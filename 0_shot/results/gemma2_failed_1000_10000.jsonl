{"File Name":"ftd-scratch3-offline\/0018-conversion-of-unknown-scratch-functions-to-c-will-fail.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOnly a subset of all scratch functions has been implemented.\\nIt might make sense to ignore unsupported functions.\\nThis would make sense when encountering sound\/image related functions.\\nSo the user could still use these functions when using the web version and not using the offline version.\\nOn the other hand the user will be surprised that some functions won't actually work.\\nFunctions silently being ignored would make troubleshooting harder.\\n","Decision":"Unknown scratch functions will cause the conversion to C to fail.\\n","tokens":86,"id":2630,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0016-don-t-support-scratch-sounds-and-sound-related-blocks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nScratch supports sounds.\\nThe ftduino has no way to play sound.\\n","Decision":"Scratch sounds and sound related blocks are not supported.\\n","tokens":19,"id":2632,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0003-use-gradle-as-build-system.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nModern software needs to be easy to build.\\nFor this reason most software uses a build system to specify how the software is build and which dependencies it needs to work.\\nPopular build systems in the Java world are Ant, Maven and Gradle.\\nAnt is pretty flexible but lacks dependency management and is also very rarely used these days.\\nMaven is more rigid than Ant but supports dependency management and is still widely used.\\nGradle is the newest build system. It can be programmed in Groovy\/Kotlin and is the most flexible build system.\\nGradle is also the only way to build for the Android platform.\\nGradle enjoys widespread usage and the authors are most proficient in it.\\n","Decision":"We will use Gradle as build system.\\n","tokens":146,"id":2633,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0008-use-scratch-saves-as-input-for-conversion-to-c.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way to get the used scratch blocks from a scratch program.\\nWe could directly use the format scratch internally (when running in the browser) uses to represent the program.\\nThis would mean that more code would have to be written in Javascript to interface with scratch.\\nThis would also couple us tightly to the internal representation scratch uses.\\nWe could use the format scratch uses to save the program.\\nIts format is defined [here](https:\/\/en.scratch-wiki.info\/wiki\/Scratch_File_Format) and it is stable for scratch3.\\nUsing the scratch saves as input is also more flexible than using an internal representation.\\n","Decision":"We will use scratch's .sb3 save files as input.\\n","tokens":135,"id":2635,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0019-allow-direct-input-of-project-json-without-a-sb3-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way to transfer the scratch save to the local web server.\\nScratch provides a function that serializes the scratch program, but directly returns a json file which would normally be the project.json in a .sb3 file.\\nRather than searching for a method that provides a .sb3 file, we chose to accept the json directly.\\n","Decision":"project.json can directly be used as input for conversion.\\n","tokens":75,"id":2636,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0010-use-travis-ci-for-continuous-integration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to use continuous integration to make sure that at any time the build is working.\\nCI will check every commit and PR.\\nPossible choices are: Travis CI, CircleCI or AppVeyor.\\nTravis CI offers Linux and Mac builds. Windows is in beta.\\nCircleCI supports all 3 platforms.\\nAppVeyor supports Linux and Windows.\\nThe authors have already used Travis CI.\\n","Decision":"We will use Travis CI for continuous integration.\\n","tokens":89,"id":2638,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0020-use-json-for-scratch-to-local-server-communication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are using POST HTTP requests to communicate with the local server.\\nTo pass multiple parameters we would use requests with `multipart\/form-data` or `text\/plain` type.\\nCreating correctly encoded multipart requests is tricky.\\n","Decision":"We will use json for scratch to local server communication.\\nI.e. responses from server to scratch will be json objects similar to this:\\n````json\\n{\\n\"status\" : \"\", | \"SUCCESS\" or \"FAILED\"\\n\"errorMessage\" : \"\", | null when no error happened, otherwise an error message\\n\"result\" : \"\" | null when there is no output (e.g. in case of an error), otherwise the result\\n}\\n````\\n","tokens":48,"id":2639,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0014-only-a-single-scratch-hat-is-supported.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nScratch can run many hats at the same time.\\nThis could be implemented on the arduino using cooperative multi-threading.\\nHowever this would be non trivial to get right.\\nThe ftduino library has not been built with multi-threading in mind as hasn't the Arduino standard library.\\nAlso program space is limited and more complex programs already take up most of the memory.\\nAdding a user mode scheduler and everything that is needed for cooperative multi-threading will likely exhaust the available memory.\\n","Decision":"We will only support a single scratch hat, i.e., multi-threading will not be supported.\\n","tokens":104,"id":2640,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0012-generate-simple-scratch-statements-in-java-and-don-t-call-a-helper-function.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAdr 0011 says that complex scratch functions will be implemented in helper functions.\\nFor statements like if\/else\/while this is harder to do.\\nAn \"if\" function would have to take a function that is executed on success.\\nThis would be more complex than simply using a c \"if\" statement.\\n","Decision":"The code for simple scratch statements will be directly generated in the java file.\\nSimple scratch statements are e.g. if\/else\/while.\\n","tokens":70,"id":2641,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ftd-scratch3-offline\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2642,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/023-dynamic-imports.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe newest versions of Javascript support `import` as a way to dynamically\\nimport modules. Modules live on a URL and can be loaded cross origin.\\nThey are therefore useful in a variety of contexts as a mechanism to lazy-load\\ncontent.\\nBrowser-support is high (~90%) but not enough to forgo a polyfill.\\n","Decision":"Support `import` via polyfills for browsers that need them.\\nAs it is not possible to directly override `import`, dynamic import is exposed\\nvia `window.guardianPolyfilledImport`.\\n","tokens":71,"id":2643,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/011-capi-elements-as-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCAPI provides a representation of content which is strongly typed and like components.\\nFrontend uses this representation to render liveblogs.\\n_Note: This document has been updated with the knowledge that the `model.liveblog` representation is a dotcom abstraction and not directly from CAPI._\\n","Decision":"The CAPI Blocks API exposes a suite of strongly typed elements. A component should be made for each element to render it.\\nEach element must have a type in dotcom rendering which specifies the data it may contain. It must have a `_type` field with a string literal representing the element's name. This should be used to identify which component to render.\\n#### Unaltered Elements\\nSome elements are converted for the liveblog into a form which is directly renderable. For instance the `TextBlockElement`. These may be sent unaltered to dotcom rendering.\\n### Altered Elements\\nSome elements contain data which is unsuitable for direct rendering, these should be transformed outside of dotcom rendering. Their `_type` field should be set.\\nThe transformed element should contain all of the `Props` required by the Components which render the element. It should not dictate HTML to be rendered into the page except where unavoidable. An element might need to directly set HTML if it contains text with markup. For instance, bold, italics and inline text in a `textBlockElement`.\\n### Functionality\\nA component must take an element, as described above and render it.\\nLogic within these components should be kept to an absolute minimum.\\n","tokens":63,"id":2644,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/000-structure-for-initial-milestone.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are significant differences between AMP and regular article rendering.\\nThese include:\\n* The markup is different\\n* The HTML structure\/limitations are different\\n* AMP does not require (or support) client-side custom Javascript, which means\\nwebpack is likely a poor fit\\n* From a project perspective, it seems that the two project change at different\\nrates (historically at least)\\n* The caching rules are different\\n* Ads are very different\\n..and more.\\nCoupling things that are different (in terms of code, building, deployment, or\\notherwise) can lead to brittle and complex solutions; it is generally better to\\nkeep things separate and share only what is absolutely required.\\nFrom this context, the question arises: should we attempt to build AMP in the\\nsame (node) package as articles, or separately?\\n","Decision":"*For our initial milestone* we will build AMP as simply a new page in the\\nexisting frontend package. The advantage of this approach is to bootstrap\\nquickly while leaving room to separate things later if necessary.\\nWithin this, we will adhere to a couple of rules:\\n* no `if (amp)` - branching is not permitted other than the absolute top-level\\nThere are many disadvantages to this kind of branching and it also would make it\\ndifficult to separate the projects later on.\\n* no divergence of data model\\nInstead, we will document pressures here and use any to bring forward\\ndiscussions around moving things into a separate project. So note, the\\nmotivation here is not that branching of data model is bad at all, but that it\\nrequires thought and possibly more significant changes.\\n","tokens":175,"id":2645,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/007-support-for-frontend-only.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMultiple 'sites' are supported in the codebase but at the cost of\\nsignificant added complexity.\\nA site is a distinct project within the repository, with it's own\\ndependencies, build and deployment. Sites cannot share code, other\\nthan through non-site packages, but do share boiler-plate tasks like\\nlinting, type-checking, and artifact-building.\\nThere are advantages to sites:\\n* helps others bootstrap their projects quicker due to easy set up\\n(i.e. all the webpack, linting, etc. is built-in)\\n* helps promote common standards for the same reason as above\\n* single repo makes sharing code easier (no need for publishing\\npackages)\\nHowever, there are also cons:\\n* distracts from our mvp\/core aim of serving the article page\\n* adds a fair bit of complexity\/overhead\\n* confuses the separation of concerns and introduces significant\\ncoupling\\nThe question of whether to keep sites has been raised before but the\\ncost has become more apparent since that discussion - as feature work\\nhas been slowed by having to consider sites-related functionality.\\nIt has also become clearer that no teams plan on using\\nsites in the near future. Neither do we want to expend effort\\nsupporting teams through this process.\\n","Decision":"Given these factors, and the general concern to remove unused code, we\\nwill remove all sites functionality.\\n","tokens":269,"id":2646,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/016-client-side-computation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen preparing data for the rendering components we currently have up to three possible locations to do so: (1) the frontend Scala backend, (2) the dotcom rendering backend and (3) the end user's client side.\\nIn the interest of the user, we should avoid postponing computation to the client side and precompute data and state on either of the two backends whenever possible.\\n","Decision":"- Favour computation in frontend over computation in dotcom-rendering\\n- Favour computation on dotcom-rendering server than computation on the client\\n","tokens":84,"id":2647,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/026-interactive-pressing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to serve 100% of articles via DCR.\\nInteractives are included in this target but are difficult to serve via DCR whilst also maintaining an acceptable design.\\n","Decision":"We will press interactives (see the [Interactive Librarian](https:\/\/github.com\/guardian\/frontend\/blob\/main\/docs\/06-features-and-components\/04-The-Interactive-Librarian\/01-README.md)) and agree which pressed interactives will be served to readers.\\nWe will migrate all other interactives - these interactives will render via DCR.\\nAfter 3 months we will review whether the migration is completed, at which point we'll remove the rendering code from frontend.\\nFull details of the migration process are [here](https:\/\/github.com\/guardian\/frontend\/blob\/main\/docs\/06-features-and-components\/04-The-Interactive-Librarian\/02-interactive-migration.md).\\n","tokens":41,"id":2648,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/001-dotcom-rendering.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Frontend rendering layer has a CSS problem. We are sending too much to CSS to the client (95% of CSS delivered to the browser is unused). The\\nfragility of the CSS is a source of presentational bugs.\\nFrontend also has a developer experience problem. The feedback cycle is slow. There is poor separation of concerns which makes it difficult to find\\nrelated code and isolate changes. There is also a lack of prescriptiveness, so developers have to make more decisions, leading to inconsistent code\\nand sub-optimal patterns.\\nWe attempted to rectify these issues by building a CSS componentisation system into Frontend, running inside JVM's Nashorn JavaScript engine. However\\nthe debugging experience was poor, and community and support for Nashorn was practically non-existent.\\n","Decision":"We will build an entirely new rendering layer that runs outside the existing frontend, via the addition of an API to frontend. Pages served from this view will\\nconsist entirely of new components. Other legacy pages are still served from the old rendering layer.\\nThis solution has the disadvantages of introducing duplication (feature X may need supporting in two separate views) and requiring a complete page as a\\nminimum deliverable. However it has the advantage of producing value even without 100% migration, as any page it serves is clean. Also this solution\\n\"fails fast\": it can be assessed and potentially rejected comparatively early on.\\nNode.js offers an excellent debugging experience, fast feedback and a vast community to draw on.\\n","tokens":163,"id":2649,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/000-technical-review-records.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are successfully using lightweight architecture decision records to document decisions being made about architecture.\\nLast year legislation changes around data privacy (GDPR, CCPA, PECR) and their impact on our obligations means we need to increase our technical measures to ensure the safety of our reader's data through:\\n-   Reducing the number of third-party scripts\\n-   Architecting our systems with a [privacy by design](https:\/\/en.wikipedia.org\/wiki\/Privacy_by_design) approach\\n-   Documenting risks and mitigations in place or to be added.\\n-   Adding new controls to prevent issues, i.e being proactive rather than reactive.\\nAdditional to this privacy concern, we have noticed in the past the difficulty to correctly estimate and measure accurately the performance and by association the revenue impact of each third-party. While they usually come with a direct revenue, there is as well indirect revenue loss through performamce impact:\\n-   performance degradation impacts our ad display and our advertising revenue\\n-   performance degradation impacts our contributions conversion rate\\n-   performance degradation impacts our Google SEO which impact our reach and indirectly all of our revenue streams (contributions, subscriptions, advertising)\\nTo address those 2 concerns, we are making a change to the process for deciding if a third-party will be added. Before a decision is made by the business to add a third-party to our website or apps, we will perform an engineering technical review that will be used to inform the business decision.\\nThe suggested format of the review is the following:\\n-   Metadata about the review\\n-   Date of the review\\n-   Author of the review\\n-   Platforms concerned\\n-   Context\\n-   Data privacy engineering review\\n-   Security engineering review\\n-   Perfomance engineering review\\n-   Recommended mitigations\\n","Decision":"For all third-party technical reviews in this project, a record will be created with a filename of `XXX-technical-review.md` where XXX is the monotonically increasing number described in the linked document.\\nPlease note that as records will be public, specific attention should be paid about information being disclosed.\\n","tokens":371,"id":2650,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/006-multiple-sites.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are several sites across the Guardian (identity-frontend, membership-frontend) that started life with a developer experience that emulated the dotcom frontend, before diverging. The duplication of effort required to keep these codebases in step with frontend was presumably too large an investment, and as a result the developer experience deteriorated and the consistency was lost.\\nOne way we could prevent this happening again is to allow teams to build their own sites in the dotcom-rendering repo. They would share the same toolchain, code style config, bundling configuration and developer experience. This would reduce duplication and help maintain consistency and best practices across projects.\\n","Decision":"We will allow teams to build their own websites under the `sites\/` directory.\\n","tokens":134,"id":2651,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/021-react-portals.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe already have partial hydration implemented using react 'islands', a design pattern where we use multiple react apps on the same page, but now we have a requirement to share data between these islands.\\nReact portals would allow us to have one root react app with multiple portals existing within it without the need to hydrate the entire dom. See: https:\/\/reacttraining.com\/blog\/portals-with-context\/\\nThere are two ways to share data between portals. Using props, or react context.\\n### React context\\nUseful because it means all child components can access the context at any point in the tree but adds 'magic'.\\n### Props\\nMore explicit but causes additional prop drilling.\\n","Decision":"Implement react portals pattern and use props to pass down shared data.\\nDon't use react context, use pure props instead. This means more prop drilling but we prefer this cost over the complexity of context.\\n","tokens":143,"id":2653,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/020-react-hooks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe've avoided using React hooks for some time in order to ensure un-desired complexity is avoided in the code base. But as hooks are now standard fare in React applications, it makes sense to review our usage of them in DCR.\\n","Decision":"- Prefer non-stateful components if possible\\n- Prefer React's official hooks to custom hooks\\n- Avoid abstractions that could lead to hooks within hooks within hooks.\\n- Prefer hooks to classes with component lifecycle methods\\n- Try to build hooks that are generic and reusable\\n","tokens":52,"id":2654,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/012-amp.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have an established aim of rendering content in Dotcom Rendering. This will dramatically improve the developer experience (measured in time to new features, time on maintenance).\\nAMP is a good candidate for early migration to the new platform:\\n- It is simpler than \u2018full\u2019 content types\\n- It is a discrete piece of functionality\\n- It receives a large volume of traffic, which will help validate the performance of our new platform\\n- It receives relatively little development outside of the Dotcom team, reducing risk during the migration period\\nThere are also some downsides. In particular:\\n- It is relatively \u2018static\u2019 (rarely developed on) so the developer experience benefits are likely less than for some non-AMP content, such as articles or liveblogs\\nIt is quite possible though that Google will weight AMP more heavily going forward, pushing us to develop the platform more than we have done so in the past.\\n","Decision":"To implement AMP using dotcom rendering.\\n","tokens":191,"id":2656,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/010-storybook.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGUUI intends to be, at some level, a library of components.\\nDotcom rendering will require a mapping of CAPI element to React Component.\\nStorybook is a widely used library which allows a series of demos and examples to be easily constructed. It also has support for typescript.\\n","Decision":"It is possible to envision a split in components:\\n- those which form our design system\\n- those which render individual elements of content from CAPI\\nEach of these should have an independant storybook, allowing the design system ones to express the variety of ways each component can and should be used. And allowing, as they are developed, each CAPI element rendering component to demonstrate the variety of content they can encapsulate.\\n","tokens":64,"id":2657,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/005-monorepo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur componentisation solution includes a framework for building components, a library of shared components and a service that generates HTML for the dotcom frontend (and maybe for other applications too). We can potentially pull out sharable code into their own repos, or keep all our componentisation code together in the same repo.\\nHaving multiple repos expresses the intention that parts of our code can be used in different applications (e.g. the GUUI component library). However, it slows down development and adds complexity. It would become necessary to clone and symlink multiple codebases, for instance in order to work on a shared component, add some new colours to pasteup and then view it in the context of dotcom. There is also the problem of discoverability, and knowing how the various codebases link together.\\n","Decision":"We will keep all the code for componentisation solution in one repo makes the dotcom frontend easy to setup. The code shares code style rules, keeping it consistent.\\nWe will organise the code as a monorepo, expressing the intention that some of our code (GUUI and Pasteup) should remain decoupled from the dotcom frontend website. Changes to these libraries will be published separately.\\n","tokens":163,"id":2658,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/002-componentisation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, our CSS is bundled in such a way that often 95% of loaded CSS is not used on a page.\\nOur CSS is fragile. It\u2019s really difficult to work out the extent to which a change changes things, because it\u2019s easy to reuse classes inappropriately.\\nFrontend can be hard to work with in part as a consequence of the markup, logic and styling being separate\\nIt can be difficult to:\\n- reason about how these concerns intersect\\n- find styles that apply to specific markup\\nIf you change or delete some styles, it\u2019s difficult to know the extent to which this changes the website\\n","Decision":"We will build our rendering layer using components, ensuring that only CSS that is used on a page is generated and included in the resultant HTML.\\nWe will colocate markup, styles and logic in the same file.\\nWe will annotate our logic with types.\\n","tokens":131,"id":2659,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/003-react.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe majority of client side developers at the Guardian have some [React](https:\/\/reactjs.org\/) experience, and it is succesfully used in several projects. Its [JSX syntax](https:\/\/reactjs.org\/docs\/introducing-jsx.html) is expressive and readable.\\nThe [Preact](https:\/\/preactjs.com\/) library has a similar API to React, and is an order of magnitude smaller in terms of file size (3.35KB vs 29.07KB Gzipped). It excludes a lot of React features that we will likely never use.\\nHowever, React has a larger community, and is driven by a dedicated team at Facebook. It comes with synthetic events that offer better cross browser support. It is easier to configure, as tools such as Babel, Webpack and Flow assume that JSX implies React. It is easier to [profile using Chrome dev tools](https:\/\/reactjs.org\/blog\/2016\/11\/16\/react-v15.4.0.html#profiling-components-with-chrome-timeline). It also has better integration with the [Enzyme](http:\/\/airbnb.io\/enzyme\/) test utilities.\\nThe React team is working on a number of new features (async rendering, time slicing, suspense). It is [unlikely](https:\/\/github.com\/developit\/preact-compat\/issues\/432) that Preact will be able to keep up with the pace of React's development.\\n","Decision":"We will build the dotcom rendering layer in React.\\n","tokens":296,"id":2660,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/008-css-in-js.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are a number of ways in which we could apply styles to components.\\n### CSS-in-JS\\nCSS-in-JS is a catch-all terms for defining CSS directly in JavaScript. It allows us to defining the styles in the same place as the markup (JSX) and component logic.\\nStyles may be defined as JavaScript objects, with some syntactical overhead (camel case rules instead of kebab case, pseudo-selectors and media queries are wrapped in quotes, ...). Alternatively, they may be defined as tagged template literals, which look much like CSS but require some additional tooling in developer IDEs (i.e. plugins for syntax highlighting).\\nCSS-in-JS is supported by a number of different libraries such as [Emotion](https:\/\/emotion.sh\/), [Styled Components](https:\/\/www.styled-components.com\/docs\/basics#getting-started) (both with TTL support), [Glamor](https:\/\/github.com\/threepointone\/glamor) and [Styletron](https:\/\/github.com\/styletron\/styletron) (no TTL support).\\n### [CSS Blocks](https:\/\/css-blocks.com\/)\\nCSS Blocks allows developers to define their CSS in `*.css` files and import them into JavaScript. It makes use of familiar CSS syntax, with clever use of attribute selector notation and the [`obj-str`](https:\/\/github.com\/lukeed\/obj-str) library to change styles based on props. The output is highly-efficient atomic CSS. The use of `obj-str` may make a steep learning curve for non-client-side-developers.\\n### CSS in `*.css` files imported with custom Webpack CSS loader\\nIt is possible to write a Webpack loader that will load the contents of `*.css` files and manipulate the content to make it usable with CSS-in-JS libraries. This would lock us in to Webpack and may become considerably complex over time.\\n### [Stylable](https:\/\/stylable.io\/)\\nSimilar to CSS Blocks, Styleable allows developers to write `*.css` files that look and feel like CSS, with added syntax that adheres \"to the spirit of CSS\". It provides access to state using custom pseudo-classes and pseudo-elements and uses a pre-processor to convert files down to vanilla CSS.\\n","Decision":"Use CSS-in-JS to represent styles.\\n","tokens":473,"id":2661,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/020-react-to-preact.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe decided to make the jump from React to Preact. Main advantages was reducing the bundle size, with minimal changes to the architecture.\\nPreact had been quite good at keeping up with React features, which was one of the main [previous concerns.](https:\/\/github.com\/guardian\/dotcom-rendering\/blob\/main\/docs\/architecture\/003-react.md)\\nPreact uses a lib called [`preact\/compat`](https:\/\/preactjs.com\/guide\/v10\/switching-to-preact) which allows us to not have a major refactoring to embrace React.\\n","Decision":"Using webpack aliases we can use `preact\/compat` to intercept `react` and `react-dom` imports. We should keep writing components as if we are using react in order to make React compatibility an easy transition if needed.\\n`preact\/compat` add only 2kb extra to preact bundle, but even this has enabled us to go from ~93kb to ~66kb in our react bundle.\\n","tokens":120,"id":2662,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/013-new-elements-models-in-frontend.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe frontend project currently parses capi block elements into a list of BodyBlock objects as part of the\\nfrontend model. This is currently used (in frontend) by the liveblog, and we also forward this structure to\\ndotcom rendering.\\n","Decision":"We have created a duplicate set of models in frontend to be used only by the dotcom rendering datamodel.\\nDoing this will allow us to iterate on the dotcom rendering models more quickly and make enhancements such\\nas better image support, without the potential to impact liveblogs.\\n","tokens":54,"id":2663,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/014-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn dotcom-Rendering we\u2019ll typically be writing 2 types of automated test, traditional unit tests (using Jest) for testing our own libs\/helpers etc, and Component integration tests that test the interactions between the various Components in our application.\\n[Jest](https:\/\/jestjs.io) is a fast JavaScript testing utility developed by Facebook. We've decided to use Jest for a number of reasons including: Jest was developed by Facebook so works particularly well with React and requires little configuration, tests are run in parallel which makes running your test suite very fast and it is used widly within The Guardian.\\nWhen we write tests for Components they should be relevant for user interfaces, they should therefore test the render logic and not the internal implementation details of our Components. We evaluated using [Enzyme](https:\/\/airbnb.io\/enzyme\/) or [react-testing-library](https:\/\/github.com\/kentcdodds\/react-testing-library). Both are complementary tools to Jest, that integrate well to provide testing abilities.\\nEnzyme is a comprehensive JavaScript Testing utility that exposes an API via the ReactWrapper. It allows you to query state, props of Component instances and interact directly with a Component's internals. The API is based on manipulating Components via the ReactWrapper. Using Enzyme you can assert, manipulate, and traverse your React Components' output, however this flexibility can lead to bad practices and facilitates the testing of irrelevant implementation details.\\nreact-testing-library is a lightweight alternative to Enzyme. Rather than dealing with React Components the utility allows you to tests against actual DOM nodes. So you don\u2019t assert against props passed\/state (implementation details) but DOM output. The utility makes you write tests from a \u201cuser-perspective\u201d, asserting against user-facing properties.\\n> The more your tests resemble the way your software is used, the more confidence they can give you.\\nKent C. Dodds (react-testing-library author)\\nHaving used react-testing-library we feel that although it's API is less comprehensive that Enzyme it is most appropriate for testing our Components as user interfaces and it minimizes the likelihood of bad practises in our tests.\\n","Decision":"We will use a combination of Jest and react-testing-library for testing our Components.\\n","tokens":438,"id":2664,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/017-javascript-loading.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe existing site (Frontend) includes Javascript in a variety of ways:\\n-   inline\\n-   script tags\\n-   dynamically added script tags\\nThere is a single (universal) boot process which conditionally loads additional\\nJS as required based on page metadata.\\nFor Dotcom Rendering we want to simplify things so that it is easy for a\\ndeveloper to know where to add a script\/functionality. We also want an approach\\nthat helps ensure good performance and, related, which is less monolithic.\\n","Decision":"-   Load scripts in the head as deferred, or as async at the bottom of the HTML\\nbody section\\n-   All other script loading (other than for commercial third-party code) is\\nprohibited\\n-   We will not use a universal 'boot' JS file, but load scripts which have a\\nunique purpose and which are self-contained aside from a few shared helpers\\nFuller discussion of this approach can be found here:\\nhttps:\/\/docs.google.com\/document\/d\/1EnkowJ7BFumrqkpkRacRl-gSkiJs9fUgRCixQTLxBvo\/edit#.\\n","tokens":110,"id":2665,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/019-remove-monorepo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe used to have a monorepo with multiple packages, including `guui` and `design`. However, once these packages were migrated to `src-foundation` we were left with a monorepo with only one package.\\n","Decision":"Remove the use of Yarn Workspaces which was being used for the monorepo. Restore a basic yarn package, merging dependencies.\\n","tokens":51,"id":2666,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dotcom-rendering\/024-script-loading.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen splitting our Javascript to use dynamic imports, we also needed to review our use of module\/nomodule. You can see more context in the write-up below.\\n","Decision":"- Move from script elements to dynamically loaded scripts\\n- Use preload for high-priority scripts to ensure we get benefits of the preparser but let low priority scripts load with script injection\\n- \"Use the link tag and place it after the resource that will insert the preloaded resource\"\\n- Preload our main media image (can we do this?), then fonts, then the high priority Javascript (Our critical path)\\n- For **dynamic imports**, implement a mechanism for **high priority** components or scripts that preloads these\\n- For all other dynamic imports, rely on fetching after parsing\\n","tokens":37,"id":2668,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/012 Stale data from fridges.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe smart fridge system is an external system that provides vital information for the customer and the *ordering system*. But, we do not control the frequency of updates, data structure, and management of available positions. Nonetheless, we have to provide our service based on the provided data and try to keep inventory on our side as precise as possible to avoid collision and situations where the user might be dissatisfied with our service.\\nThere is a risk that at some point in time we can't get actual data (offline, maintenance, throttling, caching, etc). How will the ordering system handle such situations?\\nThe occasional user (who pays cash) is a major factor in this issue. Any pre-paid meals that can still be picked up in an offline scenario don't really contribute to the stale data issue as the payment server must already have done its job.\\n","Decision":"We will cache and maintain data integrity about the available stock in fridges on our side. Until we get new data from the *smart fridge system* at which point the data will be adjusted.\\n","tokens":174,"id":2669,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/016 Use of Infrastructure as Code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nInfrastructure deployment, in the traditional imperative way, is prone to oversights and errors or beholden to extremely rigorous processes that stifle change. Hunting down a small deviation from the design, such as having accidentally added a public IP to a machine, can spell disaster.\\nPerpendicular to imperative specifications there are declarative specifications of infrastructure. These declarative specifications describe the desired state in favour of the steps to reach such a state. Infrastructure as code is a way to script the declarative specification using a Domain Specific Language or a General Programming Language.\\n","Decision":"Infrastructure as Code allows us to prevent infrastructure drift and allows us to run architectural fitness functions on the specification instead of the run-time environment. This in turn allows us to test many scenarios without actually setting the deployment of the infrastructure in motion.\\nWe are guaranteed to run the tests against what will in fact be deployed when we decide to execute the specification. Therefore, creating a very agile and evolvable ecosystem.\\nWe can recommend **Cloudformation** because it has built-in AWS specific features that assist the infrastructure engineer using visual designers but also checks on valid configurations. However, this is not officially part of the ADR as we recognize that an alternative, such as Terraform, could already be mastered by the implementation team.\\n","tokens":113,"id":2670,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/003 Tracing and Monitoring Sytem.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis is a fast growing system and justin-time scaling is essential. Also, it allows buying cheaper instances for services without overall degradation.\\nProper and trustworthy monitoring is the basis for making any decision about infrastructure changes (i.e. scaling and extracting services), proper monitoring should support business decisions and should act architecture fitness functions.\\nWe'd like to use a system that helps us get the full picture of correlated business events, connected services and amount of consumed resources.\\n","Decision":"Even though ELK stack or Graphana are the cheapest options for FoodFarmacy to go with, we decided to choose DataDog, because it requires minimal integration without maintenance (contrary to ELK Stack or Graphana). The convenience of an all-in-one alerting and monitoring tool for $15 USD per month is just too difficult to argue with. Finally, choosing an open-source solution we would have to appoint 0.2-0.5 time of a developer \/ administrator to take care of the monitoring.\\n","tokens":97,"id":2671,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/015 Integration with Map Providers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nApplication should be integrated with 3rd party geolocation providers to help the user reach the location of the Smart Fridge. Currently, we consider below map providers to integrate with our application.\\n| Provider           | Pricing Criteria                                                          | Pros                 | Links                                         |\\n| :------------------|:-------------------------------------------------------------------------:| --------------------:| ---------------------------------------------:|\\n| OpenStreet Maps    | Free                                                                      | No-cost, Open-source | [Documentation](https:\/\/wiki.openstreetmap.org\/wiki\/Main_Page)|\\n| TomTom             | Free for 2,500 requests daily,$0.42\u2013$0.50 for each subsequent 1000        | Better Navigation    | [Documentation](https:\/\/developer.tomtom.com\/)               |\\n| MapBox             | Free for 50,000 requests daily, $0.50 for each subsequent 1,000           | custom map features | [Documentation](https:\/\/docs.mapbox.com\/)     |\\n| Here Maps          | Free for 250,000 requests monthly, $1 for each subsequent 1,000 OR $449 for 1,000,000 requests monthly           | Better visualization  services | [Documentation](https:\/\/developer.here.com\/pricing)     |\\n","Decision":"Based on the business model and the volume of data now and in near future , as there will be more subscribed users, who knows the location precisely . We have decided to integrate with Here maps as it has good market reach and first 25K requests were free.\\n","tokens":253,"id":2673,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/002 System approach.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide what the dominating system approach will be. There are several options like: monolith, microservices, micro-kernels, blackboard.\\nThere are also technical constraints and business drivers even if the sky is blue and we are free of choice, developer's team experience and capabilities should be considered.\\n### Alternatives\\nKey map:\\n- + Promotes\\n- ++ Strongly promotes\\n- O Neutral\\n- - Negative\\n- -- Strongly negative\\n| | Monolith | Microservices | Micro-kernel | Modularized Monolith |\\n|----|----|----|-----|-----|\\n| Ease of Deployment  | ++ | -  | -  | ++ |\\n| Availability        | -  | ++ | +  | - |\\n| Autonomy          | -- | ++ | ++ | + |\\n| Traceability        | ++ | -  | +  | O |\\n| Performance         | ++ | +  | +  | ++ |\\n| Modifiability        | O  | ++ | +  | + |\\n| Maintainability      | -  | ++ | +  | + |\\n| Integrity           | ++ | -- | O  | + |\\n| Security            | -  | ++ | +  | ++ |\\n| Scalability         | -- | ++ | -  | + |\\n","Decision":"Based on alternatives in the context of the business needs and a development team we think that modularized monolith is the best option for now. At the same time it opens the possibility to migrate to microservices when needed.\\n","tokens":284,"id":2674,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/013  Cache the meal catalogue.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUser satisfaction and speed of the application is our priority and in case of a slow\\unstable connection, we'd like to have the smallest delay possible in catalog browsing. Users expect quick response times. We can safely assume that the menu catalog won't have drastic changes during a day and we can cache the catalog with an estimated number of meals in each fridge in the user's area. Most probably the whole menu can be formed for a week and provided for the ordering system. Based on this assumption we could upload the meal catalog once per day with the amount of available Stock Keeping Unit (SKU) in fridges. Our estimations on the update package sizes show that a catalog with 20 meals could weigh around 500kb-700kb without images. Even if it might not be a significant amount of data for a single entry, multiplying it by the number of users gives us a large amount of data to transfer which isn't free for us and the users. The catalog updates with a meal amount might be organized as additional event messages that will be very lightweight compare to the entire catalog that will save us money on the Cost of Ownership. So we can upload this information upfront.\\nThe purchase process, i.e. payment confirmation, always takes some time and users get used to it and this can be used to attempt to actualize a certain position in the menu.\\n","Decision":"We will cache the menu catalog and do an actual check during purchase confirmation. Before that moment no connection will be required.\\nWe are going to upload and keep the entire meal catalog on the users' devices.\\n","tokens":277,"id":2675,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/007 Event sourcing usage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFood Farmacy works with external individual users and depends on reputation. In such an environment the time and accuracy of resolving users' complaints are essential. Thus, there should be a way to store all intermediate changes that are used. Keeping only the final state of a domain entity won't help to understand how we end up with a specific state.\\nThe ordering system going to evolve as the company grows and not all aspects clear at the beginning. There should be an option for graceful migration from one data model and data completeness to another.\\nUser engagement is one of the business goals and there should be data for analysis of a user's actions and behaviors to build a better model for new user engagements.\\n","Decision":"The event sourcing approach gives us a full history of changes and can help investigate issues and users' complaints. With the support of EventSource storage, the main issue with synchronizing updates and read models is manageable.\\n","tokens":142,"id":2676,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/005 Service readiness checks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently there are only several services planned, but most of them require time for initialization (warming cache for instance) and they can't start processing requests from other services immediately. The situation might arise when restarting a service, introducing a new instance, or some other situations.\\nIf a service is not ready for processing a request, the timeout might happen and the supervisor might decide that there should be a new instance that leads to a cascading effect and higher bills in the end.\\n","Decision":"Health check endpoints provide information about readiness of processing requests and other services should respect this information.\\n","tokens":100,"id":2677,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/004 Health check endpoints.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe _ordering system_ is a dynamically developing system that starts as a modularized monolith. Even in the initial form, it can be split up into several services that will have to be monitored. The system starts small, to save budget for cloud instances, but has the potential to grow towards many services. Based on technical and business metrics there should be a clear way to make a decision about extracting the monolith's modules into services with smaller responsibility scopes and improved scalability.\\nThere should be a set of clear metrics and vision about health statuses of services. It includes not only consumption of hardware resources, but also information about processing user's requests.\\n","Decision":"There is a dedicated end-point per service that will provide information about its health status.\\nFor technical health checks we are using a heartbeat pattern for fault detection. It allows us to add services when necessary and the supervisor will passively listen for incoming messages, instead of pushing requests (ping pattern).\\nFor business health checks there are sets of critical path scenarios. Each scenario starts with a dummy message from a dedicated supervisor to measure execution of specific user scenarios. Measurements will includes correctness of execution and time spent (duration).\\n","tokens":134,"id":2679,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/009 Rely on payment service provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe _ordering system_ should accept a wide variety of payment methods to satisfy the user's expectations and increase the user base. The number of payment systems is significant and each of them have nuances in the API and ways of communication.\\n","Decision":"Use a 3rd party payment provider to delegate dealing with different payment systems.\\n","tokens":50,"id":2680,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/006 Zero trust architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe _ordering system_ communicates with upstream and downstream systems in a cloud environment and we can't rely on an assumption that requests from the same subnetwork are safe.\\nThe _ordering system_ is implemented as a modularized monolith and those modules will eventually be extracted to become dedicated services. We have to secure such calls at an early stage to make it easier to migrate from module to service.\\n","Decision":"Internal calls from modules should contain security info with auth and claims info from the very beginning.\\n","tokens":83,"id":2682,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"archcolider\/014 Deployment Strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOnce the solution is implemented it has to be cheap to deploy, maintain, and scale. Currently, there are several options that we could take into consideration as deployment platforms such: on-premise servers, serverless solutions, container orchestrations such as Kubernetes (on-premise or as PaaS), or OpenShift -enterprise flavor of on-premise Kubernetes. Finally, we could choose to use virtual servers provided by AWS or Azure.\\n","Decision":"We decided to go with virtual machines (EC2 instances) on AWS for two reasons. First, we can start with minimum instances and decide to scale up (scale vertically) on CPU and Memory. When we will reach the limits of upscaling than the natural thing to follow is,  to scale out (scale horizontally) increasing the number of running instances. Second, Horizontal scaling adheres to our architecture where we can scale individual components of our system according to the component's load. Another decision behind dropping popular solutions such Kubernetes is the effort of work that needs to be invested into the platform, starting from learning Kubernetes ending on provisioning Kubernetes cluster with monitoring, metrics, and inter-service networking such as a service mesh.\\n","tokens":90,"id":2683,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pay-connector\/adr-002-decrypting-google-pay-token-not-required.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are two possible tokenization methods when making a payment request to Google Pay: [PAYMENT_GATEWAY and DIRECT](https:\/\/developers.google.com\/pay\/api\/web\/guides\/tutorial#tokenization).\\nFeatures of PAYMENT_GATEWAY tokenization:\\n- Merchant id identifying GOV.UK Pay with Google Pay needs to be set up\\n- We simply pass the encrypted response from Google Pay through to the payment gateway as is\\n- No need of the operational overhead of key management as is already handled between Google Pay and the payment gateway\\nFeatures of DIRECT tokenization:\\n- No need for a Merchant id between GOV.UK Pay and Google Pay\\n- Need to be PCI DSS compliant (which we already are)\\n- Need to decrypt the response to get information to send to the payment gateway\\n- Need the operational overhead of key management as we need to [roll keys once a year and register these with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#key-management)\\nWith either tokenization method, we get a [number of data](https:\/\/developers.google.com\/pay\/api\/web\/reference\/object#response-objects), but the ones relevant to us are:\\n- Last 4 card digits\\n- Billing address (which includes name)\\n- Email address\\n- Card Network\\nWith DIRECT tokenization we decrypt to get:\\n- PAN\\n- expiration month and year\\n[and other things](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#encrypted-message).\\nChoosing whether to decrypt the payload from Google Pay or not is informed by the following two deal breakers:\\n- We need to be able to retrieve the cardholder name, email, and last 4 digits of a card\\n- We don't want services to have to manage any sort of keys\/certificates\\n","Decision":"Unlike the current implementation of Apple Pay, we have chosen to go with not decrypting the payload for the following reasons:\\n- We get all the information we require (Last 4 card digits, billing address name, email address, card network) in unencrypted form\\n- Services won't need to manage any keys\/certificates. This is because when a payment request is made to Google Pay we specify the payment gateway, e.g. \"Worldpay\".\\nGoogle Pay will return a response encrypted with the payment gateway's public key.\\nThis response is decryptable by the payment gateway as they have their private encryption key.\\nSo there is already a key management process between Google Pay and the payment gateway.\\nOur initial assumption that Google Pay encrypts with the merchant id's public key was incorrect.\\n- No operational process is needed on our part to manage keys\/certs.\\n","tokens":384,"id":2685,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"functionaut\/0005-logical-true-logical-false.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTruthy and falsy checks are well-known and established practices however they make it very easy to unintentionally discard valid values.\\nIn this example `speed` can never be set to `0` which may or may not cause bugs:\\n``` javascript\\nconst speed = n || 50;\\n```\\n","Decision":"We will adopt the concept of logical truth and logical falsity as [defined in Clojure](https:\/\/clojure.org\/reference\/special_forms#if):\\n> [\u2026]All of the other conditionals in Clojure are based upon the same logic, that is, nil and false constitute logical falsity, and everything else constitutes logical truth, and those meanings apply throughout.[\u2026]\\nLogical falsity will be defined as either `false`, `null` or `undefined`. Everything else, including `0`, `''` and `NaN`, will constitute logical truth.\\n","tokens":66,"id":2686,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"functionaut\/0004-no-type-checking.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMaking sure that a function operates with the right input and providing the user with a good debugging experience is not trivial.\\nConsider this contrived example:\\n```javascript\\nfunction inc(x) {\\nif (typeof x != 'number') throw new Error('x is not a number');\\nreturn x + 1;\\n}\\nfunction inc_array(xs) {\\nif (!Array.isArray(xs)) throw new Error('xs is not an array');\\nreturn xs.map(x => inc(x)); \/\/ <- May or may not throw\\n}\\n```\\nWe cannot guarantee that `inc_array` won't throw even if `xs` is an array. If we wanted to give early feedback, we would need to go through each `x` and do the same type checking that `inc` already does. So if the function is invoked correctly we would type check `xs` once and each `x` twice!\\n","Decision":"This project will adopt a *\"Garbage In \u2014 Garbage Out\"* philosophy and deliver a *\"no hand-holding\"* library.\\n","tokens":192,"id":2688,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"functionaut\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2690,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hackathon2020-prominent-colors\/0001-use-node-vibrant.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* Need to pick a library to use for finding the prominent colors in an image\\n* The scope of this project is subject to change in the future\\n* Unsure yet where this project will fit into Cond\u00e9 Nast platform architecture\\n* Needs to be deterministic for quality purposes\\n","Decision":"* Use node-vibrant because it's popular and therefore well maintained and tested\\n","tokens":59,"id":2691,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0007-solutions-are-data-integrated.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nData integration involves combining data residing in different sources and providing users with a unified view of them for both operational and analytical uses. Data integration encourages collaboration between internal as well as external users.\\nGiven an application landscape with a large variaty of application types (legacy, on-prem, SaaS, ...), business owners and parties involved, data integration can be challenging. Therefore it is important to make sure the need for data integration is recognized by all parties involved and technical knowledge is available for systems involved.\\n","Decision":"We integrate data through the ODH. Therefore we only make or buy solutions supporting this.\\n","tokens":105,"id":2692,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0006-openapi-guidelines.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to use a naming convention for OAuth scopes.\\n","Decision":"When creating an OpenAPI based REST API, there are the guidelines to adhere to:\\nTo get started, create a valid OpenAPI specification yaml file inside an [app folder](https:\/\/github.com\/vwt-digital\/operational-data-hub\/blob\/develop\/coding_guidelines\/adr\/0003-repo-directory-structure.md).\\nNext, use the [OpenAPI generator](  https:\/\/github.com\/OpenAPITools\/openapi-generator) to generate you API server. This will create a Readme, requirements and an openapi_server folder with controllers, models, and a generated openapi\/openapi.yaml file.\\nNormally OpenAPIGenerator is a write-once tool (any changes will be overridden by generating a new project.) However, changing requirements will force you to regenerate a project (Especially when the structure of the data changes.) To ensure a correct usage of the openapigenerator tool, perform the following steps:\\n### Instruct the SAST scanner to ignore generated files.\\n-   For flake8, these are:\\n-   app\/setup.py\\n-   app\/openapi_server\/util.py\\n-   app\/openapi_server\/models\/*.py\\n-   For yamllint this is:\\n-   app\/openapi_server\/\\_\\_init__.py\\n-   app\/openapi_server\/openapi\/openapi.yaml\\n-   Any other files that need to be edited.\\n### Making changes to the OpenAPI spec.\\nWhenever you want to make changes to the specification, it is important not to edit the openapi_server\/openapi.yaml file, this will be overridden by the generate command. It is better to change the user created source yaml file and regenerate the project. When generating, for now we'll only regenerate the models. Any other changes will need to be done manually.\\n### Enable validate_responses.\\nTo ensure that the Spec is honored, enable validate responses in the openapiserver\/\\_\\_init__.py file (Example can be found [here](https:\/\/github.com\/vwt-digital-solutions\/snbplanningapi\/blob\/develop\/api_server\/openapi_server\/__init__.py#L20).)\\n### Use models to your advantage.\\nTo ensure correctness of the response returned by the API, the developer should always work with the models provided by the generator. A model\u2019s to_dict method can be used to construct the response.\\n### Naming conventions.\\nInside the controllers, operations should be named like so:\\n-   list_<object>s for the REST list operation (GET without id)\\n-   get_<object> for the REST get operation (GET with id)\\n-   create_<object> for the REST create operation. (POST without id)\\n-   update_<object> for the update operation (POST, PUT with id)\\n","tokens":17,"id":2693,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0026-solution-facilitates-a-coherent-set-of-business-functions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA lot of different functions are required to support the business. These functions can be grouped into coherent sets, referred to as a solution. The functionality defined by the set of functions can then be implemented on the platform, fulfilling the business requirements.\\nThe solution can also implement domain crossing functionality, e.g. a service provided from one domain that is consumed in another domain.\\n","Decision":"We define a solution as the implementation of a coherent set of business functions on the platform.\\n","tokens":77,"id":2694,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0015-automatic-backup-of-each-data-component.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo protect against data loss, a backup of all storage components should be implemented. As all data components are specified in the [data catalog](0012-data-catalog-specifies-all-data-components.md), automated backup can be implemented from this specification.\\n","Decision":"We will implemented automated backup based on the [data catalog](0012-data-catalog-specifies-all-data-components.md) for each component that stores data.\\n","tokens":53,"id":2695,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0043-api-first.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAn API-first approach means that for any given development project, your APIs are treated as \u201cfirst-class citizens.\u201d That everything about a project revolves around the idea that the end product will be consumed by mobile devices, and that APIs will be consumed by client applications. An API-first approach involves developing APIs that are consistent and reusable, which can be accomplished by using an API description language to establish a contract for how the API is supposed to behave.  Establishing a contract involves spending more time thinking about the design of an API. It also often involves additional planning and collaboration with the stakeholders providing feedback on the design of an API before any code is written.\\n","Decision":"Starting a new project starts with building and documenting the API.\\n","tokens":134,"id":2697,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0060-lock-pip-requirements.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCode Injection is a specific type of injection attack where an executable program statement is constructed involving user input at an attack surface that becomes vulnerable when it can be manipulated in an unanticipated way to invoke functionality that can be used to cause harm.\\n","Decision":"To prevent dependency injection attacks we decided to have both a requirements.in file and a [pip-tools\/pip-compile](https:\/\/github.com\/jazzband\/pip-tools) generated requirements.txt\\n","tokens":51,"id":2698,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0041-deployment-through-pull-request.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDeployment of new software to the production environment should be done in a controlled way. The possibility of unauthorized deployment to production is a security vulnerability. Therefore, deployment is [automated](0049-gitops-deployment.md) in CI\/CD pipelines. Next to that, a pull request with review procedure can be used to implement 4 eyes principle through review, either by peers or other functions.\\n","Decision":"We will use pull requests to initiate a deployment of new software to production.\\n","tokens":83,"id":2699,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0011-storage-naming-guidelines.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to use naming guidelines for Google Cloud Platform (GCP) storage.\\n","Decision":"We identify rules for the storage name, the path on the storage and for the filename.\\n### Storage name\\nThe storage name should always start with the product ID of the GCP project the storage is in followed by a clear name of what kind of data the storage contains and the name should end with \"stg\". If we take as example a storage that contains data about employees, its storage-name could be ```project-id-employees-stg```.\\n### Path\\nThe path of a storage should always start with a folder that has a clear name that describes the data it contains best.\\nThe part of the path before the file should always be year\/month\/day. Where the year and month are zero padded.\\nAn example of a normal GCP path would be: ```{storage-name\/}clear-data-name\/2021\/04\/26\/file-name```.\\n#### Source\\nIf the storage files come from outside GCP, the path should then start with \u201csource\u201d followed by the clear data name, year, month and day. An example of such a path would be ```{storage-name\/}source\/clear-data-name\/year\/month\/day\/file-name```.\\n### Filename\\nA filename on a GCP storage should always be \u201ctimestamp-UUID.extension\u201d, where the timestamp\u2019s format is ISO8601 so that it is readable by humans. The timestamp is the time when the file was uploaded. An example would be ```20210426T000011Z-c6995d8d-670c-470d-8fae-498e31ad8ad7.json```\\n#### Source\\nIf the storage files come from outside of GCP, the storage file should have meta data containing the original path of the file. For example, if the file comes from an Azure fileshare with path ```azure-fileshare-name\/export\/file-name```, then this path should be added to the meta data.\\n","tokens":22,"id":2700,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0036-ssl-certificates-are-always-verified.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen using SSL, certificate verification ensures the identity of the other party we're communicating with. Using unverified certificates makes the communication more vulnerable to man-in-the-middle attacks. Certificate verification can be done using a trusted Certificate Authority (CA) or by pinning the certificate (importing a host's certificate in your trust store).\\n","Decision":"We will only use verified SSL certificates.\\n","tokens":69,"id":2702,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to standardize the directory structure for github repos.\\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0010-cloudbuild-yaml-style.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to use a specified cloudbuild.yaml style.\\n","Decision":"We identify different rules that should be followed when creating or editing a cloudbuild.yaml file:\\n### 1. Start of file\\nSince cloudbuild.yaml is a yaml file, the file should always start with `---`.\\n### 2. An ID for every step\\nMake sure that every step has an ID. Rules for this ID:\\n* It is a short but clear title containing an explanation of what the step does, a maximum of 50 characters should suffice.\\n* The words in the ID should be divided by `-`.\\n* If you cannot explain the step in 50 characters, add a comment above the step explaining what the step does.\\n### 3. Avoid \"waitFor\" field\\nTry to avoid using the `waitFor` field as much as possible since not using it makes sure that the steps are done in the order they are put in the cloudbuild.\\n### 4. Empty lines between build steps\\nUse empty lines between build steps to increase readability.\\n### 5. Order of steps\\nThe following rules should be followed when deciding on the order of the steps in the cloudbuild:\\n* The first step of the cloudbuild should always be the deployment of the data catalog.\\n* The subsequent steps should be the cloning of the necessary Github repositories.\\n* Cloud function permissions should be set in the build step after the deployment of the cloud function.\\n* If a scheduler is deployed in a step and it schedules a specific function which is also deployed in the cloud build, the step containing the deployment of the scheduler should be deployed right after the step which deploys the permissions of the cloud function.\\n* If a chain test is added, make sure that it is the last step in de cloudbuild.\\n### 6. Avoid if-else statements based on branches\\nAvoid if-else statements based on branch names.\\n### 7. Build container\\nAlways use `'gcr.io\/google.com\/cloudsdktool\/cloud-sdk:latest'` als build container (in the `name` field of the step). Different entrypoints can be used with this container.\\n### 8. Bash pipe\\nWhen using bash as entrypoint in a cloud build step, one should always use a pipe.\\n### 9. Function call in step\\nIf a function call is done in a step, its command plus subcommand should be on one line while its options or flags should be defined on separate lines. One should also use the long options if they are available, e.g. `-q` becomes `--quiet`.\\n### 10. Do not use gcloud as entrypoint\\nDo not use `gcloud` as entrypoint but use bash instead when having to call gcloud functionalities.\\n### 11. Cloud scheduler deletion\\nWhen deploying a cloud scheduler, the old \"version\" of this scheduler should first be deleted before redeployment.\\n### 12. One line commands without bash entrypoint\\nWhen a step only has one line and does not have a bash entrypoint, there are two ways you can define the step. Below is an example of a step where a github repository is cloned.\\n* ```yaml\\n- name: 'gcr.io\/google.com\/cloudsdktool\/cloud-sdk:latest'\\nid: 'cloning-repository'\\nentrypoint: 'git'\\nargs:\\n- 'clone'\\n- '--branch=git-branch'\\n- 'github-repository-url'\\n```\\n* ```yaml\\n- name: 'gcr.io\/google.com\/cloudsdktool\/cloud-sdk:latest'\\nid: 'cloning-repository'\\nentrypoint: 'git'\\nargs: ['clone', '--branch=git-branch', 'github-repository-url']\\n```\\n","tokens":19,"id":2704,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0044-openapi3.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe OpenAPI Specification (OAS) defines a standard, language-agnostic interface to RESTful APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic.\\nAn OpenAPI definition can then be used by documentation generation tools to display the API, code generation tools to generate servers and clients in various programming languages, testing tools, and many other use cases.\\n","Decision":"We will use (when possible) OpenAPI Spec version 3\\n","tokens":115,"id":2705,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0030-life-cycle-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing 3rd party components, like libraries, modules or executables, introduces the risk of importing security vulnerabilities in those components. Hackers might target these components in a software supply chain attack. A basic precaution to this is to keep all components up-to-date, a practice referred to as life cycle management.\\n","Decision":"We will keep 3rd party components up-to-date to mitigate software supply chain attack risk.\\n","tokens":66,"id":2707,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0047-shell-scripting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA Unix shell is a command-line interpreter or shell that provides a command line user interface for Unix-like operating systems. The shell is both an interactive command language and a scripting language, and is used by the operating system to control the execution of the system using shell scripts.\\n","Decision":"Within the Linux exnvironment Bash (Bourne Again Shell) is the most popular which will be used within our environment. Shell scripts should comply with the CheckCheck linter.\\n","tokens":58,"id":2708,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0032-oauth2-for-authentication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing http protocol and APIs, authentication and authorisation is mostly done by passing some token with every request. Fixed secrets are easy to understand and configure, but somewhat weak in protection. Instead, oauth2 is a stronger mechanism to create tokens, though more complicated. In oauth2 authentication and authorisation are decoupled and the security critical authentication is delegated to the authorization server. Still, oauth2 allows distributed token validation next to validating the token with the server.\\n","Decision":"We will use oauth2 for authentication and authorization.\\n","tokens":96,"id":2709,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0037-secrets-are-stored-in-gcp-secret-manager.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSecrets must be protected according to the least-privilege principle. To reduce the trusted computing base, preferrably a 3rd party secret management tool is used to manage and use secrets. Google Secret Mananager is a managed service on GCP, integrated into the platform. This makes it a suitable tool to manage our secrets.\\n","Decision":"We will use GCP Secret Manager to manage, store and use secrets.\\n","tokens":72,"id":2711,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0013-data-retention-defined-on-each-data-component.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nData LifeCycle Management is a process that helps organisations to manage the flow of data throughout its lifecycle \u2013 from initial creation through to destruction. While there are many interpretations as to the various phases of a typical data lifecycle, they can be summarised as follows:\\n1. Creation: data acquisition or retrieval\\n2. Storage: storing the data\\n3. Usage: using the data to support activities in the organisation\\n4. Archival: keeping the data in case it is needed again\\n5. Destruction: removal of the data from the organisation\\nWhen storing data, we must make sure it is available for the time it is needed, but it should also be deleted when it is not needed again. This not only saves cost, but also ensures timely removal of Personal Identifiable Information compliant to GDPR.\\nManagement of retention policies and life cycle (destruction) measures can be automated by specifying the period the data should be kept. Protecting the data from deletion during this period and purging it after ensures adherence to the temporal requirements on the data.\\n","Decision":"We will specify a temporal for each dataset in a [Project Company Data](0053-project-company-data.md) data catalog, which will automatically result in retention policies and life cycle rules.\\n","tokens":215,"id":2712,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0042-python-version-3.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn the past, there was a bit of a debate in the coding community about which Python version was the best one to learn: Python 2 vs Python 3 (or, specifically, Python 2.7 vs 3.5). Now, it's more of a no-brainer: Python 3 is the clear winner.\\n","Decision":"We will use python3  exclusively for all our python code.\\nMake sure you use version active maintained version (currently 3.6 or higher)\\n","tokens":72,"id":2713,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0054-coding-guidelines.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCoding conventions are a set of guidelines for a specific programming language that recommend programming style, practices, and methods for each aspect of a program written in that language. These conventions usually cover file organization, indentation, comments, declarations, statements, white space, naming conventions, programming practices, programming principles, programming rules of thumb, architectural best practices, etc. These are guidelines for software structural quality. Software programmers are highly recommended to follow these guidelines to help improve the readability of their source code and make software maintenance easier.\\nCoding guidelines result in enhanced efficiency, reduced risk, mininized complexity, maintainability, bug rectification, comprehensive look and cost efficiency.\\n","Decision":"We will use coding guidelines for all languages used on the platform.\\n","tokens":135,"id":2714,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0018-single-writer-for-a-topic.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTopics are used to distribute events to other applications. Systems subscribing to these events will be dependent on these events.\\n","Decision":"Every topic is limited to a single writer process.\\n","tokens":26,"id":2715,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0011-cqrs-separates-read-and-write-responsibility.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCQRS stands for Command Query Responsibility Segregation. It's a pattern that I first heard described by Greg Young. At its heart is the notion that you can use a different model to update information than the model you use to read information.\\nThe core of the ODH are the [pubsub](0016-pub-sub-implements-event-sourcing.md) topics implementing [event sourcing](0010-event-sourcing-captures-every-change-to-business-state.md). This allows consumers of these events to build a data representation (model) from these events that exactly fits their use case. The result is that different models exist, representing the same real-life entities.\\n","Decision":"We will implement Command Query Responsibility Segregation (CQRS) on the ODH between  [projects](0028-a-solution-is-implemented-by-one-or-more-gcp-projects.md).\\nCQRS is not required within a project and should only be applied at project level if the additional complexity is justified by CQRS-specific advantages.\\n","tokens":137,"id":2716,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0046-javascript-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nA Javascript framework is an abstraction in which software providing generic functionality can be selectively changed by additional user-written code. JavaScript framework is an application framework written in JavaScript where the programmers can manipulate the functions and use them for their convenience.\\nFrameworks are more adaptable for the designing of websites, and hence, most of the website developers prefer it. JavaScript frameworks are a type of tool that makes working with JavaScript easier and smoother. These frameworks also make it possible for the programmer to code the application as a device responsive. This responsiveness is yet another reason why the JavaScript frameworks are quite popular when it comes to the question of using a high-level machine language.\\n","Decision":"We have decided to use the Angular Javascript framework\\n","tokens":150,"id":2717,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0049-gitops-deployment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGitOps is a way of implementing Continuous Deployment for cloud native applications. It focuses on a developer-centric experience when operating infrastructure, by using tools developers are already familiar with, including Git and Continuous Deployment tools.\\nThe core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment match the described state in the repository. If you want to deploy a new application or update an existing one, you only need to update the repository - the automated process handles everything else. It\u2019s like having cruise control for managing your applications in production.\\n","Decision":"All cloud objects are deployed using gitops processes. A push to the git repo will trigger a cloud build process which will install all needed objects\\n","tokens":130,"id":2718,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0031-uniform-bucket-level-access-for-storage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs motivated in [14. Single confidentiality level per data component](0014-single-confidentiality-level-per-data-component.md), access level granularity is kept at data component level. For buckets, this means uniform bucket-level access will be used, the more fine grained object-level ACLs will not be used.\\n","Decision":"We will use uniform bucket-level access for storage.\\n","tokens":66,"id":2720,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0040-hpa.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHigh privilege access (HPA) limits production access for developers to only the components and period this access is required to investigate issues of check system health. This implements the [principle of least privilege](0039-least-privilege-access.md) for support on production systems.\\n","Decision":"We will use a high privilege access procedure to secure access to production systems for support.\\n","tokens":60,"id":2721,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0009-cloud-function-entrypoint.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to define a uniform way of setting entrypoints for Cloud Functions within the Google Cloud Platform\\n","Decision":"- The entrypoint should have the same name as the function directory.\\n- The entrypoint name should be clear and descriptive.\\n- The entrypoint should be in `main.py`.\\n- The entrypoint should not contain business logic.\\n","tokens":25,"id":2722,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0007-starting-a-flask-app.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to set a standard for starting a Flask App.\\n","Decision":"There should be only one entrypoint to start a Flask App. When using [openapi-generator](https:\/\/github.com\/OpenAPITools\/openapi-generator) this entrypoint defaults to `__main__.py`, which should look similar to:\\n```\\n#!\/usr\/bin\/env python3\\nimport connexion\\nfrom openapi_server import encoder\\ndef main():\\napp = connexion.App(__name__, specification_dir='.\/openapi\/')\\napp.app.json_encoder = encoder.JSONEncoder\\napp.add_api('openapi.yaml',\\narguments={'title': 'Petstore API'},\\npythonic_params=True)\\napp.run(port=8080)\\nif __name__ == '__main__':\\nmain()\\n```\\nA Flask app can be started locally by executing running `python __main__.py`.\\nWhen running a Flask app in Google App Engine (GAE), the `app.yaml` should use `gunicorn` to start Flask:\\n```\\n---\\nruntime: python37\\nentrypoint: gunicorn --pythonpath __main__.py -b :$PORT __main__:app --workers 4\\ninstance_class: F4\\n```\\nNote: `gunicorn` is automatically available in the GAE Python runtime environment.\\n","tokens":18,"id":2724,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0005-oauth-scope-naming-conventions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to use a naming convention for OAuth scopes.\\n","Decision":"For this coding standard we will follow a [Restful API Guideline](https:\/\/opensource.zalando.com\/restful-api-guidelines\/)\\nfrom Zalando.\\nGuideline [#225](https:\/\/opensource.zalando.com\/restful-api-guidelines\/index.html#225) says that permission names in\\nAPIs must conform to the following naming pattern:\\n~~~text\\n<permission> ::= <standard-permission> |  -- should be sufficient for majority of use cases\\n<resource-permission> |  -- for special security access differentiation use cases\\n<pseudo-permission>      -- used to explicitly indicate that access is not restricted\\n<standard-permission> ::= <application-id>.<access-mode>\\n<resource-permission> ::= <application-id>.<resource-name>.<access-mode>\\n<pseudo-permission>   ::= uid\\n<application-id>      ::= [a-z][a-z0-9-]*  -- application identifier\\n<resource-name>       ::= [a-z][a-z0-9-]*  -- free resource identifier\\n<access-mode>         ::= read | write    -- might be extended in future\\n~~~\\nTo meet our own naming guidelines, we will use our Solution IDs as the `<application-id>`. The naming convention of\\nthese Solution IDs are described within our Cloud naming convention page on Confluence.\\n","tokens":17,"id":2726,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0024-refer-to-blob-at-the-source-project.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAlthough inter-domain communication should be done using ODH topics, this technology is less suitable to transfer large binary objects. Therefore, references of large binaries are communicated on the ODH topics instead of the large binary itself. This reference can be used by consumers to retrieve the binary object at its source.\\n","Decision":"Instead of the large binary object itself, we will pass references to it on ODH topics, which can be used by consumers to retrieve the object at its source.\\n","tokens":63,"id":2727,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0004-create-software-defined-everything.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSoftware-defined everything (SDx) is the definition of technical computing infrastructure entirely under the control of software with no operator or human intervention. It operates independent of any hardware-specific dependencies and is programmatically extensible.\\nIn the SDx approach, an application's infrastructure requirements are defined declaratively (both functional and non-functional requirements) such that sufficient and appropriate hardware can be automatically derived and provisioned to deliver those requirements.\\nThe benefits of SDx is that it lowers\/eliminates effort towards infrastructure maintenance, allows companies to move focus to other parts of the software, ensures consistence while also allowing for extensibility, remote deployment through configuration without downtime, and allows you to leverage the power of versioning such as git.\\n","Decision":"Where possible software and infrastructure (or whatever) are deployed from source code. No human management of software and infrastructure is performed.\\n","tokens":150,"id":2728,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0033-two-identity-providers-development-run.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAccess to the solutions (applications) as a user, referred to as the _run_ environment, is based on the company identity provider. Access to the development\/operations environment, the GCP platform, is based on a separate, DevOps identity provider. This allows strict separation between Run and DevOps and makes automation of DevOps practices somewhat easier.\\n","Decision":"We will use a separate identity provider to access the platform for DevOps practices, disconnected from access to the _run_ environment.\\n","tokens":75,"id":2729,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0021-messages-are-in-json-format.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt is preferred to use a single message type for the business events. This makes it easier to handle messages on the pub\/sub system in a standerdized way.\\n","Decision":"All business events on the ODH platform topics are formatted as [JSON](https:\/\/tools.ietf.org\/html\/rfc7159)\\n","tokens":37,"id":2730,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0016-pub-sub-implements-event-sourcing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe sequence of events arising from the [event sourcing](0010-event-sourcing-captures-every-change-to-business-state.md) pattern has to be available to consumers. The pub\/sub pattern seems to be a good mechanism to decouple publisher and subscriber, making them both as independant as possible from each other.\\nFunctionality of the event handling mechanism is distribution of events and keeping a history of events that can be used for backloading and later analysis. Several implementations exist that could be applied on the platform. For example, Apache Kafka is a commonly used implementation. Google Cloud Pub\/Sub is available as a managed service on the platform. Compared to other solutions, it provides less maintenance and better integration with other components on the platform. On the other hand, it does not provide message history and ordered, one-time delivery, both properties that would simplify implementation of event sourcing patterns. Acquiring a pub\/sub implementation that has these properties would not be beneficial due to higher cost and implementation complexity.\\nCloud Pub\/Sub allows subscribers to publish their events, which makes them available to consumers. Distribution is either push, using HTTP methods, or pull, using the Pub\/Sub APIs and client libraries. By offloading the messages to persistent storage (e.g. Cloud Storage or BigQuery), the history of events can be made available.\\n","Decision":"We will use Google Cloud Pub\/Sub topics to handle the events processed on the ODH.\\n","tokens":269,"id":2731,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0034-2fa-on-all-user-identities.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTwo-Factor Authentication (2FA) is sometimes called multiple factor authentication. In simple terms, it adds an extra layer of security to every online platform you access. The first layer is generally a combination of a username and password. Adding one more step of authenticating your identity makes it harder for an attacker to access your data. This drastically reduces the chances of fraud, data loss, or identity theft.\\nPasswords have been the mainstream form of authentication since the start of the digital revolution. But, this security measure is far from infallible. Here are some worrying facts about this traditional security measure:\\n* 90% of passwords can be cracked in less than six hours.\\n* Two-thirds of people use the same password everywhere.\\n* Sophisticated cyber attackers have the power to test billions of passwords every second.\\nThe vulnerability of passwords is the main reason for requiring and using 2FA.\\n","Decision":"We will use 2FA on all user identities.\\n","tokens":187,"id":2732,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0048-testing-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe variety of JavaScript testing frameworks made available for automation testers have become a cause of confusion for many. It is only natural, the more options you have in front of you, the more time it will take for you to decide which JavaScript automation testing framework fits the best for you.\\n","Decision":"We have decided to use the cypress testing framework for frontend and e2e testing.\\n","tokens":60,"id":2733,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0008-data-is-location-and-time-aware.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAll data that is related to geographic structures is added to the ODH in a standardized way.\\n","Decision":"GeoJSON is a format for encoding a variety of geographic data structures (https:\/\/tools.ietf.org\/html\/rfc7946)\\nGeoJSON-events extends RFC 7649 GeoJSON with instants and intervals of time (https:\/\/github.com\/sgillies\/geojson-events)\\n","tokens":23,"id":2734,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0052-odrl-policy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPermissions on objects have to be specified in a clear and specific way. The [Open Digital Rights Language (ODRL)](https:\/\/www.w3.org\/TR\/odrl-model\/) is a policy expression language that provides a flexible and interoperable information model, vocabulary, and encoding mechanisms for representing statements about the usage of content and services. The ODRL Information Model describes the underlying concepts, entities, and relationships that form the foundational basis for the semantics of the ODRL policies.\\nPolicies are used to represent permitted and prohibited actions over a certain asset, as well as the obligations required to be met by stakeholders. In addition, policies may be limited by constraints (e.g., temporal or spatial constraints) and duties (e.g. payments) may be imposed on permissions.\\nThe ODRL specification can be added to the [data catalog](0012-data-catalog-specifies-all-data-components.md) to keep data storage and permissions on the data stored closely related. Also permissions on other components, like projects, can be specified using ODRL.\\nSpecifying ODRL can be done in the JSON format as described by [53. Project company data](0053-project-company-data.md).\\n","Decision":"We will use ODRL to specify permissions on components on the platform as implemented by [53. Project company data](0053-project-company-data.md).\\n","tokens":242,"id":2735,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0004-code-smells.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to eliminate practices that could lead to [coding by exception](https:\/\/en.wikipedia.org\/wiki\/Coding_by_exception). Furthermore we want to create some guidelines to prevent some common [code smells](https:\/\/en.wikipedia.org\/wiki\/Code_smell).\\n","Decision":"### 1. Low cyclomatic complexity \/ Low level of recursion.\\nCyclomatic complexity is the number of linearly independent paths in a piece of code. In short, this is determined by the depth of a coding stack. If-statements for-loops, and inline functions all increase the cyclomatic complexity of code. Higher cyclomatic complexity can lead to more coding errors, and less transparent code.\\nAnother reason for this is that a higher cyclomatic complexity can indicate a high level of recursion, which in turn can make code less efficient for larger data sets.\\nThe cyclomatic complexity of code should be no higher than 3-5 per function or class.\\nflake8 will be used to enforce this requirement.\\n### 2. \"Do not repeat yourself.\"\\nWhenever possible, we should abstract away from duplicated code. It is not a problem to copy and paste a small code snippet once.\\nWhenever a piece of code is reused twice, an abstraction should be made to remove the duplication.\\n### 3. Keep it simple\\nTo ensure reusable code and good collaboration the following rules should be followed:\\n- Use explicit variable names.\\n- Variables like `file` should be named after what they're used for e.g.: `source_pdf_file`\\n- Classes that handle data storage should be named after the type of data they store.\\n- `WorkItemStorageHandler` instead of `StoragerHandler`.\\n- Clear is better than clever.  Favor readability over conciseness.\\n- Use for loops over list comprehensions.\\n### 4. Specific small modules\\n- Write code in small reusable modules.\\n- A python function should serve one purpose and one purpose only.\\n- Generally a python file should be 100-150 lines long.\\n### 5. No code in ```__init__.py```\\nNo code should be put inside an ```__init__.py``` file, since this file is only for exposing internal dependencies to the top level of the package.\\nInstead, a main.py file should be used for code.\\n### 6. Object-oriented code\\nPython is an object-oriented language, so developers should strive to write object oriented code. This is a challenge for cloud functions specifically. Functions tend to be procedural by nature. In this case, a developer should write one file (main.py), with one entrypoint, which procedurally processes a message. To keep it OOP and testable, abstract different features away.\\nFor reading an email inbox, use a dedicated `MailService` object. for writing to storage, use a `Storage` object.\\nAn example: [https:\/\/github.com\/vwt-digital\/ews-mail-ingest\/blob\/develop\/functions\/ews-mail-ingest\/main.py](https:\/\/github.com\/vwt-digital\/ews-mail-ingest\/blob\/develop\/functions\/ews-mail-ingest\/main.py)\\n### 7. Avoid unnecessary exceptions.\\n- Only handle exceptions when they occur.\\n- When an exceptions occurs, first try to prevent the exception from occuring.\\n- If this is not possible, properly handle the exception.\\n- Don't use a `try: except:` block without an exception or with simply a  `pass` statement\\n### 8. Implement retry policies carefully.\\nWhen calling external API\u2019s and services, predetermine the retry policy. Always throw an exception after retries are exhausted. This ensures that when a service\u2019s poor health can be noticed.\\nFor exceptions that occur when calling an external service the function, a warning should be emitted that can be monitored to ensure that this does not occur to often within a given timeframe.\\n### 9. Use dependency injection\\n- Code should be loosely coupled from its dependencies.\\n- When using an external service, always write an [adapter](https:\/\/en.wikipedia.org\/wiki\/Adapter_pattern), so that it can be tested or changed for another external service.\\n### 10. No conditionals in deployment\\nEnvironments should be exactly the same.\\n- No references to the environment in a cloudbuild.yaml\\n- No environment checks in code.\\n","tokens":57,"id":2736,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0053-project-company-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSpecifying the [data catalog](0012-data-catalog-specifies-all-data-components.md) requires a clear and specific way of documenting that is also suitable for automated processing.\\nA data catalog enables a publisher to describe datasets and data services in a catalog using a standard model and vocabulary that facilitates the consumption and aggregation of metadata from multiple catalogs. The challenge is to define and name standard metadata fields so that a data consumer has sufficient information to process and understand the described data. The more information that can be conveyed in a standardized regular format, the more valuable data becomes.\\n[Project company data](https:\/\/vwt-digital.github.io\/project-company-data.github.io\/) specifies a metadata format for data catalogs in a machine readable JSON format, covering the commonly required metadata attributes.\\n","Decision":"We will use the [Project company data](https:\/\/vwt-digital.github.io\/project-company-data.github.io\/) to describe the data catalog.\\n","tokens":159,"id":2738,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0025-domains-correspond-to-business-departments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe organizational model of a company has a great influence on the communication structure in the company. Within a department, people tend to use the same terminology and definitions. This relates closely to the definition of ubiquitous language. Therefore, the domain model used on the platform should be closely related to the organizational model of the company. This is also supported by Conway's law.\\n","Decision":"We will structure the domain model on the platform around the departments organizational model of the company.\\n","tokens":76,"id":2739,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0045-code-config-separation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBoth Code and Configuration reside in source code control (Github in our case). This makes it very easy to mix-up code and configuration. However, these 2 should be clearly separated. Where possible code can be reused, but configuration is most of the times instance specific.\\n","Decision":"Code and Configuration is clearly separated. At deployment time the CI\/CD tools are responsible for bringin code and config together and deploy the code together with the correct configuration.\\n","tokens":58,"id":2741,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0035-all-communication-uses-ssl.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEncryption of communication is used to ensure that only the intented recipient can read the information. Even when communication is intercepted, the information will still be protected by the encryption. SSL is a widely used protocol to secure communication over Internet protocols (tcp\/ip).\\n","Decision":"We will secure all communication using SSL.\\n","tokens":54,"id":2742,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0002-repo-naming-conventions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to use a naming convention for github repos.\\n","Decision":"We identify three kinds of repositories:\\n### 1. General rules:\\n* Use hyphens ('-') between words in the name because:\\n* words written together without something inbetween are unclear.\\n* \"\\_\" is harder to type than \"-\"  [stack overflow](ttps:\/\/stackoverflow.com\/a\/11947816).\\n* Make repo names not longer than needed. (Because GCP project names are also limited in length).\\n### 2. config VWT DAT repositories\\nConfig repositories are repositories containing configurations of a specific Google Cloud Project (GCP) project.\\n* Should have the same name as the GCP project they are connected to minus the customer, environment and location.\\n* Name ends with `-config`.\\n### 3.  solutions VWT DAT repositories\\nSolutions repositories are repositories containing solutions, they can belong to multiple domains.\\n* Their names should always start with the domain they belong to.\\n* If the repository will handle multiple facets of the service, the name should end in `-handlers`\\n* Sometimes, two repositories are connected because they are the frontend and backend of a service. Their names should be the same except for the ending. Frontend repositories should end in `-tool` and backend repositories should end in `-api`.\\n### 4. \"normal\" VWT DAT repositories\\n\"Normal\" repositories are repositories not belonging to a solution. They contain code used specifically for the Operational Data Hub (ODH).\\n* Repository naming is equal to naming convention for solution repositories. Domains for these reposiitories is limited to `dat` and `odh`.\\n* If the repository is forked from another repository, its name should contain the name of the repository it forked from.\\n","tokens":17,"id":2743,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0006-implement-security-by-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSecure by design, in software engineering, means that the software has been designed from the foundation to be secure. Secure by Design is more increasingly becoming the mainstream development approach to ensure security and privacy of software systems. In this approach, security is built in the system from the ground up and starts with a robust architecture design.\\n\u201cThe problem is: 95 percent of successful attacks are due to poorly programmed, poorly maintained or poorly configured software,\u201d says Thomas Tschersich, Head of Internal Security & Cyber Defense at Deutsche Telekom.\\n","Decision":"Security is built-in in all our designs from the start. Security is not something which is built in afterwards.\\n","tokens":111,"id":2744,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0008-config-and-environment-variables.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe feel the need to create guidelines for the use of config variables and environment variables when using cloudbuild.\\n","Decision":"**In Short**\\nThe code that is going to be executed by the Cloud Function (aka the project that is deployed) should receive its configuration variables from a config file.\\nThe cloudbuild steps use environment variables which can be stored in the cloudbuild.yaml or an external file.\\n**Elaboration**\\nWhen developing a Cloud Function, you should store the variables in a config file. The variables are easy to read and use, and other developers (and you) don\u2019t need to build anything for the project to run (unless something other than variables need a build).\\nThis moves over to when a project is deployed: The variables used in the code of the Cloud Function are all stored within the project: A config file. The project should not receive variables from a cloudbuild, but from a file that is merged into the project by the build (or the other way around).\\nWhen building something, you might need variables that you are only going to use for building\/deployment. Or you might use an external project that needs to get some variables. These variables should be given as environment variables or CLI flags. These variables could be stored in a file that is placed inside a repository for easy access, but can be put into the build as environment files.\\n**Examples**\\nConfiguration stored in a repository and merged into the project environment (or the other way around), like this example:\\n`config.py`\\n```python\\nINDEX = 0\\nTOKEN = 'AAAAaaaaBBBBbbbb'\\n```\\n`__main__.py`\\n```python\\nfrom config import INDEX, TOKEN\\n```\\n<br>\\nConfiguration as an environment variable used to build in a specific step, like this example:\\n```yaml\\nsubstitutions:\\n_VENV: '\/venv'\\n- name: 'cloud'\\nentrypoint: 'bash'\\nargs:\\n- '-c'\\n- |\\nsource ${_VENV}\/bin\/activate\\n```\\n","tokens":27,"id":2745,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0058-api-is-trust-boundary-odh.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe trust boundary is a boundary where program data or execution changes its level of \"trust.\" The term refers to any distinct boundary within which a system trusts all sub-systems (including data). An example of an execution trust boundary would be where an application attains an increased privilege level (such as root). A data trust boundary is a point where data comes from an untrusted source. For example, user input or a network socket.\\nTrust boundaries aren\u2019t hard to find: We just need to ask questions like \u201cWhat are the consequences if this code\/data became horribly malicious? Is that likely? Can we defend against it? Do we want to defend against it?\u201d\\nTo deal with trust boundaries, we have all the usual techniques and organizing principles: input sanitization, defense in depth, sandboxing, secure authentication, least privilege, etc.\\nOn the ODH platform the first trust boundary when connecting from outside is the API, which is a result of the [API first](0043-api-first.md) principle. The API implements authentication and authorization (is identity aware), performs input validation and sanitization, protects back-end services from the outside world and is executed with only the permissions required, according to the [principle of least privilege](0039-least-privilege-access.md). This makes the API a trust boundary between the outside world and the platform.\\nConnecting platform components without an intermediate API lacks this additional layer of defense, thus reducing defense in depth. Using an API also decouples the service provided from its implementation, as the client is not using a platform specific implementation, but the API interface instead, which can be ported to another implementation without impact on the client.\\n","Decision":"We will provide all functionality to external systems through an API, using this as a trust boundary on the ODH platform.\\n","tokens":346,"id":2746,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0001-record-coding-guidelines.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the coding standard decisions made on the ODH platform.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":19,"id":2747,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0055-feature-toggles-over-feature-branching.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nContinuous integration is defined as developers doing mainline integration as soon as they have a healthy commit they can share, usually less than a day's work. For larger features, this means code changes for features are already committed to the mainline before the feature is finished. To prevent confronting users with unfinished features, these features are hidden by a keystone interface or feature toggle. The keystone interface is a user interface component that gives access to the new feature. By hiding or disabling this component, the feature is invisible to the user. The feature toggle is a configurable switch that can be used to enable or disable the feature. Both techniques allow code of partly implemented features to be integrated and deployed without exposing it to the user.\\nAnother way of concurrent development of new features on an existing code base is feature branching. With feature branching all work for a feature is put on its own branch and not integrated into mainline until the feature is complete. Feature branching appears to be the most common branching strategy in the industry at the moment, but there is a vocal group of practitioners who argue that Continuous Integration is usually a superior approach. The key advantage that Continuous Integration provides is that it supports higher, often a much higher, integration frequency.\\nHigher frequency of integration leads to less involved integration and less fear of integration. If you've lived in a world of integrating every few weeks or months, integration is likely to be a very fraught activity. It can be very hard to believe that it's something that can be done many times a day. But integration is one of those things where frequency reduces difficulty. It's a counter-intuitive notion - \"if it hurts - do it more often\". But the smaller the integrations, the less likely they are to turn into an epic merge of misery and despair. With feature branching, this argues for smaller features: days not weeks (and months are right out).\\nContinuous integration allows a team to get the benefits of high-frequency integration, while decoupling feature length from integration frequency. If a team prefers feature lengths of a week or two, continuous integration allows them to do this using feature toggles while still getting all the benefits of the highest integration frequency by merging to the mainline often.\\n","Decision":"We prefer continuous integration (using feature toggles) over feature branching.\\n","tokens":450,"id":2748,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0051-declarative-preferred-over-actions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn computer science, declarative programming is a programming paradigm\u2014a style of building the structure and elements of computer programs\u2014that expresses the logic of a computation without describing its control flow.\\nMany languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives (the how being left up to the language's implementation). This is in contrast with imperative programming, e.g. actions in Deployment Manager, which implements algorithms in explicit steps.\\nDeclarative specifications are more clear on the expected result, once you are familair with the syntax. To understand the result of imperative specifications, interpretation of the statements in order of execution is required. Declarative syntax more explicitly states the result. Next to that, declarative specifications are context-independant. They state what should be there, instead of continuing on things that happened (or should have happened) before.\\n","Decision":"We will prefer declarative syntax over imperative syntax.\\n","tokens":203,"id":2750,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0012-data-catalog-specifies-all-data-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMetadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information resource (NISO 2004, ISBN: 1-880124-62-9). A Data Catalog is a collection of metadata, combined with data management and search tools, that helps analysts and other data users to find the data that they need, serves as an inventory of available data, and provides information to evaluate fitness data for intended uses.\\nBy specifying the data catalog as part of the application code in a schema defined by [Project Company Data](0053-project-company-data.md), deployment of the data storage can also be done from the this metadata. By only creating storage from the data catalog, it is assured that all storage on the platform is defined in the data catalog.\\nThe data catalogs can be used to populate a Data Portal Platform (data catalog viewer). This makes the metadata in the data catalog accessible for users.\\nTogether this leads to the implementation of Enterprise Metadata Management, the business discipline for managing the metadata about the information assets of the organization.\\nDevelopers specify the metadata with the storage declaration in the data catalog, which is part of the application programming code. On deployment the storage specified in the data catalog is created and the data catalog is published to the Data Portal Platform. Data governance employees, data analysts and business analysts use the Data Portal Platform to access the metadata.\\n","Decision":"We will specify and deploy all storage on the ODH platform using a data catalog in the schema of [Project Company Data](0053-project-company-data.md).\\n","tokens":291,"id":2751,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0020-topic-messages-have-gobits.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nData lineage includes the data origin, what happens to it and where it moves over time. Data lineage gives visibility while greatly simplifying the ability to trace errors back to the root cause in a data analytics process.\\nBy adding tracing information to every single message it is possible to trace back a single business event to its source (and all the systems inbetween).\\n","Decision":"Every event published on a pub\/sub topic has a gobits record added. Every applications handling\/modifing or relaying the event should add a gobits record to that single business event.\\n","tokens":76,"id":2753,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0014-single-confidentiality-level-per-data-component.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGranularity of access determines the boundaries of control regarding protection of confidential information. For example, a bucket has [uniform bucket level access](0031-uniform-bucket-level-access-for-storage.md). Therefore, all information in the bucket should have the same confidentiality level to make sure the right access permissions are applied.\\n","Decision":"We will only store data of a single confidentiality level on a data component.\\n","tokens":66,"id":2754,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0010-event-sourcing-captures-every-change-to-business-state.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMartin Fowler: \"Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\"\\nEvent sourcing persists the state of a business entity such an Order or a Customer as a sequence of state-changing events. Whenever the state of a business entity changes, a new event is appended to the list of events. Since saving an event is a single operation, it is inherently atomic. The application reconstructs an entity\u2019s current state by replaying the events.\\nA system is eventsourced when\\n- the single source of truth is a persisted history of the system\u2019s events;\\n- and that history is taken into account for enforcing constraints on new events.\\n","Decision":"All data on the ODH is available as a series of events. Events are published on the ODH topics and applications can process these events by creating a subscription on the topic.\\n","tokens":175,"id":2758,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0028-a-solution-is-implemented-by-one-or-more-gcp-projects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA [solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md). Those functions can originate from multiple domains. A [project always belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md). Therefore, a solution can be implemented by multiple projects, either due to the fact that it requires functions from multiple domains, or because projects allow better modularization of the solution, or both.\\n![Structure of projects, domains and solutions](solution_project_domain.png \"Projects in different domains implementing a solution\")\\n","Decision":"We implement a solution by one or more projects.\\n","tokens":126,"id":2760,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2761,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0022-locations-are-specified-in-geojson.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA large numer of the events on the ODH contain location specific data. It is usefull to standardize the usage for location events.\\n","Decision":"[GeoJSON](https:\/\/tools.ietf.org\/html\/rfc7946) will be used for XYZ (locations) on the ODH. When XYZt data is needed [GeoJSON-events](https:\/\/github.com\/sgillies\/geojson-events) should be considered.\\n","tokens":32,"id":2762,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/006-homogeneous-build-across-environments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* In order to develop and test their build and deployment pipeline the teams need be able to run the same build tools as the CI server offline on their machines.\\n* In order to be able to trace user acceptance down to code and to enable efficient and fast pipelines we must not build the same code multiple times for different environments \/ configurations.\\n","Decision":"* We make a **pipeline development environment** available to teams. See #34\\n* The CI server only **build once** per pipeline run, so that the commit hash of the system can be used as reliable reference.\\n","tokens":72,"id":2764,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/003-library-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhat is our testing strategy in terms of tooling, coverage and test types.\\nhow to test\\n- unit tests\\n- testing libs \/ frameworks\\n- testing approaches\\n- what to test\\n- how to e2e test before merging \/ releasing\\n- architecture check\\n","Decision":"- For every public groovy Class method there is at least one unit test\\n- We aim for 100% branch coverage\\n- we use JUnit 4\\n- we use AssertJ\\n- we use ArchUnit\\n","tokens":59,"id":2765,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/004-depend-on-linux.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nShould we depend on running in an unix environment with support for e.g. `sh` commands.\\n","Decision":"We depend on linux and therefore reduce the overhead of maintaining linux and windows support.\\n","tokens":24,"id":2767,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/007-use-docker-as-default-for-tool-provisioning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n1) There different provisioning mechanisms for build tools\\n1) [Jenkins global tools](https:\/\/wiki.jenkins.io\/display\/JENKINS\/Manage+global+settings+and+tools+installations)\\n1) [Docker containers](https:\/\/hub.docker.com\/)\\n1) Tool specific managers like [jEnv](https:\/\/www.jenv.be\/) or [SDKMan](https:\/\/sdkman.io\/)\\n1) We want to ensure a [homogeneous build across environments](006-homogeneous-build-across-environments)\\n","Decision":"We use docker as default for tool provisioning\\n","tokens":117,"id":2769,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"CICD-pipeline\/008-backwards-compatibility.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n1) We have commonly used existing Jenkins libraries\\n1) We want to be able to migrate a large number of projects with minimal effort\\n","Decision":"We provide a compatibility layer for the current library\\n","tokens":31,"id":2770,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"thorleifurb.github.io\/0001-what-map-provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to base our user interface on map software and calculations in routing engine.\\nWhich map provider should we use?\\n## Decision Drivers\\n* Since this is research project of using free software, we require that the map usage MUST be free.\\n* Required is that the map has supporting library to generate and draw actual routes between two points.\\n* Required is that the map has supporting library or methods to calculate actual routes and time for driving, cycling and walking between two points.\\n","Decision":"* Since this is research project of using free software, we require that the map usage MUST be free.\\n* Required is that the map has supporting library to generate and draw actual routes between two points.\\n* Required is that the map has supporting library or methods to calculate actual routes and time for driving, cycling and walking between two points.\\nChosen option: \"[`Openstreetmap`](https:\/\/openstreetmap.org) with service provided by [`OSRM routing engine`](http:\/\/project-osrm.org\/)\", because\\n* Only option that is free for implementatation and does not require registration of any kind.\\n* The product is research only, and not intended to installation to production.\\n* `ja.is` did not provide any documentation about using routing engine and tiles, so it cannot be considered.\\nWe accept that usage of `Google maps` would be lot easier to implement. `Google maps` does host and maintain all required api's and geographical data.\\n`Google directions api` seems to have better geographical data for Iceland than we see in accessable `OSRM routing profiles`\\nIf product was intended for production we must add in our decision conideration about the price of using `Google maps` compared with the cost of hosting and maintaining `OSRM routing engine`.\\n### Positive Consequences\\n* We do not need to pay for usage of map.\\n### Negative Consequences\\n* We need to install, host and maintain `OSRM routing engine`.\\n* We need to update and maintain routing info on local `OSRM routing engine` to avoid outdated map data and route calculations.\\n<hr>\\nContent of decision: Copyright \u00a9 2020 \u00deorleifur Bjarnason, All Rights Reserved.\\n","tokens":105,"id":2771,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"exercise3-group-6\/0004-make-repository-iterable.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nObservationRepository and WhaleRepository both implement `Iterable<T>` to enable searching in their\\nrespective lists. Since ObservationRepository and WhaleRepository both implement interface `Repository<T>`\\nit may make sense to extend `Iterable<T>` in Repository rather than it's subclasses.\\n","Decision":"Extend `Iterable<T>` in Repository and remove `Iterable<T>` from ObservationRepository\\nand WhaleRepository.\\n","tokens":58,"id":2773,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"exercise3-group-6\/0001-create-whale-comparator.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are a variety of whales with many attributes, thus it may be necessary to sort these whales into\\nvarious different groupings. In order to sort these objects, a _function object_ must be implemented using\\none of three possible designs: nested classes, anonymous classes, and lambda expressions.\\n","Decision":"Implement `Comparable<Whale>`, create `compareTo` default method for field `species` (Species) and a nested comparator\\nclass for field `whaleId` (long) in Whale.\\n","tokens":64,"id":2774,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"exercise3-group-6\/0005-create-observation-repository-sort.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nResearchers may require methods for sorting the lists of Whale Observations to extract key information regarding\\nwhale migration, population, etc. Through Observation's available comparison methods the proposed sorting is possible.\\n","Decision":"Create `sortByDate` and `sortById` methods in ObservationRepository.\\n`sortByDate` uses the comparator `compareByDate` from the `Comparator<Observation>` interface and `Collections.sort()`\\nto sort the list of observations by field `sightingTime` (Date).\\n`sortById` uses the default `compareTo` method from the `Comparable<Observation>` interface and `Collections.sort()`\\nto sort the list of observations by field `ObservationId` (long).\\n`Collection.sort()` implements the abstract strategy of the `Comparator` interface using the concrete strategy\\n`compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":42,"id":2776,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"exercise3-group-6\/0002-create-repository-interface.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe system has various data types that users may want aggregated together so that they are easily accessible and sortable.\\ne.g. Types `Whale` and `Observation`. The system should have a consistent interface so that the user may access various\\ntypes of records.\\n","Decision":"We decided to implement a `Repository <T>` interface that can be realised by `Whale` or `Observation` objects. Users\\nwho need to access a large list\/repository of Whale's or Observation's will do so through the `Repository <T>` interface.\\n","tokens":58,"id":2777,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kore\/2019-01-14-Function-Evaluation-Simplification.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Problem\/Questions\\n-----------------\\nShould we add an attribute differentiating definition axioms from simplification\\naxioms?\\nShould we just change the function evaluation code to work with both types of\\naxioms?\\nDecision: Function definition attribute\\n---------------------------------------\\nLong term, we should add a function simplification attribute, since it's the\\nsafest way to solve the problem.\\nNote that it may make more sense to add a function definition attributes since,\\nin principle, definition axioms are in `O(symbol-count)`, while we could have\\nany number of simplification axioms. However, in practice it would probably\\nbe undesirable to add these attributes to the main semantics (especially\\nto the existing semantics), so we'll use simplification attributes.\\nDecision: No matching constraints with functions over axiom variables\\n---------------------------------------------------------------------\\nConstraints with normal functions (i.e. not constructors) over axiom variables\\nare not easily solvable and almost surely do not come from function definition\\naxioms. Therefore we should not attempt to apply axioms generating these\\nwhen matching since the result will probably be more complex than the input\\nand it does not make sense to apply a simplification axiom in order to get\\nsomething more complex.\\nIf we have these, matching should fail, i.e. it should throw an error.\\nDecision: Allow matching constraints with functions without axiom variables\\n---------------------------------------------------------------------------\\nIf any variables in the result have constructor-like symbols on top of them,\\nthe constraint seems more like a function definition one than like a\\nsimplification one, so we will allow it.\\nDecision: Free pass for inputs made of constructor-like stuff\\n-------------------------------------------------------------\\nWe will allow matching on axiom patterns made of constructor-like symbols\\nregardless of how the resulting constraint looks.\\nNote that this is actually a subcase of\\n\"Allow matching constraints with functions without axiom variables\". However,\\nin the future we may want to allow only this kind of matching.\\n","Decision":"---------------------------------------\\nLong term, we should add a function simplification attribute, since it's the\\nsafest way to solve the problem.\\nNote that it may make more sense to add a function definition attributes since,\\nin principle, definition axioms are in `O(symbol-count)`, while we could have\\nany number of simplification axioms. However, in practice it would probably\\nbe undesirable to add these attributes to the main semantics (especially\\nto the existing semantics), so we'll use simplification attributes.\\nDecision: No matching constraints with functions over axiom variables\\n---------------------------------------------------------------------\\nConstraints with normal functions (i.e. not constructors) over axiom variables\\nare not easily solvable and almost surely do not come from function definition\\naxioms. Therefore we should not attempt to apply axioms generating these\\nwhen matching since the result will probably be more complex than the input\\nand it does not make sense to apply a simplification axiom in order to get\\nsomething more complex.\\nIf we have these, matching should fail, i.e. it should throw an error.\\nDecision: Allow matching constraints with functions without axiom variables\\n---------------------------------------------------------------------------\\nIf any variables in the result have constructor-like symbols on top of them,\\nthe constraint seems more like a function definition one than like a\\nsimplification one, so we will allow it.\\nDecision: Free pass for inputs made of constructor-like stuff\\n-------------------------------------------------------------\\nWe will allow matching on axiom patterns made of constructor-like symbols\\nregardless of how the resulting constraint looks.\\nNote that this is actually a subcase of\\n\"Allow matching constraints with functions without axiom variables\". However,\\nin the future we may want to allow only this kind of matching.\\n","tokens":406,"id":2779,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kore\/2018-10-24-And-Not-Exists-Simplification.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Problem\/Questions\\n-----------------\\nWe could implement generic handling for simplifying `not` terms based on\\nconstructors, but a `not` would be expanded to an `or` of a very large list\\nof terms, which would need to be simplified. On the other hand, this should\\nsolve the `not` evaluation once and for all.\\nExample:\\nIf `C1(a, b)`, `C2(a)` and `C3(a, b, c)` are the constructors of a certain sort,\\nthen\\n```\\n\u00acC1(phi, psi) =\\nC2(T)\\n\u2228 C3(T, T, T)\\n\u2228 C1(phi, \u00acpsi)\\n\u2228 C1(\u00acphi, T)\\n```\\nWe could also implement specific handling for the configuration left-overs,\\nleaving the generic solution for later. This would be much simpler and faster,\\nbut would not solve the general problem.\\nDecision: Implement what's needed for configuration left-overs\\n--------------------------------------------------------------\\nWe will implement particular handling for now, as described in\\n`docs\/2018-11-08-Configuration-Splitting-Simplification.md`.\\nReasoning\\n---------\\nFor performance reasons, we would need to implement specific handling anyway.\\nThe generic handling part may or may not be needed in the future. It seems\\nthat the only reason for implementing generic handling first is to reduce the\\ndevelopment time in the short term, and that would happen only if we actually\\nneed to solve both in the short term.\\n","Decision":"--------------------------------------------------------------\\nWe will implement particular handling for now, as described in\\n`docs\/2018-11-08-Configuration-Splitting-Simplification.md`.\\nReasoning\\n---------\\nFor performance reasons, we would need to implement specific handling anyway.\\nThe generic handling part may or may not be needed in the future. It seems\\nthat the only reason for implementing generic handling first is to reduce the\\ndevelopment time in the short term, and that would happen only if we actually\\nneed to solve both in the short term.\\n","tokens":331,"id":2781,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"life-dashboard\/20191019 GUI lib.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMust be free! Python GUI libs to make a fancy clean lovely UI.\\n","Decision":"Questioning if it's actually worth setting up a GUI or if the python project should just look at spitting out data to be consumed by some kind of dashboard system like \"thingsBoard\". Arch thinking in next decision record. Postponing until the architectural overview is thought out!\\n","tokens":19,"id":2782,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"life-dashboard\/20191026 arch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNeed to decide on a high level structure for this app to take. It's goals are:\\n- Me to learn\\n- Fun times \/ be aspirational (as a selling point if it ever gets published seriously)\\n- be a potential thing I'll publish\\n- make some elements of life easier \/ be an enabler to life automation.\\n","Decision":"\"Federated syncing\". It's aspirational, one hell of a challenge, awesome for the CV. The main con for this is complexity... but this _is_ a learning project so: bring it on! Next up is to sketch out the layout of the pieces.\\n","tokens":74,"id":2783,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"life-dashboard\/20191017 Language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nShould be a new language to me for learning.\\nShould be an appropriate one for native desktop apps (if it works cross platform that's a bonus!)\\nShould be one that improves future job options \/ pay ()\\n","Decision":"Python. The size of the community (it's _crazy_ popular! That pretty much wins it alone), it works with GIS (Chrissy), ml\/ai potential. May not be as (possibly) future focused with the whole function thing, but that'll likely be decades away and it's still doable with JS and python (I think).\\n","tokens":49,"id":2784,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0004-use-log-framework-for-logging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUse of a logging mechanism other than javascript's console.log should provide more feature in logging.\\n## Decision Drivers\\n*   `console.log` is default for aws lambda and is simple, however have no features other then plot a text to be viewed somewhere\\n*   Startup time and memory consumption are very important factors\\n*   Complete log solutions like `winston` provide a [lot of features](https:\/\/github.com\/winstonjs\/winston)\\n","Decision":"*   `console.log` is default for aws lambda and is simple, however have no features other then plot a text to be viewed somewhere\\n*   Startup time and memory consumption are very important factors\\n*   Complete log solutions like `winston` provide a [lot of features](https:\/\/github.com\/winstonjs\/winston)\\nChosen option as for \"wrapped `console.log` calls\". Since `console.log` is just a simple way to log things, it doesn't meant for more advanced logging purposes. `winston` came to fill this gap with a complete logging solution for nodejs applications. However, in the scope of a lambda function, specially in a AWS environment, the major part of those features will not be used (for example the transports, coloring, advanced templating).\\nIn this sense, wrap a `console.log` with some basic features like leveling, and basic templating can fit better to current context. Only for the sake of *futurism*, the wrapper must be implemented following the same interface of `winston` (eg: `logger.info(...)`, `logger.error(...)`, etc).\\n","tokens":98,"id":2785,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0005-use-of-dynamodb-streams-for-enqueuer.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe [ADR-0003] states that the main purpose of DynamoDB is not hold historical data and must only be used as state holder while the particular schedule is not ready to be in the delayer queue. However, the state will be changed from *NEW* to *PROCESSED* exactly when this schedule is posted in the delayer queue. This proposal aims change this behavior, removing the records that are already in delayer queue instead change it to *PROCESSED*.\\n## Decision Drivers\\n*   Cost: to be able to deal with thousands of schedules, the only way is to increase provisioned capacity of DynamoDB. Do it of hundreds of thousands, or even millions of records is acceptable, but not for few thousands.\\n","Decision":"*   Cost: to be able to deal with thousands of schedules, the only way is to increase provisioned capacity of DynamoDB. Do it of hundreds of thousands, or even millions of records is acceptable, but not for few thousands.\\nChosen option was to move to a solution based on Dynamo Streams. The proposed solution doesn't use TTL, since according [AWS docs](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/howitworks-ttl.html), the deletion of a expired record should occurs within 48 hours - which, for this case, is not acceptable.\\nThe solution is based in the fact that records generates events in DynamoDB that can be consumed by lambda functions. In this sense, when a new schedule is deleted, a DELETE event is raised, and the function will schedule it for the proper pointInTime. In other hand, when a schedule is placed within the delayer period (the timeframe that events are waiting in the delayer queue), this function deletes this record which generates another DELETE that will be handled. If the schedule is not in the delayer period, it will be stored until the *task-1minute-enqueuer* finds it and then deletes the record.\\nHey, but why *api-schedule-post* don't simply put this in que delayer queue directly?\\nThe short answer is: because each lambda should do 0, 1 or tops 2 data transformation.\\nEach lambda should be very restrict on what it does. *api-schedule-post* is the function responsible to handle the POST request from API Gateway. Handle and transform the data and store it somewhere is the \"reason to exists\" of this function. How this information is processed later, is not it's concern.\\nHave in mind that in this stage of development, keep the integrity of the design is more important that this fine performance tunning.\\nBelow, the new roles of the involved lambdas:\\n*   *task-1minute-enqueuer* - this function still be triggered by the cloudwatch events and simply deletes the records that entered in the delayer period.\\n*   *stream-dynamodb-fasttrack_enqueuer* - send the records that are raised in DELETE event to the SQS queue. If the event is an INSERT and it will be in the delayer period, it deletes the record.\\nPositive Consequences:\\n*   Keeps the entire architecture design more *push based*, which is very important for a serverless architecture style.\\n*   Keeps the integrity of the flow: only 1 lambda sends records do sqs, only 1 lambda inserts records in the dynamodb, the event the pushes records to sqs is always the same.\\nNegative consequences:\\n*   Still keeps the increased writes in DynamoDB (each delete in Dynamo counts as a write operation)\\n","tokens":159,"id":2786,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0006-use-build-and-code-coverage-tools.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUse build and code coverage tools to automate continuous integrated tests.\\n## Decision Drivers\\n*   Test automation\\n*   Build automation\\n*   Display badges in project repository - brings more\\n","Decision":"*   Test automation\\n*   Build automation\\n*   Display badges in project repository - brings more\\n(( to be taken ))\\nPositive Consequences:\\n*\\nNegative consequences:\\n*\\n","tokens":44,"id":2787,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0001-select-proper-name-for-the-project.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nEven this is not essentially a technical decision, I believe that this \"simple\"\\nname probably will drive some other important decisions, like the creation of\\nterminologies, naming packages and other stuff, helping defining\\nfunctionalities, etc. According to [Martin Fowler](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html)\\nnaming things figures out as one of the hardest things in computer science.\\nApart of a joke, I believe that choose a good name is a good success factor,\\nbut this is not a technical decision, which can make things harder.\\n## Decision Drivers\\n*   Consider the serverless nature of the project\\n*   Consider the simplicity\\n*   Consider the vendor specific nature\\n*   Consider the fact that this could not be a scheduler\\n","Decision":"*   Consider the serverless nature of the project\\n*   Consider the simplicity\\n*   Consider the vendor specific nature\\n*   Consider the fact that this could not be a scheduler\\nChosen option: \"delayer-aws\".\\nAll the conception and idea behind of this project was made over\\nthe \"scheduler\" word, which sounds natural, since the main purpose of the project is to *schedule* a\\ntask to run in the future. However, the term \"scheduler\" reemsemble a lot of things that already are\\nin place, like *recurrence* e *orchestration*. The main objective of this project is to provide a way\\nto easily and cost-effectively execute tasks in future. Easily because it will be based in a Restful\\nAPI to deal with tasks (in face of config files or configuration screen), and cost-effective by using\\nserveless architectural approach. This is very different of current set of tooling, which could be\\ncalled \"schedulers\" too (and actually are!), but do a LOT of thing other then this, like but not\\nlimited to, recurrence and orchestration.\\nIn summary, I would like to think that people would find here more a \"system-to-system-todo-list\"\\nthan in a complete \"scheduler\". That's why I think that \"delayer\" reflects much more what it will do\\nthen \"scheduler\".\\nThe suffix \"-aws\" aims to:\\n-   confirm that it is not multicloud\\n-   confirm that there are plans to build this for other cloud providers\\n### Positive Consequences\\n*   clearly defines what this project do, and what it will do for the next iteractions\\n### Negative consequences\\n*   not using \"scheduler\" in the name can reduce the comprehensiveness about the project\\n","tokens":169,"id":2788,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0002-Recursive-lambda-or-triggered-lambda.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n~~In the WARM stage~~ When a schedule is in the delayer queue, messages become visible after delayed period. However, those messages must be published in corresponding *action topic*, and it cannot be done automatically, being needed to use a lambda function to do this publishing.\\n## Decision Drivers\\n*   This project is a scheduler, so the latency is an important concern\\n*   Flexibility: ability to be good enough in high or low concurrent executions\\n","Decision":"*   This project is a scheduler, so the latency is an important concern\\n*   Flexibility: ability to be good enough in high or low concurrent executions\\nChosen option was to \"Cloudwatch-triggered lambda\".\\nChaining lambda functions is a good solution when the volume of messages is flat or well known. In a scenario of a variable\/high-volume of messages, the function would introduce some latency, due to the lack of paralelism of execution. To achieve paralelism, the solution would become much more complicated.\\nThe chosen solution introduces the folowing concepts:\\n-   lambda function timeout: configured time on when a function will be forcibly stopped by it's container\\n-   lambda execution baseline: amount of time that a lambda function should be ideally working\\n-   cloudwatch event timer: amount of time on cloudwatch event will call lambda function\\nThe execution baseline and the event timer should have the same values, and timeout should be greater. Function starts executing and during the execution baseline, it waits for messages. If no messages arrives when this time finishes, the function stops itself. However, if there's more messages waiting in the queue, the function keeps consuming messages until 1. there's no more messages, 2. reach the function timeout. Whilst, the function will keep running and as new function are spawned by CW events, those function will run in parallel.\\n```\\n|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----| <- CW events\\n|-----------------------------| <- function timeout\\n|-----| <- function baseline\\n```\\n**Example of how it would work**\\n```\\n|-----| <- function - instance 1\\n|-----| <- function - instance 2\\n|-----| <- function - instance 3\\n************************ received a bunch of messages - parallel\\n|-----------------------------| <- function - instance 4\\n|-----------------------------| <- function - instance 5\\n|-----------------------------| <- function - instance 6\\n|-----------------------------| <- function - instance 7\\n```\\nPositive Consequences:\\n*   Enable parallelism and, for instance, reduces latency. This is very important for a scheduler\\nNegative consequences:\\n*   More effort in develop and maintain this lambda function\\n","tokens":100,"id":2789,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0007-schedule-internal-data-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nDefine a default data structure for the schedule information within interal structures of delayer-aws. The outside schedule model was already defined.\\n## Decision Drivers\\n*   Structure is an internal representation, and format should be concerned with it's interal functionalityes\\n*   Could be used in future event dispatchers\\n","Decision":"*   Structure is an internal representation, and format should be concerned with it's interal functionalityes\\n*   Could be used in future event dispatchers\\nThe json structure of the schedule will be:\\n```javascript\\n{\\nscheduleId: \"484a127d-6ce3-11e8-8fdd-fb8b5aab61ea\",\\npointInTime: 1528662600,\\napiKey: \"nVMIr6J5Do1qutXsZt1dhaaJfjyq4YgzThPQWkQ1\",\\ntopicArn: \"arn:aws:sns:us-east-1:472249637553:http-request_action-topic\",\\nactionConfig: '{ \"method\" : \"GET\", \"url\" : \"https:\/\/google.com\/\" }',\\ncontext: '{ \"headers\" : [], \"payload\": \"\" }'\\n}\\n```\\nWhere:\\n*   `scheduleId`: unique identifier of the schedule - generated\\n*   `pointInTime`: unix timestamp created in UTC basis to the exact time of the execution of the schedule - informed by the client in request body\\n*   `apiKey`: if informed, the client's api key - informed by the client in headers\\n*   `topicArn`: the Amazon Resource identifier of the action topic on where the schedule will be published. If only the topic name was provided, the current account and the region of the *api-schedule-post* currently in execution will be used to build the arn.\\n*   `actionConfig`: config object provided by the user based on the type of the action\\n*   `context`: the data that will be used as context in the actions\\n","tokens":70,"id":2790,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"govuk-kubernetes-discovery\/0006-exposing-kubernetes-services.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nKubernetes services can be exposed to the public using Kubernetes managed resources\\nthat can integrate with Cloud environments, such as the Loadbalancer service resource,\\nor with Cloud native services, such as AWS ELBs.\\nWhich solution to use will affect how we manage associated resources and processes, for\\ninstance EIPs and DNSs, HTTPS certificates, green-blue deployments, etc.\\nInitially we want to find a solution that meets some basic requirements:\\n- HTTP and HTTPS support\\n- Internal-only and Internet facing services\\n- Probably we want to route traffic to services based on request URI\\n","Decision":"Initially we tested a Loadbalancer Kubernetes service on AWS. This service configures a\\nclassic ELB that connects with the pod service port. It requires uses Kubernetes secrets\\nto read the HTTPS certificate but doesn't support request URI routing. It also needs a\\nfixed public IP to start the ELB and a DNS entry managed externally.\\nKubernetes Ingress resources allow us to define rules to route traffic to Kubernetes\\nservices. Google Cloud has native support for Ingress resources, but on AWS we need to deploy\\na controller service. On non-cloud environments (for instance, a local Minikube VM),\\nit is also possible to deploy a Ingress controller that can replicate the behaviour\\nexpected in the cloud. The catalogue of Ingress controllers can be found here:\\nhttps:\/\/github.com\/kubernetes\/ingress\/blob\/master\/docs\/catalog.md\\nWe tested Traefik and CoreOS ALB Ingress controller and finally decided to use the CoreOS\\nsolution:\\n- Traefik feedback?\\n- CoreOS ALB ingress controller https:\/\/github.com\/coreos\/alb-ingress-controller\\n- AWS managed certificates, with AWS Certificate Manager\\n- Uses AWS ALB loadbalancers\\n- Adds Route53 entries\\n- Can optionally access services only on specific namespaces\\nWe decided to use the CoreOS ALB controller because it uses AWS native solutions that meet our requirements.\\nAlso, it's supported by CoreOS.\\nOther links and discussions about loadbalancer support for Kubernetes on cloud platforms:\\n- http:\/\/www.sandtable.com\/a-single-aws-elastic-load-balancer-for-several-kubernetes-services-using-kubernetes-ingress\/\\n- Kubernetes discussion around Ingress\/AWS LBs support: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/30518\\n- Internal ELB, useful for services that don't need to be open to the Internet: https:\/\/github.com\/kubernetes\/kops\/issues\/2011\\n- ELB and HTTPS: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/22854\\n- Traefik working branch: https:\/\/github.com\/alphagov\/govuk-hosting-discovery\/compare\/traefik-ingress\\n","tokens":122,"id":2792,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"govuk-kubernetes-discovery\/0004-structure-for-terraform-projects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe wanted to agree on our Terraform code organisation to manage resources in different stacks and\\navoid having to recreate things every time we refactor code.\\n","Decision":"- We want to separate code from data, so in the future we can opensource the code without disclosing our implementation details\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects (stacks, tiers), each one representing a logical tier. This is specially\\nimportant to separate resources between GOV.UK applications.\\nThe initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks\/tiers\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 google\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 container_cluster\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dns_managed_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 network.tf\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 public_subnetwork\\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 main.tf\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n","tokens":33,"id":2794,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"govuk-kubernetes-discovery\/0003-using-templates-for-manifests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nKubernetes requires someone to write very verbose manifest files in YAML. Templating\\ncan allow someone to separate the manifest and data and allow a tool to generate\\nand populate manifest files with ease.\\nIt allows easily understanding the values of a specific application.\\nIt requires us to use a tool on top of plainly writing manifests ourselves.\\n","Decision":"We will not use templates for the time-being, but will re-evaluate our use of\\npopulating common values in the future.\\n","tokens":72,"id":2795,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"govuk-kubernetes-discovery\/0002-structure-for-kubernetes-apps.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe wanted to agree a structure for Kubernetes applications so that we could create\\nconsistent layouts when creating new applications.\\nThis structure should be iterated upon when we start to work more heavily with\\nmigrating apps over to Kubernetes.\\nWe need to also be aware that developers may want to create their own applications\\nand so must be easily understood.\\nWe want to split open and private repositories easily when we come to start publishing\\nour work.\\n","Decision":"We have agreed on the following tree:\\n```\\n\u251c\u2500\u2500 data\\n\u2502   \u2514\u2500\u2500 my-awesome-app\\n\u2502   |   \u251c\u2500\u2500 development_secrets.yaml\\n|   |   \u2514\u2500\u2500 development_configmaps.yaml\\n|   \u2514\u2500\u2500 global\\n|       \u2514\u2500\u2500 development_configmaps.yaml\\n\u2514\u2500\u2500 manifests\\n\u2514\u2500\u2500 my-awesome-app\\n\u251c\u2500\u2500 deployment.yaml\\n\u2514\u2500\u2500 service.yaml\\n```\\nThis allows us to split the manifests section of Kubernetes configuration from the\\ndata values that get created and mounted into any deployments.\\n","tokens":95,"id":2796,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"govuk-kubernetes-discovery\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2797,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gorouter\/0002-change-tls-route-pruning-behavior.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis is related to story [#158847588](https:\/\/www.pivotaltracker.com\/story\/show\/158847588)\\nPrior to the story above, when route-integrity was turned on (soon to be the\\ndefault) we did not prune routes that received most [retriable]() errors. The\\ncode ensures that there are only two types of errors that [result in a\\nprune](https:\/\/github.com\/cloudfoundry\/gorouter\/blob\/b63e6fb16c2a422ec5108a19debc9adb81f2d1dd\/route\/pool.go#L369-L372):\\n[Hostname Mismatch and Attempted TLS with Non TLS\\nBackend](https:\/\/github.com\/cloudfoundry\/gorouter\/blob\/b63e6fb16c2a422ec5108a19debc9adb81f2d1dd\/proxy\/fails\/classifier_group.go#L17-L20)\\nThe prune operation should have little user impact - the route will get added\\nagain the next time the route-registrar runs if the application is still\\nrunning.\\n","Decision":"We will prune any TLS route that has had a failure immediately. Consequently,\\nwe are immediately pruning on more errors, such that the final list includes the\\nfollowing errors: AttemptedTLSWithNonTLSBackend, Dial, RemoteFailedCertCheck,\\nRemoteHandshakeFailure, HostnameMismatch, UntrustedCert\\nWe will also add logging to the cases when an endpoint is pruned.\\n","tokens":237,"id":2798,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gorouter\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2799,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"akvo-product-design\/ADR-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a framework that serve as base for the front-end development of\\nAkvo DASH. There are several options on the market, some of them are\\nJavaScript based others languages that compile to JavaScript\\n(transpilers) like ClojureScript.\\n","Decision":"After discussing with the team and taking into account the skill set at\\nhand, we have decided that JavaScript and\\n[React](https:\/\/facebook.github.io\/react\/) it's a safer approach to\\nbuild the UI. It has a large community and we can use some available\\ncomponents as the base of our UI.\\n","tokens":55,"id":2800,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"akvo-product-design\/ADR-006.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, when user requests data point sync from either DataPointsMapFragment or SurveyedLocaleListFragment and a service,\\nSurveyedDataPointSyncService is started and for each batch of datapoints both fragments get notified of new data available via local Broadcast which both subscribe too. While data is being loaded a non-blocking \"loading\" ui is shown. Once the loading is finished\\nthat loading needs to be dismissed. We also display a snackBar with the number of updated datapoints or some eventual errors.\\n","Decision":"The current implementation is not ideal since if user leaves the fragments view, the service will still send the broadcast even\\nif there is no ui waiting for it. Both fragments have to subscribe to the broadcast even if only one of them that will display\\nthe error\/success snackBars. It would be much better to have the ui subscribe to changes in the database data instead of broarcasts.\\nThat way the service does not need to notify ui of new data being available instead when the service inserts data into the database\\nthe ui will automatically be notified. RX Java can be used for this, an example is this library https:\/\/github.com\/square\/sqlbrite.\\nThis task also requires the refactor of this feature (independently from the rest of the application). We will separate the ui and\\nview (fragment and presenter) logic from domain logic (business logic, use cases) and the data logic (data sources sych as database\\nand network). This separation of concerns will allow us to add unit tests to separate components which will have a Single\\nResponsability.\\n","tokens":106,"id":2801,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"akvo-product-design\/ADR-003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# Context\\nAkvo DASH is a system that allows the user to connect to different _data\\nsources_ and import the data. After that user must be able to transform\\nit (clean, aggregate, etc).\\nWe have been looking at different open source ETL frameworks like:\\n* [Pentaho Data\\nIntegration](http:\/\/www.pentaho.com\/product\/data-integration)\\n* [Clover ETL](http:\/\/www.cloveretl.com\/)\\n* [Onyx Platform](http:\/\/www.onyxplatform.org\/about.html)\\n* [Bubbles](http:\/\/bubbles.databrewery.org\/)\\n* [Kiba ETL](http:\/\/www.kiba-etl.org\/)\\nSome of them provide a GUI to build transformations, others require\\ncoding.\\nThere are other ones that are based on Hadoop ecosystem, which is really\\ntoo much for our current needs:\\n* [Luigi](https:\/\/luigi.readthedocs.org\/en\/stable\/)\\n* [Oozie](https:\/\/oozie.apache.org\/)\\n* [Azkaban](https:\/\/azkaban.github.io\/)\\n# Decision\\nBased on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https:\/\/github.com\/michaelklishin\/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https:\/\/gist.github.com\/iperdomo\/7af984b9f32c117678de) with Onyx author\\n","Decision":"Based on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https:\/\/github.com\/michaelklishin\/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https:\/\/gist.github.com\/iperdomo\/7af984b9f32c117678de) with Onyx author\\n","tokens":418,"id":2803,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"akvo-product-design\/ADR-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# Context\\nWe need to store the imported data from the _Connect_ process in an\\nhomogeneous way for easy retrieval.\\n# Decision\\nAfter checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nAccepted\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n","Decision":"After checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nAccepted\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n","tokens":161,"id":2804,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"profiles\/0002-store-oauth2-clients-in-json.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProfiles acts as a proxy for ORCID's OAuth2 service, and has its own list of (eLife) clients, eg Journal. This list is small and reasonably static.\\n","Decision":"The list of clients will be maintained in a JSON file.\\n","tokens":40,"id":2805,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"profiles\/0001-proxy-orcids-oauth2-service.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to allow applications (eg Journal) to use ORCID's OAuth2 service. We will need to combine data provided by ORCID with other sources in the future.\\n","Decision":"We will proxy the ORCID OAuth2 service, which allows Profiles to see the access token and use ORCID's API.\\n","tokens":39,"id":2806,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"prosemirror-elements\/000-element-fields-model.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt the Guardian, elements currently exist as block-level elements within a document. They manage an arbitrary amount of state that ranges from relatively simple (e.g. pullquote \u2013 one non-rich text field, a few supporting properties) to more complex (e.g. image \u2013 many rich text fields, communication with outside code for [Grid](https:\/\/github.com\/guardian\/grid) integration, etc.)\\n![example pullquote](.\/images\/example-pullquote.png)\\n![example element](.\/images\/example-element.png)\\nElements are responsible for managing their own state and update their parent ProseMirror instance when changes occur. At the moment, elements model their state as fields within the attributes on their Prosemirror node. These attributes can contain arbitrary data \u2013 but they\u2019re flat properties of that node, and we can only update these properties by replacing their parent object wholesale.\\nThis works fine for most cases in a single editor, but becomes more problematic when collaborative editing is introduced. Because we replace node properties wholesale, any edits made to these elements in a collaborative context will be last-write-wins. In addition, we won\u2019t be able to see cursor positions whilst other users are editing elements, and we won\u2019t be able to see any decorations that the parent node might want to add (like noting, for example, or [Typerighter](https:\/\/github.com\/guardian\/typerighter) annotations).\\nTo enable these things, we\u2019ll need to model element properties in a way that more closely matches the document structure \u2013 treating, for example, rich text fields as document nodes, rather than flat attributes.\\n","Decision":"We\u2019d like to pursue options 3 or 4. Which we choose depends on how impactful we think last-write-wins is for non-text attributes, and the complexity of the implementation it requires \u2013 but option 4 is likely to have the least confusing behaviour for users.\\n","tokens":332,"id":2807,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"prosemirror-elements\/001-transformers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe type of data representing elements in consumers may differ from the type of data native to prosemirror-elements elements. This is the case at the Guardian, where our document format (flexible-model) has an element definition that doesn\u2019t correspond with the elements we define with prosemirror-elements.\\nThis is always likely to be the case \u2013 there\u2019s usually a difference between data as it\u2019s represented in a model, and data represented in forms designed to feed that model.\\nAs a result, currently prosemirror-elements has a `transformIn` and `transformOut` API, that allows consumers to include the transformation between external and internal types in their element.\\nAs we worked with this API, a few concerns were raised:\\n- It makes our types harder to reason about, both in terms of writing them, and in terms of reasoning about them as a consumer when things go wrong.\\n- Reasoning about partial representations of external data, and hydrating to their complete or partial internal counterparts felt complex.\\n- Coupling the definition of our element with a particular set of data requirements for a particular consumer felt arbitrary.\\nUltimately, these concerns led us to explore alternatives to this approach.\\n","Decision":"We decided to remove the transformers API, and make it the consumer\u2019s responsibility to do this work.\\n","tokens":246,"id":2808,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ski-resort-manager\/0002-use-modular-monolith-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis application will have severals functionnalities on severals domains. They must be independent to increase maintainability.\\n","Decision":"I decided to choose the modular monolith for some reasons:\\n- The time to develop is shorter than microservices\\n- It can evolve to microservices in the future\\n- The [Modular monolith DDD](https:\/\/github.com\/kgrzybek\/modular-monolith-with-ddd) project is a great inspiration for this approach\\n","tokens":27,"id":2809,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ski-resort-manager\/0003-use-identity-server.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis application need a user authentication and a fine granularity for roles.\\n","Decision":"I decided to choose IdentityServer 4 because it is a mature, use by many people and well documented project.\\nIt implement OpenID connect and support external authentication (Google, Facebook, ...)\\n","tokens":17,"id":2810,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ski-resort-manager\/0001-record-architecture_decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis project is design to explore architecture, so I have to record my decisions.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":20,"id":2811,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ski-resort-manager\/0004-backend-for-frontend.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis application will have 2 front end and possibly a mobile app.\\nThese differents access will use common and specifics API. Somes datas will be use differently.\\n","Decision":"Every front end will have a specific backend to manage the differents needs.\\n","tokens":38,"id":2812,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"operability\/001-container-deployment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nWe've decided to use [Hashicorp's Waypoint](https:\/\/www.waypointproject.io\/) to handle the build\/deployment task for us. It is a pretty new tool (released for public in 2020), but Hashicorp made some of the most used and stable tools in the market (like Terraform and Vault).\\nWaypoint can handle the container build process, deployment, and release. Since we're using a Gitops process, it still lacks some plugins for our deployment\/release steps, but using it only to build even helps us make the deployment process more accessible.\\nWe've tried this tool in our [Design System application](https:\/\/github.com\/budproj\/design-system). It was straightforward to create containers for our storybooks (the front-end of this application).\\nThe next step is to create a Gitops plugin that can deploy the updated image manifest in our Argo Git repository or find a way to make Argo receive information regarding Waypoint deployment and release.\\n","tokens":45,"id":2813,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bananatabs\/0003-testing-model-in-dom-integration-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nWhen it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.  It could be more sensible to only verify the state\/model.\\nBut, we had a small problem described in ADR-0002.\\n","Decision":"We will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly.\\n","tokens":126,"id":2814,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bananatabs\/0005-using-environment-variable-instead-of-config-singleton.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe had a `config.ts` that exported a single `Config` object with a single `debug` boolean property.\\nThis was being used in DebugUtils.tsx to control whether or not to render some extra debug information.\\nOne problem was that in order to activate this we had to change the value in a file that was under version control, and then remember to revert it and never to commit it with `debug: true`.\\n","Decision":"We removed the `config.ts` file and used an environment variable instead. This environment variable is set is set when running a new script defined in `package.json`.\\n","tokens":93,"id":2815,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bananatabs\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nadr new Implement as Unix shell scripts\\nThis will create a new, numbered ADR file and open it in your\\neditor of choice (as specified by the VISUAL or EDITOR environment\\nvariable).\\nTo create a new ADR that supercedes a previous one (ADR 9, for example),\\nuse the -s option.\\nadr new -s 9 Use Rust for performance-critical functionality\\nThis will create a new ADR file that is flagged as superceding\\nADR 9, and changes the status of ADR 9 to indicate that it is\\nsuperceded by the new ADR.  It then opens the new ADR in your\\neditor of choice.\\n3. For further information, use the built in help:\\nadr help\\n","tokens":16,"id":2817,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bananatabs\/0004-using-react-context.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# 4. Using React Context\\nDate: 2019-07-01\\n## Context\\nThere was a lot of prop-drilling in the component tree.\\n### Old Render Tree -- A lot of prop-drilling\\n```\\n<App\/>\\n<BananaTabs \/>\\n<MainView \/> (\u2705session, \u274csessionMutator, \u274cwindowMutator, \u274ctabMutator, \u2705browserController)\\n<Title \/>\\n<WindowListView \/>          (\u2705windows, \u2705sessionMutator, \u274cwindowMutator, \u274ctabMutator)\\n<WindowView \/>            (\u2705window, \u274cwindowMutator, \u274ctabMutator)\\n<WindowHeader \/>        (\ufe0f\ufe0f\u26a0\ufe0fwindow\ufe0f, \u2705windowMutator, \u274ctabMutator)\\n<DisclosureButton \/>  (\u2705window, \u2705windowMutator)\\n<VisibilityIcon \/>    (\u2705window, \u2705windowMutator, \u2705tabMutator)\\n<WindowTitle \/>       (\u2705window, \u2705windowMutator)\\n<TabList \/>             (\u2705window, \u274ctabMutator)\\n<TabView \/>           (\u2705window, \u2705tab, \u2705tabMutator)\\n<TabToolsView \/>\\n<MainViewCmdButtons \/> (none)\\nLegend:\\n\u2705prop: actually used by component\\n\u26a0\ufe0fprop: only reading id (e.g. window.id, tab.id)\\n\u274cprop: only passing it down to children\\n```\\n","Decision":"I'm going to try to use React Context with the `useContext` hook to see if I can reduce or eliminate prop-drilling.\\n","tokens":332,"id":2818,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0004-use-spring-boot.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe choice of using spring-boot is not related to any specific purpose. It just happened to be lying around.\\n","Decision":"OpenCHS server will use Spring boot\\n","tokens":26,"id":2819,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0013-handling-change-of-catchments-in-the-client.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA change in catchment in a device can be caused by two reasons.\\n1. The user-catchment assignment has changed in the server\\n2. A new user has logged in.\\nIn both these conditions,\\n- It is possible that there is unsynced data in the device\\n- The new user might not have control over old data\\nA new catchment means that the last update date times are wrong, and the data is also wrong on the device, so this means a full sync from the server. We should do this, but not allow existing unsynced data to be wiped out.\\nAlso note that the catchment an individual belongs to is set when he\/she is created. Therefore, a sync of data will not affect any unsynced data.\\n","Decision":"When a change in catchment is detected by openchs-client, existing unsynced data will be synced to the server, all data deleted, and a fresh sync started.\\n","tokens":163,"id":2820,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0008-use-monorepo-for-client.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDevelopment in health-modules often requires tandem work in openchs-android as well. It is much easier to have them in a single repo for ease of development. The repos will be set up using Lerna.\\n","Decision":"All client side libraries will be set up as a monorepo using Lerna.\\n","tokens":45,"id":2821,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0007-separate-models-and-health-modules-from-the-android-application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn the future, once all product features are built, there will be a lot of work on building and enhancing health modules. This might eventually also become its own project.\\nThere needs to be a contract that a health module exposes. We will have interfaces defined in the health module, and communication using the models (which will be common across openchs-android and openchs-health-modules). This will allow common domain logic to live in modules and health module specific business logic to live in the models.\\n","Decision":"The android client will be broken down into 3 components - openchs-android for the app, openchs-health-modules for the health modules and openchs-models for the contract objects that will be exchanged by openchs-android to openchs-health-modules.\\n","tokens":103,"id":2822,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome rules regarding usage of health-module apis.\\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0002-openchs-client-will-be-offline-first.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOpenCHS client will be used in places of low or no connectivity. This means the application should be usable at any point in time without internet. However, data needs to be pushed to a central server for reporting, as well as for backup. This means no functionality other than sync to server should require connectivity to the server.\\n","Decision":"OpenCHS client should be usable offline.\\n","tokens":70,"id":2825,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0003-use-react-native.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen we started OpenCHS, we wanted to choose between native development and React Native. The reasons to choose RN are\\n- React-Native is mature\\n- It has a comparatively lower learning curve for the team\\n- Development cycles are faster compared to native\\n","Decision":"OpenCHS client will use react-native.\\n","tokens":56,"id":2826,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0011-moving-forward-forms-api-will-not-be-used-to-create-update-concepts.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nConcepts can be created\/updated using Concepts API or along with FormElement through Forms API.\\nThis doubles the test cases and increases maintanence.\\nWe consistently saw bugs in Forms API when creating or updating coded concepts along with it.\\n","Decision":"We will create Concepts through Concepts API. Forms API will refer concepts.\\n","tokens":53,"id":2827,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2830,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openchs-adr\/0010-all-implementation-rules-will-temporarily-stay-in-the-main-health-modules-repository.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe currently don't have a good solution of having the code for implementation specific rules separate. The current solution is to add them into the health-modules repository, and switch them on or off through switches in the server.\\nWe will eventually move away from the solution, but until then, this stays.\\n","Decision":"Implementations specific rules to stay in the health-modules repository with switching on or off of rules provided by the server\\n","tokens":64,"id":2831,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kafkarator\/0006-future-adrs-in-pig-repository.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nADRs are important documents detailing how we do things in our domain.\\nOften an ADR will point to a new feature or even an entirely new service to create.\\nWhen an ADR initiates the creation of a new service, where does the ADR belong?\\nIn the new repo, or in the repo where the idea originated?\\nWould it simply be better to collect ADRs in a central location?\\n","Decision":"When writing new ADRs, they will be in the [PIG repo](https:\/\/github.com\/navikt\/pig).\\n","tokens":90,"id":2832,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kafkarator\/0002-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":2835,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kafkarator\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2836,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"marsha\/0004-soft-deletion.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have users and objects, and everything is tied together. Deleting\\nsomething may cause some problems, like deleting other things in\\ncascade, or losing some relatioship, like not knowing who is the author\\nof a video.\\n","Decision":"We don't want things to be deleted, instead we'll keep them in the\\ndatabase in a \"deleted\" state, so that they won't show up anywhere.\\nLooking at the [Safe\/Soft\/Logical deletion\/trashing and\\nrestoration\/undeletion](https:\/\/djangopackages.org\/grids\/g\/deletion\/)\\npage on djangopackages, we can make a choice with these constraints:\\n-   support for python 3 and django 2\\n-   simple, not over-featured\\n-   can manage relationships\\n-   supports the django admin\\n-   is maintained\\nRegarding this, we choose\\n[django-safedelete](https:\/\/django-safedelete.readthedocs.io\/en\/latest\/)\\nwhich proposes many options to handle deletion, and so will fit our\\nneeds.\\n","tokens":52,"id":2842,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"marsha\/0002-videos-languages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"marsha\/0003-content-organization-and-accesses.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have actors &lt;0001-actors&gt;. And\\nvideos &lt;0002-videos-languages&gt;, with their auxiliary files, that\\nwe'll call for now \"content\". We want this content to be organized for\\nactors to manage\/view them.\\n","Decision":"Videos are grouped in playlists, which is a Django model named\\n`Playlist`. A playlist belongs to an organization (`Organization` model\\ndefined in actors &lt;0001-actors&gt;) and is created by someone who can\\nbe a manager of this organization, or an author who belongs to this\\norganization. This link is materialized by an `author` field on the\\n`Playlist` model, a `ForeignKey` to the `User` model.\\nThe manager can allow many actors to manage a playlist, so there is a\\n`ManyToMany` field on the `Playlist` model, named `editors`, pointing to\\nthe `User` model. And instead of relying on the hidden model created by\\nDjango when creating a `ManyToMany`, we'll define this model and use it\\nvia the `through` argument, to be able to add more rights constraints\\nlater if needed.\\nThe author of the playlist is automatically added to this list of\\neditors. And can be removed from it by a manager, still staying marked\\nas the author, but being the author itself doesn't involve any rights.\\nA playlist can be duplicated by a manager, and if it stays in the same\\norganization, the manager can clear or keep the list of editors. If it\\nis to be duplicated in another organization, the list of editors will be\\ncleared of actors not belonging to the new organization, and the manager\\nwill still be able to clear it all or keep the remaining editors.\\nWhen duplicated, a new instance of `Playlist` is created, with a link to\\nthe original playlist, keeping the author. We do the same for each\\ninstances of the `Video` linked to this playlist, but we will still\\npoint to the same files (videos\/audios\/subtitles...) on the hosting\\nprovider, to keep cost manageable.\\nAnd finally, there is a flag named `is_public` on the playlist, that can\\nbe toggled by a manager, to tell if the playlist can be viewed by anyone\\nor only by people who were granted access to it. This kind of access is\\nnot in the scope of this document.\\n","tokens":60,"id":2844,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"marsha\/0001-actors.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are different kinds of actors that need to interact with Marsha.\\nFirst we have the people managing a Marsha instance.\\nThen we have people linking their own website (this website is a\\n\"consumer site\") to a Marsha instance, to host videos.\\nThese consumer sites can host many publishers. We call these\\n\"organizations\". And these organizations have managers that can\\nadministrate some things about authors, video sharing between authors...\\nNext we have the video authors, belonging to the organizations.\\nAnd finally we have the users coming to watch videos.\\n","Decision":"Let's separate those 5 actors \/ roles:\\n### \"staff\"\\n#### Purpose\\nTo manage a Marsha instance\\n#### Implementation\\nThese are simply instances of the Django `User` model, with the flag\\n`is_staff` set to `True`.\\n### \"admins\"\\n#### Purpose\\nTo manage the link between a consumer site and a Marsha instance, and\\nthe organizations allowed to access this consumer site on the instance.\\n#### Implementation\\nTo represent a consumer site on a Marsha instance, we have a\\n`ConsumerSite` Django model. With a `ManyToMany` link to the `User`\\nmodel, named `admins` (not a single admin, to avoid having no admin if\\nthe only one existing is not available anymore)).\\n### \"managers\"\\n#### Purpose\\nTo manage the authors in the organization (an organization could be\\npresent on many consumer sites). To allow videos to be private to\\nauthors or public for all authors. And create courses.\\n#### Implementation\\nTo represent an organization on a Marsha instance, we have an\\n`Organization` Django model. With a `ManyToMany` link to the `User`\\nmodel, named `managers`.\\n### \"authors\"\\n#### Purpose\\nTo post videos on a Marsha instance to be used on a consumer site.\\n#### Implementation\\nAn author is simply an instance of the `User` model, but has a link to\\nan `Organization` via a `ManyToMany` link, named `organizations` (we can\\nimagine an author working for many organizations).\\n### \"viewers\"\\n#### Purpose\\nTo watch videos hosted on a Marsha instance.\\n#### Implementation\\nFor the viewers we don't need to save anything in the database, so there\\nis no instances of the `User` Django model for them.\\nEach time a user does an action to view a video, they access it via a\\nurl containing a unique token, with a limited life span. It's this token\\nthat grant them access to the video.\\nThis is not the scope of this document to address token generations.\\nTo store the user preferences regarding languages, video resolution,\\netc, it can simply be done via a cookie.\\n","tokens":120,"id":2845,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pupperware-party\/0002-use-semver-for-versioning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHow software is versioned conveys a lot of meaning to the end user. If you have a consistent, known way of versioning softeare, the end users can plan for upgrades and have a better overall expereince, and not be suprised by breaking changes.\\n","Decision":"Use [SemVer 2.0.0](https:\/\/semver.org\/spec\/v2.0.0.html) for the guidelines of version numbers for this project\\n","tokens":61,"id":2846,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pupperware-party\/0006-installation-and-config-file-locations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSoftware needs to be installed in order to be useful, and also normally needs config files. Where to put these and where to install to can have an impact on user experience.\\n","Decision":"We will follow the example set by Puppet, since this software is part of that ecosystem\\n","tokens":39,"id":2847,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pupperware-party\/0003-use-readme-driven-development.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDocumentation for a project is critical - without it, you don't know if a project is \"complete\", or as a user, how to use it.  However, too much documentaion can delay the project, or lead to a good implementation of the wrong thing. This process, and the reasoning is much better explained at [this blog post](http:\/\/tom.preston-werner.com\/2010\/08\/23\/readme-driven-development.html)\\n","Decision":"This project will be developed using the readme driven decelopment process.\\n","tokens":97,"id":2848,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pupperware-party\/0004-project-license-scheme.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order for a project to be useful, it has to be shared with others. In order for it to be shared effectively, there needs to be a shared expectation of what can be done with it, how to make changes, whether or not changes can be made, and what if any warranty or suitablity for use claims there may be.\\nThe license dictates how other people can use the software, and will set the tone for the community - making it easier or harder for individuals and organziations to use the software and contribute back to it.\\nIn order to foster an active community, it is important to pick the correct license.\\n","Decision":"The decision was made to use the Apache License version 2.0\\n","tokens":133,"id":2849,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pupperware-party\/0005-participate-in-puppet-contributor-summit.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFor 2018, Puppet has changed the format of their annual conference, doing what they are calling Puppetize Live instead of PuppetConf.  This also brings about a change to their contributor summit.  Instead of a single day event on site before the conference, it is now a month long virtual event.\\nThis decision does not directly impact the design, other than neccessitating the creation of a charter document and providing some pre-determined milestones. It does, however, provide some additional developmnet and design resources, as well as some tme constratins to help with motivation.\\n","Decision":"I have decided to enter this project into the Puppet Contributor Summit online event for 2018.\\n","tokens":124,"id":2850,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pupperware-party\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2851,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-with-a-gp-beta-web\/0004-cookies-encryption.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n### Protect Personal Data at Rest\\nIf a users' device gets compromised then the attacker will not be able to see\\nthe values they've submitted to us.\\n### Prevent Session Tampering\\nWe don't want to open an attack vector where people can fuzz values that go\\ninto our system so we would either need to sign or encrypt our cookies.\\n### Data in Transit\\nThis will not make data in transit any safer as it's already encrypted at the\\ntransport level.\\n### Encryption (and MAC) Algorithm\\n### AES-256\\nThe NSA have [declared](http:\/\/csrc.nist.gov\/groups\/ST\/toolkit\/documents\/aes\/CNSS15FS.pdf) AES-256 safe enough for top secret data.\\nAES is a low ram and high speed algorithm which is important when trying to keep\\npage load times down.\\n#### Salt\\nA salt is used to ensure that two items with the same value don't, when\\nencrypted equal the same output value. We're using a buffer of 256 bits salt the\\nencrypted cookie.\\n### Iron Library\\nAt a high level the seal process follows these general steps:\\n* generate encryption salt `saltE`\\n* derive an encryption key `keyE` using `saltE` and a password\\n* generate an integrity salt `saltI`\\n* derive an integrity (HMAC) key `keyI` using `saltI` and the password\\n* generate a random initialization vector `iv`\\n* encrypt the serialized object string using `keyE` and `iv`\\n* mac the encrypted object along with `saltE` and `iv`\\n* concatenate `saltE`, `saltI`, `iv`, and the encrypted object into a URI-friendly string\\nYou can read more about it in the Iron\\nrepository: https:\/\/github.com\/hueniverse\/iron\\n","Decision":"### We will encrypt cookies using the [Iron] library with AES-256\\nsalted with 256 bits.\\n","tokens":383,"id":2852,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-with-a-gp-beta-web\/0003-use-persistant-cookies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n### What is a cookie?\\nA cookie is a small amount of data sent along with every HTTP request to a\\nwebsite. We can set it to any value we like. In this project we are using the\\ncookie as a way to store data that a person who is registering has given us.\\nCookies constitute a security risk as they can be intercepted in flight or\\non machine of the user. That's why we want them hanging around for as little\\ntime as possible.\\n### What is a Session Cookie?\\nA session cookie is a cookie that disappears when a browser window is closed.\\n### What is a Persistent cookie?\\nA persistent cookie is one that disappears after a set time, called its\\n`max-age`.\\n### Assumptions\\n* More people are using their own devices than public ones\\n* We will set a sensible (short) `max-age` on a persistent cookie\\n* People rarely close their browsers\\n","Decision":"### Use session cookies\\nGiven that most people will be using their own devices, and most likely their\\nphones it makes more sense to use persistent cookies with a short timeout rather\\nthan a session cookie which could lie around on their device for months.\\n","tokens":198,"id":2853,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-with-a-gp-beta-web\/0005-add-synthetic-max-age-to-session-cookie.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nin [ADR#4](0004-cookies-encryption.md) we decided to use session cookies but\\ndidn't address the problem of sessions lasting an infinitely long time if the\\nuser doesn't ever close their browser (all tabs).\\nA user could abandon their registration and then return months or years later to register again and we'd have their information pre-populated. This will be extremely disconcerting for the user.\\n","Decision":"Using middleware, add an extra key to the session cookie called max-age. If the middleware sees a cookie with a max-age < now then it will delete that cookie. We can set to a long time like 4 hours for now and reduce as we continue to learn about our users' behaviour.\\n","tokens":92,"id":2854,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-with-a-gp-beta-web\/0002-use-hapi-js-framework-for-http.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs we start to standardise on what our services look like we want to use the\\nbest tools for the job when it comes to node web frameworks. Hapi is more of a\\nframework than [express] and is more modular allowing many developers to work on\\na single project without trampling over each others' routes.\\n","Decision":"We have decided to use [hapi] as our http server.\\nThere are many other [alternatives] but none are as actively developed as Hapi\\n","tokens":70,"id":2855,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-with-a-gp-beta-web\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2856,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-documentation\/0002-pull-acs-info-out-of-modules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUsing acs-info inside our standard and component modules is creating pain. Does it make more sense to expect products to pass the necessary variables in, allowing them to use acs-info if they want?\\n## Decision Drivers <!-- optional -->\\n* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\n","Decision":"* Terraform's lack of optimization with modules and data sources\\n* Usage of modules in non-OIT accounts\\n* We want our modules to be simple to use\\n* We want development teams to understand the infrastructure of their products.\\nChosen option: \"Pull acs-info out of modules\", because it was the best option based on an analysis of pros and cons (see below).\\n<!-- ### Positive Consequences optional -->\\n<!-- * TBD -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * TBD -->\\n","tokens":103,"id":2857,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-documentation\/0001-discard-component-modules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTerraform recommends not nesting modules into a deep hierarchy. We felt that varying from this recommendation made sense. We've started feeling some pain due to that decision. Do we follow this recommendation, or continue trying to organize modules for reusability and abstraction?\\n## Decision Drivers <!-- optional -->\\n* Terraform best practice\\n* Simplicity (maintenance, use, enhancement, fixes)\\n* Abstraction\\n* Reuse\\n* The way Terraform handles perceived changes to state\\n","Decision":"* Terraform best practice\\n* Simplicity (maintenance, use, enhancement, fixes)\\n* Abstraction\\n* Reuse\\n* The way Terraform handles perceived changes to state\\nChosen option: \"Discard component modules\", the danger of breaking products and the pain of enhancement is too high with component modules.\\n<!-- ### Positive Consequences optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n<!-- ### Negative Consequences optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n","tokens":106,"id":2858,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"protagonist\/0000-api-project-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n### Context\\nHow best can we structure the API project to make it easy for developers to navigate and comprehend.\\n","Decision":"\"Organise by Feature and use MediatR\", because we can use MediatR to encapsulate the various use-cases of the API. Together with feature folders this should allow developers to more easily identify the capabilities of the API and target the section they need to.\\n","tokens":29,"id":2860,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0045-support-jmap-push-with-mailbox-changes-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to handle the **state** property that comes with JMAP get\/set request. This means that James needs to be able\\nto record a new state for objects whenever a change happens as well as return the most recent state to the client when fetching objects.\\nFirst step is to implement Mailbox\/changes.\\n","Decision":"We will implement a mechanism to record all the changes happening to Mailbox objects in the form of a list of **mailboxId**. When an event such as\\ncreated\/updated\/destroyed occur, or when message is appended to a mailbox we will store their **mailboxIds** along with a **state** object\\nin a Cassandra table.\\nEach state will have a list of changes, and all the **mailboxId** will be stored as separated lists corresponding to the change which they represent: **created**, **updated**, **destroyed**.\\nFor the case when messages are appended to a mailbox, it will be counted as an updated event and that mailboxId should be stored in **updated** list.\\nLeveraging the **MailboxChanges** table, we can now fetch all the changes that have occurred since a particular **state**.\\nStates are stored in Cassandra as time based UUID (**TimeUUID**). This ensures that no conflicting changes will happen in the case when two or more events occur at the same point in time.\\n**TimeUUID** also allows **state** to be sorted in chronological order.\\nComponents that need to be implemented:\\n- MailboxChangesRepository: Allows storing and fetching the **state** along with the lists of **mailboxId** in **MailboxChanges** table.\\n- MailboxChangeListener: Listens to changes and triggers the record creation in **MailboxChanges** table.\\n- MailboxChangeMethod: Handles the **state** property, allowing client to fetch the changes since a particular state.\\n- MailboxSetMethod\/MailboxGetMethod needs to query the MailboxChangesRepository for their states properties.\\n","tokens":100,"id":2861,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0033-use-scala-in-event-sourcing-modules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt the time being James use the scala programming language in some parts of its code base, particularily for implementing the Distributed Task Manager,\\nwhich uses the event sourcing modules.\\nThe module `event-store-memory` already uses Scala.\\n","Decision":"What is proposed here, is to convert in Scala the event sourcing modules.\\nThe modules concerned by this change are:\\n-  `event-sourcing-core`\\n-  `event-sourcing-pojo`\\n-  `event-store-api`\\n-  `event-store-cassandra`\\n","tokens":50,"id":2862,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0011-remove-elasticsearch-document-source.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThough very handy to have around, the source field does incur storage overhead within the index.\\n","Decision":"Disable `_source` for ElasticSearch indexed documents.\\n","tokens":22,"id":2863,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0006-task-serialization.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need to have a way to describe the `Task` to be executed and serialize it in order to be able to store it in the `Created` event. Which will be persisted in the Event Store, and will be send in the event bus.\\nAt this point in time a `Task` can contain any arbitrary code. It's not an element of a finite set of actions.\\n","Decision":"* Create a `Factory` for one `Task`\\n* Inject a `Factory` `Registry` via a Guice Module\\n* The `Task` `Serialization` will be done in JSON, We will get inspired by `EventSerializer`\\n* Every `Task`s should have a specific integration test demonstrating that serialization works\\n* Each `Task` is responsible of eventually dealing with the different versions of the serialized information\\n","tokens":112,"id":2864,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0043-avoid-elasticsearch-on-critical-reads.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA user willing to use a webmail powered by the JMAP protocol will end up doing the following operations:\\n- `Mailbox\/get` to retrieve the mailboxes. This call is resolved against metadata stored in Cassandra.\\n- `Email\/query` to retrieve the list of emails. This call is nowadays resolved on ElasticSearch for Email search after\\na right resolution pass against Cassandra.\\n- `Email\/get` to retrieve various levels of details. Depending on requested properties, this is either\\nretrieved from Cassandra alone or from ObjectStorage.\\nSo, ElasticSearch is queried on every JMAP interaction for listing emails. Administrators thus need to enforce availability and good performance\\nfor this component.\\nRelying on more services for every read also harms our resiliency as ElasticSearch outages have major impacts.\\nAlso we should mention our ElasticSearch implementation in Distributed James suffers the following flaws:\\n- Updates of flags lead to updates of the all Email object, leading to sparse segments\\n- We currently rely on scrolling for JMAP (in order to ensure messageId uniqueness in the response while respecting limit & position)\\n- We noticed some very slow traces against ElasticSearch, even for simple queries.\\nRegarding Distributed James data-stores responsibilities:\\n- Cassandra is the source of truth for metadata, its storage needs to be adapted to known access patterns.\\n- ElasticSearch allows resolution of arbitrary queries, and performs full text search.\\n","Decision":"Provide an optional view for most common `Email\/query` requests both on Draft and RFC-8621 implementations.\\nThis includes filters and sorts on 'sentAt'.\\nThis view will be stored into Cassandra, and updated asynchronously via a MailboxListener.\\n","tokens":296,"id":2865,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0016-distributed-workqueue.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy switching the task manager to a distributed implementation, we need to be able to run a `Task` on any node of the cluster.\\n","Decision":"For the time being we will keep the sequential execution property of the task manager.\\nThis is an intermediate milestone toward the final implementation which will drop this property.\\n* Use a RabbitMQ queue as a workqueue where only the `Created` events are pushed into.\\nInstead of using the brittle exclusive queue mechanism described in [3. Distributed WorkQueue](0003-distributed-workqueue.md), we will\\nnow use the natively supported [Single Active Consumer](https:\/\/www.rabbitmq.com\/consumers.html#single-active-consumer) mechanism.\\n","tokens":32,"id":2866,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0005-distributed-task-termination-ackowledgement.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2867,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0046-generalize-event-bus.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUser email storage usage is limited both in size and count via quotas (IMAP RFC-2087). In order to ease administrating large user bases, the quota search extension allows administrator\\nto retrieve all users whose email usages are exceeding a given occupation ratio.\\nWhen searching for users by quota ratio if we set the value of the parameters to 0, for example: `\/quotas\/users?minOccupationRatio=0&maxOccupationRatio=0`, the search feature is supposed to return newly created users\\nwho have not received any email yet at that point. However, this is not the case because the quotas are currently being initialized only after\\na user has received the first email.\\nWe need to initialize user quotas upon user creation time. The problem is: there is currently no event at user creation\\nand since the quota-search feature is a plugin of James, it cannot be hardwired into the domain logic of user management to initialize the quota for a just created user.\\n","Decision":"For quota-search to be initialized\/removed for a given user while keeping this feature as a plugin, we decided to adopt the Event Driven pattern we already use in Mailbox-api.\\nWe can create new events related to user management (UserCreated, UserRemoved and so on).\\nTo achieve that, we will extract the EventBus out of mailbox-api in order to make it a utility component (eventbus-api), then we will make both mailbox-api and data-api depend on that new module.\\n","tokens":207,"id":2868,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0015-objectstorage-blobid-list.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2869,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0035-distributed-listeners-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames enables a user to register additional mailbox listeners.\\nThe distributed James server is handling mailbox event processing (mailboxListener execution) using a RabbitMQ work-queue\\nper listener.\\nMailbox listeners can be registered to be triggered every time an event is generated by user interaction with their\\nmailbox. They are being executed in a distributed fashion following the workqueue messaging pattern. The \"group\" is an\\nattribute of the mailbox listener identifying to which work queue they belong.\\nMore information about this component can be found in [ADR 0036](0037-eventbus.md).\\nCurrently, mailbox listeners are determined by the guice bindings of the server and additional mailbox listeners defined\\nvia configuration files.\\nWhile the configuration might be specific for each James server, what actually is defined in RabbitMQ is common.\\nHeterogeneous configuration might then result in unpredictable RabbitMQ resource status. This was left as a limitation\\nof [26. Removing a configured additional MailboxListener](0026-removing-configured-additional-mailboxListeners.md).\\n","Decision":"We need to centralize the definition of mailbox listeners.\\nAn event sourcing system will track the configured mailbox listeners.\\nIt will have the following commands:\\n- **AddListener**: Add a given listener. This should be rejected if the group is already used.\\n- **RemoveListener**: Remove a given listener.\\nConfiguration changes are not supported. The administrator is expected to remove the given listener, then add it again.\\nIt will have the following events:\\n- **ListenerAdded**: A mailbox listener is added\\n- **ListenerRemoved**: A mailbox listener is removed\\nA subscriber will react to these events to modify the RabbitMQ resource accordingly by adding queues, adding or removing\\nbindings.\\nThis event sourcing system differs from the one defined in\\n[26. Removing a configured additional MailboxListener](0026-removing-configured-additional-mailboxListeners.md) by the\\nfact that we should also keep track of listener configuration.\\nUpon start, James will ensure the **configured mailbox listener event sourcing system** contains the guice injected\\nlisteners, and add them if missing (handling the RabbitMQ bindings by this mean), then starts the eventBus which will\\nconsume the given queues.\\nIf a listener is configured with a class unknown to James, the start-up fails and James starts in a degraded state\\nallowing to unconfigure the faulty listener. This downgraded state will be described in a separate ADR and the link will\\nbe updated here.\\nThis differs from [26. Removing a configured additional MailboxListener](0026-removing-configured-additional-mailboxListeners.md)\\nby the fact we no longer need to register all listeners at once.\\nA WebAdmin endpoint will allow:\\n- **to add a listener** to the one configured. Such a call:\\n- Will fail if the listener class is not on the local classpath, or if the corresponding group already used within\\nthe **configured mailbox listener aggregate**.\\n- Upon success the listener is added to the **configured mailbox listener aggregate**, and the listener is\\nregistered locally.\\n- **to remove a listener**. Such a call:\\n- Will fail if the listener is required by Guice bindings on the current server or if the listener is not configured.\\n- Upon success, the listener is removed from the **configured mailbox listener aggregate**, and the listener is\\nunregistered locally.\\nA broadcast on the event bus will be attempted to propagate topology changes, by the mean of a common registrationKey\\nto all nodes, a \"TopologyChanged\" event, and a mailbox listener starting the MailboxListeners on local node upon\\ntopology changes. `registrationKey` concept is explained in [ADR 0036](0037-eventbus.md).\\nIf a listener is added but is not in the classpath, an ERROR log is emitted. This can happen during a rolling upgrade,\\nwhich defines a new guice binding for a new mailbox listener. Events will still be emitted (and consumed by other James\\nservers) however a local James upgrade will be required to effectively be able to start processing these events. The\\nbinding will not need to be redefined.\\nWe will also expose an endpoint listing the groups currently in use, and for each group the associated configuration, if\\nany. This will query the **configured mailbox listener aggregate**.\\nWe will introduce a health check to actually ensure that RabbitMQ resources match the configured listeners, and propose\\na WebAdmin endpoint to add\/remove bindings\/queue in a similar fashion of what had been proposed in\\n[26. Removing a configured additional MailboxListener](0026-removing-configured-additional-mailboxListeners.md). This\\ncan happen if the James server performing the listener registration fails to create the group\/queue. This health check\\nwill also report if this James server does not succeed to run a given listener, for instance if its class is not on the\\nclasspath.\\n","tokens":215,"id":2870,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0024-polyglot-strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context & Problem Statement\\nJames is written in Java for a very long time. In recent years, Java modernized a lot after a decade of slow progress.\\nHowever, in the meantime, most software relying on the JVM started supporting alternative JVM languages to keep being relevant.\\nIt includes Groovy, Clojure, Scala and more recently Kotlin, to name a few.\\nNot being open to those alternative languages can be a problem for James adoption.\\n","Decision":"Nowadays, libraries and framework targeting the JVM are expected to support usage of one or several of these alternative languages.\\nJames being not only a mail server but also a development framework needs to reach those expectations.\\nAt the same time, more and more developers and languages adopt Function Programming (FP) idioms to solve their problems.\\nWe decide for options 4, 5 and IV.\\nThat means we need to write some mailets in Scala and demonstrate how it's done and then used in a running server.\\nIt also means writing and\/or refactoring some server components in Scala, starting where it's the most relevant.\\n###\u00a0Positive Consequences\\n* Modernize parts of James code\\n* Leverage Scala richer FP ecosystem and language to overcome Java limitations on that topic\\n* Should attract people that would not like Java\\n###\u00a0Negative Consequences\\n* Adds even more knowledge requirements to contribute to James\\n* Scala build time is longer than Java build time\\n","tokens":93,"id":2871,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0038-distributed-eventbus.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n","Decision":"Provide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n","tokens":69,"id":2872,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0002-make-taskmanager-distributed.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2874,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0048-cleanup-jmap-uploads.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJMAP allows users to upload binary content called blobs to be later referenced via method calls. This includes but is not\\nlimited to `Email\/set` for specifying the blobId of attachments and `Email\/import`.\\nThe [specification](https:\/\/jmap.io\/spec-core.html#binary-data) strongly encourages enforcing the cleanup of these uploads:\\n```\\nA blob that is not referenced by a JMAP object (e.g., as a message attachment) MAY be deleted by the server to free up\\nresources. Uploads (see below) are initially unreferenced blobs.\\n[...] An unreferenced blob MUST NOT be deleted for at least 1 hour from the time of upload; if reuploaded, the same\\nblobId MAY be returned, but this SHOULD reset the expiry time.\\n```\\nDeleting such uploads in a timely manner is important as:\\n- It enables freeing server resources.\\n- failing to do so may compromise privacy: content the user have uploaded and long forgotten might still be accessible\\nin the underlying data-store. Failing to delete uploads in a timely fashion may jeopardize for instance GDPR compliance.\\nToday, uploads are stored along side email attachments. This means:\\n- We can hardly apply a specific lifecycle that cleans up uploads, as distinguishing attachment from uploads is not\\ntrivial.\\n- We currently have a complex right resolution system on attachment, handling both the upload case (were the attachment\\nis linked to a user) and the 'true' attachment case (linked to a message, those who can access the message can access\\nthe attachment). This leads to sub-optimal code (slow).\\n","Decision":"We need to create a separate interface `UploadRepository` in `data-jmap` to store uploads for each user. We would provide a memory\\nimplementation as well as a distributed implementation of it.\\nThe distributed implementation would host metadata of the upload in Cassandra, and the content using the BlobStore API,\\nso object storage.\\nThis `UploadRepository` would be used by JMAP RFC-8620 to back uploads (instead of the attachment manager), we will\\nprovide a `BlobResolver` to enable interactions with the uploaded blob. Similarly, we will use the `UploadRepository` to\\nback uploads of JMAP draft.\\nWe will implement cleanup of the distributed `UploadRepository`. This will be done via:\\n- TTLs on the Cassandra metadata.\\n- Organisation of the blobs in time ranged buckets, only the two most recent buckets are kept.\\n- A WebAdmin endpoint would allow to plan a CRON triggering the cleanup.\\n","tokens":339,"id":2876,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0020-cassandra-mailbox-object-consistency.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMailboxes are denormalized in Cassandra in order to access them both by their immutable identifier and their mailbox\\npath (name):\\n- `mailbox` table stores mailboxes by their immutable identifier\\n- `mailboxPathV2` table stores mailboxes by their mailbox path\\nWe furthermore maintain two invariants on top of these tables:\\n- **mailboxPath** unicity. Each mailbox path can be used maximum once. This is ensured by writing the mailbox path first\\nusing Lightweight Transactions.\\n- **mailboxId** unicity. Each mailbox identifier is used by only a single path. We have no real way to ensure a given mailbox\\nis not referenced by two paths.\\nFailures during the denormalization process will lead to inconsistencies between the two tables.\\nThis can lead to the following user experience:\\n```\\nBOB creates mailbox A\\nDenormalization fails and an error is returned to A\\nBOB retries mailbox A creation\\nBOB is being told mailbox A already exist\\nBOB tries to access mailbox A\\nBOB is being told mailbox A does not exist\\n```\\n","Decision":"We should provide an offline (meaning absence of user traffic via for exemple SMTP, IMAP or JMAP) webadmin task to\\nsolve mailbox object inconsistencies.\\nThis task will read `mailbox` table and adapt path registrations in `mailboxPathV2`:\\n- Missing registrations will be added\\n- Orphan registrations will be removed\\n- Mismatch in content between the two tables will require merging the two mailboxes together.\\n* [Discussion on the mailing list](https:\/\/www.mail-archive.com\/server-dev@james.apache.org\/msg64432.html)\\n","tokens":230,"id":2877,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0003-distributed-workqueue.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy switching the task manager to a distributed implementation, we need to be able to run a `Task` on any node of the cluster.\\n","Decision":"For the time being we will keep the sequential execution property of the task manager.\\nThis is an intermediate milestone toward the final implementation which will drop this property.\\n* Use a RabbitMQ queue as a workqueue where only the `Created` events are pushed into.\\nThis queue will be exclusive and events will be consumed serially. Technically this means the queue will be consumed with a `prefetch = 1`.\\nThe queue will listen to the worker on the same node and will ack the message only once it is finished (`Completed`, `Failed`, `Cancelled`).\\n","tokens":32,"id":2878,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0012-jmap-partial-reads.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJMAP core RFC8620 requires that the server responds only properties requested by the client.\\nJames currently computes all of the properties regardless of their cost, and if it had been asked by the client.\\nClearly we can save some latencies and resources by avoiding reading\/computing expensive properties that had not been explicitly requested by the client.\\n","Decision":"Introduce two new datastructures representing JMAP messages:\\n- One with only metadata\\n- One with metadata + headers\\nGiven the properties requested by the client, the most appropriate message datastructure will be computed, on top of\\nexisting message storage APIs that should remain unchanged.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":73,"id":2881,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0042-james-cli-based-on-webadmin.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2882,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0013-precompute-jmap-preview.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2883,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0009-java-11-migration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJava 11 is the only \"Long Term Support\" java release right now so more and more people will use it exclusively.\\nJames is known to build with Java Compiler 11 for some weeks.\\n","Decision":"We adopt Java Runtime Environment 11 for James as a runtime to benefits from a supported runtime and new features\\nof the languages and the platform.\\n","tokens":44,"id":2885,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0028-Recompute-mailbox-quotas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJMAP custom quota extension, as well as IMAP [RFC-2087](https:\/\/tools.ietf.org\/html\/rfc2087) enables a user to monitor\\nthe amount of space and message count he is allowed to use, and that he is effectively using.\\nTo track the quota values a user is effectively using, James relies on the\\n[eventBus](..\/site\/markdown\/server\/manage-guice-distributed-james.md#mailbox-event-bus) to increment a Cassandra counter\\ncorresponding to this user.\\nHowever, upon Cassandra failure, this value can be incorrect, hence the need of correcting it.\\n","Decision":"We will implement a generic corrective task exposed via webadmin.\\nThis task can reuse the `CurrentQuotaCalculator` and call it for each and every quotaRoot of each user.\\nThis way, non-Cassandra implementation will also benefit from this task.\\n","tokens":132,"id":2887,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0044-against-the-use-of-cassandra-lightweight-transactions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs any kind of server James needs to provide some level of consistencies.\\nStrong consistency can be achieved with Cassandra by relying on LightWeight transactions. This enables\\noptimistic transactions on a single partition key.\\nUnder the hood, Cassandra relies on the PAXOS algorithm to achieve consensus across replica allowing us\\nto achieve linearizable consistency at the entry level. To do so, Cassandra tracks consensus in a system.paxos\\ntable. This `system.paxos` table needs to be checked upon reads as well in order to ensure the latest state of the ongoing\\nconsensus is known. This can be achieved by using the SERIAL consistency level.\\nExperiments on a distributed James cluster (4 James nodes, having 4 CPU and 8 GB of RAM each, and a 3 node Cassandra\\ncluster of 32 GB of RAM, 8 CPUs, and SSD disks) demonstrated that the system.paxos table was by far the most read\\nand compacted table (ratio 5).\\nThe table triggering the most reads to the `system.paxos` table was the `acl` table. Deactivating LWT on this table alone\\n(lightweight transactions & SERIAL consistency level) enabled an instant 80% throughput, latencies reductions\\nas well as softer degradations when load breaking point is exceeded.\\n","Decision":"Rely on `event sourcing` to maintain a projection of ACLs that do not rely on LWT or SERIAL consistency level.\\nEvent sourcing is thus responsible for handling concurrency and race conditions as well as governing denormalization\\nfor ACLs. It can be used as a source of truth to re-build ACL projections.\\nNote that the ACL projection tables can end up being out of synchronization from the aggregate but we still have a\\nnon-questionable source of truth handled via event sourcing.\\n","tokens":276,"id":2889,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0027-eventBus-error-handling-upon-dispatch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames allows asynchronous processing for mailbox events via MailboxListener. This processing is abstracted by the\\nEventBus.\\nIf the processing of an event via a mailbox listener fails, it is retried, until it succeeds. If a maxRetries parameter\\nis exceeded, the event is stored in deadLetter and no further processing is attended.\\nThe administrator can then look at the content of deadLetter to diagnose processing issues and schedule a reDelivery in\\norder to retry their processing via webAdmin APIs.\\nHowever no such capabilities are supported upon dispatching the event on the eventbus. A failed dispatch will result in message loss.\\nMore information about this component can be found in [ADR 0036](0037-eventbus.md).\\n","Decision":"Upon dispatch failure, the eventBus should save events in dead letter using a dedicated group.\\nReprocessing this group an admin can re-trigger these events dispatch.\\nIn order to ensure auto healing, James will periodically check the corresponding group in deadLetter is empty. If not a\\nre-dispatching of these events will be attempted.\\n","tokens":153,"id":2890,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0004-distributed-tasks-listing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2891,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0034-mailbox-api-visibility-and-usage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAll mailboxes implementations rely on `mailbox-store` module that defines some common tools to implement the `mailbox-api`\\n(representing the API defining how to use a mailbox).\\nFor example, a `CassandraMailboxmanager` has to extend `StoreMailboxManager` (that implements `Mailboxmanager` from the\\n`mailbox-api`) that requires the implementation of some `Mapper`s.\\n`Mapper`s are designed to provide low-level functions and methods on mailboxes. It's recurrent that we are tempted in\\nJames, outside of the `mailbox` modules, to rely on some of those common tools located in `mailbox-store` to have an\\neasier access on some user's mailboxes or messages.\\nLike for example, using a `Mapper` outside to be able to retrieve a message with only its `MessageId`, which is not\\ncurrently possible at the `Manager`'s level, which tends to violate `mailbox-api`'s role and primary mission.\\nAs a matter of fact, we have currently such uses of `mailbox-store` in James:\\n* `mailbox-adapter` because `Authenticator` and `Authorizator` are part of the `mailbox-store`\\nThe manager layer do further validation including right checking, event dispatching (resulting in message search index\\nindexing, current quota calculation mainly), quota validation. Not relying on the manager layer is thus error prone\\nand can lead to security vulnerabilities.\\n","Decision":"We should never rely on classes defined in `mailbox-store` outside of the `mailbox` modules (except on some cases\\nlimited to the test scope). The right way would be to always rely on the `Manager`s defined in `mailbox-api` module to\\naccess mailboxes and messages, as the `mailbox-api` module defines the API on how to use a mailbox.\\nWe should ensure the correctness of `Manager`s implementations by providing contract tests and not by sharing abstract\\nclasses.\\nRegarding the modules wrongly relying already on `mailbox-store`, we can:\\n* `mailbox-adapter`: move `Authenticator` and `Authorizator` to `mailbox-api`\\n","tokens":305,"id":2893,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0026-removing-configured-additional-mailboxListeners.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames enables a user to register additional mailbox listeners.\\nThe distributed James server is handling mailbox event processing (mailboxListener execution) using a RabbitMQ work-queue\\nper listener.\\nThe distributed James server then declares a queue upon start for each one of these user registered listeners, that it\\nbinds to the main event exchange.\\nMore information about this component, and its distributed, RabbitMQ based implementation, can be found in\\n[ADR 0036](0037-eventbus.md).\\nIf the user unconfigures the listener, the queue and the binding are still present but not consumed. This results in\\nunbounded queue growth eventually causing RabbitMQ resource exhaustion and failure.\\n","Decision":"We need a clear consensus and auditability across the James cluster about **required groups** (and their changes). Thus\\nEvent sourcing will maintain an aggregate tracking **required groups** (and their changes). Audit will be enabled by\\nadding host and date information upon changes. A subscriber will perform changes (binds and unbinds) in registered groups\\nfollowing the changes of the aggregate.\\nEvent sourcing is desirable as it allows:\\n- Detecting previously removed MailboxListener upon start\\n- Audit of unbind decisions\\n- Enables writing more complex business rules in the future\\nThe event sourcing system will have the following command:\\n- **RequireGroups** the groups that the **EventBus** is starting with.\\nAnd the following events:\\n- **RequiredGroupAdded** a group is added to the required groups.\\n- **RequiredGroupRemoved** a group is removed from the required groups.\\nUpon start the aggregate will be updated if needed and bindings will be adapted accordingly.\\nNote that upon failure, registered groups will diverge from required groups. We will add a health check to diagnose\\nsuch issues. Eventually, we will expose a webadmin task to reset registered groups to required groups.\\nThe queues should not be deleted to prevent message loss.\\nGiven a James topology with a non uniform configuration, the effective RabbitMQ routing will be the one of the latest\\nstarted James server.\\n","tokens":141,"id":2894,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0018-jmap-new-specs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHistorically, James has been an early adopter for the JMAP specification, and a first partial implementation was conducted when JMAP was just a draft.\\nBut with time, the IETF draft went with radical changes and the community could not keep this implementation up to date with the spec changes.\\nAs of summer 2019, JMAP core ([RFC 8620](https:\/\/tools.ietf.org\/html\/rfc8620)) and JMAP mail ([RFC 8621](https:\/\/tools.ietf.org\/html\/rfc8621)) have been officially published.\\nThus we should implement these new specifications to claim JMAP support.\\nWe need to keep in mind though that part of the community actively relies on the actual 'draft' implementation of JMAP existing in James.\\n","Decision":"We decided to do as follow:\\n* Rename packages `server\/protocols\/jmap*` and guice packages `server\/container\/guice\/protocols\/jmap*` to `jmap-draft`. `JMAPServer` should also be renamed to `JMAPDraftServer` (this has already been contributed [here](https:\/\/github.com\/apache\/james-project\/pull\/164), thanks to @cketti).\\n* Port `jmap-draft` to be served with a reactive technology\\n* Implement a JMAP meta project to select the JMAP version specified in the accept header and map it to the correct implementation\\n* Create a new `jmap` package\\n* Implement the new JMAP request structure with the [echo](https:\/\/jmap.io\/spec-core.html#the-coreecho-method) method\\n* Implement authentication and session of the new JMAP protocol\\n* Implement protocol-level error handling\\n* Duplicate and adapt existing mailbox methods of `jmap-draft` to `jmap`\\n* Duplicate and adapt existing email methods of `jmap-draft` to `jmap`\\n* Duplicate and adapt existing vacation methods of `jmap-draft` to `jmap`\\n* Support uploads\/downloads\\nThen when we finish to port our existing methods to the new JMAP specifications, we can implement these new features:\\n* Accounts\\n* Identities\\n* EmailSubmission\\n* Push and queryChanges\\n* Threads\\nWe decided to support `jmap` on top of memory-guice and distributed-james products for now.\\nWe should ensure no changes is done to `jmap-draft` while implementing the new `jmap` one.\\nRegarding the versioning in the accept headers:\\n* `Accept: application\/json;jmapVersion=draft` would redirect to `jmap-draft`\\n* `Accept: application\/json;jmapVersion=rfc-8621` would redirect to `jmap`\\n* When the `jmapVersion` is omitted, we will redirect first towards `jmap-draft`, then to `jmap`\\nwhen `jmap-draft` becomes deprecated\\nIt's worth mentioning as well that we took the decision of writing this new implementation using `Scala`.\\n","tokens":161,"id":2895,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0021-cassandra-acl-inconsistency.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMailboxes ACLs are denormalized in Cassandra in order to:\\n- given a mailbox, list its ACL (enforcing rights for example)\\n- discover which mailboxes are delegated to a given user (used to list mailboxes)\\nHere is the tables organisation:\\n- `acl` stores the ACLs of a given mailbox\\n- `UserMailboxACL` stores which mailboxes had been delegated to which user\\nFailures during the denormalization process will lead to inconsistencies between the two tables.\\nThis can lead to the following user experience:\\n```\\nALICE delegates her INBOX mailbox to BOB\\nThe denormalisation process fails\\nALICE INBOX does not appear in BOB mailbox list\\nGiven a delegated mailbox INBOX.delegated\\nALICE undo the sharing of her INBOX.delegated mailbox\\nThe denormalisation process fails\\nALICE INBOX.delegated mailbox still appears in BOB mailbox list\\nWhen BOB tries to select it, he is being denied\\n```\\n","Decision":"We can adopt a retry policy of the `UserMailboxACL` projection update as a mitigation strategy.\\nUsing `acl` table as a source of truth, we can rebuild the `UserMailboxACL` projection:\\n- Iterating `acl` entries, we can rewrite entries in `UserMailboxACL`\\n- Iterating `UserMailboxACL` we can remove entries not referenced in `acl`\\n- Adding a delay and a re-check before the actual fix can decrease the occurrence of concurrency issues\\nWe will expose a webAdmin task for doing this.\\n","tokens":216,"id":2898,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0050-jmap-web-push.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n**Notification for newly received emails** is a common feature of modern mail applications. Furthermore,\\n**real time** updates across devices are a common feature. We wish to provide support for this on top\\nof Apache James JMAP implementation for a large variety of devices.\\nDealing with mobile devices yield a unique set of challenges at the protocol level, including\\nlow bandwidth connection, long latencies, and the need to save battery. As such, mobile devices\\noperating systems tend to limit background connections for applications. Thus, in order to receive\\nnotifications when the application is running on the background, one cannot rely on\\n[ADR 0047](0047-jmap-push-over-websockets.md) (JMAP websocket push) - as it would imply\\nthe application maintaining a persistent connection to the server, which is battery\/network\\nconsuming.\\nAs such, [RFC-8620 section 7.2](https:\/\/jmap.io\/spec-core.html#pushsubscription) `JMAP specification for web push`\\nintroduces the use of [RFC-8030](https:\/\/datatracker.ietf.org\/doc\/html\/rfc8030) `Generic Event Delivery Using HTTP Push`\\nusing an intermediate push server: a Push Gateway is used by the device to multiplex all push of all applications on a\\nsingle connection. The device then register the URL of its push gateway on the application server (here JMAP server) for\\nit to forward `StateChange` events to the push gateway.\\n","Decision":"Implement [RFC-8620 section 7.2](https:\/\/jmap.io\/spec-core.html#pushsubscription) `JMAP specification for web push`.\\nWe will store StateChanges, provide a Cassandra implementation of this storage for the Distributed application.\\nWe will implement a PushClient based on reactor-netty to send events to the push gateway.\\nWe will rely on a Group listener plugged on the JMAP event bus to handle the web push - See\\n[ADR 37: Event bus](0037-eventbus.md).\\nWe will implement the `PushSubscription\/get` and `PushSubscription\/set` JMAP methods.\\nWe will also implement the verification code mechanism (a first round-trip to verify the push gateway works well).\\nWe also decided to allow a single push subscription per devideId (which includes device and APP identifier...)\\n","tokens":311,"id":2899,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0008-distributed-task-await.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `await` a `Task` running on any node of the cluster.\\n","Decision":"* Broadcast `Event`s in `RabbitMQ`\\n","tokens":35,"id":2900,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0010-enable-elasticsearch-routing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur queries are mostly bounded to a mailbox or a user. We can easily\\nlimit the number of ElasticSearch nodes involved in a given query by\\ngrouping the underlying documents on the same node using a routing key.\\nWithout a routing key, each shard needs to execute the query. The coordinator\\nneeds also to be waiting for the slowest shard.\\nUsing the routing key unlocks significant throughput enhancement (proportional\\nto the number of shards) and also a possible high percentile latency enhancement.\\nAs most requests are restricted to a single coordination, most search requests will\\nhit a single shard, as opposed to non routed searches which would have hit each shards\\n(each shard would return the number of searched documents, to be ordered and limited\\nagain in the coordination node). This allows to be more linearly scalable.\\n","Decision":"Enable ElasticSearch routing.\\nMessages should be indexed by mailbox.\\nQuota Ratio should be indexed by user.\\n","tokens":172,"id":2901,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0040-quality-levels-definitions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2902,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0042-applicative-read-repairs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCassandra eventual consistency is all about \"replication\", but \"denormalization\" consistency needs\\nto be handled at the applicative layer (due to the lack of transactions in a NoSQL database).\\nIn the past we did set up \"Solve inconsistency\" tasks that can be assimilated to Cassandra repairs. Such\\ntasks, after being scheduled, ensure that the according entity denormalization is correctly denormalized.\\nHowever, the inconsistencies persist between runs. We experienced inconsistencies in some production platform\\nfor both the mailbox entity, and the mailbox counter entity (whose table structure is exposed in\\n[these](0020-cassandra-mailbox-object-consistency.md), [ADRs](0023-cassandra-mailbox-counters-inconsistencies.md)).\\nMonitoring is required to detect when to run them and is time consuming for the platform administrator.\\nGiven a large dataset, it could even be impossible to run such tasks in a timely fashion.\\nAnother classic eventual consistency mechanism, that enables auto-healing is read-repair. Randomly piggy back upon reads\\nsynchronous or asynchronous consistency checks. If missed a repair is performed.\\nIn order to achieve denormalization auto-healing, we thus need to implement \"applicative read repairs\".\\n","Decision":"Provide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities.\\nThis enables read path simplification (and performance enhancements) for the mailbox object.\\nIMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying the\\nprice of read repairs for this operation.\\nProvide a comprehensive documentation page regarding \"Distributed James consistency model\".\\n","tokens":263,"id":2903,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0030-separate-attachment-content-and-metadata.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome mailbox implementations of James store already parsed attachments for faster retrieval.\\nThis attachment storage capabilities are required for two features:\\n- JMAP attachment download\\n- JMAP message search \"attachment content\" criteria\\nOnly Memory and Cassandra backends can be relied upon as a JMAP backend.\\nOther protocols relies on dynamic EML parsing to expose message subparts (IMAP)\\nHere are the POJOs related to these attachments:\\n- **Attachment** : holds an attachmentId, the attachment content, as well as the content type\\n- **MessageAttachment** : composes an attachment with its disposition within a message (cid, inline and name)\\n- **Message** exposes its list of MessageAttachment when it is read with FetchType Full.\\n- **Blob** represents some downloadable content, and can be either an attachment or a message. Blob has a byte array\\npayload too.\\nThe following classes work with the aforementioned POJOs:\\n- **AttachmentMapper** and **AttachmentManager** are responsible of storing and retrieving an attachment content.\\n- **BlobManager** is used by JMAP to allow blob downloads.\\n- Mailbox search exposes attachment content related criteria. These criteria are used by the JMAP protocol.\\nThis organisation causes attachment content to be loaded every time a message is fully read (which happens for instance\\nwhen you open a message using JMAP) despite the fact that it is not needed, as attachments are downloadable through a\\nseparate JMAP endpoint, their content is not attached to the JMAP message JSON.\\nAlso, the content being loaded \"at once\", we allocate memory space to store the whole attachment, which is sub-optimal. We\\nwant to keep the consumed memory low per-message because a given server should be able to handle a high number of messages\\nat a given time.\\nTo be noted that JPA and maildir mailbox implementations do not support attachment storage. To retrieve attachments of a\\nmessage, these implementations parse the messages to extract their attachments.\\nCassandra mailbox prior schema version 4 stored attachment and its metadata in the same table, but from version 5 relies\\non the blobStore to store the attachment content.\\n","Decision":"Enforce cassandra schema version to be 5 from James release 3.5.0. This allows to drop attachment management prior version\\n5.\\nWe will re-organize the attachment POJOs:\\n- **Attachment** should hold an attachmentId, a content type, and a size. It will no longer hold the content. The\\ncontent can be loaded from its **AttachmentId** via the **AttachmentLoader** API that the **AttachmentManager**\\nimplements.\\n- **MessageAttachment** : composes an attachment with its disposition within a message (cid, inline and name)\\n- **Blob** would no longer hold the content as a byte array but rather a content retriever (`Supplier<InputStream>`)\\n- **ParsedAttachment** is the direct result of attachment parsing, and composes a **MessageAttachment** and the\\ncorresponding content as byte array. This class is only relied upon when saving a message in mailbox. This is used as\\nan output of `MessageParser`.\\nSome adjustments are needed on class working with attachment:\\n- **AttachmentMapper** and **AttachmentManager** need to allow from an attachmentId to retrieve the attachment content\\nas an `InputStream`. This is done through a separate `AttachmentLoader` interface.\\n- **AttachmentMapper** and **AttachmentManager** need the Attachment and its content to persist an attachment\\n- **MessageManager** then needs to return attachment metadata as a result of Append operation.\\n- **InMemoryAttachmentMapper** needs to store attachment content separately.\\n- **MessageStorer** will take care of storing a message on the behalf of `MessageManager`. This enables to determine if\\nattachment should be parsed or not on an implementation aware fashion, saving attachment parsing upon writes for JPA\\nand Maildir.\\nMaildir and JPA no longer support attachment content loading. Only the JMAP protocol requires attachment content loading,\\nwhich is not supported on top of these technologies.\\nMailbox search attachment content criteria will be supported only on implementation supporting attachment storage.\\n","tokens":449,"id":2904,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0019-reactor-netty-adoption.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAfter adopting the last specifications of JMAP (see\\n[new JMAP specifications adoption ADR](https:\/\/github.com\/apache\/james-project\/blob\/master\/src\/adr\/0018-jmap-new-specs.md)),\\nit was agreed that we need to be able to serve both `jmap-draft` and the new `jmap` with a reactive server.\\nThe current outdated implementation of JMAP in James is currently using a non-reactive [Jetty server](https:\/\/www.eclipse.org\/jetty\/).\\nThere are many possible candidates as reactive servers. Among the most popular ones for Java:\\n* [Spring](https:\/\/spring.io)\\n* [Reactor-netty](https:\/\/github.com\/reactor\/reactor-netty)\\n* [Akka HTTP](https:\/\/doc.akka.io\/docs\/akka-http\/current\/introduction.html)\\n* ...\\n","Decision":"We decide to use `reactor-netty` for the following reasons:\\n* It's a reactive server\\n* It's using [Reactor](https:\/\/projectreactor.io\/), which is the same technology that we use in the rest of our codebase\\n* Implementing JMAP does not require high level HTTP server features\\n","tokens":180,"id":2905,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0023-cassandra-mailbox-counters-inconsistencies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCassandra maintains a per mailbox projection for message count and unseen message count.\\nAs with any projection, it can go out of sync, leading to inconsistent results being returned to the client, which is not acceptable.\\nHere is the table organisation:\\n- `mailbox` Lists the mailboxes\\n- `messageIdTable` Holds mailbox and flags for each message, lookup by mailbox ID + UID\\n- `imapUidTable` Holds mailbox and flags for each message, lookup by message ID and serves as a source of truth\\n- `mailboxCounters` Holds messages count and unseen message count for each mailbox.\\nFailures during the denormalization process will lead to inconsistencies between the counts and the content of `imapUidTable`\\nThis can lead to the following user experience:\\n- Invalid message count can be reported in the Mail User Agent (IMAP & JMAP)\\n- Invalid message unseen count can be reported in the Mail User Agent (IMAP & JMAP)\\n","Decision":"Implement a webadmin exposed task to recompute mailbox counters.\\nThis endpoints will:\\n- List existing mailboxes\\n- List their messages using `messageIdTable`\\n- Check them against their source of truth `imapUidTable`\\n- Compute mailbox counter values\\n- And reset the value of the counter if needed in `mailboxCounters`\\n","tokens":204,"id":2906,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0049-deduplicated-blobs-gs-with-bloom-filters.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe body, headers, attachments of the mails are stored as blobs in a blob store.\\nIn order to save space in those stores, those blobs are de-duplicated using a hash of their content.\\nTo attain that the current blob store will read the content of the blob before saving it, and generate its id based on\\na hash of this content. This way two blobs with the same content will share the same id and thus be saved only once.\\nThis makes the safe deletion of one of those blobs a non-trivial problem as we can't delete one blob without ensuring\\nthat all references to it are themselves deleted. For example if two messages share the same blob, when we delete\\none message there is at the time being no way to tell if the blob is still referenced by another message.\\n","Decision":"To solve this, we will propose a simple two steps algorithms to provide a background deduplication job.\\nThe **first step** consists in building a bloom filter using the entities referencing the blobs.\\nIn the **second step** we iterate over blobs and check in the bloom filter to predict if they are referenced on not.\\n**Bloom filters** are probabilistic data structures. Here the reference prediction can produce false positives: we might\\nskip some non referenced blobs that should have been garbage collected. However, the associated probability can be tuned\\nand by adding a salt we can ensure subsequent runs will have different sets of false positives and thus that all blobs is\\neventually garbage collected.\\nTo avoid concurrency issues, where we could garbage collect a blob at the same time a new reference to it appear,\\na `reference generation` notion will be added. The de-duplicating id of the blobs which before where constructed\\nusing only the hash of their content,  will now include this `reference generation` too. To avoid synchronization\\nissues, the `generation` will be time based.\\nSo only blobs belonging to the `reference generation` `n-2` will be eligible for garbage collection to avoid\\nconcurrency issues, and allow for a clock skew.\\nFinally, we wish to offer the opportunity to configure, and reconfigure, the `generation` duration. In order to do so,\\nwe introduce a `generation family` part in the blobId. Incremented by the administrator on each configuration changes on\\nthe generation duration it allows avoiding conflicts in generations getting the same number before and after the change:\\nall blobIds with a different family are considered belonging to a distinct generation ready to be garbage collected. This\\nallows arbitrary changes in the generation duration.\\n","tokens":168,"id":2907,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0037-eventbus.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMany features rely on behaviors triggered by interactions with the mailbox API main interfaces (`RightManager`,\\n`MailboxManager`, `MessageManager`, `MessageIdManager`). We need to provide a convenient extension mechanism for\\norganizing the execution of these behaviours, provide retries and advanced error handling.\\nAlso, protocols enable notifications upon mailbox modifications. This is for instance the case for `RFC-2177 IMAP IDLE`,\\nleveraged for `RFC-3501 IMAP unsolicitated notifications` when selecting a Mailbox, as well as maintaining the\\n`IMAP Message Sequence Number <-> Unique IDentifier` MSN <-> UID mapping. Changes happening for a specific entity\\n(mailbox) need to be propagated to the relevant listeners.\\n","Decision":"James mailbox component, a core component of James handling the storage of mails and mailboxes, should use an event\\ndriven architecture.\\nIt means every meaningful action on mailboxes or messages triggers an event for any component to react to that event.\\n`MailboxListener` allows executing actions upon mailbox events. They could be used for a wide variety of purposes, like\\nenriching mailbox managers features or enabling user notifications upon mailboxes operations performed by other devices\\nvia other protocol sessions.\\nInteractions happen via the managers (`RightManager`, `MailboxManager`, `MessageManager`, `MessageIdManager`) which emit an\\nevent on the `EventBus`, which will ensure the relevant `MailboxListener`s will be executed at least once.\\n`MailboxListener` can be registered in a work queue fashion on the `EventBus`. Each work queue corresponds to a given\\nMailboxListener class with the same configuration, identified by their group. Each event is executed at least once\\nwithin a James cluster, errors are retried with an exponential back-off delay. If the execution keeps failing, the event\\nis stored in `DeadLetter` for later reprocessing, triggered via WebAdmin.\\nGuice products enable the registration of additional mailbox listeners. A user can furthermore define its own\\nmailboxListeners via the use of `extension-jars`.\\nMailboxListener can also be registered to be executed only on events concerning a specific entity (eg. a mailbox). The\\n`registrationKey` is identifying entities concerned by the event. Upon event emission, the manager will indicate the\\n`registrationKey` this event should be sent to. A mailboxListener will thus only receive the event for the registration\\nkey it is registered to, in an at least once fashion.\\n","tokens":155,"id":2908,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0014-blobstore-storage-policies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nAlso, the capabilities of the various implementations of BlobStore have different strengths:\\n- CassandraBlobStore is efficient for small blobs and offers low latency. However it is known to be expensive for big blobs. Cassandra storage is expensive.\\n- Object Storage blob store is good at storing big blobs, but it induces higher latencies than Cassandra for small blobs for a cost gain that isn't worth it.\\nThus, significant performance and cost ratio refinement could be unlocked by using the right blob store for the right blob.\\n","Decision":"Introduce StoragePolicies at the level of the BlobStore API.\\nThe proposed policies include:\\n- SizeBasedStoragePolicy: The blob underlying storage medium will be chosen depending on its size.\\n- LowCostStoragePolicy: The blob is expected to be saved in low cost storage. Access is expected to be unfrequent.\\n- PerformantStoragePolicy: The blob is expected to be saved in performant storage. Access is expected to be frequent.\\nAn HybridBlobStore will replace current UnionBlobStore and will allow to choose between Cassandra and ObjectStorage implementations depending on the policies.\\nDeletedMessageVault, BlobExport & MailRepository will rely on LowCostStoragePolicy. Other BlobStore users will rely on SizeBasedStoragePolicy.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":188,"id":2909,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0039-distributed-blob-garbage-collector.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe body, headers, attachments of the mails are stored as blobs in a blob store.\\nIn order to save space in those stores, those blobs are de-duplicated using a hash of their content.\\nTo attain that the current blob store will read the content of the blob before saving it, and generate its id based on\\na hash of this content. This way two blobs with the same content will share the same id and thus be saved only once.\\nThis makes the safe deletion of one of those blobs a non trivial problem as we can't delete one blob without ensuring\\nthat all references to it are themselves deleted. For example if two messages share the same blob, when we delete\\none message there is at the time being no way to tell if the blob is still referenced by another message.\\n","Decision":"To address this issue, we propose to implement a distributed blob garbage collector built upon the previously developed\\nDistributed Task Manager.\\nThe de-duplicating blob store will keep track of the references pointing toward a blob in a `References` table.\\nIt will also keep track of the deletion requests for a blob in a `Deletions` table.\\nWhen the garbage collector algorithm runs it will fetch from the `Deletions` table the blobs considered to be effectively deleted,\\nand will check in the `References` table if there are still some references to them. If there is no more reference to a blob,\\nit will be effectively deleted from the blob store.\\nTo avoid concurrency issues, where we could garbage collect a blob at the same time a new reference to it appear,\\na `reference generation` notion will be added. The de-duplicating id of the blobs which before where constructed\\nusing only the hash of their content,  will now include this `reference generation` too.\\nAt a given interval a new `reference generation` will be emitted, since then all new blobs will point to this new generation.\\nSo a `garbage collection iteration` will run only on the `reference generation` `n-2` to avoid concurrency issues.\\nThe switch of generation will be triggered by a task running on the distributed task manager. This task will\\nemit an event into the event sourcing system to increment the `reference generation`.\\n","tokens":167,"id":2910,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0007-distributed-task-cancellation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA `Task` could be run on any node of the cluster. To interrupt it we need to notify all nodes of the cancel request.\\n","Decision":"* We will add an EventHandler to broadcast the `CancelRequested` event to all the workers listening on a RabbitMQ broadcasting exchange.\\n* The `TaskManager` should register to the exchange and will apply `cancel` on the `TaskManagerWorker` if the `Task` is waiting or in progress on it.\\n","tokens":32,"id":2911,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to be more community-oriented, we should adopt a process to have a structured way to have open architectural decisions.\\nUsing an Architectural Decision Records-based process as a support of discussion on the developers mailing-lists.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/web.archive.org\/web\/20190824074401\/http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nEach ADR will be discussed on the Apache James' developers mailing-list before being accepted.\\nFollowing [Apache Decision Making process](https:\/\/community.apache.org\/committers\/decisionMaking.html), we provide the following possible status, with their associated meaning:\\n- `Proposed`: The decision is being discussed on the mailing list.\\n- `Accepted (lazy consensus)` : the architecture decision was proposed on the mailing list, and a consensus emerged from people involved in the discussion on the mailing list.\\n- `Accepted (voted)` : the architecture undergo a voting process.\\n- `Rejected` : Consensus built up against that proposal.\\n","tokens":49,"id":2912,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"james-project\/0029-Cassandra-mailbox-deletion-cleanup.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCassandra is used within distributed James product to hold messages and mailboxes metadata.\\nCassandra holds the following tables:\\n- mailboxPathV2 + mailbox allowing to retrieve mailboxes informations\\n- acl + UserMailboxACL hold denormalized information\\n- messageIdTable & imapUidTable allow to retrieve mailbox context information\\n- messageV2 table holds message metadata\\n- attachmentV2 holds attachments for messages\\n- References to these attachments are contained within the attachmentOwner and attachmentMessageId tables\\nCurrently, the deletion only deletes the first level of metadata. Lower level metadata stay unreachable. The data looks\\ndeleted but references are actually still present.\\nConcretely:\\n- Upon mailbox deletion, only mailboxPathV2 & mailbox content is deleted. messageIdTable, imapUidTable, messageV2,\\nattachmentV2 & attachmentMessageId metadata are left undeleted.\\n- Upon mailbox deletion, acl + UserMailboxACL are not deleted.\\n- Upon message deletion, only messageIdTable & imapUidTable content are deleted. messageV2, attachmentV2 &\\nattachmentMessageId metadata are left undeleted.\\nThis jeopardize efforts to regain disk space and privacy, for example through blobStore garbage collection.\\n","Decision":"We need to cleanup Cassandra metadata. They can be retrieved from dandling metadata after the delete operation had been\\nconducted out. We need to delete the lower levels first so that upon failures undeleted metadata can still be reached.\\nThis cleanup is not needed for strict correctness from a MailboxManager point of view thus it could be carried out\\nasynchronously, via mailbox listeners so that it can be retried.\\n","tokens":255,"id":2913,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"banking-cqrs-es-go\/0003-drive-development-with-tdd.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe goal of this project is to build out a small, demonstration CQRS and Event Sourced application.\\nTest Driven Development is an excellent way to keep the development cycle short and on track whilst naturally keeping test coverage high.\\nThis project is also a learning tool that is helping me get familiar with the Go programming language.\\n","Decision":"Allow the tests to drive the development of this application.\\n","tokens":70,"id":2915,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"banking-cqrs-es-go\/0005-avoid-short-variable-names.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIdiomatic Go calls for \"short, descriptive\" variable names, which is fine, but I abhor needlessly short variable names.\\nCode is for humans to read and understand. Having single character variable names is rarely helpful. Reducing the cognitive load on software engineers trying to understand what code is doing should be one of the top priorities of any shared codebase.\\n","Decision":"In this project, single character variable names will typically only be used for temporary loop variables (e.g. i, j for indexes).  Variable names will be as short as possible but not at the cost of not being descriptive enough.  Exceptions are fine.\\n","tokens":79,"id":2916,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"banking-cqrs-es-go\/0004-explore-using-oop-and-ddd-with-go.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nI am comfortable writing Object Oriented code and using the Domain Driven Design tactical design patterns.\\nI would like to leverage these approaches for this Go project and see how much my thinking has to change to adapt to the strengths of the Go programming language.\\n","Decision":"Use OOP and DDD tactical patterns where possible.  Recognise when to adapt and change if necessary.\\n","tokens":73,"id":2917,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"banking-cqrs-es-go\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2918,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"origin-frontend-challenge\/0002-testcafe-for-end-to-end-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe project needs some framework to test end-to-end cases.\\n","Decision":"Decied to use [Testcafe](https:\/\/devexpress.github.io\/testcafe\/) as a main framework to end-to-end tests.\\n","tokens":16,"id":2919,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"aspan\/0002-aspan-router.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRouting is required for Apspan Client applicatoin. I am not using a location bar to navigate between pages. Threfore React Router is not an option.\\n","Decision":"I will implement custom router with GraphQL based state.\\nRouter will control type of the main screen component based on state variables.\\nApplication starts with a Folder component showing root folder content. Possible successors - Image and MetaData. From Image user can go further to MetaData or back to Folder containing that image.\\nTransition from Folder to Image:\\n{\\nscreen: Folder\\nid: ID\\n}\\n{\\nscreen: Folder\\nid: ID\\n}\\n","tokens":37,"id":2920,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"aspan\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2921,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/unify-build-stage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur CI\/CD Pipeline has a build stage, which contains a `buildBackend` and a `buildFrontend` component.\\n`buildFrontend` is optional, and only run if the project has a frontend, which is determined by the existence of a `package.json` file.\\nIf both are run, they are executed in parallel.\\n`buildFrontend` actually does not build anything, it only downloads dependencies.\\n\"Building\" the frontend is facilitated via maven, which is run in `buildBackend`.\\n","Decision":"Both `buildBackend` and `buildFrontend` are unified into one `build` stage.\\n","tokens":113,"id":2923,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/sidecar-container.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome projects need downstream systems, e.g. a database, for their Integration-tests.\\nTo enable tests with a downstream system in a containerized Jenkins environment it is required to spin up sidecar-containers on demand.\\nJenkins supports sidecar-containers with help of the [docker-workflow-plugin](https:\/\/github.com\/jenkinsci\/docker-workflow-plugin). There is also an official [example](https:\/\/jenkins.io\/doc\/book\/pipeline\/docker\/) how to use sidecar-containers in Jenkins.\\nUnfortunately with the current implementation of the plugin it is not possible to connect the sidecar-container to multiple docker networks.\\nAn [issue](https:\/\/issues.jenkins-ci.org\/browse\/JENKINS-56561) has already been created in Jenkins JIRA.\\n","Decision":"To provide the consumer of the pipeline the possibility to use sidecar-containers in their pipelines (in docker as well as kubernetes environments) we decided to use the already available functionality of the `dockerExecute` step provided by [jenkins-library](https:\/\/github.com\/SAP\/jenkins-library).\\n","tokens":161,"id":2925,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/cx-server-in-container.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\\nRecently, we added a \"companion\" Docker image which is used by `cx-server` to run scripts.\\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\\n","Decision":"We move the bash script inside the `s4sdk\/cxserver-companion` Docker image.\\nThe old `cx-server` script just delegates the command to the script inside the companion container.\\nA new `cx-server.bat` script is added, doing the same for Windows.\\nWe don't use PowerShell to increase compatibility with Windows.\\n","tokens":175,"id":2926,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/project-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSAP S\/4HANA Cloud SDK encompasses components such as Java libraries, Maven archetypes, a Virtual Data Model and a Continuous Delivery Toolkit.\\nThe Continuous Delivery Toolkit must be able to build, test and deploy arbitrary SAP S\/4HANA Cloud SDK Maven projects developed by teams within and outside of SAP.\\nBased on best practices gained when developing cloud-native applications, we learned that it should be possible to separate unit- and integration tests to run them in isolation.\\nReasoning for requiring running them in isolation are:\\n* Dependencies for unit tests might not be required for integration tests and vice versa, or even conflict with them\\n* Unit- and integration test can be run independently and parallel, locally and on CI\/CD\\nAlternative options to separate unit- and integration tests:\\n* Use [Maven profiles](https:\/\/maven.apache.org\/guides\/introduction\/introduction-to-profiles.html)\\n* Use [Maven multi-module](https:\/\/maven.apache.org\/guides\/mini\/guide-multiple-modules.html) projects\\nArchetypes for generating valid projects are provided.\\n[Example projects](https:\/\/github.com\/sap\/cloud-s4-sdk-examples) are available.\\n","Decision":"Projects built by SAP S\/4HANA Cloud SDK Pipeline need to be Maven multi-module projects.\\nRequired modules are `application`, `integration-tests` and `unit-tests`.\\nAdditional modules are **not** allowed.\\n","tokens":248,"id":2927,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/automatic-release.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the \"build once\" principle.\\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\nFor this document, the term \"release\" (noun) refers to a uniquely identifiable version of software.\\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\\nThe verb \"to release\" refers to the process of creating a new release.\\nPart of this process is to determine the version number of the release candidate.\\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http:\/\/maven.apache.org\/maven-release\/maven-release-plugin\/).\\nUsing this plugin does not satisfy our requirements as described below.\\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\\nArtifact uploads and deployments happen only for commits on the so co called \"productive branch\" (`master` by default).\\nMaven's versioning schema appends `SNAPSHOT` suffix to versions which are not released.\\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\\n","Decision":"We implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\\nThis feature is enabled by default, but can be disabled.\\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\\nAn auto-created release does not imply creating a tag in the repository.\\nCreating tags may be done manually to mark noteworthy versions by the developer.\\n### Reasoning\\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team's definition of done.\\nMerging to `master` is implicitly approval for release.\\n* Feature can be disabled: You might still have builds which don't follow this release approach.\\nFor those, it must be possible to disable automatic versioning.\\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\\n* Don't create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\\nIf we created tags automatically for each version, tags would be cluttered very quickly.\\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\\n","tokens":336,"id":2928,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/ignore-end-to-end-tests-on-non-produtive-branches.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSAP S\/4HANA Cloud SDK Pipeline can execute end-to-end tests, which simulate how a human would test the application.\\nEnd-to-end tests tend to run quite long, which might impede how fast pull requests can be merged.\\n### Decision\\nWe allow to skip running end-to-end tests on non-productive branches.\\nWe do not allow skipping them on the productive branch.\\nThis feature is disabled by default.\\n","Decision":"We allow to skip running end-to-end tests on non-productive branches.\\nWe do not allow skipping them on the productive branch.\\nThis feature is disabled by default.\\n","tokens":93,"id":2929,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SAP-Cloud\/zero-downtime-deployment-for-end-to-end-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn the End-To-End Tests stage of the SAP S\/4HANA Cloud SDK Pipeline the newly built application will be deployed to SCP Neo \/ SCP Cloud Foundry before executing the End-To-End Tests.\\nSome consumers of the SAP S\/4HANA Cloud SDK Pipeline are working in one single test landscape for all of their microservices with one instance per microservice.\\nDeploying multiple services at once in this scenario can cause issues when running End-To-End Tests, because a dependency of the tested service might not be available.\\n### Decision\\nWe provide the possibility to activate Zero Downtime Deployment in the End-To-End Tests.\\nThis feature is disabled by default.\\n","Decision":"We provide the possibility to activate Zero Downtime Deployment in the End-To-End Tests.\\nThis feature is disabled by default.\\n","tokens":145,"id":2930,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0005-Interpreter.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n","Decision":"An interpreter uses the [import IR](.\/0003-Import_IR.md) and the [mapping rule IR](.\/0004-Mapping_IR.md)\\nto execute and map operations from one framework to nd4j's file format and back.\\nThis also allows execution of different frameworks via conversion in the nd4j engine.\\nA combination of the 2 allows a uniform interface to be used for the interpreter.\\n1 or more MappingRules will be used to transform 1 file format to another.\\n","tokens":3,"id":2935,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0007-configuration_objects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome Ops (esp. convolution) have many parameters. Many of them can have reasonable defaults, but even then creating\\nsignatures for evey reasonable configuration may be impossible, as those signatures would require different naming in\\norder to be actually distinguishable from each other.\\nIn other cases, an op may have a lot of same typed parameters that are required (e.g. GRU, LSTM, SRU) but it is very\\neasy to mix them up.\\nFor both of those cases (many optional parameters, easily mixed up required parameters) it is reasonable to use a\\nconfig holder with builder pattern in languages that do not support named or default parameters.\\nIn our current codebase those configurations are often used across several related ops.\\n","Decision":"We add a `Config(\"name\"){ ... }` section to the namespace context. It supports `Input` and `Arg` definitions in the same\\nway that `Op` does.\\nOps that want to use that config can use `useConfig(conf)`. As configs are often reused across related objects, this\\nwill have the effect of a mixin: All inputs and args defined in that config will also be automatically defined on that\\nOp. If there is a naming conflict, an exception will be thrown at construction time.\\nFor default signatures, configs will be passed at the end, in the order that they were added to the Op.\\nIf other signatures are desired, configs, like regular inputs and args, can be passed to `Signature`.\\nIn languages that do not support default or named parameters, a config holder will be created, that will take the\\nparameters of the config using a builder pattern. For languages with default and named parameters, no additional config\\nholder will be created, and the parameters of the config will be treated as if they were directly configured on the Op.\\n### Example\\nThis example shows a very simple case in order to highlight how this feature would be used.\\n```kotlin\\nfun RNN() = Namespace(\"RNN\"){\\nval sruWeights = Config(\"SRUWeights\"){\\nInput(FLOATING_POINT, \"weights\"){ description = \"Weights, with shape [inSize, 3*inSize]\" }\\nInput(FLOATING_POINT, \"bias\"){ description = \"Biases, with shape [2*inSize]\" }\\n}\\nOp(\"SRU\"){\\nInput(FLOATING_POINT, \"x\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"initialC\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"mask\"){ description = \"...\" }\\nuseConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n}\\nOp(\"SRUCell\"){\\nval x = Input(FLOATING_POINT, \"x\"){ description = \"...\" }\\nval cLast = Input(FLOATING_POINT, \"cLast\"){ description = \"...\" }\\nval conf = useConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n\/\/ Just for demonstration purposes\\nSignature(x, cLast, conf)\\n}\\n}\\n```\\n","tokens":156,"id":2936,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0001-kotlin_dsl_as_source_of_truth.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\nencountered while trying to define a language that would work for a few simple examples.\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\ncontributors would have to use.\\nUsing a Java builder pattern based approach is very verbose.\\n","Decision":"We use a Kotlin-based DSL to define Ops.\\n","tokens":218,"id":2939,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0005-optional_parameters_and_signatures.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNot all inputs or args (= parameters) are always required.\\nOften there are sensible defaults available. We want to be able to make those defaults explicit where possible.\\nEven though some parameters may be optional, they might become required in the presence of other optional parameters.\\nWe need a way to explicitly define what combinations are possible.\\n","Decision":"We drop the `optional` property on parameters. Instead, parameters get an additional property `defaultValue`. It can be\\nset to either a fixed literal value (e.g. `7`, `\"something\"`, `null`), an Arg, or it may reference the specific methods\\n`shape()` and `dataType()` on inputs and outputs. Parameters with `defaultValue` specified are treated as optional.\\nTo be able to deal with languages that do not support default values for arguments, Signatures will be specified.\\nSignatures are specified using a `Signature(a,b,c){ \"signature specific documentation\" }` section for each signature.\\nWith the signature specific documentation being optional.\\nSignatures making use of outputs will only be generated for NDArray programming mode, not in SameDiff mode. This also\\nmeans that parameters with a `defaultValue` based on an output will be treated as required in SameDiff mode.\\nIf signatures are specified, only the specified signatures will be generated.\\nIf no signatures are explicitly specified, only the \"all-arg\" and \"no-optional-arg\" signatures will be generated. In\\nNDArray programming mode, the default signatures also include a variant that includes the output.\\n","tokens":71,"id":2940,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0006 - Test architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDL4J was a junit 4 based code based for testing.\\nIt's now based on junit 5's jupiter API, which has support for [Tags](https:\/\/junit.org\/junit5\/docs\/5.0.1\/api\/org\/junit\/jupiter\/api\/Tag.html).\\nDL4j's code base has a number of different kinds of tests that fall in to several categories:\\n1. Long and flaky involving distributed systems (spark, parameter-server)\\n2. Code that requires large downloads, but runs quickly\\n3. Quick tests that test basic functionality\\n4. Comprehensive integration tests that test several parts of a code  base\\nDue to the variety of behaviors across different tests, it's hard to tell what's actually needed\\nfor running and validating whether changes work against such a complex test base.\\nMuch of the time, most of the tests aren't related to a given change.\\nOften times, quick sanity checks are all that's needed in order to make sure a change works.\\nA common set of tags is used to filter which tests are needed to run when.\\nThis allows us to retain complex integration tests and run them on a set schedule\\nto catch regressions while allowing a defined subset of tests to run for a quick feedback loop.\\n","Decision":"A few kinds of tags exist:\\n1. Time based: long-time,short-time\\n2. Network based: has-download\\n3. Distributed systems: spark, multi-threaded\\n4. Functional cross-cutting concerns: multi module tests, similar functionality (excludes time based)\\n5. Platform specific tests that can vary on different hardware: cpu, gpu\\n6. JVM crash: (jvm-crash) Tests with native code can crash the JVM for tests. It's useful to be able to turn those off when debugging.: jvm-crash\\n7. RNG: (rng) for RNG related tests\\n8. Samediff:(samediff) samediff related tests\\n9. Training related functionality\\n","tokens":268,"id":2941,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0002-separate_object_graph_for_serialization.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSerialization and Code Generation have different needs when it comes to how the object graph should be laid out. When\\ngenerating code, it is a lot easier to be able to directly access referenced objects and traverse through the graph.\\nHowever, if the same object graph is used for serialization, the same object will appear in multiple places.\\nThis becomes very apparent when defining constraints. A single constraint might be referring to an input multiple times,\\nand then there can also be multiple constraints that refer to multiple inputs.\\nThe main reason why we want to keep serialization in mind, is that we want to keep code generators in other languages as\\na viable option. Just serializing the graph that is meant to be used during runtime in code generation however, would\\ncan easily become a problem when object identity is required for equality comparisons.\\nAn implementer in a different language would therefore need work through that graph and find identical objects AND would\\nhave to know where that identity is a coincidence and where it is meant to be that way. By creating an object graph that\\nmakes this explicit, we make that work easier.\\n","Decision":"We use two distinct object graphs. One is used for code generation, and the other is used for serialization.\\n","tokens":229,"id":2942,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0004-Mapping_IR.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGenerally, every neural network file format defines a sequence of operations\\nto execute mathematical operations that comprises a neural network.\\nEach element in the sequence is a node that contains information such as the\\ndesired operation, and a set of attributes that represent parameters\\nin to the mathematical function to execute.\\nIn order to write import\/export for different frameworks, we need to adapt\\nan attribute based format from various popular deep learning frameworks.\\nNd4j  has a different list based format for operation execution arguments.\\nIn the [previous ADR](.\/Import_IR.md), we added an IR which makes it easier to\\ninterop with other frameworks.\\nIn this ADR, this work is extended to add a file format for\\ndescribing lists of operations as MappingRules which allow transformations\\nfrom one framework to another.\\nThese transformations manipulate protobuf as input and output Nd4j's\\nnew OpDescriptor format as output.\\n","Decision":"We implement a mapping process framework that defines transforms on an input file format.\\nA MappingProcess defines a list of MappingRules which represent a sequence of transformations\\non each attribute of an op definition.\\nTo assist in mapping, a mapping context with needed information like rule arguments\\nfor transformation, current node, and whole graph are used as input.\\nThe input is a protobuf file for a specific framework and the output is an op descriptor\\ndescribed [here](.\/0003-Import_IR.md).\\nA MappingRule converts 1 or more attributes in to 1 more or arg definitions. A potential definition\\ncan be found in Appendix E.\\nAttributes are named values supporting a wide variety of types from floats\/doubles\\nto lists of the same primitive types. See Appendix C for a theoretical definition.\\nArg Definitions are the arguments for an OpDescriptor described in [the import IR ADR.](.\/0003-Import_IR.md)\\nSee Appendix D for a potential definition of arg definitions.\\nAll of this together describes how to implement a framework agnostic\\ninterface to convert between a target deep learning framework and the nd4j format.\\n","tokens":194,"id":2943,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0004-auto_initialization_for_inplace_operations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome operations work in-place on the inputs that they are given in ND4J, but in SameDiff the same operations will\\ngenerate an array from a given shape. Examples for this include `BernoulliDistribution`, and other random ops, that\\neffectively initialize the array that they are given.\\nFrom a consistency point of view, it would be nice if both API's would support both ways of using those ops.\\n","Decision":"We introduce an option to mark inputs as `inPlace = true` to make it clear that this input is going to be changed\\nin-place. In addition we introduce an option `supportsInPlaceInit = true` to mark an input as initialize-able. If the\\n`supportsInPlaceInit` option is enabled, two signatures for the Op will be created, one that takes an input, and one\\nthat takes the appropriate shape and data type information in its stead.\\n","tokens":92,"id":2946,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0006-op_specific_enums.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":2947,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"deeplearning4j\/0002-ONNX_Runtime.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way of providing nd4j a way of running onnx modules\\nthat is easily compatible with the onnx community. The gold standard for this\\nis is using [onnxruntime](https:\/\/github.com\/microsoft\/onnxruntime\/blob\/master\/docs\/Java_API.md).\\n","Decision":"We will use javacpp's onnxruntime bindings in a similar manner to [nd4j-tensorflow](..\/nd4j-tensorflow)\\nallowing nd4j to be used as an ndarray format that interops with onnxruntime.\\nWe will implement a simple api similar to the [GraphRunner](..\/nd4j-tensorflow\/src\/main\/java\/org\/nd4j\/tensorflow\/conversion\/graphrunner\/GraphRunner.java)\\nThis will sit on top of javacpp's lower level onnxruntime bindings.\\nThis module will follow a similar structure to the nd4j-tensorflow module\\nfocusing on INDArrays as a data interchange format, but otherwise pass execution\\ndown to onnxruntime.\\nThe main api to the graph runner works as follows:\\n```java\\ntry(GraphRunner runner = new GraphRunner(...)) {\\nMap<String,INDArray> inputs = new HashMap<>();\\n\/\/ ..initialize inputs\\nMap<String,INDArray> outputs = runner.run(inputs);\\n\/\/ process outputs...\\n}\\n```\\nThe core logic will contain the following components:\\n1. Loading onnx pb files\\n2. A graph runner in similar nature to nd4j-tensorflow\\n3. Interop with onnxruntime's version of an ndarray\/tensor\\nUsing different accelerators\/backends\\n-----------------------------------------\\nSimilar to nd4j-tensorflow which uses javacpp for the specific version of\\ntensorflow to use, this module will rely on the user picking the right dependency\\nto link against. Different builds of cpu, gpu, .. exist [here](https:\/\/repo1.maven.org\/maven2\/org\/bytedeco\/tensorflow\/1.15.3-1.5.4\/)\\nThe equivalent of this in onnxruntime can be found [here](https:\/\/repo1.maven.org\/maven2\/org\/bytedeco\/onnxruntime\/1.4.0-1.5.4\/)\\nThe user will need to include the version of onnxruntime they wish to use\\nsimilar to how you link against a particular implementation in a c library\\nor include a backend in nd4j. This will happen via maven.\\n","tokens":62,"id":2948,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pcmt\/adr-003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDeploying to the cloud is a fundamental part of increasing software delivery\\nfrequency.  However once the frequency is increased, it becomes increasingly\\nneeded to manage the provisioning and configuration of those cloud resources\\nlest a step is forgotten, a security vulnerability is found, or an instance is\\ncorrupted.  Following patterns found in software development, such as managing\\ninfrastructure as code (IaC) and tracking changes in source control, are\\nwidely regarded as good practice today.\\nMany cloud providers have bespoke tooling that helps manage the creation of\\nresources on their infrastructure.  While this tooling is hugely powerful, the\\ncontext of PCMT is that our cloud resources should be kept as simple as\\npossible, in part due to our constraint to allow for on-prem deployments.\\nFurther an organization that chooses to deploy PCMT as a SaaS may not use the\\nsame cloud provider as the one the PCMT project uses.\\n","Decision":"We will use Terraform to provision cloud resources:  compute instances,\\nstorage, DNS, etc.\\n","tokens":197,"id":2949,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pcmt\/adr-006.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPCMT is intended to be an open and free software product that's aligned with\\nthe [Open Source Definition][osd] and the\\n[Principles for Digital Development][pdd].  To this end PCMT needs to release\\nthe software and documentation under appropriate software licenses.\\nWhile PCMT has it's pick of open-source licenses, our starting point of Akeneo\\ncomes with the OSL v3 license, a copy-left license.\\n","Decision":"- We will abide by the Akeneo PIM Community Edition's [OSL v3][osl3] software\\nlicense. We will include the License and Copyright notice from Akeneo in the\\nroot of every source repository which distributes Akeneo PIM CE.\\n- We will license PCMT software (that which isn't a contribution to Akeneo)\\nunder the [Non-Profit OSL v3][nposl3].  We will include a copy of this license\\nin the root of every source repository created.  We will mark every file\\ncreated with this license notice.\\n- We will license all PCMT documentation and non-software materials under\\n[CC BY-SA 4.0][cc-by-sa]. We will include this mark with all documentation\\nand materials.\\n- We will copyright all software and documentation, that's not a contribution to\\nAkeneo, to [VillageReach][villagereach].  We will include this notice in every\\nfile created.\\n- We will require that all contributions to PCMT be licensed under the\\naforementioned licenses and copyright assigned to VillageReach.  We will\\nnote this in a contribution guide available at the root of the PCMT project.\\n","tokens":98,"id":2950,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pcmt\/adr-007.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pcmt\/adr-005.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe PCMT group is interested in starting from an existing code-base to decrease\\nour time-to-market and make use of our short available project time-line.  To do\\nthis the group created a simple [evaluation process][eval-process] by which we\\n[evaluated 14 mature software products][eval-matrix].  Out of this process a\\n[recommendation][recommendation] narrowed the options down to 2, highlighting a\\nfew business-focused trade-offs between them. These trade-offs were captured in\\ndetail as a [pre-read to project stakeholders][pre-read], with the overall\\nconsensus being that neither option would be a wrong-option.\\n[eval-process]: https:\/\/drive.google.com\/open?id=1vG-kVU5Jfh28g63mI21Lp3OwH4Vmq562101g8VdAoLQ\\n[eval-matrix]: https:\/\/drive.google.com\/open?id=19MLRJvzgIio41XjeefCc6z3ygrjiuVJLC11IxOHppoE\\n[recommendation]: https:\/\/docs.google.com\/document\/d\/1GCAWtYuwSrUBp5zmaP5rCqVXPBdH_Rats4ib7ZZIuoo\/edit?usp=sharing\\n[pre-read]: https:\/\/drive.google.com\/open?id=17CWD5MBOTSuG1HtLHo0JQ_Y6E3L73IuVdIMzdfsJg48\\n","Decision":"We will build PCMT from [Akeneo PIM Community Edition][akeneo-ce].\\nWe will sign the Akeneo SAS [Contributor's Agreement][contrib-agreement] should\\nwe need to patch or otherwise contribute to Akeneo PIM Community Edition.\\nWe will maintain an active down-stream fork of the Akeneo PIM Community Edition\\ngit [repository][repo].\\nWe will make every effort to be forward compatible with future Akeneo PIM\\nCommunity Edition releases.\\n[akeneo-ce]: https:\/\/www.akeneo.com\/community-edition\/\\n[contrib-agreement]: https:\/\/www.akeneo.com\/contributor-license-agreement\/\\n[repo]: https:\/\/github.com\/akeneo\/pim-community-dev\\n","tokens":323,"id":2952,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pcmt\/adr-004.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhile PCMT's basic unit of deployment is Docker and Docker-Compose\\n([ADR #2](adr-002.md)), and Terraform is able to provision a cloud environment\\n([ADR #3](adr-003.md)), we still need to address a gap where the computing\\nenvironment needs to be provisioned and client configuration needs to be loaded.\\n","Decision":"We will use ready-made Ansible roles to install the latest versions of\\nDocker and Docker-Compose, utilized through Terraform.\\nWe will build PCMT Ansible role's that will serve as a template and are\\navailable in the Ansible Galaxy repository.\\nConfiguration of a PCMT instance will be managed as part of an Ansible Playbook.\\n","tokens":75,"id":2954,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pcmt\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe need to deliver PCMT in such a way that it's possible for interested\\norganizations to provide it at a reasonable cost in a SaaS model, as well\\nas for government organizations to have control over their tools and data by\\ninstalling it on-prem calls for a containerization tool that has wide adoption\\nand which can run in the major host operating systems.\\n","Decision":"We will provide each component of PCMT as a production ready docker image.\\nWe will utilize docker-compose to orchestrate the docker containers that a\\nsingle PCMT instance requires.\\n","tokens":80,"id":2955,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0004-disable-group-chats.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.\\nGroup chat is not as vital as 1-1 chat for the chat experience. It is also\\nlikely not as useful for an MVP as public chat is, since this gives people a\\nchance to talk to strangers and it is open by default.\\nWhether group chats are in scope or not has been a recurring discussing and a\\nlot of effort has been wasted related to this.\\nThere are currently a lot of outstanding regressions related to group chat: https:\/\/github.com\/status-im\/status-react\/labels\/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.\\n","Decision":"Disable group chat for beta and don't work on bugs related to it until after. See https:\/\/github.com\/status-im\/status-react\/issues\/3995\\nThis ensures we can release beta without blockers, and then we can take some\\ntime to fix group chats properly.\\n","tokens":161,"id":2956,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0003-geth-node.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOn the path to releasing a usable beta we've faced a variety of performance\\nissues:\\n- https:\/\/github.com\/status-im\/ideas\/issues\/55\\n- https:\/\/ideas.status.im\/ideas\/071-low-traffic\\n- https:\/\/ideas.status.im\/ideas\/076-smooth-ui\\n- https:\/\/ideas.status.im\/ideas\/083-energy-efficient\\nSome of these performance issues are related to the use of LES, especially on\\nMainnet. Examples of problems include: slow\/hot sync, disk filling up, bandwidth\\nusage, etc. This can partly be remedied by the use of CHT to speed up sync\\n(https:\/\/github.com\/status-im\/status-go\/wiki\/Updating-CHT-BloomTrie).\\nIn order to simplify the problem, we decided earlier this year to not spend too\\nmuch time looking into performance improvements related to LES, instead focusing\\nour efforts on improving the performance on Whisper and in client-side code.\\nThis also means we don't have a full picture of to what extent LES is a\\nperformance bottleneck for the Status app. It is possible proper use of CHT,\\nslowing sync down, vary load depending on if you are on WiFi or not, etc, can\\nsolve these problems. However, the consensus right now is that ULC is the more\\npromising route to take.\\nHowever, since ULC isn't ready yet, this means we are relying on Infura. This\\nis a bad idea from a decentralization and security point of view. This can be\\npartly remedied by allowing the user to run an upstream node. It is still a bad\\ntrust model, relying on third-party Ethereum nodes, and we want to get away\\nfrom this as soon as possible.\\nWith ULC it is possible to connect to more than one node, which increases\\ndecentralization and security.\\n","Decision":"1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases\\nQ2 we will provide two ways of accessing the Ethereum network:\\n- Upstream RPC with Infura (default)\\n- Custom upstream node (https:\/\/github.com\/status-im\/status-react\/issues\/3994)\\nFrom a decentralization point of view we need the latter before we can release\\nthe beta.\\n2. ULC MVP. Additionally, as soon as beta is released, assess and start to\\nintroduce ULC as an experimental non-default mode in the app.\\nBy doing this we also support the network, since it is still a very new\\nprotocol. ULC requires trusted peers to operate, and right now trust is\\nestablished by config (list of trusted peers).\\nInitially we could hardcode these nodes, with option to add your own. Later on\\nwe can have some form of distributed directory with a rating system. This also\\nlikely requires merging ULC upstream, so we aren't the only ones running trusted\\nnodes.\\n3. LES track (optional). If we still have resources and there's interest,\\noptionally run a parallel track to clean up LES usage (update CHT, cluster) and\\nunderstand exact performance impact and viablity of running LES on\\nresource-constrained devices.\\nThese three decisions balances the following trade-offs:\\n- get a usable beta released as soon as possible\\n- provide users with options to get minimal amount of decentralization\\n- pave the way for trustless usage by default (ULC\/LES node)\\n","tokens":388,"id":2957,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0005-discovery-protocol.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently peers for messaging are hardcoded in the app. This means we can't\\neasily scale, as some of these nodes might fill up when we have more than N\\nactive users.\\nDiscovery v5 protocol (https:\/\/ideas.status.im\/ideas\/092-disc-v5-research,\\nhttps:\/\/github.com\/fjl\/p2p-drafts\/blob\/master\/discv5-eip.md) allow us to\\ndiscover new peers from the app and scale server peers in our cluster\\nindependently. This is vital for high availability\/reliability and security.\\n","Decision":"- Use and enable Discovery v5 for all networks\\n- Do this before beta is released\\nThis means we have capacity to achieve 5000+ DAU \/ 500 concurrent connection. It\\nseems unlikely we can do so with reliable messaging in the current setup.\\nTrade-off for this is:\\n- Uncertainty in introducing a new protocol leading to possible delays\\n- Possible increase in CPU\/Network (we can disable protocol once peers are found though)\\n","tokens":121,"id":2960,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0013-tribute-to-talk.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and \"cold\" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.\\nThe whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.\\n","Decision":"Considering:\\n- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)\\n- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be pratical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)\\n- the limited time to develop the feature\\nWe opted for a solution that:\\n- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it\\n- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment\\n- doesn't store tribute information on-chain\\n","tokens":79,"id":2961,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0006-wallet-compatibility.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIdea [142-Wallet-Compatibility](https:\/\/ideas.status.im\/ideas\/142-wallet-compatibility\/README) changes the way we generate keys starting from the 12 mnemonic words.\\nThis change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.\\n### Breaking Changes\\nThe 12 mnemonic words will generate different addresses and whisper identities (public key). We should communicate this properly and recommend users to create new accounts.\\n### Features\\nIf users used a dapp in an external app\/wallet, they would be able to continue to use the same dapp with the same account in Status.\\n","Decision":"* update the BIP39 seed generation to use the salt `\"mnemonic\"` instead of `\"status-im\"` following [BIP39](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0039.mediawiki#from-mnemonic-to-seed).\\n* update the master key generation using the constant `\"Bitcoin seed\"` instead of `\"status-im\"`, following [BIP32](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0032.mediawiki#master-key-generation).\\n* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device).\\n### Relevant reading\\n* [BIP39](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0039.mediawiki)\\n* [BIP32](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0032.mediawiki)\\n* [BIP44](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0044.mediawiki)\\n","tokens":139,"id":2962,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0002-extension-uri.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"manuela\/0006-single-node-edge-server.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context [2. Overall context and Container Platform](0002-overall-context-and-container-platform.md)\\n## Context\\nEdge Node is part of OpenShift Cluster\\n+ OS management done by OCP\\n+ Network namespace can extend to edge node\\n+ supported config\\n+ ArgoCD support\\n- Additional load due to OCP services on edge node\\n- firewall impact needs to be assessed (Bosch requested secure, unidirectional communication between zones)\\n- The line data server services must run autonomously without network connection to the master for certain time period (30-60min?).\\nEdge Node is standalone RHEL w\/ standalone Kubelet + git sync\\n+ kubelet can start\/stop pods based on filesystem contents\\n- Support questionable\\n- No integration in ArgoCD\\n- OS Management\\n+ firewall compatible (connection only to git server + container registry)\\nEdge Node Is standalone RHEL w\/ custom Operator running on Edge node\\n(Operator looks at CRD 'RemotePod' and creates Pods on Podman \/ has finalizer to remove Pods from Podman)\\n- effort to write operator\\n- Support\\n+ ArgoCD compatible\\n- OS Management\\n+ firewall compatible (only to k8s API server + container registry)\\n+ feedback to user\\nEdge Node Is standalone RHEL w\/ standalone Podman\\n(unclear which mechanisms starts\/stops pods on podman)\\n- No integration in ArgoCD\\n- OS Management\\n+ aligned with RHEL Edge strategy\\n+ firewall compatible (connection only to git server + container registry)\\nEdge Node is single-node \/ three-node OCP (KVM?)\\n+ ArgoCD Compatible\\n-\\nEdge Node + centralized Operator via SSH (e.g. on bastion host)\\n","Decision":"TBD - The change that we're proposing or have agreed to implement.\\n","tokens":365,"id":2967,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"manuela\/0004-argocd-as-deployment-agent.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context [2. Overall context and Container Platform](0002-overall-context-and-container-platform.md)\\nRelated [3. Gitops Approach](0003-gitops-approach.md)\\nRelated [5. Kustomize as templating tool](0005-kustomize-as-templating-tool.md)\\n## Context\\nWe need a deployment agent to execute the deployment information stored in git. At the time of writing, multiple such tools exist:\\n* Razee\\n* ArgoCD\\n* WeaveWorks Flux\\n* ...\\nOn 2019-11-14, ArgoCD and WeaveWorks announced to work together to develop ArgoFlux as next-gen GitOps tool.\\nIn addition, it is feasible to build an automated deployment of git content to kubernetes via Ansible.\\n","Decision":"ArgoCD since\\n1) proofs-of-concept and RHTE lab sessions showed it to be easy to use and powerful\\n2) Red Hat reference architectures seem to be most closely aligned\\n3) is available today\\n4) does not require engineering effort\\n","tokens":167,"id":2970,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"manuela\/0007-how-to-implement-multiple-demo-instances.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen multiple demo instances exist in parallel, we need to avoid to step on each others toes. The scope is\\n- the manuela-gitops repository, where component deployment and configuration takes place as part of a demo run\\n- the manuela-dev repository, where coding changes take place as part of a demo run\\nOptions:\\n1. **BRANCHES** inside the existing sa-mw-dach repos, for each manuela demo env there would be a branch (e.g. name based on a convention).\\nPros:\\n- Easiest to set up since it's literally just cloning the central repos.\\n- only need to adjust branch names for new demo instance (however the number of required adjustments is likely the same)\\nCons:\\n- need to coordinate the creation\/assignment of branches\\n- all demoers need write access to the repos\\n- Danger of cluttering the repo with \"abandoned\" branches\\n2. **FORKS**: The \"owner\" of the new installation forks all repos in GitHub or creates new repositories from scratch. The changing remote URL needs to be adjusted when setting up the demo. Preferably, there would be somewhere one central configuration (GIT_BASE_URL) defining from which fork to pull from.\\nPros:\\n- demo instances can be set up without any coordination\\n- No conflicts when branches are used for e.g. production staging approvals\\nCons:\\n- need to adjust\\n- pipeline configs (pointing to forks of manuela-dev\/manuela-gitops)\\n- argoCD applications (poingint to forks of manuela-gitops)\\n3. **DEDICATED GIT**: We could deploy a dedicated GIT(LAB?) to a namespace, place the manuela-gitops +  manuela-dev repos there. As long as we can use only cluster-internal URLs these will not change across instances.\\nPros:\\n- Consistent URLs at least in a single-cluster scenario, no need to adapt anything\\nCons:\\n- Will require similar adaptations as **FORKS** in a multi-cluster scenario, since we will need to use external URLs for cross-cluster communication\\n","Decision":"Use **FORKS**: this requires least coordination, access rights or additional components.\\n","tokens":439,"id":2971,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"manuela\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2972,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"manuela\/0008-need-for-time-series-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe expect sensor data that is collected to be (temporarily) stored in Kafka as distributed event log, from where it can easily be moved to other sinks and replicated into other enviroments such as the central DC. It is not really used as a durable persistence layer.\\nData processing at the edge however requires more than just the ability to buffer and process discrete events. It especially requires long-term persistence and the ability to retrieve data and perform calculations over time windows, e.g. to render graphs or the ability to look up historic values.\\nThese kinds of requirements are typically addressed in time series databases. A Timeseries database is used to store and analyze time-series data. They typically include functions and operations related to time-series data analysis such as data retention policies, continuous queries, flexible time aggregations, etc.\\nA primary purpose of a TSDB is to analyze change over time. Traditional DBs don't scale well for inserting and querying data points.\\nCharacteristics of time-series data:\\n* Append only (inserts only)\\n* Inserts ordered by time\\n* time is primary axis\\nIntroducing this additional component to the architecture means it needs to be deployed, managed, requires backup and restore functionality, has runtime footprint, etc.\\nWe consider the data in the time series DB to be authoritative and the data in Kafka to be ephemeral. Discrepancies between the data in Kafka and the data in the Time Series DB may occur when the TSDB has not yet caught up ingesting the Kafka data (a transient problem) or because it has a longer retention period than the Kakfa brokers.\\n","Decision":"We want to introduce a time series database.\\n","tokens":331,"id":2973,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architectural-decision-log\/0006-hamlet-docker-layer-release-mgmt.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe hamlet engine is made up of a collection of software components which are based on a number of different platforms ( bash, freemarker, java ).\\nOur current distribution process collects these components into a multipurpose docker image that can handle most build and deploy tasks.\\nThe collection process uses git to pull down the code from the software components and includes this in the image\\nThe problem with this process is that it really only works in a controlled docker environment, making this image multipurpose has resulted in a large docker image and installing other versions of hamlet requires pulling a new image.\\n## Decision Drivers\\n* Having a simple installation process for user local installations or integration in existing processes\\n* Simple version management support with a focus on handling the usage of features under active development within deployment pipelines\\n* A tested and verified release process\\n","Decision":"* Having a simple installation process for user local installations or integration in existing processes\\n* Simple version management support with a focus on handling the usage of features under active development within deployment pipelines\\n* A tested and verified release process\\nChosen `Docker image layer as component artefacts with support for installing these images` as this is the most flexible option and provides a foundation for managing the distribution of hamlet code\\n### Positive Consequences\\n* Establishes a process for distributing packages which supports the different platforms we use\\n* Opens up support for enhancing our release process to be safer and more flexible\\n* Doesn't add too much overhead and is reasonably straightforward to understand\\n### Negative Consequences\\n* Is essentially implementing our own package manager which a lot of people already have and implement\\n* Doesn't necessarily cover off the software platform dependencies but at least makes it get the scripts that can tell us that easily\\n","tokens":174,"id":2974,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architectural-decision-log\/0002-layer-deployments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want a way to define and manage the deployment of resources which are shared across multiple solutions.\\nA number of services offered by cloud providers, namely AWS have services which can only be deployed once per AWS Account or enable a service across an entire account\\nHow should we manage these resources to scale with our plugin support and to support the features we have introduced for our other deployments\\n## Decision Drivers\\n* Support required for multi region deployments to allow global image registry\\n* Deprecation of the fragment and case style template processing which is currently used for account level deployments\\n","Decision":"* Support required for multi region deployments to allow global image registry\\n* Deprecation of the fragment and case style template processing which is currently used for account level deployments\\nChosen option: \"Extend occurrence support to work at any layer\", because this allows for greater flexibility over the resources we can deploy and aligns with our other deployment processes\\n### Positive Consequences\\n* Occurrences will be the approach we use for all deployments\\n* Opens support for any occurrence features that we have in place already or develop in the future\\n* Makes our existing deployment model easier to follow ( our current tiers deployments will belong to the Segment and show that relationship )\\n### Negative Consequences\\n* Complexity in extending the occurrence building process\\n* May introduce some complexity in managing default and required singleton instances of deployments. We can manage this and could potentially be a useful feature\\n","tokens":120,"id":2976,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architectural-decision-log\/0001-executor-engine-separation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHamlet is currently built on two application stacks, a set of bash scripts which run deployments and invoke the generation of outputs, and a freemarker wrapper with supporting templates which handles the creation of outputs. There is a tight coupling between these components and it is difficult to determine the authoritative source of context which we share between the these to stacks.\\n## Decision Drivers\\n* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\n","Decision":"* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\nChosen option: \"Create a clearly defined abstraction layer between the existing bash scripts and the freemarker wrapper\"\\nMakes our architecture cleaner and provides a clear way for hamlet to scale in the future\\n### Positive Consequences\\n* We have more explicit definitions of what hamlet is made of and how it works\\n* We can make choices on appropriate tools for specific services within hamlet\\n### Negative Consequences\\n* Defining and building the layer separation will add complexity to hamlet\\n","tokens":142,"id":2977,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architectural-decision-log\/0007-placement-support.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nFor some time hamlet has had the idea of a `placement` - a mechanism to dynamically determine where resources should be hosted.\\nPlacements are currently part of the hierarchy of the state tree, and the resource groups within the occurrence structure were intended to allow groups of resources to be placed into different provider accounts.\\nHowever to date, a number of limitations and inefficiencies stemming from hamlet's original design has prevented full exploitation of the placement concept;\\n- the assembly of the solution before the engine is run\\n- the focus of a template run on a single segment\\n- the generation of account templates uses different concepts to those of product templates\\n- the maintenance of critical configuration information such as environment to account mappings external to the cmdb.\\nThe introduction of dynamic cmdb loading and the input pipeline processor with full cmdb qualification presents an opportunity to address these long standing issues and complete the implementation of placement support. The question is how.\\n## Decision Drivers <!-- optional -->\\n* reuse existing concepts and ways of doing things as much as possible\\n* given a cmdb, no other configuration information should be necessary to deploy a product\\n","Decision":"* reuse existing concepts and ways of doing things as much as possible\\n* given a cmdb, no other configuration information should be necessary to deploy a product\\nChosen option: \"Extend link semantics\", because it addresses the decision drivers best (see below) and aligns with the ADR-0002 decision previously made.\\n### Positive Consequences <!-- optional -->\\n* reduced configuration complexity - everything in one place\\n* better validation of configurations\\n* links have proven very flexible so expect the proposed changes to be re-purposed for other things in the future\\n* straightforward transition process once infrastructure in place\\n### Negative Consequences <!-- optional -->\\n* Expansion of semantics of existing concepts may be harder to document, understand and explain than dedicated new concepts\\n","tokens":236,"id":2978,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architectural-decision-log\/0004-provide-error-codes-on-handled-exception.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHamlet Deploy is not a compiled binary file nor is it written in a single programming or scripting language. It is a combination of all of the above;\\n* Java for the Freemarker wrapper,\\n* Freemarker has its own DSL,\\n* the original Hamlet Deploy Executor is presently primarily Bash scripts and\\n* the \"Click\" CLI Executor is written in Python.\\nWhen a handled exception occurs, there is no consistency in the error information provided. A handled error in Freemarker is likely to provide a large JSON object for review, whilst an error in the Bash Executor will provide a brief summary of the problem. Because these summaries are not always unique-per-error, many provide only a generic error message.\\n","Decision":"Chosen option: introduce logging classes, that group together messages of a simmilar kind. Later we will re-evaluate to determine if a more specific approach to error handling (such as individual error codes outlined above) would remain useful \/ necessary and if so can build upon this framework.\\n### Manually-maintained library of error codes\\n* Good - will provide a method for providing tailored error messages and possible fixes to the user\\n* Good - could be extended to provide an avenue for review of the most-encountered error messages\\n* Good - improved reporting \/ communication around errors experienced\\n* Good - consistency across the spectrum of potential error sources\\n* Bad - increase in maintenance overhead\\n* Bad - may be a challenge to enforce consistently\\n* Bad - difficult to make extensible by community content\\n### Logging classes\\n* Good - provide a method for tailoring messages, whilst allowing for re-usability\\n* Good - accessible to community content\\n* Good - consistency across the spectrum of potential error sources\\n* Good - framework could be extended \/ upgraded at a later time\\n* Good - minor maintenance overhead once established\\n* Bad - less specificity is possible when compared to unique error messages\\n","tokens":151,"id":2979,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"grout\/adr-2-rename-package.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"aoide-rs\/0002-use-milliseconds-for-encoding-audio-positions-and-durations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPositions and durations in digital audio could be represented using different\\nmeasurements:\\n* time-based, e.g. in seconds or sub-seconds\\n* sample-based, i.e. in *sample frames* (1 frame = samples from all channels)\\nThe sample-based measurement must always consider complete sample frames with\\none sample from each channel (interleaved) to avoid any dependencies on the\\nactual number of channels.\\nThe value of a sample-based measurement will vary depending on the actual sample\\nrate while the time-based measurement is independent of the sample rate.\\nTime-based measurements could be represented by different units. A common\\nrepresentation are seconds (SI unit) stored with fractional digits, i.e.\\na floating-point number or fixed-point representation.\\nDepending on the use case an integer representation might be sufficient\\nif sub-seconds are used as the unit, i.e milliseconds or microcseconds.\\nMilliseconds should provide an acceptable compromise between readability\\nand precision.\\n","Decision":"We will use time-based measurements for encoding positions and durations in\\ndigital audio streams to avoid dependencies on the actual encoding (sample rate,\\nnumber of channels) of the data.\\nValues will be encoded as floating-point numbers using milliseconds as the unit.\\n","tokens":199,"id":2983,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"aoide-rs\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2984,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"upd-articles-api\/ADR-003-16012019-configure-checkstyle-findbugs-support.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**\\nCode quality is very important and we need some way to ensure the code we are sending to production is clean code based on good practices and without bugs that can cause some unexpected behaviors. Sonar is a good option in our Integration server but how we can check our code in the local environment?\\n**Decision**\\nThere are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\n","Decision":"There are two very famous plugins to help us. [Findbugs](http:\/\/findbugs.sourceforge.net\/) will scan the code and detect any possible bug that can cause an unexpected exception or some vulnerability in our code. Same way [Checkstyle](http:\/\/checkstyle.sourceforge.net\/) will help us to be aligned with the code standard depending on the platform, in this case Java.\\n**Status**\\nAccepted\\n**Consequences**\\nBoth plugins will be executed when the build command is executed. Currently Findbugs is activated to prevent build to be done if detects some code vulnerability or bug. It will generate a report in HTML code to analyze the point of failure. Checkstyle is configured to alert us about the code bad syntax, but will not stop the build.\\nThese two configurations can be modified on the `build.gradle` file.\\n","tokens":241,"id":2985,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"upd-articles-api\/ADR-001-16012019-configuring-lombok.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**\\nCurrent application was generated by Spring Initializr platform. We want to configure come useful libraries to avoid repeated code through our model classes.\\n**Decision**\\nLombok (https:\/\/projectlombok.org\/) is an useful library to avoid writing of getter, setter, constructor methods and allows to implement builder pattern just using annotations.\\n**Status**\\nAccepted\\n**Consequences**\\nOnce implemented the Lombok dependency in the `build.gradle` file, we need to download the right plugin for our IDE in order to work without any compilation problems.\\n- For IntelliJ (And other JetBrains tools): https:\/\/plugins.jetbrains.com\/plugin\/6317-lombok-plugin\\n- Once downloaded, enter to Preferences -> Build, Execution, Deployment -> Annotation Processors and enable the annotation processing.\\nNow we are ready to start using the Lombok features (https:\/\/projectlombok.org\/features\/all).\\n","Decision":"Lombok (https:\/\/projectlombok.org\/) is an useful library to avoid writing of getter, setter, constructor methods and allows to implement builder pattern just using annotations.\\n**Status**\\nAccepted\\n**Consequences**\\nOnce implemented the Lombok dependency in the `build.gradle` file, we need to download the right plugin for our IDE in order to work without any compilation problems.\\n- For IntelliJ (And other JetBrains tools): https:\/\/plugins.jetbrains.com\/plugin\/6317-lombok-plugin\\n- Once downloaded, enter to Preferences -> Build, Execution, Deployment -> Annotation Processors and enable the annotation processing.\\nNow we are ready to start using the Lombok features (https:\/\/projectlombok.org\/features\/all).\\n","tokens":187,"id":2986,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"upd-articles-api\/ADR-002-16012019-add-docker-support.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**\\nThe application can run on any machine with JVM installed, but what if we don't have a correct Java version in our machine or server environment? Containers comes to the rescue.\\n**Decision**\\nDocker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n","Decision":"Docker (https:\/\/www.docker.com\/) is an easy to configure container platform to manage our applications in isolated containers. That will help us to deploy our application in any environment with the Docker daemon installed.\\n**Status**\\nAccepted\\n**Consequences**\\nBy the moment the container can be used just for deploys, development support can be added sharing a Volume in the Dockerfile. Instructions to modify the Dockerfile can be found [here](https:\/\/docs.docker.com\/engine\/reference\/builder\/).\\n","tokens":150,"id":2987,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"upd-articles-api\/ADR-004-16012019-configuring-integration-test.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**\\nIn order to make sure we are retrieving the expected status and object responses from our API, we need to add an integration test layer in order to ensure the API is working as a single unit.\\n**Decision**\\nWe gonna use the built-in Spring test module and [RestAsured](http:\/\/rest-assured.io\/) in order to make request to the API in a more human readable way.\\n**Status**\\nAccepted\\n**Consequences**\\nTo execute the integration tests we just need to execute the `intTest` task configured in our `build.gradle` file.\\nThe main context is loaded from the Core application, in the future this can be configured in a own context loader for integration test.\\n","Decision":"We gonna use the built-in Spring test module and [RestAsured](http:\/\/rest-assured.io\/) in order to make request to the API in a more human readable way.\\n**Status**\\nAccepted\\n**Consequences**\\nTo execute the integration tests we just need to execute the `intTest` task configured in our `build.gradle` file.\\nThe main context is loaded from the Core application, in the future this can be configured in a own context loader for integration test.\\n","tokens":153,"id":2988,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oauth-spike\/adr-03-prettier.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCode formatters programmatically apply consistent formatting to a codebase to ensure\\nconsistent style. The intended goals are improving readability and reducing bikeshedding\\nover code style.\\nFor JavaScript, this trend may have started with `eslint --fix` but gained momentum with\\n[Prettier](https:\/\/prettier.io\/). This trend is not specific to JavaScript (consider gofmt\\nand rustfmt).\\nAt Guidewire:\\n- The InsuranceNow code base has recently adopted autoformatting\\n- The Enterprise pod uses Prettier extensively\\n","Decision":"We will use Prettier as a precommit git hook. Developers can also install IDE plugins to\\napply Prettier on file save.\\n","tokens":115,"id":2990,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oauth-spike\/adr-01-rollup.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA module bundler combines multiple files into fewer files (this is likely a single file\\nfor libraries but may be multiple files for web apps). Common choices include Webpack and\\nmore recently rollup.js; less common choices include Parcel and perhaps SystemJS.\\nWhile Webpack is currently the most popular bundler for web applications, rollup.js is\\nan increasingly popular choice for libraries. A notable example of a library using rollup.js is\\n[React](https:\/\/github.com\/facebook\/react\/tree\/master\/scripts\/rollup).\\n### Rollup.js Pros\\nRollup.js produces smaller bundles (due to treeshaking and omitting a runtime).\\n### Rollup.js Cons\\n- Smaller community. This means fewer plugins and less unofficial documentation\\n(blog posts, StackOverflow questions, etc).\\n- Missing support for code-splitting and hot module reloading (due to missing runtime). This\\nis a bigger concern for web apps.\\n","Decision":"We will use [Rollup.js](https:\/\/rollupjs.org\/guide\/en) for the smaller bundle size (~4K savings).\\nHowever, it's unclear how significant this benefit will be to end users and we can re-evaluate\\nif needed.\\n","tokens":194,"id":2991,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"eq-translations\/0001-context-in-translations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# 1. Context in Translations\\n## Context\\nFor some translatable strings, we create a message context (`msgctxt`) that contains the question text. This means strings can be translated in the context of the question being asked. There are two issues with this approach:\\n- For some elements the context is incorrect. For example, the context for an answer guidance title is `\"Answer for: {question_text}\"`.\\n- Most translatable strings at the question level and below don't have context. This causes problems when the English text doesn't vary between question variants, but the translated text needs to - https:\/\/trello.com\/c\/JEdbgtlT\/3538-e-translations-add-context-to-all-child-elements-of-a-question-m\\n","Decision":"- Remove the current message context `Answer for: {question_text}`.\\n- Use a new `Question: {question_text}` message context for all translatable properties within a question other than title. This allows any text within a question to vary based on the question.\\n- Add a new `Type: {property_type}` comment for every string extracted based on the values in the table below e.g. `Type: Question definition list item`. This helps a translator understand what they are being asked to translate.\\n| JSON Path | Type | Notes |\\n|-----------|------|-------|\\n| `$.title` | Questionnaire title | |\\n| `$.legal_basis` | Questionnaire legal basis | |\\n| `$.messages` | Global answer error message | `message` rather than `messages` as each message is an individual translation |\\n| `$.sections[*].title` | Section title | Used for section summaries, final summary and the hub |\\n| `$.sections[*].repeat.title` | Section title (repeating section) | |\\n| `$.sections[*].summary.items[*].title` | Custom section summary item title | |\\n| `$.sections[*].summary.items[*].add_link_text` | Custom section summary list add link | |\\n| `$.sections[*].summary.items[*].empty_list_text` | Custom section summary empty list text | |\\n| `$..groups[*].title` | Group title | Used as heading in summaries |\\n| `$..blocks[*].title` | Block title | Deprecated? |\\n| `$..blocks[*].summary.title` | List collector summary heading | Deprecated when section summary blocks are removed from section definitions. |\\n| `$..blocks[*].summary.item_title` | List collector summary item | Defines how to form the string to summarise each item in the list. Strictly speaking this doesn't need to be translated for all current usages |\\n| `$..add_block.cancel_text` | List collector add block cancel link | |\\n| `$..content.title` | Content page heading | The main heading on a Content page |\\n| `$..content.contents[*].title` | Content page contents title | |\\n| `$..content.contents[*].description` | Content page contents description | |\\n| `$..content.contents[*].list` | Content page contents list | |\\n| `$..question.title` | Question text | |\\n| `$..question.description` | Question description | |\\n| `$..question.instruction` | Question instruction | Instructions for field interviewers |\\n| `$..question.definitions[*].title` | Question definition link | |\\n| `$..question.definitions[*].contents[*].title` | Question definition title | |\\n| `$..question.definitions[*].contents[*].description` | Question definition description | |\\n| `$..question.definitions[*].contents[*].list` | Question definition list  | |\\n| `$..question.guidance.contents[*].title` | Question guidance title | |\\n| `$..question.guidance.contents[*].description` | Question guidance description | |\\n| `$..question.guidance.contents[*].list` | Question guidance list | |\\n| `$..answers[*].validation.messages` | Answer error message | |\\n| `$..answers[*].label` | Answer label | |\\n| `$..answers[*].description` | Answer description | |\\n| `$..answers[*].playback` | Relationships playback template | Used for playback before a relationship is chosen e.g. \"{person2} is {person1}'s ...\" |\\n| `$..answers[*].options[*].label` | Answer option | |\\n| `$..answers[*].options[*].description` | Answer option description | |\\n| `$..answers[*].options[*].detail_answer.label` | Detail answer label | |\\n| `$..answers[*].options[*].detail_answer.description` | Detail answer description | |\\n| `$..answers[*].options[*].title` | Relationships answer option question text | Question text to set when this answer option is selected e.g. \"Thinking about {person1}, {person2} is their husband or wife\"|\\n| `$..answers[*].options[*].playback` | Relationships answer option playback text | Used for playback when a relationship is chosen e.g. \"{person2} is {person1}'s husband or wife\" |\\n| `$..answers[*].guidance.show_guidance` | Answer guidance show link | |\\n| `$..answers[*].guidance.hide_guidance` | Answer guidance hide link | |\\n| `$..answers[*].guidance.contents[*].title` | Answer guidance title | |\\n| `$..answers[*].guidance.contents[*].description` | Answer guidance description |\\n| `$..answers[*].guidance.contents[*].list` | Answer guidance list | |\\n- Where it is helpful to reference another property value, an additonal comment should be added. Initially this will be:\\n| JSON Path | Comment |\\n|-----------|---------|\\n| `$..answers[*].description`                           | For answer label: {answer label} |\\n| `$..answers[*].options[*].description`               | For answer option: {answer option label} |\\n| `$..answers[*].options[*].detail_answer.label`       | For answer option: {answer option label} |\\n| `$..answers[*].options[*].detail_answer.description` | For answer option: {answer option label} |\\n| `$..answers[*].options[*].title`                     | For answer option: {answer option label} |\\n| `$..answers[*].options[*].playback`                  | For answer option: {answer option label} |\\n### Example\\nFor a detail answer \"Enter main language\":\\n```\\n#. Type: Detail answer label\\n#. For answer option: Other, including British Sign Language\\nmsgctxt \"Question: What is <em>{person_name_possessive}<\/em> main language?\"\\nmsgid \"Enter main language\"\\nmsgstr \"\"\\n```\\n","tokens":154,"id":2992,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ErisCasper-Data\/adr-003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFields in the JSON structures used by the Discord API can be optional, nullable, or both\\n(see [here](https:\/\/discordapp.com\/developers\/docs\/reference#nullable-and-optional-resource-fields)).\\nThis means that we can not treat `Optional.empty()` and `null` the same.\\nAdding a `@Nullable` annotation to an `Optional<T>` field is not handled well by Immutables.\\nIt will not generate the methods need to make using such a field in a builder 'nice'.\\nImmutables supports [custom encodings](https:\/\/immutables.github.io\/encoding.html) for types.\\nIt does not allow detecting the encoding to use via annotation.\\nIt requires that the encoding be compiled before the main compilation stage.\\n","Decision":"We will create a `Nullable<T>` class, similar in functionality to `Optional<T>`.\\nWe will create an encoding for that class, making the builder easier to use for\\nfields of that type.\\n","tokens":161,"id":2993,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ErisCasper-Data\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSerializing and deserializing JSON in Java works best when performing it to\/from plain old java objects (POJOs).\\nWhile it is possible to serialize into generic `JsonNode` or `Map` objects this results in untyped values that\\nmust be referenced via String keys, meaning the compiler is limited in what it can check for us.\\nThe classes required for JSON serialization and deserialization are reptitive and error prone to write by hand.\\n[Immutables](https:\/\/immutables.github.io\/) helps to some extent, but there are still many classes and fields to be written.\\nThe [Discord API documentation](https:\/\/discordapp.com\/developers\/docs\/intro) is mostly consistent with how it documents\\nthe JSON structures it returns and accepts.\\nCode generation is often difficult to use with IDEs.\\nCode is often referring to classes that will not be committed, so we depend upon the IDE to generate these classes\\nso that the IDE can provide useful feedback regarding compilation errors.\\n","Decision":"We will use code generation to generate the POJOs to be used for JSON serialization and deserialization.\\nWe will generate classes that make use of Immutables.\\n","tokens":208,"id":2994,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ErisCasper-Data\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe will be generating a number of classes as per [ADR 1](adr-001.md).\\nGenerated classes should be generated during the build process and not committed to GitHub.\\nThis ensures we have reproducable builds and makes it clear that generated classes are not to be edited manually.\\nWe wish to publish artifats to the central maven repository.\\nThis does not necessarily limit us to working with Maven, but it encourages us to stick it.\\nUsing a compiled language to run the code generation would require two passes of compilation\\n(or another\/project module) - one to compile the codegen scripts, another to run it.\\n","Decision":"We will use ruby to write the code generation scripts.\\nWe will run them using the [jruby-maven-plugins](https:\/\/github.com\/torquebox\/jruby-maven-plugins) as part of the\\n`generate-sources` phase of the\\n[maven lifecycle](https:\/\/maven.apache.org\/guides\/introduction\/introduction-to-the-lifecycle.html)\\n","tokens":134,"id":2995,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0005-using-aws-ec2-as-platform.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI'm having lots of issues trying to detect and inject the IP address of the\\ncontainer to the env var so the etcd cluster can get up.\\nI can start it in single node, but I have no progress with the clustering.\\nI'm running out of time...\\nTo speed up deployments, I can create my own AMI images with\\nall the needed packages, and launch them from the provisioning system,\\nTerraform.  Then, using a Configuration Management Tool like Puppet or\\nAnsible I could finish the configuration to adapt to the environment.\\n","Decision":"After failing trying to use AWS ECS to launch an etcd cluster, I have switched\\nto use AMI's instead of Docker containers.\\nThis way I have to do a lot of things, but I think it will be easier to see\\nprogress.\\nI'm going to build my own images using Packer.  I used veewee before, but\\nPacker is not so different.\\nAnother option could be to use a cloud-init and a APT repository, but I have\\nnot enough time to make everything it can need.\\nI need to use Terraform to create the infrastructure and provision network,\\nsecurity groups, and load balancers.\\nAlso, this way I can provision the rest of the instances I want, like Prometheus\\nand Taurus (for benchmarking).\\nTo manage configuration, I'm going to use Ansible.  More about this later.\\n","tokens":120,"id":2996,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n","Decision":"Done.\\n","tokens":42,"id":2997,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0002-use-docker-for-12factor-app-goal.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOne of the main goals I'm interested on is in to provide a 12factor app\\narchitecture to the solution.\\netcd is a daemon that runs using:\\na. environment variables or arguments in command line\\nb. config-file\\nAll are valid for a 12factor app, but which is faster\/better to implement?\\nI have read most of etcd invocations use command line arguments, but that's not\\ngoing to work fine with Docker.\\nI can create a config file in a volume and map it from the docker container in\\nruntime.  That is how it should work in kubernetes using config maps.\\nBut for something more 12factor alike, like in heroku, I would rather prefer to\\nuse ENV VARS.  Etcd has a good support for them.\\n","Decision":"I'm going to try first with a config file, and then I will try to inject env\\nvars through a docker env-file option.\\nAs I want to create a three node cluster, I would need three env_files... Umm.\\n","tokens":169,"id":2999,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0009-use-terraform-to-provision-infrastruture-as-code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSince I moved to AWS EC2 I knew I had to implement Terraform to manage the\\ninfrastructure.\\nWhy? Because for me it's the only tool I can use to provision for AWS.  Yes,\\nthere is an alternative called AWS Cloudformation, but I prefer not to kill\\nmyself.\\nTerraform is very powerful, and I have worked with it before.\\nTerraform also implements a nice feature called remote state.  It allows me\\nto share work with other users just sharing the credentials for the AWS.\\nTerraform allows me to create a big file with all the resources and\\nprogressively split it into different files to document better the solution.\\nLast, Terraform has a simple provision system that can help me to launch the\\nconfiguration management solution after creating the instances.\\n","Decision":"Implement Terraform to launch the infrastructure.\\n","tokens":173,"id":3000,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0006-use-prometheus-as-monitoring-service.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis was a clear goal of the challenge: to observe the solution.\\nAs etcd has a metrics endpoint with the structure supported by prometheus,\\nit's a logical solution to use it.\\nPrometheus has several discovery services that can help in a cloud environment.\\n- static target file\\n- service discovery modules\\n- sd_ec2: plugin to discover AWS instances.\\n","Decision":"I have tested the sd_ec2 plugin and it works fine.  I prefer to implement it\\nrather than trying to implement a provisioner in the configuration management\\nsystem that modifies the static target file to add and remove instances.\\n","tokens":80,"id":3001,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0010-use-ansible-for-configuration-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen I decided to use AMI's instead of docker containers, I knew I had to\\nuse some tool for configuration management.\\nThis tool would help me to execute configuration tasks.  But as my intention\\nis to create an inmutable system, I don't want to manage changes, just set\\nthe resources.\\nFor doing this I could implement Puppet, Chef, or Ansible.  I have experience\\nwith all of them, but recently I have been working more with Ansible.  And for\\na challenge like this, I prefer something modern, without servers.\\n","Decision":"I have used Ansible to configure all the services for the three kind of nodes\\nin the challenge: etcd, monitor, and benchmark.\\n","tokens":122,"id":3002,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0003-etcd-cluster-mode.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEtcd has three ways to setup a cluster:\\n- Statically: setting all the IP's and listeners of the cluster members from\\ncommand line\\n- Dynamically: using an external etcd cluster (https:\/\/discovery.etcd.io\/new?size=X)\\n- Dynamically: using a DNS SRV record.\\nI have tested two first options, and found that the main problem is that the\\ncluster member is identified by its name AND it's peer URL, not only its name.\\nThat's make my job very difficult as I cannot just open a 0.0.0.0\/0 listener\\nfor listening and peering with other members.  It has to use always its IP.\\nI need to find a way to identify each etcd cluster member with the IP address\\nfor peering.\\n","Decision":"There is an option: setting a network in docker \/ aws vpc and force the IP\\naddress used for that container \/ instance.\\n","tokens":173,"id":3003,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0011-use-a-ci-cd-tool-to-ease-provisioning-workflow.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI have tested Github Actions, it's similar to CircleCI.  Also I intended to use\\nthe Gitlab pipeline solution, but at the end I have no time to waste.\\n","Decision":"I'm going to try first CircleCI to see if I can deploy to AWS EC2.\\n","tokens":149,"id":3005,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0009-event-driven-framework-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\\n* Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with\\n* We can reduce the number of threads used by relying on event-driven behavior\\n","Decision":"The framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register\/unregister for events).\\nDispatch queues will be provided to assist with the event driven model.\\nPlatform examples will default to dispatch-based processing models.\\n","tokens":66,"id":3007,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0016-how-to-handle-meson-library-targets-w-r-t-libc-and-libcxx-header-includes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen compiling static libraries within the framework, we need to have access to libc and libc++ includes. For example, when creating the nRF52840 library, there were build failures related to the library using system include headers instead of the intended headers.\\nThis is an error-prone process and we need to figure out how to handle it.\\n","Decision":"Because of the way the build system is structured, we can't easily use include_directories() and separate native\/target header include alternatives. It will be easier to handle this instruction through documentation instead.\\nOne option we considered was to add libc and libc++ include directories to the framework_includes, so all framework classes pick them up. However, this also impacts the test code (which could be worked around), which does not include our internal stdlib headers. However, we would need multiple variables - to differentiate between native and target headers.\\nAnother option that was considered was to use add_project_options to foist includes on every target in the project. This would allow us to differentiate between native and target. However, the tests would have been affected by this without ability to work around it.\\n","tokens":73,"id":3008,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0010-dispatch-callbacks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBecause we are building an [event-driven framework], we need to think about how to handle callback functions. We want to use them without causing threads\/functions to block unexpectedly while executing callbacks.\\n","Decision":"We will dispatch callbacks to a global dispatch queue which will execute them as processing time is available.\\n","tokens":41,"id":3009,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0008-virtual-platform-as-mediator-pattern.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are trying to build a reusable embedded system framework. However, many embedded system implementation details are particular to the underlying hardware details. Systems become tightly coupled because the various software elements interact directly with RTOS, processor, and hardware functionality. If these areas were contained behind generic interfaces and kept from knowing about each other, we increase the odds of creating reusable software designs.\\n","Decision":"The virtual platform abstraction will serve as the [mediator](..\/patterns\/mediator.md) for the system. All platform-specific behavior will be implemented within the virtual platform.\\nThe platform will contain all of the hardware implementation details, ensuring that the rest of the application is decoupled from these details. This decoupling will work through generic drivers (set up by the platform) and platform APIs (basic set defined by us, extensible by user).\\nThe software layer will interact with hardware through the generic driver interfaces and the platform-provided APIs. This ensures that the software application is portable across components and across platform implementations (assuming API guarantees are met on the new platform).\\n","tokens":77,"id":3010,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0017-virtual-destructor-usage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen trying to link an embvm application that did not use any dynamic memory allocations, we failed to link because of a missing `free` symbol. This is because a virtual destructor in a base class was causing the compiler to generate a deleting destructor.\\n","Decision":"By default, make destructors in abstract classes protected and non-virtual.\\nIf you encounter a situation where you need to provide a virtual destructor (because you are destroying an object with a base class pointer), then you need to implement a default `operator delete` in your final class if dynamic memory management isn't used.\\nIf you don't expect a deleting destructor to be called (you would need to manually call `delete` on the base class), then trigger an assertion in case it is called.\\n```\\nvoid operator delete(void* p)\\n{\\nassert(0); \/\/ should not be called\\n}\\n```\\n","tokens":54,"id":3011,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0002-layering-scheme.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe framework is being designed to provide portability for embedded systems applications. We also want to maximize reuse of software across embedded products to minimize bring-up time and to increase our confidence in the software.\\nIn order to accomplish this, we must think about how to isolate the various platform-specific forces of change that occur within embedded products:\\n* The processor\\n* The layout\/connections on a given board\\n* The memory availability\\n* The component ICs that are included on the board\\nBy isolating these various forces of change and platform dependence, we can improve the possibility of re-use on embedded platforms.\\n","Decision":"The architecture is structured in three layers:\\n1. Virtual Processor Layer (Base)\\n2. Virtual Platform Layer\\n3. Software Layer\\nThe virtual processor layer is a standalone layer which manages the chip and HAL.\\nThe virtual platform layer contains platform-specific implementations for the C\/C++ runtime, RTOS, and board\/platforms.\\nThe software layer contains standalone software which is built to use the virtual platform abstractions.\\n","tokens":126,"id":3012,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0004-track-documentation-alongside-source.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProject documentation is:\\n* Usually tracked in a variety of locations (google drive, various hard drives, wiki, repository)\\n* Rarely maintained by an organization\\n* Not updated to match source changes\\n* Copied multiple times, often with differences between copies\/locations\\n* Usually not given a responsible party to own it\\nAs a result, documentation is scattered and rarely kept up to date. If it is used, documenation is often generated and referred to once.\\nQuality documentation, especially when kept up-to-date, enables us to support more clients and developers with less direct involvement.\\n","Decision":"All architectural, design, development, and user documentation will be maintained in the same repository as the source code.\\nThis means that documentation should be generated when possible, rather than manually constructed in a Microsoft Word document.\\n","tokens":125,"id":3013,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0005-provide-non-blocking-interfaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBlocking interfaces are those that only return when an operation has been completed.\\nNon-blocking interfaces initiate a request and return when the request is submitted. Completion is annouced via a callback or by checking the status through another interface.\\nNon-blocking interfaces provide a more flexible implementation route.\\nBlocking implementations are typically avoided on embedded systems. Typically, a thread should sleep while waiting for an action to complete, rather than hogging processor resources.\\n","Decision":"Interfaces provided by the framework will be non-blocking. Users can write their own blocking wrappers if blocking code is needed.\\n","tokens":92,"id":3014,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0003-no-dynamic-memory-allocation-in-core.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMany systems, teams, and designs require that the system will not utilize dynamic memory allocation. We should maintain a flexible design by allowing users to use dynamic memory allocation if they desire. However, we should be able to support the strictest operating model for maximum flexibility and potential use of the framework.\\n","Decision":"No dynamic memory allocation will be utilized by framework core components\\n","tokens":63,"id":3015,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0015-use-embvm-top-level-namespace-for-core-interfaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAll of the framework code was placed into the global namespace. We need to find a home to group our code in.\\n","Decision":"The `embvm` (Embedded Virtual Machine) namespace will be used for framework constructs.\\n","tokens":28,"id":3016,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0019-virtual-platform-takes-in-thwplatform-type.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs a consequence of [ADR 0018](0018-driver-registration-in-hw-platform.md), we moved the [Driver Registry](..\/components\/core\/driver_registry.md) definition to the hardware platform and removed the global singleton from the platform. We also want the platform APIs to forward to the hw platform. However, we needed a way to access the hw platform object for successful forwarding. This requires the platform base class to know about the type.\\n","Decision":"The hardware platform type is now a template parameter for the Virtual Platform base class. A local variable will be declared (`hw_platform_`), and a `protected` API will be provided to access that variable as well.\\n","tokens":94,"id":3017,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0012-switch-to-catch-for-unit-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe started unit testing with doctest. However, when trying to compile with exceptions disabled, it seems to be a problem.\\nCatch provides support for compiling with -fno-exceptions, and even lets us configure the termination handler.\\n","Decision":"We're proposing to use Catch for unit testing. But we need to actually try it out firs.t\\n","tokens":51,"id":3018,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0018-driver-registration-in-hw-platform.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPreviously, the [Driver base class](..\/components\/core\/driver.md) interacted with the [Driver Registry](..\/components\/core\/driver_registry.md). This eliminated work that was needed from the user's side, but it also complicated the base class and introduced a dependency of drivers on the registry.\\n","Decision":"The Driver Registry dependency will be removed from the Driver base class. Drivers will instead be manually registered in the hardware platform.\\nThis comes with a few related changes:\\n- There is no more global driver registry instance - it's part of the hardware platform, and the virtual platform interfaces just forward the call to the hardware platform\\n- The virtual hardware platform base class takes a driver registry as a template parameter, which allows us to defer the selection of our driver registry until we declare one in the platform.\\n- From the hardware-platform's perspective, we don't care if we are using static or dynamic registry creation. We can't have our hardware platform deciding that it needs dynamic memory, when our platform is set up to use no dynamic memory.\\n- The virtual platform base class will be adjusted to forward the calls to the hardware platform APIs\\n- There is no more driver registry global singleton (in the virtual platform), because we don't need to access the registry *before* the hardware platform is created. Now, the instance is kept in the virtual hardware platform.\\n","tokens":64,"id":3019,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0013-use-templates-to-switch-between-dynamic-and-static-memory.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to build the framework components to work with both dynamic and static memory allocation schemes.\\n","Decision":"Rather than duplicating implementations, we will use Template Metaprogramming to use a single structure which supports both static and dynamic memory allocation. Classes can be templated with a size parameter. This size is evaluated to determine the underlying data structure that will be used. If the size is equal to 0, dynamic memory is assumed. Otherwise, static memory allocation will be used.\\n","tokens":22,"id":3020,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0022-unified-gpio-base-class.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"To simplify the implementation for drivers, we will create a \"unified\" GPIO base class, which provides the standard interfaces that can be used to set the mode (input, output, special function). This will provide a single class to implement for GPIO support, rather than one class for each mode.\\nIdeally, we will find a way to zero-overhead create wrapper classes that constrain the interface appropriately, so we can mark pins as `GPIOInput` or `GPIOOutput` and have the compiler report an error if we try to do an invalid operation for the given type.\\n","tokens":21,"id":3021,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0007-differentiate-between-processor-implementation-and-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to support a wide variety of chips. However, there are many chips that utilize the same architecture, and we dont' want to duplicate information across those chips.\\nWe can abstract out architecture-related behavior from the vendor-specific processor implementations.\\n","Decision":"A processor will be represented by two orthogonal abstract models:\\n1. Processor architecture (e.g., ARM Cortex-M0, x86_64, PIC32)\\n2. Processor implementation (e.g., a Nordic nRF52 has a unique architecture & set of peripherals)\\nA specific processor will be repersented by inheriting from an architecture and a processor family. The peripherals, definitions, and other details for that processor which are unique will be contained within the specific processor implementation.\\nThis means that an nRF52822 inherits from nRF52 and Cortex-M4F.\\nThe processor architecture tree with contain all architecture-specific behavior (e.g., how to turn on\/off MMU on Cortex-M0, M4, A5, etc.).\\nThe processor implementation tree will contain all vendor-specific behaviors and constructs.\\n","tokens":53,"id":3022,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0006-differentiate-drivers-and-hal.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProcessor peripherals often require additional interfaces and information that generic peripheral interfaces don't. For instance, we need to initiate our ARM chip's SPI driver with an SFR base address and some configuration options. If I am writing an accelerometer attached to the SPI bus, I only care about read\/write and providing my CS address. I don't need any knowledge of the processor details for the SPI bus.\\n","Decision":"We will provide two sets of interfaces:\\n* HAL (drivers) which manage low-level processor hardware devices\\n* Drivers with generalized interfaces which abstract away the low-level details\\n* The software layer doesn't need to know about the clock settings or location of the SPI0 SFRs\\nThe platform and software layers will interact with the generic drivers for application features. The platform layer will create the generic drivers with references to the processor HAL.\\n","tokens":82,"id":3024,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0011-generic-startup-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSystems go through a predictable set of boot behaviors, which we can commonize:\\n* Relocate\\n* zero data in the .bss section\\n* Call constructors\\n* Initialize RTOS\\n* Set default hardware pin states\\n* Create drivers\\n* Configure system\\n","Decision":"We will provide a generic startup library which manages the initial boot sequence in a predictable manner.\\nThe library will gain control from the processor startup sequence and transition the boot control process to the platform layer. It will call pre-defined platform APIs to initialize the RTOS, board, etc.\\nWe will call the same order ever time, but allow the platform to define actions that occur at each stage.\\n","tokens":58,"id":3025,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0020-hardware-platform-options-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"We will create a new required `hw_platform_options.hpp` file. This file is defined in the platform level, and can be used to configure any necessary compile-time parameters without the use of templates.\\n","tokens":21,"id":3026,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"embvm-core\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3028,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0004-define-project-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHow should the folders and files in this repo be organized and named?\\n","Decision":"1. Add a folder in \/src\/components.\\n- Name the folder the same as the component name, in title case without any spaces.\\n- Beneath that, add a `v1` folder. Each new version of the component that changes its appearance will get a new version folder.\\n- Do not create any additional layers of subfolders\\n1. In the folder, add a .js file, a .md file, and a .test.js file, all of them named the same as the component name. If the component needs any component-specific utility functions (to keep the code in the .js file clean), add a folder named `utils` and put them in there. Here is an example folder structure:\\n```text\\n\/src\\n\/components\\n\/Button\\n\/v1\\nButton.js\\nButton.test.js\\nButton.md\\nutils\\nmyUtilFunction.js\\nmyUtilFunction.test.js\\nmyOtherUtilFunction.js\\nmyOtherUtilFunction.test.js\\n```\\n1. Define a single React component in the .js file. Export it as default. (Refer to \"Component Guidelines\" section.)\\n1. Write tests in the .test.js file, using Jest. (Refer to \"Writing and Running Tests\" section.)\\n1. Write style guide documentation and examples for the component in the .md file.\\n1. Add your component to an appropriate style guide section in `styleguide.config.js`\\n1. If any utility functions used by your component can be helpful to other components, too, put them in a `\/src\/utils\/[functionName].js` file instead of the component-specific `utils` folder\\n","tokens":18,"id":3029,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0006-style-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThese are our requirements for component styling:\\n- A component has baked-in styles that make it look good out of the box. These are used in the Style Guide app.\\n- Some but not all aspects of a component's style are overrideable by the user, i.e., theming\\n- Try as much as possible to isolate components from any generic app styles. For example, when rendered in an app that pulls in all Bootstrap CSS, it should still appear as expected. Conversely, no styles included with the component should affect the appearance of any other component in an app.\\nAlso potential requirement? Works in a React Native app\\n","Decision":"Use [styled-components](https:\/\/www.styled-components.com\/)\\nThe primary reason to use styled-components library is because they have a good theming solution that works for our use case: https:\/\/www.styled-components.com\/docs\/advanced#theming\\nAlso:\\n- Uses normal CSS that people know\\n- Supports media queries, :hover, animations, etc.\\n- Injects stylesheets that take precedence over global stylesheets, though you can override them in a global stylesheet in an emergency if you use tricks.\\n- Server side rendering\\n- Works with React Native if we eventually try to make the components universal\\n","tokens":132,"id":3030,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0002-12factor-config-from-env.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFrom [12factor.net](https:\/\/12factor.net\/config):\\n#### Store config in the environment\\nAn app\u2019s config is everything that is likely to vary between deploys (staging,\\nproduction, developer environments, etc). This includes:\\n* Resource handles to the database, Memcached, and other backing services\\n* Credentials to external services such as Amazon S3 or Twitter\\n* Per-deploy values such as the canonical hostname for the deploy\\nApps sometimes store config as constants in the code. This is a violation of\\ntwelve-factor, which requires strict separation of config from code. Config\\nvaries substantially across deploys, code does not.\\nA litmus test for whether an app has all config correctly factored out of the\\ncode is whether the codebase could be made open source at any moment, without\\ncompromising any credentials.\\nThe twelve-factor app stores config in environment variables (often shortened\\nto env vars or env). Env vars are easy to change between deploys without\\nchanging any code; unlike config files, there is little chance of them being\\nchecked into the code repo accidentally; and unlike custom config files, or\\nother config mechanisms such as Java System Properties, they are a language-\\nand OS-agnostic standard.\\nAnother aspect of config management is grouping. Sometimes apps batch config\\ninto named groups (often called \u201cenvironments\u201d) named after specific deploys,\\nsuch as the development, test, and production environments in Rails. This\\nmethod does not scale cleanly: as more deploys of the app are created, new\\nenvironment names are necessary, such as staging or qa. As the project grows\\nfurther, developers may add their own special environments like joes-staging,\\nresulting in a combinatorial explosion of config which makes managing deploys\\nof the app very brittle.\\nIn a twelve-factor app, env vars are granular controls, each fully orthogonal\\nto other env vars. They are never grouped together as \u201cenvironments\u201d, but\\ninstead are independently managed for each deploy. This is a model that scales\\nup smoothly as the app naturally expands into more deploys over its lifetime.\\n","Decision":"Follow the twelve-factor advice and use configuration from the environment.\\nWe will centralize the interface for the environment into a single location so\\nthat we can enforce strict requirements on vars. For example, the application\\nshould fail fast if a requirement is not valid. A `PORT` environment variable\\nshould be validated that it is positive number within the accepted port values.\\n#### Strictness\\nWe're currently using the npm package [`envalid`][1] to provide this\\nfunctionality. There are several libraries that serve this purpose.\\n[`envalid`][1] was specifically chosen because it doesn't have any\\nfunctionality that would allow developers to break this decision.\\nSome libraries allow merge of config files. Some libraries allow usage of\\n`NODE_ENV`. These features allow (if not encourage) developers to do the wrong\\nthing.\\n","tokens":449,"id":3032,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0008-keep-styles-with-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSo that developers do not have to think too hard, we want a simple rule about where the styles related to a React component should live. The options are \"always in a separate styles.js file in the same folder\" or \"always in the same file as the React component, above the component\". (This is referring to JSX or styled-components styles, and not CSS\/SASS styles.)\\n### Separate File\\nPros:\\n- Smaller files, easier to read\\nCons:\\n- Overkill for a single style\\n- More files for Babel to transform and watchers to watch\\n- Extra work for dev (minimal)\\n### Same File\\nPros:\\n- Simple and less work for dev\\n- Fewer files for Babel to transform and watchers to watch\\nCons:\\n- Larger files, harder to read (but you can code fold in IDE)\\n","Decision":"All styles always live in the same file as the React component, above the component.\\n","tokens":181,"id":3033,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0005-test-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur React Components need to be well tested. At a minimum:\\n- Snapshots of what is rendered for the most common props, to see when the output changes and confirm that it was intentional.\\n- Test that all function props are called at the proper time with the proper arguments, as documented.\\n- Generate a coverage report to prove that everything is tested.\\n","Decision":"Use Jest.\\nIt is popular, backed by Facebook, runs tests in parallel and reruns only changed tests, has built-in coverage, mocking, and `expect` patterns, has a snapshot feature, runs well on CI, and is built on Jasmine, which is battle tested.\\n","tokens":78,"id":3034,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0004-use-create-react-app-and-npm-eject-to-create-the-basis-of-the-project-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nReact projects can be created from scratch, by setting up webpack scripts, installing dependencies etc. or they can be set up to a predetermined configuration using the create-react-app CLI.\\n","Decision":"We will use create-react-app to generate a project scaffold using Facebook-defined best practices.\\nWe will use the `npm run eject` command to make full project configuration available to us, such as Webpack.\\n","tokens":39,"id":3036,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0003-implement-cssinjs-as-default-approach-to-styling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe govuk-react components library uses CSSinJS with Emotion for styling. Our approach to styling needs to be efficient and maintainable, and this could be achieved either through a preprocessor like SCSS\/SASS, or by using CSSinJS.\\n","Decision":"We will use CSSinJS with the [Emotion library](https:\/\/emotion.sh\/), with SCSS available as a fallback should it be necessary.\\n","tokens":55,"id":3038,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0007-switch-from-create-react-app-to-next-js.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nReact can be implemented from scratch, using the `create-react-app` CLI, or by using a 3rd-party framework such as Next.js.\\n","Decision":"We will re-start the scaffold.\\nWe will use Next.js as the basis for this project.\\nWe will extend and customise Next.js to fit our more bespoke requirements.\\n","tokens":34,"id":3039,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0006-implement-eslint-with-default-create-react-app-and-prettier-rules-and-scope-for-additional-rules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJavaScript linting is required to enforce consistency of code style and best practices. There are different options for linting tools but ESLint is the most common. Within ESLint, there are many options for rules, plugins, etc.\\n","Decision":"We will use ESLint as our linting tool.\\nWe will extend the `react-app` and `prettier` linting rules.\\nWe will use the Prettier 'recommended' config to disable any conflicting rules and enforce Prettier as the standard.\\n","tokens":50,"id":3040,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"register-a-food-business-front-end\/0005-implement-yarn-as-a-cli-for-node-package-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nYarn and NPM can both manage the Node packages for a project. Recent updates to NPM mean that Yarn only has a negligible performance advantage over NPM.\\n","Decision":"We will use `yarn`, `yarn start`, `yarn add`, `yarn remove` etc. for the management of Node packages in our project.\\n","tokens":38,"id":3042,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rufus\/0000-rufus-decision-records.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and problem statement\\nThere are many decisions to make in the design of Rufus, and each one has many\\ntradeoffs to consider. We want to record the context around important language\\ndecisions, both as part of the design process and as documentation for users.\\n","Decision":"* A decision record must clearly describe the context that motivates a solution,\\nthe options considered, and the decision made.\\nChosen option: a subset of the MADR template, because it's the most easily\\nadaptable to our needs. Rufus decision records are assigned an RDR-_nnnn_ ID\\nwhen they're accepted. See [RDR-0001](0001-template.md) for a template.\\n","tokens":58,"id":3043,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rufus\/nnnn-exported-identifiers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and problem statement\\nA package should only export public modules, and a module should only export\\npublic types, constants and functions. It should not be possible for a user to\\naccess private identifiers in a package they're using. What facilities should\\nRufus provide for users to manage encapsulation?\\n","Decision":"* A user can tell whether an identifier is public or private at a glance.\\n* Works well for all users, including those that uses non-latin languages and\\nalphabets.\\n* It's not possible to access private modules from outside their package.\\n* It's not possible to access private types, constants, or functions from\\noutside their module.\\nChosen option: option 1, because it avoids Rufus source files in the same\\ndirectory have names that differ only by case, which is likely to lead to\\nconfusion.\\n","tokens":64,"id":3044,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rufus\/nnnn-list-syntax.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and problem statement\\nWhat syntax should Rufus use for list types, literals, and cons expressions?\\n","Decision":"* The syntax should be consistent, especially between literal and cons\\nrepresentations, and avoid unnecessary boilerplate where doing so helps\\nreadability.\\n* The syntax should work well in match expressions as well as patterns in\\nfunction heads.\\n* The syntax should sit cleanly alongside tuples and maps (and eventually sets).\\n* The syntax should scale to custom types.\\n* Parsing and typechecking the syntax should be straightforward to reduce\\ncomplexity in the compiler and ensure that other tools, such as `rf fmt`, can\\neasily work with the syntax.\\nChosen option: option 2, because it will simplify compiler implementation, and\\nbecause making the list type explicit should make code easier to read because\\nthere will be less guessing about what the type is.\\n","tokens":23,"id":3045,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"signals-frontend\/0002-using-local-state-in-settings-module.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\\n","Decision":"The structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer\/context approach in favour of `redux` complexity.\\n","tokens":133,"id":3046,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"signals-frontend\/0003-application-configuration-setup.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\\n- Docker registry URLs\\n- URLs of API endpoints, machine learning service, Authz service and map server\\n- Nginx configuration\\n- HTML `<title \/>`\\n- [`Sentry` Package](https:\/\/www.npmjs.com\/package\/@sentry\/browser) dependency and configuration\\n- PWA manifest\\n- Logo\\n- Favicon\\n- PWA icons\\n- Hardcoded strings\\n- Main menu items\\n- Theme configuration\\n- Maps settings\\n- [`amsterdam-stijl` Package](https:\/\/www.npmjs.com\/package\/amsterdam-stijl) Package dependency\\n- Package URL and description\\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\\n","Decision":"Taking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side \/ container](#Server-side \/ container) option.\\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\\nConfiguration is injected in the `index.html` file so it can be read at run-time.\\n","tokens":246,"id":3048,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"signals-frontend\/0006-forms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently the form library [`react-reactive-forms`](https:\/\/github.com\/bietkul\/react-reactive-form) is being used. This poses the following problems:\\n- It is difficult and time consuming to customize the form to specific needs and requirements, because the form logic consists of many layers and you have indirect control over the end result.\\n- Many forms are defined through a configuration.\\n- Forms are created with a form builder.\\n- Form elements are defined through components\\n- Rendering is defined throught a prop on a field component.\\n- There is no easy interaction through hooks.\\n- Not actively maintained anymore (latest release from 2018-08-19).\\nIt does provide us with the following benefits:\\n- Additional questions in the incident form can come from the backend in a json definition and easily be inserted into the form.\\n- Has built-in validation.\\n- Small footprint, no dependencies.\\n","Decision":"Only [react-hook-form](https:\/\/github.com\/react-hook-form\/react-hook-form) has integration with React hooks and doesn't rerender on input changes to form fields. Furthermore, it has a small footprint, built in validation, requires very little code to use and follows HTML standards closely.\\nGenerating a form based on a definition coming from the back end can easily be done with this library using native JavaScript\/React logic.\\n","tokens":193,"id":3049,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"signals-frontend\/0011-embedded-application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhile signals\/front-end is a React web app, the client wants to provide it to mobile app users as well.\\nFurthermore, some changes are requested specifically for the 'app version':\\n- The application header should be hidden\\n- The button 'Doe een melding' should be replaced with a 'Sluit venster' button which closes the window\\n- The possibility to initialize the map center location\\nThe above requirements are described in more detail in [Jira](https:\/\/datapunt.atlassian.net\/browse\/SIG-3933).\\nAt this moment, rebuilding the frontend in a native app architecture is not possible due to time and financial constraints.\\n","Decision":"Add a feature flag `appMode` to the application configuration, and deploy the application with `appMode=true` to a separate domain.\\nUsing the feature flag, the application conditionally applies the app-specific requirements.\\nThe mobile app embeds the web app (e.g. using [React Native WebView](https:\/\/github.com\/react-native-webview\/react-native-webview)),\\nand provides additional information via query parameters (e.g. map coordinates).\\nConversely, communication from the web-app to the mobile app occurs via the `postMessage()` API, but its use should be limited.\\nBecause an embedded application cannot close itself, the `postMessage()` can be used to notify the app that it should be closed.\\n","tokens":138,"id":3051,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"signals-frontend\/0007-typescript.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently TypeScript is not being used. For typing of component props PropTypes is being used.\\n","Decision":"Making use of typing will make our code more readable and more reliable. Furthermore we don't need to use the PropTypes dependency anymore. TypeScript's interfaces and types are more reusable and adaptable than PropTypes.\\nTransition to TypeScript will be done gradually.\\n","tokens":21,"id":3052,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"boxwise-flask\/graphql-ariadne-apollo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBoxwise is in the middle of a planned migration from the old PHP-based Dropapp, to a new app based on a new stack (Python\/React). In the old app, the backend and the data model were closely intertwined, with SQL strings written directly within PHP. This made it challenging to evolve the data model for the app, which in turn imposed many product functionality constraints and slowed development time - especially since the Dropapp data model was a prototype rather than designed for scalability and product-market fit. As the team migrates to the new app and explores the possibility of entering new markets, it is time for us to reexamine if and how the team might benefit from a separation of concerns on the data layer.\\n","Decision":"1. **Scalability:** how well will it support expected future changes such as DB restructuring, database migrations, etc.? What timescale is the technology expected to be defunct on?\\n2. **Developer experience:** given the rotating environment of loosely affiliated developers of different backgrounds, what will support the rapid onboarding of developers with our data structure? Once onboarded, is the chosen technology pleasant to use? Is it useful from a career progression standpoint?\\n3. **Maintainability:** we expect to have rapid changes to the DB structure as we expand functionality. How easy is our solution to maintain and evolve? What about documentation?\\n4. **Support and production-readiness:** Is the library mature enough to use in a production environment? Is there an active community or support channel should we run into problems?\\nGraphQL with single endpoint for everything was selected, paired with Ariadne server-side and Apollo client side.\\nReasoning: while GraphQL may have a steeper learning curve for professional developers who are not familiar with the standard, in the long run this should be more scalable for iterations and easier to maintain than  multiple REST endpoints. In the future, should we end up ingesting external data APIs such as the UNHCR data, it will be easier to pull that all from the GraphQL endpoint as well. This should also be more favorable from a developer experience standpoint for both onboarding and maintaining the codebase, due to GraphQL's introspective capabilities, human-readable JSON query structure, and degree of client-side specificity in requesting field-level data.\\nApollo was selected on the client-side due to its maturity as a product, robust features including sophisticated caching, excellent documentation, and huge community. Ariadne was selected over Graphene on the server side due to it being designed to deliberately intended to mimic Apollo Server. With Ariadne being under active development by Mirumee software, and its excellent documentation that developers can cross-reference with Apollo server documentation, I believe this outweighs any cons that come from it being a less mature library than Graphene.\\nFinally, I believe any performance concerns that could result from queries being abstracted from SQL into resolvers will be compensated for by less load on the network due to overfetching, as long as no N+1 queries are created.\\n","tokens":149,"id":3054,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"boxwise-flask\/adr_python-dev-env.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe main programming language for the present repository is Python. The Python ecosystem features a rich set of tools and libraries that facilitate developing an industry-standard codebase. Development includes implementation of production and test code in an environment built on linting and formatting tools.\\n","Decision":"1. Test-driven development\\n1. Code maintainability\\n1. Code scalability\\n1. Code format consistency\\n1. Testing: `pytest`. Compared to unittest, test code is more concise yet readable, test fixture setup is straightforward, and test assertion output is very clear and helpful for debugging.\\n1. Code formatting: `black`. Developed by the Python Software Foundation themselves. Uncompromising, fast, deterministic.\\n1. Linting: `flake8`. Detection of common code smells. `pylint` tends to be pickier and might hinder rapid initial development.\\n1. Integration with `git`: `pre-commit`. Automatic style checks prior to committing. Orchestration of style check tools.\\n1. Isolated Python development environment: `venv`. Isolation of system and project Python packages in so called 'virtual environments'.\\n","tokens":54,"id":3055,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/simulator_description.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nApi need to receive vehicles locations informations. Simulator will generate vehicle movement for testing purposes.\\n","Decision":"Team decide to make separate simulator service.\\n","tokens":21,"id":3056,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/api_description.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nApi will be responsible for administering vehicle timetables. It will use map service for real time vehicle tracking and will recive informations about it localizations from any source.\\n","Decision":"Team decide to make API wich allows to organize timetables, receive vehicles localizations and send informations to front-end api.\\n","tokens":38,"id":3057,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/api_architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBy using the correct architecture, the application will be flexible and legible.\\n","Decision":"REST Api\\n","tokens":19,"id":3058,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/map.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe choice of database should depend on the information that the database will store.\\n","Decision":"Mapbox\\n","tokens":19,"id":3059,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/containerization.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\\n","Decision":"Docker\\n","tokens":24,"id":3060,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe choice of database should depend on the information that the database will store.\\n","Decision":"PostgreSQL\\n","tokens":19,"id":3061,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/api_framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/api_language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nChoosing proper programming language is the primary choice for new project.\\n","Decision":"PHP 7.4\\n","tokens":16,"id":3063,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"otm-docs\/simulator_language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSimulator should do multiple calculations in a short time. Choice should be dictated by the speed of code execution.\\n","Decision":"Golang\\n","tokens":26,"id":3064,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-s3-static-website-module\/0003-optionally-create-route53-resource.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt cannot be assumed that users of this module would manage DNS in AWS or that users may have a cross account design that prevents access to the hosted zone.\\n","Decision":"Provide a means to create the custom Route53 resource as an option\\n","tokens":35,"id":3065,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-s3-static-website-module\/0004-dual-support-for-terraform-version-0-11-and-0-12.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\\n","Decision":"This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\\n","tokens":46,"id":3066,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-s3-static-website-module\/0002-designed-to-require-a-tls-certificate-dependency.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs https connections have become the standard for web connections this module needed to implement a TLS connection. An abstraction of how certificates and DNS is managed be users of this module needed to be abstracted away.\\n","Decision":"While this module may have created a certificate resource, it was decided the use cases of any DNS management and certificate providers was too vast and that this module will take a certificate arn as a dependency.\\n","tokens":44,"id":3067,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"moneycount-api\/001-Choose_Spring_Boot.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI have to choose a framework to implement moneycount-api project. It could be a familiar framework, such as Spring Boot, Spark, Servlets + Jersey or even Python + Flask, or I could try another different framework to learn something new.\\n","Decision":"I decided to use Spring Boot 1.5.10, the last stable 1.X release at the time, because I want to have a first version of the software in a well known framework that allows me to implement it fast, with easy integration with other features that I can choose in future improvements.\\n","tokens":53,"id":3072,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0026-use-snyk-vulnerability-scanning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAn area of security risk to MyMove is supply-chain risk.  In the context of software development, supply-chain risk is the risk that a malicious actor could insert malicious code somewhere into a source code dependency.  The actor could attack a dependency directly by modifying a git repo, upload bad code to a package manager, man in the middle during the build process, or a wide variety of other scenarios.\\nScanning software for known vulnerabilities (e.g., CVEs) is a critical step for mitigating supply-chain risk.  However, it does not completely mitigate against zero-days, custom attacks, or other supply-chain attacks.\\n## Decision Drivers\\n* Seamlessly integrates into our CI\/CD workflow\\n* Scans JavaScript for known CVEs\\n* Scans Golang dependencies for known CVEs\\n* Maintenance cost\\n","Decision":"* Seamlessly integrates into our CI\/CD workflow\\n* Scans JavaScript for known CVEs\\n* Scans Golang dependencies for known CVEs\\n* Maintenance cost\\nChosen option: \"Use Snyk for Vulnerability Scanning\".  We'll use `snyk test` and `snyk monitor` for one-time tests and monitoring of our dependency tree.\\n","tokens":173,"id":3073,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0058-replace-loki-with-happo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Problem\\nMilMove currently uses a tool called Loki for performing visual regression tests on all components rendered in Storybook. While Loki has been helpful with catching unintended visual changes, it also has several downsides:\\n- Loki is executed in local development & CI environments, so reference image assets must be checked into the code repo, and running\/updating the tests locally is time- and resource-intensive.\\n- Because reference images are checked into the repo, the action of approving changes requires an additional commit, push, and CI build.\\n- Loki only runs tests in Chrome, so visual implementations in other browsers (Firefox, IE, Safari) are not tested.\\n- Review and approval of changes happens in local dev environments, so actual changes can be relatively opaque and may not be explicitly reviewed by designers.\\n- Loki only supports visual testing of Storybook components, so we would not be able to add visual tests to the application if we wanted.\\n","Decision":"- **Chosen Alternative: Switch to Happo**\\n- `+` Tests run on Happo's hosted platform, so engineers and CircleCI don't need to spend time\/resources running them\\n- `+` Visual tests can run against all of the browsers we support: Chrome, Firefox, IE11, Safari, iOS Safari, Edge (on all pricing plans, including free open source)\\n- `+` Happo provides a UI for users with access to view and approve or reject changes, and approving does not require committing changes\\n- `+` Happo has a plugin to use with Storybook components, but can also be used to screenshot the application itself (for example, as part of Cypress E2E tests)\\n- `~` Happo has an on-premise option available if security is a concern (but pricing is by request, and will require additional infra support to set up and maintain)\\n- `-` Happo runs tests on its hosted platform, so it requires giving Happo access to our Github repo (which is currently public)\\n- `-` Happo costs money for non-open source projects (pricing tiers are $125 \/ $250 \/ \\$500 \/ month depending on usage)\\n- `-` Test reports for free open source projects are public to anyone who has the link\\n### Strategy\\nMy proposed steps for migrating from Loki to Happo are:\\n1. Add the [Happo Github app](https:\/\/github.com\/apps\/happo) to this repo\\n1. Install the JS dependencies (`happo.io, happo-plugin-storybook`)\\n1. Add required configuration for Happo & Storybook\\n1. Test trigger a Happo run from local environment\\n1. Skip components that are currently skipped in Loki if needed\\n1. Replace Loki script with Happo in CI (and test)\\n1. Update relevant documentation around Storybook tests\\n1. Remove deprecated Loki scripts, files, documentation from the repo\\nSince Happo offers a 30 day free trial, the above _should be able to_ be completed independently of providing payment information for an account.\\n### References\\n- [Happo: Getting Started](https:\/\/docs.happo.io\/docs\/getting-started)\\n- [Happo: Storybook plugin](https:\/\/docs.happo.io\/docs\/storybook)\\n- [Cross-browser screenshot testing with Happo.io and Storybook](https:\/\/medium.com\/happo-io\/cross-browser-screenshot-testing-with-happo-io-and-storybook-bfb0b848a97a)\\n### Additional Questions\\n- **Do we have budget for this? Estimate # of runs, expected cost.**\\n- Budget is TBD. If we determine no budget, we can investigate whether MilMove qualifies for Happo's free open source plan (this would require test reports be public to anyone with the link, though). Budget should be weighed against the current amount of time Loki is requiring from both engineers and CircleCI, which is not insubstantial. Running the tests in CircleCI is one of our longest-running jobs, and debugging or even approving intended changes requires additional engineering time.\\n- **Does it actually help us ensure that we are maintaining IE compatibility?**\\n- Loki does no testing in IE, and Happo does, so: yes.\\n- **Are we losing anything that Loki is providing?**\\n- No.\\n- **Estimated time cost to implement (very rough estimate fine, determined by reading Happo \u201cgetting started\u201d docs, etc.)**\\n- 1 day of engineering time to implement the strategy outlined above.\\n- **Does this feasibly integrate with our CI pipeline?**\\n- [Yes](https:\/\/docs.happo.io\/docs\/continuous-integration#happo-ci-circleci)\\n- **Does this feasibly run in a local dev environment? Is this extra development\/infra effort?**\\n- Happo tests can be triggered from local dev environments, but the report and screenshots will be generated in Happo's cloud environment. If we don't want to use the on-premise version, no extra dev\/infra effort is required.\\n","tokens":192,"id":3074,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0067-ppm-db-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Decision Drivers\\n* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we need more information?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\n","Decision":"* Time and complexity\\n* Do we have bandwidth to make these changes?\\n* Do we have enough capacity handle the unknowns or deal with future issues this decision may cause?\\n* Do we need more information?\\n* Do we have enough future capacity to delay this?\\n* Flexibility\\n* How likely is this solution to support new shipment types or even existing shipments?\\n* Consistency\\n* How consistent are our design patterns?\\n* Are they intuitive?\\n* Chosen Alternative: [Create a new table](#create-a-new-table)\\n* Positive Outcomes: We will have a pattern that is easier to understand and will potentially be helpful in the future when implementing new shipments. The tentative plan is to test this new pattern out with PPMs, and if things go smoothly, revisit this to be the default DB pattern for all new shipment types moving forward.\\n* Consequences: If this new pattern is not utilized elsewhere we may be adding more complexity by introducing another pattern that is only partially used.\\n","tokens":101,"id":3077,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0063-use-openapi-to-compile-api-specs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Problem Statement\\nWe have a specification-first development cycle for our APIs. This means that editing our API - adding endpoints,\\nediting responses, changing functionality - starts in the YAML file that contains the API definition. From that, we use\\n`go-swagger` to read our specification and generate Go types for use in our backend.\\n**The good:** With this model, we can focus on the API design without worrying about how to convert that into usable Go\\ncode - `go-swagger` does this for us. Code is neatly organized into separate packages for each API, so they can function\\nindependently.\\n**The problem:** Our APIs are all concerned with the same data models, so even though they are _technically_\\nindependent, they are highly related. We're defining the same objects over and over again in our YAML specs. All APIs\\nhave a shipment, a move, an orders object, and the list goes on. When we make one change to these objects, we have to\\nmake changes to each and every YAML file.\\nThis means our YAML files quickly get out of sync. We've had to deal with bugs stemming from this disconnect many times.\\nThis is also hugely redundant - there are hundreds of lines that are essentially identical in each API.\\nWe have to do a lot of manual type conversions in the backend to turn the Swagger-generated Go types in our general\\nmodel types. These type conversions are also redundant, and they're another place where we can miss changes that add or\\nmodify fields. One possible negative is that having shared types between APIs would threaten their ability to function\\nindependently. However, our APIs are extremely interconnected on the backend and use many of the same services, so there\\nis a question of how independent they could possibly be regardless.\\nLastly, we struggle with maintaining the same standards in each API. Some are more resistant to change, and we don't\\nhave a good method for incrementally standardizing those APIs.\\n","Decision":"### Chosen Alternative: _Use the `openapi` CLI tool to compile shared API definitions (Option 3)_\\nThis looks like the most complicated solution by far. And for the initial implementation, it is. We have already\\nintroduced the `openapi` tool to the project so that we can preview our API documentation, but now we will be dependent\\non it for our development process. We will also have to work in a new folder, so all of our engineers will have to\\nacclimate to the development cycle.\\nHowever, the benefits are significant. The `openapi` compiler dictates a structure that is organized and fairly\\nintuitive, making it easy to create, find, and reference separate definition files. Like option 2, edits to one file can\\napply to all of our APIs. Furthermore, the compiler can handle our files as-is, so we can gradually split our\\ndefinitions as we move forward.\\nUnlike option 2, this method won't change the outward behavior of our APIs. External tools like Load Testing, and\\neventually the Prime integration, won't need to change the way they consume our content. This was ultimately the\\ndeciding factor because, even though this option _looks_ more complicated, the overall impact of the switch will be\\nminimal. Load Testing was also completely non-functional with option 2, and I have not yet figured out how to make it\\nwork.\\n","tokens":409,"id":3079,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0064-use-stateless-services-with-context.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Problem statement\\nWe want our services to be composable, so that one service can call\\nanother. We also want to be able to have a per request trace id\\nassociated with a logger so that we can correlate log messages in a\\nsingle request.\\nRight now, most services are initialized with a database connection pool\\nand a logger.\\nWhen using the `database\/sql` package, each database\\nrequest uses a different connection to the database. Some of our\\nservices start transactions and then want to call other services. The\\n\"sub-service\" uses its own connection pool and thus does not have\\nvisibility to the changes made within the transaction.\\nSome services have a way to set the connection used the service, but\\nsince we have a single service object, it seems almost certain that\\nwith goroutines handling each request that we'll have errors\\nwhen multiple requests are running simultaneously.\\nThe same problem exists for logging because services are logging using\\nthe \"global\" logger and not one initialized per request. That means\\nservice logs don't include the per request trace id.\\n- Modify all service methods to accept *both* a `Context` and a custom interface\\n- Modify all service methods to accept a `Context`\\n- Do nothing\\n","Decision":"- Chosen Alternative: Modify all service methods to accept a custom interface\\n","tokens":257,"id":3080,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mymove\/0075-cli-spinner.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Decision Drivers\\n- We want to resolve the race conditions that appear when the `-race` flag is turned on in Go\\n","Decision":"- We want to resolve the race conditions that appear when the `-race` flag is turned on in Go\\n- Remove all the spinner code\\n- Use [YAC Spin](https:\/\/github.com\/theckman\/yacspin)\\n- Replace [pterm](https:\/\/github.com\/pterm\/pterm) completely\\n- Continue to use [pterm](https:\/\/github.com\/pterm\/pterm) spinner\\nRemove all spinner code.\\n- This allows us to resolve races conditions that are flagged by Go's `-race` flag. Resolving these race conditions helps resolve [STIG V-70185](https:\/\/www.stigviewer.com\/stig\/application_security_and_development\/2018-12-24\/finding\/V-70185).\\n- Having a spinner display is not critical to the operation of the application. The text of the message should be retained to maintain context as to what is being run.\\n### Remove all spinner code\\n- `+` No race conditions created from spinners.\\n- `+` Less code to maintain.\\n- `-` Work to be done to remove the spinners.\\n- `-` No visibility on if the task that would be using the spinner is active.\\n### Use YAC Spin\\n- `+` No race conditions from spinners because YAC Spin is thread safe.\\n- `-` The features that we use for the terminal user interface are split across two libraries.\\n- `-` Some work will need to be done to replace pterm's spinner.\\n### Replace pterm\\n- `+` Wouldn't have to deal with race conditions from pterm.\\n- `+` All the features for a terminal user interface are in one library.\\n- `-` Have to find a sutable replacement that is thread safe and does everything that MilMove uses pterm for.\\n- `-` Work needs to be done to refactor and replace pterm.\\n### Continue to use pterm's spinner\\n- `+` All the terminal user interface related stuff are in one library.\\n- `+` No code changes needed.\\n- `-` MilMove is stuck with race conditions since the race conditions are occurring in a third party library.\\n","tokens":26,"id":3082,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0003-tagging-for-releases-and-long-term-support.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAtlas needs to provide long-term support (LTS) to internal users and also iterate quickly and have frequent releases.  We need some agreed versioning and release scheme so users and developers know what is supported and what will be supported in the future.\\n","Decision":"Atlas is versioned with tags of the form X.Y.Z.  For the purpose of discussion let A.B.C be the lexicographically largest version of atlas and let a.b.c be them most recent version of atlas on your branch (these are the same if you are on **develop** but are different if you are working on an **LTS branch**).\\n### To Publish A New Release of AtlasDB\\nIf you are on **develop** then you tag with **a.b+1.0** (which is the same as **A.B+1.0**) otherwise tag with **a.b.c+1**.  The patch version will be primarily incremented when backporting fixes to an old branch of AtlasDB.\\n### When Publishing A Release with Long-Term Support\\n1. From the tip of develop create a new protected branch on Github called **A.B+1.x** (e.g. \u201c0.6.x\u201d, x here is literally the character 'x')\\n2. Tag the forking point of that branch with **A.B+1.0** (e.g. \u201c0.6.0\u201d)\\nThe **A.B.x** branch serves as an LTS branch and all further critical changes needed go on that branch and get a new tag (following the previous rule).\\n","tokens":54,"id":3084,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0011-retry-long-running-locks-via-blockingtimeoutexception.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur implementation of AtlasDB clients and the TimeLock server were interacting in ways that were causing the TimeLock\\nserver to experience large thread buildups when running with HTTP\/2. This manifested in\\n[issue #1680](https:\/\/github.com\/palantir\/atlasdb\/issues\/1680). The issue was eventually root caused to long-running\\nlock requests in excess of the Jetty idle timeout; the server would close the relevant HTTP\/2 stream, but *not*\\nfree up the resources consumed by the request. Eventually, all server threads on the leader node would be busy handling\\nlock requests that had timed out. The leader would thus not respond to pings, resulting in other nodes proposing\\nleadership. This is problematic as leader elections cause all locks to be lost, and thus most\\ninflight transactions will fail. A concrete trace is as follows:\\n1. Client A acquires lock L\\n2. Client B blocks on acquiring lock L, with request B1\\n3. The idle timeout for Client B's connection runs out, and we close the HTTP\/2 stream\\n4. Client B retries, and blocks on acquiring lock L, with request B2\\n5. Client A releases L\\n6. Request B1 is granted, but client B is no longer listening on it\\n7. (2 minutes) The idle timeout for Client B will expire four times, and Client B retries with requests B3, B4, B5, B6\\n8. The lock granted to request B1 is reaped, and request B2 is granted, but client B is not listening for it\\nSince we retry every 30 seconds by default but only \"service\" one request every 2 minutes and 5 seconds, we accumulate\\na backlog of requests. Also, observe that setting the idle timeout to 2 minutes and 5 seconds does not solve the\\nproblem (though it does mitigate it), since multiple clients may be blocking on acquiring the same lock.\\n","Decision":"### Time Limiting Lock Requests\\nWe decided that solutions to this problem should prevent the above pattern by satisfying one or both of the following:\\n1. Prevent the idle timeout from reasonably occurring.\\n2. Ensure resources are freed if the idle timeout triggers.\\nWe introduced a time limit for which a lock request is allowed to block - we call this the *blocking timeout*.\\nThis is set to be lower than the Jetty idle timeout by a margin, thus achieving requirement 1. Even if we lose the\\nrace, we will still free the resources shortly after, thus achieving requirement 2.\\nIn the event that a lock request blocks for longer than the blocking timeout, we interrupt the requesting thread and\\nnotify the client by sending a `SerializableError` which wraps a `BlockingTimeoutException`. Upon receiving a\\n503, a client checks the nature of the error occurring on the server; in the event of `BlockingTimeoutException`,\\nwe retry on the same node, and reset the counter of the number of times we've failed to zero.\\nThe timeout mechanism is implemented using Guava's `TimeLimiter`, and server time is thus considered authoritative; we\\nbelieve this is reasonable as it is used for both our time limiting and for Jetty's handling of idle timeouts. Thus,\\nwe are relatively resilient to clock drift relative to the client or to other nodes in the TimeLock cluster.\\n### Alternatives Considered\\n#### 1. Significantly increase the Jetty idle timeout\\nWe could have configured the recommended idle timeout for TimeLock to be substantially longer than we expect any lock\\nrequest to reasonably block for, such as 1 day.\\nThis solution is advantageous in that it is simple. However, the current default of 30 seconds is already longer than\\nwe would expect any lock requests to block for. Furthermore, in the event of client or link failures, it would be\\npossible that resources would be unproductively allocated to the associated connections for longer periods of time.\\nWe would also introduce a dependency on the idle timeout on the HTTP client-side, which would also need to be\\nincreased to account for this (the current default is 60 seconds).\\n#### 2. Convert the lock service to a non-blocking API\\nWe could have changed the lock API such that lock requests return immediately regardless of whether the lock being\\nasked for is available or not. If any lock being asked for was not available yet, the server would return a token\\nindicating that the request was to be satisfied. The client can then, at a later time, poll the server with its token\\nto ask if its request had been satisfied; alternatively, we could investigate HTTP\/2 or WebSocket server push.\\nThis solution is likely to be the best long-term approach, though it does involve a significant change in the API\\nof the lock service which we would prefer not to make at this time.\\n#### 3. Implement connection keep-alives \/ heartbeats\\nWe close the connection if no bytes have been sent or received for the idle timeout. Thus, we can reset this timeout\\nby sending a *heartbeat message* from the client to the server or vice versa, at a frequency higher than the idle\\ntimeout. We would probably prefer this to live on the server, since the idle timeout is configured on the server-side.\\nThis solution seems reasonable, though it does not appear to readily be supported by Jetty.\\n#### 4. Send a last-gasp message to the lock service to free resources before the stream closes\\nAn idea we considered was to have Jetty free resources on the lock service before closing the HTTP\/2 stream.\\nThis solution appears to be the cleanest of the \"free resources\"-based solutions, including the one we chose to\\nimplement. Unfortunately, while this feature has been requested in Jetty, as at time of writing this has not\\nbeen implemented yet; see [Jetty issue #824](https:\/\/github.com\/eclipse\/jetty.project\/issues\/824).\\n#### 5. Have clients truncate individual requests to the idle timeout\\nAn alternative to having the server return `BlockingTimeoutException`s on long-running requests would be for clients\\nto trim down any requests to an appropriate length (or, in the case of `BLOCK_INDEFINITELY`, indefinitely send\\nrequests of a suitable length). For example, with the default idle timeout of 30 seconds, a client wishing to block\\nfor 45 seconds could send a lock request that blocks for 30 seconds, and upon failure submit another request that\\nblocks for just under 15 seconds (suitably accounting for network overheads).\\nThis solution is relatively similar to what was implemented, though it requires clients to know what the\\naforementioned \"appropriate length\" should be (it needs to be the idle timeout or less) which is inappropriate as\\nthat timeout is configured on the server side.\\n#### 6. Implement a \"magic\" HTTP status code or header to ask clients to retry\\nAn alternative to serializing exceptions into `SerializableError` s could be defining a specific HTTP status code\\nand\/or custom header to indicate that a blocking timeout has occurred and\/or clients should retry. This is used\\nin practice e.g. in nginx, where a 495 indicates an error with a client's SSL certificates.\\nThis solution would be simpler than serializing exceptions; our existing `AtlasDbErrorDecoder` already switched on the\\nstatus code returned in an HTTP response. However, we prefer not to introduce any custom status codes where feasible\\n(since clients are unlikely to understand these status codes). A similar argument, though perhaps weaker, applies\\nfor headers as well.\\n","tokens":410,"id":3085,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0004-create-schema-lock-table-via-a-one-off-cli-command.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n","Decision":"We will implement the CLI based solution as:\\n- It is safe\\n- Requires a very simple set of code changes\\n- Should have minimal drawbacks if people can automate it\\n","tokens":210,"id":3086,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0008-add-heartbeat-for-schema-lock-holders.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs of version 0.16.0, anyone trying to acquire the a schema mutation lock that is already held by someone else\\nwould keep trying to acquire the lock until a fixed timeout threshold. Once that threshold was reached, we would\\nstop trying to acquire the lock and throw a timeout exception. This would effectively kill the client trying to grab\\nthe lock and necessitate a restart. We have a timeout threshold instead of retrying indefinitely to handle cases\\nwhere the lock holder dies without releasing the lock.\\nHowever, we think this has caused a lot of unnecessary exceptions since the threshold is static. We believe we could\\navoid a lot of timeouts if we have some way of informing anyone trying to acquire the lock if the lock holder is alive\\nor dead. Once we have that information, we can keep retrying as long as we know that the lock holder is alive without\\ntiming out needlessly.\\n","Decision":"To inform anyone trying to acquire the lock if the lock holder is dead or alive, we decided to have the lock holder\\nperiodically update its lock (heartbeat) in the locks table while its still holding the lock. That way, anyone else\\ntrying to acquire the lock can see the updates to the lock and keep retrying as long as the updates continue.\\nCurrently, the lock value in the locks table consists of a randomly generated identifier. To incorporate the heartbeat\\nupdates into the same value, we decided to change the lock value to following format:\\n`<randomly generated id>_<heartbeat value>`. The heartbeat value would essentially be incremented by 1 for every\\n\"heartbeat\" by the lock holder.\\nFor example, a lock with identifier `123456789` and heartbeat value `23` would have `123456789_23` as its lock value.\\nOnce we had the heartbeat mechanism in place, we needed to decide whether to allow someone waiting on a lock\\nto override the lock and grab it once it detects that the lock holder is dead (via its heartbeat). Allowing locks\\nthat could be overridden weakened the guarantee provided by the lock and could cause corruption in rare situations.\\nOn the plus side, overridable locks would minimize human intervention when dealing with dead lock holders.\\nHowever, it was decided that we should avoid the possibility of data corruption as much as possible. Therefore, we won't\\nhave overridable locks for now and simply use the information about the health of the lock holder to better inform\\nthe exception we throw when we timeout on the lock. This will still require manual intervention (truncating the locks\\ntable) when a lock holder dies without releasing the lock but provides a better picture of the situation\\nto the human user.\\n","tokens":189,"id":3089,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0012-batch-timestamp-requests-on-the-client-side.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA TimeLock cluster can serve multiple services. Under these circumstances, each TimeLock client node can separately\\nopen connections to TimeLock and request timestamps and locks. As timestamp and lock requests are typically very common\\n(for instance, a single write transaction needs to contact TimeLock multiple times), TimeLock tends to experience\\nrelatively heavy loads. This affects overall throughput, as TimeLock takes longer to respond to requests.\\nThe `PersistentTimestampService` supports querying for a contiguous range of timestamps. This functionality has\\nalready been used in production, in Palantir's large internal product.\\n","Decision":"- Use the `RequestBatchingTimestampService` to coalesce timestamp requests to TimeLock, and enable it by default.\\nThis means that a TimeLock client will, at any time, have at most one timestamp request in flight to TimeLock.\\nWhen a client makes a request, if there is no request in-flight, the request is submitted immediately.\\nOtherwise, requests accumulate in a batch. Once the in-flight request completes, the batch request is sent as a\\n`getFreshTimestamps` request for the relevant number of timestamps, and the `TimestampRange` returned is distributed\\namong the threads that made the request.\\n- Configure `RequestBatchingTimestampService` with a delay interval of zero milliseconds. In practice, this means that\\nwhen a request returns, the next request will immediately be dispatched (provided timestamps have actually been\\nrequested). This does imply that, on average, requests take half a round-trip time longer. In practice, things\\nare more complex:\\n- In addition to the half-RTT, we also incur costs involved in synchronization of the batching.\\n- TimeLock itself tends to respond more quickly, though, as it is less heavily loaded. For many internal contexts,\\nTimeLock being less heavily loaded is a particularly compelling reason to enable batching by default, as it\\nserves timestamps and locks for many services.\\n- Maintain the ability to disable request batching. This was kept, because request batching can cause a modest\\nincrease in latency especially where client loads are light. Request batching might not be appropriate for\\napplications that require real-time responses.\\n","tokens":121,"id":3093,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0005-stop-allowing-embedded-lock-and-timestamp-services-in-production.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently a completely normal way to set up AtlasDB is to run an embedded lock and timestamp service that only your atlas client talks to in Java. If this happens with 2 different clients, then they can easily clash with each other and cause undefined issues in the KVS. It also stops CLIs that require knowledge of lock and timestamp being able to run against these atlas clients.\\nThere is no reason for this situation to arise in production. Lock and Timestamp services can still be deployed in a sensible fashion without spinning up a separate service by registering the lock and timestamp server endpoints in your application, and having your own nodes act as the servers. Alternatively, you can spin up an completely external lock and timestamp service whose whole reason for existence is to perform these roles.\\n","Decision":"1. Remove the ability to run embedded lock and timestamp services in configuration\\n2. Provide an easy to run Timelock Server which provides lock and timestamp services to applications\\n3. Provide a sensible development story for testing against a TimelockServer that can be started from Java.\\n","tokens":157,"id":3095,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0010-use-partial-row-complete-cell-batching-in-gettimestampsbycell.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs of version 0.35.0 , our implementation of `DbKvs.getTimestampsByCell` was creating a MultiMap with all pairs of\\n(Cell, timestamp) from the range determined by the row batch size. In case of wide rows, or simply a row batch size\\nthat is too large, this could cause us to run out of memory; see [issue #982](https:\/\/github.com\/palantir\/atlasdb\/issues\/982).\\n","Decision":"We decided to use a block (a triple of row name, column name, and timestamp) batch size, defaulting to 1000000 to avoid\\nloading too many blocks into memory. The algorithm will fetch blocks by row, column, and increasing timestamp until the\\nbatch size is met. In that case:\\n1. If at least one row was fully processed, we return RowResults with all timestamps each of those rows, and continue\\nprocessing from the beginning of current row when more RowResults are necessary.\\n2. If no rows were fully processed, we continue processing blocks until the end of the current cell (row, column),\\nfetching all timestamps for that cell. We return a (partial) RowResult that contains all timestamps for that row\\nuntil and including the final cell processed. To create the next RowResult we then continue processing the row after\\nthe last cell processed, effectively splitting the wide row into several RowResults.\\nExample:\\nAssuming a batch size of 10 and the following table\\n|   |         1          |          2         |     3     |\\n|   | ------------------ | ------------------ | --------- |\\n| 1 |     (1, 2, 3)      |      (4, 5, 6)     | (7, 8, 9) |\\n| 2 |    (1, 2, 3, 4)    |    (4, 5, 6, 7)    | (7, 8, 9) |\\n| 3 | (1, 2, 3, 4, 5, 6) | (4, 5, 6, 7, 8, 9) | (7, 8, 9) |\\n| 4 |     (1, 2, 3)      |                    |           |\\nThe RowResults will be as follows:\\n- (1 -> (1 -> (1, 2, 3); 2 -> (4, 5, 6); 3 -> (7, 8, 9)))\\n- (2 -> (1 -> (1, 2, 3, 4); 2 -> (4, 5, 6, 7); 3 -> (7, 8, 9)))\\n- (3 -> (1 -> (1, 2, 3, 4, 5, 6); 2 -> (4, 5, 6, 7, 8, 9)))\\n- (3 -> (3 -> (7, 8, 9)))\\n- (4 -> (1 -> (1, 2, 3)))\\nOther options considered:\\n1. Partial row batching: fetch up to 1000000 blocks. If the block batch size is hit, stop immediately (cells can be split across multiple batches).\\n2. Full row batching: fetch up to 1000000 blocks. If the block batch size is hit in the middle of a row, continue fetching until the row is exhausted.\\n3. No batching, but throw an error when the block batch hint is reached\\n- Option 1 was considered, but was replaced by the modified version above. This is because sweep must ensure that all blocks\\nexcept for the most recent (before the immutable timestamp) are deleted. This can be achieved by repeating the last\\nblock from one batch in the next batch, or by keeping this last result in sweep's memory, so that it knows to remove it,\\nif further blocks for the same cell are encountered. Neither option is compelling: the first is hard to reason about,\\nand the second reduces the scope for parallelisation, and risks introducing a correctness bug.\\n- Option 2 was not chosen, because it does not guard well against wide rows (many cells) that have many overwrites.\\n- Option 3 was considered, but was ultimately discarded because it relies on properties of the code that calls getCellTimestamps.\\nIn particular, there needs to be retry logic that detects that an error was thrown, and reduces the batch size accordingly.\\n","tokens":100,"id":3097,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3098,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0014-use-bats-for-testing-bash-wrappers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe write wrapper scripts for simpler and consistent operations.\\nHow should we test these scripts?\\n## Decision Drivers\\n* We want to use the best tools out there.\\n* We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool.\\n* We want to make contributions inviting.\\n","Decision":"* We want to use the best tools out there.\\n* We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool.\\n* We want to make contributions inviting.\\nChosen option: \"bats\", because the benefit of using a standard and rather light tool outweighs the cost of collective training on the new tool.\\n### Positive Consequences\\n* We use a pretty standard tool for testing in the bash universe.\\n* We do not risk re-inventing the while by writing our own wrappers around `alias`, `diff` and `test`.\\n### Negative Consequences\\n* We need to learn another tool, fortunately, it seems pretty light.\\n","tokens":87,"id":3100,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0001-use-rook-storage-orchestrator.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCompliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?\\n## Decision Drivers\\n* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\n","Decision":"* Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.\\n* Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.\\n* Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)\\nChosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.\\n### Positive Consequences\\n* We no longer need to worry about cloud provider without native storage.\\n### Negative Consequences\\n* We need to deprecate our NFS storage provider.\\n* Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.\\n","tokens":158,"id":3101,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0012-do-not-persist-dex.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n> Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.\\nWhat persistence option should we use?\\n## Decision Drivers\\n* CRDs add complexity\\n* Storage adds complexity\\n","Decision":"* CRDs add complexity\\n* Storage adds complexity\\nChosen option: \"use memory\", because it simplified operations with little negative impact.\\n### Positive Consequences\\n* Dex brings no additional CRDs, which simplified upgrades.\\n* Dex brings no state, which simplified upgrades.\\n### Negative Consequences\\n* The authentication flow is disrupted, if Dex is rebooted *exactly* during an authentication flow. There is no user impact if Dex is restarted after the [JWT](https:\/\/jwt.io\/) was issued. Cristian tested this with `kubectl` and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future.\\n","tokens":54,"id":3102,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0006-use-standard-kubeconfig-mechanisms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo increase adoption of Compliant Kubernetes, we were asked to observe the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment). Currently, Compliant Kubernetes's handing of kubeconfig is astonishing. Most tools in the ecosystem use the standard `KUBECONFIG` environment variable and kubecontext implemented in the client-go library. These tools leave it up to the user to set `KUBECONFIG` or use the default `~\/.kube\/config`. Similarly, there is a default kubecontext which can be overwritten via command-line. Tools that get cluster credentials generate a context related to the name of the cluster.\\nTools that behave as such include:\\n* `gcloud container clusters get-credentials`\\n* `az aks get-credentials`\\n* `kops`\\n* `helmfile`\\n* `helm`\\n* `kubectl`\\n* `fluxctl`\\n## Decision Drivers\\n* Compliant Kubernetes needs to observe the Principle of Least Astonishment.\\n* Compliant Kubernetes needs to be compatible with various \"underlying\" vanilla Kubernetes tools.\\n* Compliant Kubernetes needs to be usable with various tools \"on top\".\\n","Decision":"* Compliant Kubernetes needs to observe the Principle of Least Astonishment.\\n* Compliant Kubernetes needs to be compatible with various \"underlying\" vanilla Kubernetes tools.\\n* Compliant Kubernetes needs to be usable with various tools \"on top\".\\nWe chose using standard kubeconfig mechanism, because it improves integration both with tools \"below\" Compliant Kubernetas and \"on top\" of Compliant Kubernetes.\\nTools that produce Kubernetes contexts are expected to use an approach similar to `kubectl config set-cluster`, `set-credentials` and `set-context`. The name of the cluster, user and context should be derived from the name of the cluster.\\nTools that consume Kubernetes contexts are expected to use an approach similar to `kubectl`, `helm` or `helmfile` (see links below).\\n","tokens":251,"id":3103,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0007-make-monitoring-forwarders-storage-independent.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn the context of this ADR, **forwarders** refers to any components that are necessary to forward monitoring information -- specifically traces, metrics and logs -- to some monitoring database. As of February 2021, Compliant Kubernetes employs two projects as forwarders:\\n* [Prometheus](https:\/\/prometheus.io\/) for metrics forwarding;\\n* [fluentd](https:\/\/www.fluentd.org\/) for log forwarding.\\nSimilarly, two projects are employed as monitoring databases:\\n* [InfluxDB](https:\/\/www.influxdata.com\/) for metrics;\\n* [Elasticsearch](https:\/\/opendistro.github.io\/for-elasticsearch\/) for logs.\\nOverall, the monitoring system needs to be one order of magnitude more resilient than the monitored system. Forwarders improve the resilience of the monitoring system by providing buffering: In case the database is under maintenance or down, the buffer of the forwarders will ensure that no monitoring information is lost.\\nHence, forwarders are subject to the following tensions:\\n* More buffering implies storage, which make the forwarders vulnerable to storage outages (e.g., disk full, CSI hiccups);\\n* Less buffering implies higher risk of losing monitoring information when the database is under maintenance or down.\\n## Decision Drivers\\n* We want a robust monitoring system.\\n* We want to monitor the storage system.\\n* We want VM-template-based rendering of the workload cluster, which implies no cloud native storage integration.\\n* We want to make it easier to \"cleanup and start from a known good state\".\\n* We want to have self-healing and avoid manual actions after failure.\\n* We want to be able to find the root cause of an incident quickly.\\n* We want to run as many components non-root as possible and tightly integrate with [securityContext](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/security-context\/#configure-volume-permission-and-ownership-change-policy-for-pods).\\n","Decision":"* We want a robust monitoring system.\\n* We want to monitor the storage system.\\n* We want VM-template-based rendering of the workload cluster, which implies no cloud native storage integration.\\n* We want to make it easier to \"cleanup and start from a known good state\".\\n* We want to have self-healing and avoid manual actions after failure.\\n* We want to be able to find the root cause of an incident quickly.\\n* We want to run as many components non-root as possible and tightly integrate with [securityContext](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/security-context\/#configure-volume-permission-and-ownership-change-policy-for-pods).\\nChosen option: emptyDir for Prometheus as forwarder, because it allows monitoring of the storage system in some cases (e.g. Rook) and can redeploy automatically after node failure. It also keeps the complexity down without much risk of data loss.\\nFluentd as forwarder is deployed via DaemonSet. Both, emptyDir and hostPath can be used.\\n### Positive Consequences\\n* We can monitor the storage system.\\n* Failure of the storage system does not affect monitoring forwarder.\\n* Forwarder can be easier deployed \"fresh\".\\n### Negative Consequences\\n* Buffered monitoring information is lost if node is lost.\\n* emptyDir can cause disk pressure. This can be handled by alerting on low disk space.\\n","tokens":412,"id":3105,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0015-we-believe-in-community-driven-open-source.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product\/project X already has feature Y\". Needless to say, this can cause a [\"Simpsons Already Did It\"](https:\/\/en.wikipedia.org\/wiki\/Simpsons_Already_Did_It) feeling.\\nThis ADR clarifies one of the core values of the Compliant Kubernetes project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make.\\n## Decision Drivers\\n* We do not want to depend on the interests of any single company, be it small or large.\\n* Our customers need to have a business continuity plan, see [ISO 27001, Annex A.17](https:\/\/www.isms.online\/iso-27001\/annex-a-17-information-security-aspects-of-business-continuity-management\/). Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management.\\n* We want to use the best tools out there.\\n","Decision":"* We do not want to depend on the interests of any single company, be it small or large.\\n* Our customers need to have a business continuity plan, see [ISO 27001, Annex A.17](https:\/\/www.isms.online\/iso-27001\/annex-a-17-information-security-aspects-of-business-continuity-management\/). Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management.\\n* We want to use the best tools out there.\\nChosen option: \"prefer community-driven open source solutions\".\\n### Positive Consequences\\n* We do not depend on the interests of any single company.\\n* Our customers do not depend on the interests of any single company.\\n* Business continuity is significantly simplified for our customers.\\n* We have better chances at influencing projects in a direction that is useful to us and our customers. The smaller the project, the easier to influence.\\n### Negative Consequences\\n* Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves.\\n* As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.\\n","tokens":238,"id":3106,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0005-use-individual-ssh-keys.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCurrently, we create per-cluster SSH key pairs, which are shared among administrators. This is problematic from an information security perspective for a few reasons:\\n1. It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes control plane Nodes.\\n2. It makes credential management challenging, e.g., when onboarding\/offboarding administrators.\\n3. It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all administrators.\\n4. It encourages storing the SSH key pair without password protection.\\n5. It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey.\\n6. It violates the Principle of Least Astonishment.\\n## Decision Drivers\\n* We need to stick to information security best-practices.\\n","Decision":"* We need to stick to information security best-practices.\\nWe will manage SSH keys via an Ansible role, since it allows rotating\/adding\/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The [compliantkubernetes-kubespray](https:\/\/github.com\/elastisys\/compliantkubernetes-kubespray) project will make it easy to configure SSH keys.\\n### Bootstrapping\\nThe above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows.\\nFor cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init:\\n* [AWS](https:\/\/github.com\/kubernetes-sigs\/kubespray\/blob\/release-2.15\/contrib\/terraform\/aws\/variables.tf#L9)\\n* [Exoscale](https:\/\/github.com\/kubernetes-sigs\/kubespray\/blob\/master\/contrib\/terraform\/exoscale\/variables.tf#L24)\\n* [GCP](https:\/\/github.com\/kubernetes-sigs\/kubespray\/blob\/release-2.15\/contrib\/terraform\/gcp\/variables.tf#L57)\\n* [OpenStack](https:\/\/github.com\/kubernetes-sigs\/kubespray\/blob\/release-2.15\/contrib\/terraform\/openstack\/variables.tf#L81)\\nThe administrator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other administrators.\\nBYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email\/Slack to the VM\/metal administrator.\\n","tokens":175,"id":3107,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0002-use-kubespray-for-cluster-lifecycle.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCompliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house `ck8s-cluster` implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters.\\n## Decision Drivers\\n* We want to differentiate on top of vanilla Kubernetes cluster.\\n* We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible.\\n* We promise building on top of best-of-breeds open source projets.\\n* We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.\\n","Decision":"* We want to differentiate on top of vanilla Kubernetes cluster.\\n* We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible.\\n* We promise building on top of best-of-breeds open source projets.\\n* We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.\\nWe chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters.\\n### Positive Consequences\\n* We learn how to use a widely-used tool for cluster lifecycle management.\\n* We support many cloud providers.\\n* We can differentiate on top of vanilla Kubernetes.\\n### Negative Consequences\\n* We need training on kubespray.\\n* We need to port our tooling and practices to kubespray.\\n* We need to port `compliantkubernetes-apps` to work on kubespray.\\n","tokens":196,"id":3108,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0009-use-cluster-issuers-for-letsencrypt.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nData protection regulations require encrypting network traffic over public networks, e.g., via HTTPS. This requires provisioning and rotating TLS certificates. To automate this task, we use the [cert-manager](https:\/\/cert-manager.io\/), which automates provisioning and rotation of TLS certificates from [Let's Encrypt](https:\/\/letsencrypt.org\/).\\nThere are two ways to configure Let's Encrypt as an issuers for cert-manager: [Issuer and ClusterIssuer](https:\/\/cert-manager.io\/docs\/concepts\/issuer\/). The former is namespaced, whereas the latter is cluster-wide. Should we use Issuer or ClusterIssuer?\\n## Decision Drivers\\n* We want to make compliantkubernetes-apps less fragile, and LetsEncrypt ratelimiting is a cause of fragility.\\n* We want to make it easy for users to get started with Compliant Kubernetes in a \"secure by default\" manner.\\n* We want to have a clear separation between user and administrator resources, responsibilities and privileges.\\n* We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes clusters that hosts both service cluster and workload cluster components.\\n","Decision":"* We want to make compliantkubernetes-apps less fragile, and LetsEncrypt ratelimiting is a cause of fragility.\\n* We want to make it easy for users to get started with Compliant Kubernetes in a \"secure by default\" manner.\\n* We want to have a clear separation between user and administrator resources, responsibilities and privileges.\\n* We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes clusters that hosts both service cluster and workload cluster components.\\nChosen option: \"Use ClusterIssuers in the service cluster; optionally enable ClusterIssuers in the workload cluster(s)\", because it reduces fragility, clarifies responsibilities, makes it easy to get started securely.\\nEach cluster is configured with an optional ClusterIssuer called `letsencrypt-prod` for LetsEncrypt production and `letsencrypt-staging` for LetsEncrypt staging. The email address for the ClusterIssuers is configured by the administrator.\\n","tokens":242,"id":3111,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0017-persist-dex.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n> Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.\\nWhat persistence option should we use?\\n## Decision Drivers\\n* CRDs add complexity\\n* Storage adds complexity\\n* We want to frequently reboot Nodes for security patching\\n* We want to deliver excellent user experience\\n","Decision":"* CRDs add complexity\\n* Storage adds complexity\\n* We want to frequently reboot Nodes for security patching\\n* We want to deliver excellent user experience\\nChosen option: \"use CRD-based storage\", because it improves user experience when Nodes are rebooted.\\nWith \"memory\" storage, Dex loses the OpenID keys when restarted, which leads to the user being forced to eventually re-login. Worst off, this forced re-login happens unexpectedly from the user's perspective, when the Kubernetes apiserver chooses to refresh the OpenID keys.\\nHere is the experiment to illustrate the issue:\\n```console\\n$ curl https:\/\/dex.$DOMAIN\/.well-known\/openid-configuration > before-openid-configuration.json\\n$ curl https:\/\/dex.$DOMAIN\/keys > before-keys.json\\n$ kubectl delete pods -n dex -l app.kubernetes.io\/instance=dex\\n$ curl https:\/\/dex.$DOMAIN\/.well-known\/openid-configuration > after-openid-configuration.json\\n$ curl https:\/\/dex.$DOMAIN\/keys > after-keys.json\\n$ diff -y before-openid-configuration.json after-openid-configuration.json\\n[empty output, no differences]\\n$ diff -y before-keys.json after-keys.json\\n[all keys are replaced]\\n```\\n### Positive Consequences\\n* Nodes which host Dex can be rebooted for security patching\\n* User experience is optimized\\n### Negative Consequences\\n* Dex will have a more permissions in the Service Cluster (see [`rbac.yaml`](https:\/\/github.com\/dexidp\/helm-charts\/blob\/dex-0.6.3\/charts\/dex\/templates\/rbac.yaml))\\n* We will need to closely monitor migration steps for Dex\\n","tokens":75,"id":3112,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0011-let-upstream-projects-handle-crds.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCustomResourceDefinitions (CRDs) are tricky. They are essentially a mechanism to change the API of Kubernetes. Helm 2 had zero support for CRDs. Helm 3 has [support for installing CRDs](https:\/\/helm.sh\/docs\/topics\/charts\/#custom-resource-definitions-crds), but not upgrading them.\\nHow should we handle CRDs?\\n## Decision Drivers\\n* CRDs add complexity and need to be treated specially.\\n* Generally need to \u201ctrim fat\u201d and rely on upstream.\\n","Decision":"* CRDs add complexity and need to be treated specially.\\n* Generally need to \u201ctrim fat\u201d and rely on upstream.\\nChosen option: \"Rely on upstream\", because it trims fat and reduces astonishment.\\nAt installation, rely on upstream's approach to install CRDs (see below). At upgrade, propagate upstream migration steps in CK8s migration steps in each release notes. An [issue template](https:\/\/github.com\/elastisys\/compliantkubernetes-apps\/pull\/436) was created to ensure we won't forget.\\nSince we \"vendor in\" all Charts, CRDs can be discovered using:\\n```\\ngrep -R 'kind: CustomResourceDefinition'\\n```\\n### Positive Consequences\\n* Less astonishing, compared to installing Chart \"by hand\".\\n* Less maintenance, i.e., there is only one source of truth for CRDs.\\n### Negative Consequences\\n* None really.\\n","tokens":110,"id":3113,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0010-run-managed-services-in-workload-cluster.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo truly offer our users an option to run containerized workloads in EU jurisdiction, they also need additional managed services, like databases, message queues, caches, etc.\\nWhere should these run?\\n## Decision Drivers\\n* Some of these services are chatty and need low latency.\\n* Some of these services might assume trusted clients over a trusted network.\\n* We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Compliant Kubernetes features around monitoring, logging, access control and network segregation.\\n* We want to make it difficult for Compliant Kubernetes users to negatively affect managed services.\\n* We want to keep support for multiple workload cluster, i.e., application multi-tenancy.\\n* Many cloud providers do not support Service Type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes cluster.\\n* Service cluster might not exist in a future packaging of Compliant Kubernetes.\\n","Decision":"* Some of these services are chatty and need low latency.\\n* Some of these services might assume trusted clients over a trusted network.\\n* We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Compliant Kubernetes features around monitoring, logging, access control and network segregation.\\n* We want to make it difficult for Compliant Kubernetes users to negatively affect managed services.\\n* We want to keep support for multiple workload cluster, i.e., application multi-tenancy.\\n* Many cloud providers do not support Service Type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes cluster.\\n* Service cluster might not exist in a future packaging of Compliant Kubernetes.\\nChosen option: \"run managed services in workload cluster\".\\n","tokens":204,"id":3114,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0016-gid-0-is-okey-but-not-by-default.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nOpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes.\\nHowever, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see [\"Support arbitrary user ids\"][openshift-docs]) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain:\\n```Dockerfile\\nRUN chgrp -R 0 \/some\/directory && chmod -R g=u \/some\/directory\\n```\\nDuring execution, OpenShift assigns `gid=0` as a supplementary group to containers, so as to give them access to the required files.\\nIn contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., [CVE-2020-8554][cve]\\n), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign `gid=0` as a supplementary group.\\nThe `gid=0` practice above seems to have made its way in [quite a few Dockerfiles][github-search], however, it is far from being the default outside OpenShift.\\nWhat should Compliant Kubernetes do with the `gid=0` practice?\\n## Decision Drivers\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\n","Decision":"* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\nChosen option: \"disallow `gid=0` by default\". Enabling it on a case-by-case basis is okay.\\n### Positive Consequences\\n* We do not unnecessarily add a permission to containers.\\n### Negative Consequences\\n* Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster.\\n","tokens":451,"id":3115,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0003-push-metrics-via-influxdb.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters:\\n* Dex, for authentication;\\n* Elastisearch, for pushing logs (append-only).\\nCurrently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster.\\n## Decision Drivers\\n* We want to support workload multi-tenancy.\\n* We want to untangle the life-cycle of the service cluster and workload cluster.\\n* The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster.\\n","Decision":"* We want to support workload multi-tenancy.\\n* We want to untangle the life-cycle of the service cluster and workload cluster.\\n* The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster.\\nWe chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible.\\n### Positive Consequences\\n* All of `*.$opsDomain` can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup.\\n* Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy.\\n* The service cluster can be set up first, followed by one-or-more workload clusters.\\n* Workload clusters become more \"cattle\"-ish.\\n### Negative Consequences\\n* Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the `prometheus.$opsDomain` DNS entry.\\n* The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints.\\n* The workload clusters will have to properly label their metrics.\\n* Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage.\\n","tokens":163,"id":3116,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"compliantkubernetes\/0018-use-probe-to-measure-internal-uptime.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to measure uptime for at least two reasons:\\n1. To serve as feedback on what needs to be improved next.\\n2. To demonstrate compliance with our SLAs.\\nHow exactly should we measure uptime?\\n## Decision Drivers\\n* We want to reduce tools sprawl.\\n* We want to be mindful about capacity and infrastructure costs.\\n* We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.\\n","Decision":"* We want to reduce tools sprawl.\\n* We want to be mindful about capacity and infrastructure costs.\\n* We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.\\nChosen option: \"use Probe for measuring uptime of internal Compliant Kubernetes services\", because it measures uptime as observed by a consumer. Although this requires a bit of extra capacity for running Blackbox, the costs are worth the benefits.\\nInstead of configuring Blackbox directly, `Probe` is a cleaner abstraction provided by the Prometheus Operator.\\nThe following is an example for a Probe:\\n```yaml\\napiVersion: monitoring.coreos.com\/v1\\nkind: Probe\\nmetadata:\\nname: google-is-up\\nlabels:\\nprobe: google\\nrelease: kube-prometheus-stack\\nspec:\\ninterval: 60s\\nmodule: http_2xx\\nprober:\\nurl: blackbox-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115\\ntargets:\\nstaticConfig:\\nstatic:\\n- https:\/\/www.google.com\\n```\\nThis will generate a metric as follows: `probe_success{cluster=\"ckdemo-wc\", instance=\"https:\/\/www.google.com\", job=\"probe\/demo1\/google-is-up\", namespace=\"demo1\"}`.\\n### Positive Consequences\\n* We measure uptime as observed by a consumer.\\n* Increasing redundancy, reducing failure time, etc. will contribute positively to our uptime, as desired.\\n### Negative Consequences\\n* We don't currently run Blackbox in the workload cluster, so we'll need a bit of extra capacity.\\n","tokens":116,"id":3117,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"racing-tips\/0002-use-kubernetes-and-docker.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA lot of options exist to create applications nowadays - serverless, PaaS, SaaS etc. The purpose of this project is to hone Kubernetes and Docker skills in particular which may mean use of these technologies seem over engineered.\\n","Decision":"* Use Kubernetes and Docker\\n* Use Amazon EKS\\n","tokens":49,"id":3118,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0006-no-more-memory-caches.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn `Wegwijs` we used a memory cache across all event handlers.\\nThese memory caches, however, were built up from certain projections. An example of this was the `organisation names` cache, which was built up from the `OrganisationDetail` projection.\\nUsing the memory cache outside the in-process event handlers, where the `OrganisationDetail` projection was built, causes a dependency between projections. This means that if we now want to rebuild projection X while the OrganisationDetail is not fully built, the memory cache is in an incorrect state. This defeats the purpose of the memory cache.\\n","Decision":"We will re-evaluate the use of memory caches. If we do use them, we will use them fully isolated within the containing handler.\\n","tokens":127,"id":3120,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0007-use-mediatr.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen doing cqrs, we need a way to send commands to their command handler.\\nSimilarly, we need a way to send events to their event handlers (note: plural).\\nMediatR is a low-ambition OSS library trying to solve a simple problem - decoupling the in-proc sending of messages from handling messages. Cross-platform, supporting .NET 4.5 and netstandard1.1.\\nMediatR can accomodate both handling Request\/Reply (commands) and Fire\/Forget (events) scenarios. Handlers are simply decorated with a marker interface, are self-contained and easily tested.\\n","Decision":"We will use MediatR for sending commands to their command handler.\\nWe will use MediatR for sending events to their event handlers.\\nWe will keep the pipelines for these two scenarios separate from each other.\\n","tokens":130,"id":3121,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0002-event-sourcing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur back-end needs to store state somehow.\\nTraditional Object-Relational systems store the current state of the application as the source of truth.\\nWhile these are simple to build, they are less conducive to changes in the way data is stored in the long run.\\nSystems built under our current shareholders have shown to be very prone to change over time, which gives us the need for systems that are conducive to change.\\nOur experience with our previous system under these shareholders, `Wegwijs`, has shown to be very permissive to change. `Wegwijs` had rather simple data manipulations, with the difficulty of the system lying in the presentation of this data.\\nWe built `Wegwijs` using event sourcing, which uses a record of all changes as its source of truth. These events can then be used to create different representations of the data using `projections`.\\nDue to this, we were able to capture the manipulations easily, while still allowing the stakeholders to change their mind about the presentation of this data.\\n","Decision":"We will use event sourcing the single source of truth for our application.\\n","tokens":219,"id":3122,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0003-async-event-handlers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhile we had great benefits from using an event sourced system in `Wegwijs`, the specific implementation had some drawbacks, specifically the transactional part.\\nTo get up and running quickly, we used a cqrs-es template application as the starting point of `Wegwijs`. To protect us from some of the harder parts of event sourcing, we choose to use an in-process, transactional approach to the way our events and projections were stored, ie: in one all-or-nothing transaction.\\nThis enabled us to get a quick start on `Wegwijs`, delivering features fast. Though we still stand by that decision, the template application also had some drawbacks in terms of testability and modularity. Since we also had in-memory state, this forced us to create rollback logic for this in-memory state in case the transaction failed.\\nIn short: we feel that this approach was great for fast start-up, but over time brings in more complexity than needed.\\nWe feel that, with the knowledge and experience we built on `Wegwijs`, we could now take the step a more modular approach using async event handlers.\\n","Decision":"We will use async event handlers, and thus eventual consistency in our application.\\n","tokens":236,"id":3125,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0011-sometimes-include-code-instead-of-packages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nInstead of reinvinting the wheel, we reuse OSS libraries. Some of these libraries are written as finished products, to be reused as is, without any alteration to the library. Examples of these are `Microsoft.Net.Aspnet`, `Newtonsoft.Json`, `Xunit` or `FluentAssertions`.\\nOther OSS libraries are a collection of best practices, and are specifically recommended by the library authors to be included into your own project. These projects realize and admit that there is no single solution, no silver bullet to the problem they're trying to solve. While these libraries typically have a packaged version to be included, you are instead advised not to use them unless absolutely necessary. Examples of these are `AggregateSource` and `Cedar.CommandHandling`.\\n","Decision":"We will include code in a separate csproj per library, instead of including packages, where advised by the library authors.\\nWe will also include the tests for these projects, to maintain quality when adapting these libraries to our needs.\\nWe will revise this approach if it starts to negatively impact our build times. A possible alternative for this could be to repackage our altered versions on a private NuGet server.\\n","tokens":159,"id":3128,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"publicservice-registry\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n### Changes to Nygard's approach\\nWe will keep ADRs in the project repository under `docs\/adr\/NNN-explanation-of-adr.md`.\\n","tokens":16,"id":3130,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pul_solr\/0004-revert-memory-tuning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe experimentally changed the GC settings for lib-solr1 (see ADR 0003). The\\nresult is that the heap grows quite large, right up to the limit, before going\\ninto GC. We aren't comfortable with letting the heap get so large.\\nHeap sizes don't grow as much on the other solr machines, and we also are not seeing the\\nOutOfMemoryErrors we had been observing when we initiated this change on\\nlib-solr1. Settings on the other machines are mostly pulled from the [default solr\\nrecommendations](https:\/\/github.com\/apache\/lucene-solr\/blob\/5f2d7c4855987670489d68884c787e4cfb377fa9\/solr\/bin\/solr.in.sh#L48-L62).\\n","Decision":"Revert the memory tuning on lib-solr1 to match those on the other machines.\\n","tokens":171,"id":3132,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pul_solr\/0002-increase-zookeeper-timeout.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA ZooKeeper timeout last night brought several Solr servers down. Factors that might make servers unresponsive\\nare garbage collection, network traffic\/latency, and high amounts of disk I\/O.\\n","Decision":"Increase the ZooKeeper timeout from 15 seconds to 60 seconds.\\n","tokens":43,"id":3133,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pul_solr\/0005-catalog-configset-rotation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSometimes we make a change to the catalog config set that will break search results if it's deployed without a new index already in place. In these cases the config set in use cannot be updated with this change and we have to deploy a different config set for the reindexing collection, which is then swapped in once it's fully populated.\\n","Decision":"For most config changes we will continue updating and reloading the config set in production.\\nIn the cases described above we will copy a new config set from the most recent production config set, incrementing the version suffix. Once the index is created and swapped in, the previously-used config set should be deleted.\\n","tokens":72,"id":3134,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pul_solr\/0003-remove-memory-tuning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have seen a number of OutOfMemoryErrors with Solr, mostly related to garbage collection.  Our memory\\nsettings include many overrides of the defaults, and it's not clear what they do individually or as a group.\\nThese settings have also been in use for several years, and we're not sure if they are still a good match for\\nour hardware and usage.\\n","Decision":"Remove the following memory tuning options from the Java options:\\n* `-Xss=256k`\\n* `-XX:NewRatio=3`\\n* `-XX:SurvivorRatio=4`\\n* `-XX:TargetSurvivorRatio=90`\\n* `-XX:MaxTenuringThreshold=8`\\n* `-XX:ConcGCThreads=4`\\n* `-XX:ParallelGCThreads=4`\\n* `-XX:PretenureSizeThreshold=64m`\\n* `-XX:CMSInitiatingOccupancyFraction=50`\\n* `-XX:CMSMaxAbortablePrecleanTime=6000`\\nAdd the following option to disable throwing an OutOfMemoryError if garbage collection takes too long:\\n* `-XX:-UseGCOverheadLimit`\\nDecrease total memory allocation from 72 GB to 40 GB.\\n","tokens":81,"id":3135,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0013-use-iac-network-tester-to-test-connectivity-rules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[IaC network tester](https:\/\/aws.amazon.com\/blogs\/networking-and-content-delivery\/integrating-network-connectivity-testing-with-infrastructure-deployment\/) is a programmatic wrapper around the [AWS Reachability Analyzer](https:\/\/docs.aws.amazon.com\/vpc\/latest\/reachability\/what-is-reachability-analyzer.html). It supports automated executions of the Reachability Analyzer, with feedback indicating if the network connectivity test was successful or not. Such a tool could allow us to automatically test network connectivity and take actions on the results.\\n","tokens":65,"id":3136,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0012-use-tgw-route-analyzer-to-check-desired-state-for-route-tables.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence..\\n","Decision":"[Transit Gateway Route Analyzer](https:\/\/docs.aws.amazon.com\/vpc\/latest\/tgw\/route-analyzer.html) is an AWS tool allowing the analysis of routes in Transit Gateway Route tables. It analyzes the routing path between a specified source and destination, and returns information about the connectivity between components. It is useful in validating and troubleshooting configuration. As such, it could be used to assess the desired state for transit gateway route table configuration, providing feedback on issues.\\n","tokens":65,"id":3137,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0003-use-aws-sso.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Modernisation Platform will be used by a large group of people across the Justice Digital and Technology estate, and each person will need their access to the Modernisation Platform managed.\\n","Decision":"Rather than managing the administrative burden of the Joiners, Movers and Leavers (JML) process ourselves, we can use AWS SSO with SCIM automated provisioning to allow access management through an Identity Provider that is already managed within Ministry of Justice, such as G-Suite. AWS SSO provides some advantages, and some disadvantages, over [IAM Federated Access](0002-use-iam-federated-access.md), as listed below.\\nWe can, if we wish, still utilise a different service (e.g. GitHub) as a IdP through Auth0, whilst SCIM provisioning with a different IdP (e.g. G-Suite).\\nWe decided to use AWS SSO in favour of [IAM Federated Access](0002-use-iam-federated-access.md) to allow us to centrally manage identities across the Ministry of Justice at the organisational level rather than at a team level.\\nA further benefit of this is that AWS SSO can be used across all AWS accounts, not just ones provisioned within the Modernisation Platform, as long as the AWS account is part of the AWS organisation.\\n","tokens":39,"id":3138,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0004-use-bash-node-python-as-core-languages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs we build the Modernisation Platform, we should define preferred languages for anything written and maintained by the Modernisation Platform team, including GitHub Actions and AWS Lambda functions.\\n","Decision":"We have decided to use `bash`, `nodejs`, and `python` as our core languages, as they are both: three of the most popular languages by repository in the Ministry of Justice, and the team already have knowledge of these languages.\\nWe ran a poll for what languages people already know within the team and the results were:\\n- 3 votes for `nodejs`\\n- 3 votes for `python`\\nWe have also includes `bash` in this list to be explicit in that we should and can use it when needed.\\nWe will leave it to each team member to decide what language is best for the problem they are trying to solve.\\n","tokens":37,"id":3139,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0002-use-iam-federated-access.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Modernisation Platform will be used by a large group of people across the Justice Digital and Technology estate, and each person will need their access to the Modernisation Platform managed.\\n","Decision":"Rather than managing the administrative burden of the Joiners, Movers and Leavers (JML) process ourselves, we can automate this for the team by utilising IAM Federated Access to allow access management through an identity provider that is already managed within Ministry of Justice. For example, we can use GitHub, which is included as part of the department's JML process, as an identity provider for access to our AWS account.\\n","tokens":39,"id":3140,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0008-use-kms-in-shared-services-for-cross-account-encryption.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMember account users will need to share encrypted things such as backups and snapshots between AWS accounts.\\n","Decision":"We've decided to use [AWS Key Management Service (KMS)](https:\/\/aws.amazon.com\/kms\/) for cross account encryption.\\nKeys will be created per business unit as standard, if users require application level keys we will create these as and when needed.\\n","tokens":22,"id":3141,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0017-monitoring-and-alerting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to monitor the Modernisation Platform infrastructure. This breaks down into the following areas:\\n- core platform security baseline Cloudwatch alarms set up for GuardDuty, SecurityHub and Config\\n- member account security baseline Cloudwatch alarms set up for GuardDuty, SecurityHub and Config\\n- core platform additional alarms that we may want to config\\n- member additional alarms that they may want to configure\\n- metrics and alarms for ECS\/Fargate\\n- visualisation of metrics for the core platform\\n- visualisation of metrics for member accounts\\n- how do we get alerts\\nWe have talked with the Cloud Platform team, there is currently limited capacity and adding additional metrics to their Prometheus stack is not a good option.  We have also discussed this with Operations Engineering who will be looking into centralised monitoring, but this work has not been started yet - <https:\/\/github.com\/ministryofjustice\/operations-engineering\/issues\/147>.\\nWith this in mine, the Modernisation Platform will start to develop it's own solution, aligning technology choices with the other platform teams where possible.\\n","Decision":"### Visualisation\\nThe main visualisation tool in use across the MoJ is [Grafana](https:\/\/grafana.com\/), this would be the most logical tool choice, with AWS managed being the preferred way of hosting. At the time of writing there is currently no Terraform provider, although this is on the AWS provider backlog.\\nThe Modernisation do not currently have any metrics that we want to view with Grafana, and we currently have no users that have expressed a desire to use Grafana.\\nDecision:\\nWait until we have a solid use case for visualising metrics, then re-evaluate at the time the right visualisation tool.\\n### Core Platform security baseline alarms\\nWe have a number of Cloudwatch alarms set up for GuardDuty, SecurityHub and Config.\\nDecision:\\nWe want to be alerted to these.\\n### Member account security baseline alarms\\nWe have a number of Cloudwatch alarms set up for GuardDuty, SecurityHub and Config.\\nDecision:\\nWe want to be alerted to these.\\n### Core Platform additional alarms\\nWe do not currently have any additional alarms configured, but we know there are some we would like to implement, such as notification if an account becomes detached from the Transit Gateway.\\nDecision:\\nThe solution should support the ability to add additional alarms.\\n### Member account additional alarms\\nWe would like to be able to offer users easy to use out of the box alerting. We do not yet know what users want or what this could look like.\\nDecision:\\nWe will not be doing this at this time. This will be covered in - <https:\/\/github.com\/ministryofjustice\/modernisation-platform\/issues\/1332>.\\n### Metrics and Alarms for ECS\/Fargate applications\\nWe have applications that use ECS and will possibly have more, we could follow Cloud Platform and use a Prometheus\/Alert manager stack for these. We would most likely use AWS managed Prometheus. There is however currently no user demand for this.\\nDecision:\\nWe will not be doing this at this time. When we have a solid user need then re-evaluate what is the right tool.\\n### Receiving alerts\\nOther platform teams use [PagerDuty](https:\/\/moj-digital-tools.pagerduty.com\/) and [Slack](https:\/\/slack.com\/). These tools are know in the organisation, used by other platform teams and fulfil our needs.\\nDecision:\\n- We will create a Modernisation Platform team in PagerDuty and add our permanent staff to our team. <https:\/\/github.com\/ministryofjustice\/modernisation-platform\/issues\/1312>\\n- We will integrate Slack with PagerDuty to push our alarms to Slack. <https:\/\/github.com\/ministryofjustice\/modernisation-platform\/issues\/1312>\\n- We will integrate our existing alarms that we want to monitor with PagerDuty. <https:\/\/github.com\/ministryofjustice\/modernisation-platform\/issues\/1334> and <https:\/\/github.com\/ministryofjustice\/modernisation-platform\/issues\/1335>\\n- We will not yet consider an on-call rota, this will be decided here - <https:\/\/github.com\/ministryofjustice\/modernisation-platform\/issues\/1333>.\\n","tokens":226,"id":3143,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0014-create-ecr-in-the-shared-services-account.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nApplications using Docker will sometimes need an ECR repo to store their Docker images.  Since we use a separate AWS account for each application environment, this will need to be in a shared location.\\nSome options which were considered:\\n* ECR repository per application in the shared services account\\n* Use a service provided by Operations Engineering (there is only Docker Hub currently)\\n* Use a hosted service such as Docker Hub\\n* ECR repository per application in one of the applications accounts which is then cross shared to other accounts\\n","Decision":"We will create application ECR repositories in the shared-services account, and share them to the relevant accounts and account deployment CICD users.\\nFor the moment we will create repositories on behalf of teams as and when they need them [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/blob\/main\/terraform\/environments\/core-shared-services\/ecr_repos.tf) using the ECR module.\\n","tokens":110,"id":3146,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0005-use-github-actions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBefore we start automating any part of the Modernisation Platform, we should define our CI\/CD runner.\\n","Decision":"We have decided to use GitHub Actions as our CI\/CD runner due to the following:\\n- we don't have to roll our own infrastructure for it\\n- we get unlimited free running minutes on all of our public repositories\\n- it offers centralised CI\/CD alongside our code storage\\n- it helps us meet our goal of working in the open\\n- other teams within Ministry of Justice are moving toward GitHub Actions themselves, so we can align ourselves\\n","tokens":26,"id":3148,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0007-use-terratest-opa-and-go-for-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn the Modernisation Platform, we want to have confidence that we can make changes without breaking things.\\n","Decision":"We've decided to use the following tools for testing:\\n* [Terratest](https:\/\/terratest.gruntwork.io\/) for infrastructure testing\\n* with [localstack](https:\/\/github.com\/localstack\/localstack) where possible\\n* in a suitable account if not\\n* [Open Policy Agent](https:\/\/www.openpolicyagent.org\/) for JSON and policy tesing\\n* with [conftest](https:\/\/www.conftest.dev\/)\\n* [Go testing framework](https:\/\/golang.org\/pkg\/testing\/) for anything else\\n* We will run tests on pipelines where possible\\n","tokens":24,"id":3149,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0016-ip-address-range-allocation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Modernisation Platform needs to be able to communicate privately with various other areas of the MoJ. The main use cases are as follows:\\n- Access from various MoJ sites to the platform\\n- Access from VPNs for remote devices to the platform\\n- Connectivity from a business units existing hosting to the platform during migration\\n- Connectivity from a business units existing hosting to the platform for applications not yet migrated\\n- Connectivity to other MoJ hosting platforms\\nThis connectivity between different networks is provided by the MoJ Transit Gateway. In order to enable to required routing we need to ensure that our IP range allocation is unique to avoid overlaps or clashes.\\nOur current IP ranges are not being managed centrally and we have identified some clashes with existing networks.\\n","Decision":"The Modernisation Platform will be allocated IP ranges from the Network Operations Team and Vodafone who maintain the MoJ record of IP ranges. This will avoid clashes with any existing or future infrastructure registered in the same way.\\n","tokens":154,"id":3151,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"modernisation-platform\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs we build the Modernisation Platform, we will collectively making decisions around the architecture, processes, and tooling for the Modernisation Platform.\\nWhen making these decisions, we should record them, both to help us understand and remember why we made them, and to also act as a reference for onboarding new team members and for anyone who is interested to view.\\nFinally, as outlined in the [Government Design Principles](https:\/\/www.gov.uk\/guidance\/government-design-principles), these should be publicly accessible as [making things open makes things better](https:\/\/www.gov.uk\/guidance\/government-design-principles#make-things-open-it-makes-things-better).\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in the article \"[Documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\".\\n","tokens":145,"id":3152,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"alfa\/adr-003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"alfa\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMajor decisions on architecture and design have so far primarily [been recorded in code comments](https:\/\/github.com\/Siteimprove\/alfa\/blob\/b92105fba4b961e8b67c91490aa1d7c6f13a8328\/packages\/alfa-dom\/src\/types.ts#L1-L17) or not recorded at all. While the former has been an acceptable approach, the latter has not as important information has resided only with the person who made the decision.\\nWe want to avoid siloing of information on the reasons for why important decisions were made and we want to do so in a way that is both transparent and easy to maintain. Documenting architecture decisions should be no different than documenting code in that the documentation is written as plain text and lives with the rest of the code base.\\n","Decision":"Moving forward, we will make use of so-called architecture decision reports (ADR) as described in [Documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard. The ADRs will live in the [`docs\/architecture\/decisions`](..\/decisions) directory and will follow the format outlined in [`docs\/architecture.md`](..\/..\/architecture.md).\\n","tokens":172,"id":3154,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"alfa\/adr-004.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHaving decided on [ADR 2](adr-002.md), we will no longer be tied to any one specific browser and are left with the choice of what to do about things that browsers are not aligned on. For example, if Alfa needs to implement an API to perform a given task, and a given set of browsers do not agree on how the API should be implemented, Alfa can do several things:\\n1. Implement the API the way it is implemented in the _least_ capable browser, in terms of support for the API, among a given set of browsers.\\n2. Implement the API the way it is implemented in the _most_ capable browser, in terms of support for the API, among a given set of browsers.\\n3. Provide several implementations of the API that cover all the different implementations of the API among a given set of browsers.\\nOur proprietary accessibility conformance testing engine at Siteimprove takes the second approach in practice as it is usually run within the latest version of the Chrome browser and relies heavily on browser APIs. Other tools that operate with the concept of an [accessibility support baseline][] will typically take a combination of the first and second approach where some browser APIs are used and then others implemented manually according to the capabilities of the [accessibility support baseline][].\\nThe quality shared by the first and second approach is that they both produce just a single implementation of the API, though with potentially different capabilities. However, both approaches suffer from the fact that depending on how the API is implemented, an accessibility issue could be found in either the least or most capable browser, while being missed in the other. For example, a contrast issue might appear in a less capable browser that does not support a specific color format, while the contrast would be perfectly fine in a more capable browser that does support the specific color format. The third approach avoids this issue entirely by implementing the API in all possible ways for a given set of browsers.\\n","Decision":"We will take the third approach described in the previous section and incrementally adopt it throughout Alfa where it makes sense. To do so, we will introduce the concept of _browser specific values_ that model abstract values that can take on multiple concrete values across different sets of browsers. This way, any API that we may want to implement can either return just a single value or a set of browser specific values in the case where the API would behave differently in different sets of browsers.\\nTo decrease the complexity of working with browser specific values, we will implement them as [monads](<https:\/\/en.wikipedia.org\/wiki\/Monad_(functional_programming)>) and expose a set of operations that allow consumers to perform computations on browser specific values without having to worry about whether they take on one or multiple concrete values.\\n","tokens":401,"id":3155,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"alfa\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"test-assignments\/0001-id-type.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide what data structure to use as the identifier for domain entities.\\n","Decision":"We will use a [GUID](https:\/\/msdn.microsoft.com\/en-us\/library\/system.guid%28v=vs.110%29.aspx?f=255&MSPPError=-2147217396) type.\\n","tokens":20,"id":3157,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"test-assignments\/0002-command.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide the format and semantics of a command type within the actor system.\\n","Decision":"We will use a regular class for a command. The data of a command is added as properties with public getters and setters. A command should be handled as an immutable type.\\n","tokens":21,"id":3158,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hmpps-interventions-service\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3160,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hmpps-interventions-service\/0003-decouple-transactions-on-asynch-calls.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to ensure that when asynchronous calls are made that they do not cause issues with database transactions scopes.\\nIn normal circumstances (without @Async) a transaction gets propagated through the call hierarchy from one Spring @Component to the other.\\nHowever, when a @Transactional Spring @Component calls a method annotated with @Async this does not happen.\\nThe call to the asynchronous method is being scheduled and executed at a later time by a task executor and is thus handled as a 'fresh' call.\\n","Decision":"We decided to prevent issues with @Async calls that we only sent primitive values or DTOs rather than a database entity.\\nThis ensures that there is no unexcepted behaviour when performing a lazy-fetch of the database entity.\\n","tokens":103,"id":3161,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-data-deputy-reporting\/0003-resubmitted-reports.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAn erroneous report would be unsubmitted by the case manager, corrected and resubmitted by the deputy. In that case, we'd probably make the request again with the same source, report dates and year (though a different date submitted).\\n","Decision":"The original report submission needs to be retained, and the 'context' of that decision retained also. The context means all the supporting files around the original submission.\\nWhen the report is re-submitted, a new document is created in Supervision, and also the context surrounding THAT submission.\\nPractically, what this means is that all of the supporting documents need to be submitted again, because it's impractical for us to check for documents added, removed, or modifiedfor this new submission\\n","tokens":52,"id":3162,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-native-app\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4215,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-cli\/0003-use-docs-adr-as-folder.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n`adr-cli` needs to store the markdown files somewhere.\\n[Michael Nygard's article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) proposes `doc\/arch\/adr-NNN.md` as directory and filename pattern.\\n[adr-tools](https:\/\/github.com\/npryce\/adr-tools) uses `docs\/adr\/NNNN-title-with-dashes.md` as pattern.\\n","Decision":"Use `docs\/adr\/NNNN-title-with-dashes.md` as pattern to be\\n1) consistent with adr-tools and\\n2) enable rendering in GitHub pages, because GitHub pages can be rendered from the `docs\/` sub folder, but not from `doc` subfolder.\\n","tokens":97,"id":4219,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-cli\/0002-use-iso-8601-date-format.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n`adr-cli` seeks to communicate the history of architectural decisions of a project. An important component of the history is the time at which a decision was made.\\nTo communicate effectively, `adr-tools` should present information as unambiguously as possible. That means that culture-neutral data formats should be preferred over culture-specific formats.\\nExisting adr-tools deployments format dates as dd\/mm\/yyyy by default. That formatting is common formatting in the United Kingdom (where the project was originally written), but is [easily confused](https:\/\/xkcd.com\/1179\/) with the mm\/dd\/yyyy format preferred in the United States.\\n","Decision":"`adr-cli` will use the ISO 8601 format for dates:  `yyyy-mm-dd`\\n","tokens":131,"id":4220,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"acs-deployment-aws\/0002-use-aws-cloudformation-for-provisioning-aws-resources.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs part of the ACS with AWS Services initiative, we are looking to use more Amazon services around our Kubernetes deployment. To do this we have to find a way to provision Amazon services like S3 buckets, EKS Cluster, Amazon MQ, Aurora DB which are outside of our current helm deployment. We have investigated 3 options for doing this provisioning.\\nThe first option would be to use CloudFormation templates to do the provisioning.\\nCloudFormation would be in alignment with our AWS First company direction and it can allow us to provision all types of Amazon resources needed.\\nAn additional plus is that we have experience working with this tool within our team.\\nHowever, CloudFormation locks us to Amazon only services and makes us have separated tools for provisioning Alfresco Content Services and the adjacent resources.\\nThe second option is having Terraform as an outside of AWS provisioner.\\nTerraform allows us to provision and make use of services from different cloud providers in our solutions as well as totally unrelated services like Github, Consul, PagerDuty and more importantly Bare Metal on-prem provisioning. Terraform is also abstracting away a good part of the required metadata needed for the provisioning of resources.\\nHowever, we have limited experience in using terraform.\\nThe final option is using kubernetes controllers to deploy Amazon resources as part of the helm deployment for acs.\\nImplementing kubernetes controllers for dynamically provisioning resources along with the usual kubernetes deployment for Alfresco Content Services would make us more consistent in how we deploy our applications and would ease up maintenance in the future.\\nHowever, we would still need another way for provisioning the actual kubernetes cluster and our experience in developing custom resource definitions used in kubernetes controllers is inexistent.\\n","Decision":"We will use Amazon CloudFormation templates as it is in alignment with Alfresco's AWS First direction. Also, we have experience in developing and using this tool within the company. It also brings us closer to potentially having a quickstart template for deploying the Alfresco Digital Business Platform.\\n","tokens":351,"id":4221,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"acs-deployment-aws\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4222,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0003-use-rollup-to-build-distributables.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":4227,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0002-use-typescript.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https:\/\/www.typescriptlang.org\/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https:\/\/github.com\/microsoft\/TypeScript\/network\/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n","Decision":"We will use TypeScript.\\n","tokens":134,"id":4228,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0019-update-release-notes-every-pr.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFollowing the release of v3.5.0 of Horace,\\nit was found that breaking changes were not effectively communicated to users.\\nA discussion on the best way to communicate these changes,\\nas well as the best way for developers to track such changes, was had.\\nIt was decided release notes provided a solution to this problem.\\nThe workflow for producing release notes needs to be decided.\\n","Decision":"A file is to be kept in the repository to track changes between releases.\\nThis file is to be updated on every pull request in which:\\n- an API change is made.\\n- a feature is added.\\n- a user-facing bug is fixed.\\nAdditions to the file will be written as if addressing users.\\nThese additions will document behaviour changes,\\nas well as changes users will be required to make to their workflows\/scripts.\\nBefore every release, this file will be used to compile release notes.\\nThese release notes will be published alongside releases on GitHub\\nand with user documentation.\\n","tokens":83,"id":4230,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0014-use-jenkins-for-network-path-storage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe [large data storage area](.\/0012-use-network-storage-for-large-datafiles.md) is shared by all the PACE projects and accessed via as network path.\\nThis path should not be hard-coded in any scripts in order that it can be easily updated if the network storage location is moved.\\nFor security reasons it is undesirable for this path to be stored in configuration files in the GitHub repositories.\\n","Decision":"The data will be stored in ANVIL through the available Jenkins [Credentials](https:\/\/plugins.jenkins.io\/credentials\/) plugin.\\nThe path will be stored the `PACE-neutrons`  store with the ID `SAN_path`.\\n","tokens":86,"id":4231,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0005-use-gtest-in-cpp-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nC++ tests are required for the project. For writing these, a testing framework\\nshould be used.\\nBelow is a shortlist of C++ testing frameworks:\\n* [GoogleTest](https:\/\/github.com\/google\/googletest)\\n* [Catch2](https:\/\/github.com\/catchorg\/Catch2)\\n* [CXXTest](https:\/\/cxxtest.com\/)\\n","Decision":"We will use GoogleTest as the framework for our C++ tests.\\nGoogleTest was chosen because it is:\\n* in active development\\n* used by other teams within ISIS (IBEX, Mantid use the mocking framework)\\n* required by [Google Benchmark](https:\/\/github.com\/google\/benchmark), which\\ncould become useful in the future.\\n* equipped with a mocking framework\\nCXXTest has not had any development for several years and Catch2, whilst being\\neasy to set-up (it's header only), is not used elsewhere within ISIS.\\n","tokens":84,"id":4232,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0008-use-pipeline-builds.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nContinuous builds are needed for each target platform and operating system to maintain a set of build artifacts for the `master` branch.\\nBuilds are also required for pull requests made on [GitHub](https:\/\/github.com\/pace-neutrons).\\nThese builds will be the same except for the commit to build (for `master` this will be the branch HEAD, for a pull request this will be a merge commit between the branch and `master`).\\n","Decision":"Jenkins builds pipelines will be scripted (`tools\/build_config\/Jenkinsfile`) with minimal configuration set in the Jenkins UI.\\nThis configuration will be:\\n- set required variables (e.g. build agent type, MATLAB and compiler versions)\\n- manage Git triggers and branches\\nDifferences between the `PR`, `Branch` and `master` builds will be split between the UI (extraction of information about pull request is required for `PR` builds) and `Jenkinsfile` scaffold (target commit to post build status).\\nThe pipeline script will call a `build.[sh|ps1]` script with appropriate arguments to perform the build and test steps. The `Jenkinsfile` will provide a scaffold that can be applied to all build targets.\\nThe build jobs will be organised into `Horace` and `Herbert` folders. Separate projects will be written for:\\n```\\nHorace\/PR-builds\\n\/master-builds\\n\/branch-builds\\nHerbert\/PR-builds\\n\/master-builds\\n\/branch-builds\\n```\\nEach project name will follow the pattern: `[PR-|Branch-]<target-os>-<target-matlab>`.e.g. `PR-Scientific-Linux-2018b`, `osx-2019a`, `win64-2018a`.\\n","tokens":96,"id":4233,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0006-use-jenkins-for-ci.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA Continuous Integration (CI) server is essential for maintaing build quality and creating reproducable builds. The build agents will need access to MATLAB licenses.\\nOptions considered:\\n- [Anvil](https:\/\/anvil.softeng-support.ac.uk\/) is a service run centrally by STFC, based on Jenkins, with access to third party software for STFC staff and colleagues.\\n- [Travis](https:\/\/travis-ci.org\/) is a cloud hosted CI service, free for Open Source projects.\\n- Create a private Jenkins instance for the PACE project.\\n","Decision":"We will use the Anvil managed Jenkins instance for build and testing.\\nThe use of Anvil removes any complications with the avilability of specific target platforms and MATLAB licensing on build nodes.\\n- The service has access to pool of STFC MATLAB licenses.\\n- Additional build nodes can be provisioned and added to the pool if currently unsupported targets are required.\\nThe overhead of provisioning and managing a private Jenkins instance is not justified at this time.\\n","tokens":121,"id":4235,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0009-use-standard-naming-for-build-artifacts.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe continuous builds (of `master` and the pull requests) will create many release artifacts containing the archived (`.zip` or `tar.gz`) source and binary folders.\\nThese should follow a consistent naming convention so that the following can be quickly determined and the required build identified:\\n- target platform\\n- build date\\n- MATLAB version\\n- whether this is a PR build or branch build.\\nWhen production releases are made these build artifacts will be pushed to [GitHub](https:\/\/github.com\/pace-neutrons).\\n","Decision":"The release files will be named: `<application>-<version>-<target>-<matlab>[-<yymmdd>][-<sha>].<ext>`\\n- Release files: `<application>-<version>-<target>-<matlab>`\\n- PR builds will be:`<application>-<version>-<target>-<matlab>-<sha>`\\n- Nightly builds will be: `<application>-<version>-<target>-<matlab>-<yymmdd>-<sha>`\\n| Argument | Description |\\n|------|-----|\\n|`version`| `m`.`n`.`o` taken from top ` CMakeList.txt` file |\\n|`target` | `win64`, `osx` or `linux` |\\n|`matlab` | MATLAB version, e.g. `R2018b` |\\n|`ext`    | `.zip` will be used for Windows releases; `.tar.gz` for MacOS and Linux |\\n","tokens":111,"id":4237,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0015-store-pixel-data-in-single-precision.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe pixel data is a N-row array of detector-element data, where N is O(2^32) and is expected to grow by an order of magnitude over the lifetime of Horace.\\nThe current (Horace 3.4) SQW data file stores data as single-precision (float32) values. Previous versions of the file format stored the pixel data as double-precision (float64) values.\\nRead\/write of full SQW files scales linearly with file size and for a \"typical\" data file can take an hour on a desktop machine or 20-30 minutes on a parallel file system. As most of the data contained within the SQW file is the pixel data, the conversion halved the file size and consequently halved the time required to perform read\/write operations.\\nThe raw data is typically captured with precision not exceeding that which my be represented in a single precision value.\\n","Decision":"The data pixel data array will be stored as single-precision floats.\\n","tokens":189,"id":4238,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0013-use-jenkins-for-secrets-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe build processes for all of the PACE projects require credentials to access GitHub and the SAN area.\\nThese need to be stored securely and be accessible to steps in the Jenkinsfile and any scripts launched from that.\\nFor security reasons this data cannot be stored in the build or pipeline scripts as these are stored in GitHub.\\n","Decision":"The data will be stored in ANVIL through the available Jenkins [Credentials](https:\/\/plugins.jenkins.io\/credentials\/) plugin.\\nA `file` object will be used for the SAN credentials as this maps directly onto the format required by be `gio mount` syntax on Linux build nodes.\\nA `string` object will be used for other credentials types, e.g. GitHub access tokens.\\nCredentials will be stored in the `PACE-neutrons`  store and use `snake_case` IDs that clearly identify their use. The first word will identify the associated system, e.g. `SAN_credentials_file`.\\n","tokens":69,"id":4239,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0011-version-project-with-cmake.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEvery instance of the project should carry an up-to-date version number. This\\nversion number should be accessible from within the Matlab and C++ code.\\n","Decision":"The version number will be defined in a top-level VERSION file. This file\\nshould contain a version number of format \\<major\\>.\\<minor\\>.\\<patch\\> and\\nnothing else, including whitespace.\\nCMake will read the VERSION file and formate template Matlab and C++ files.\\nThese templates will be copied into the Matlab\/C++ source tree by CMake at\\nconfigure time.\\nIf CMake is building a developer version (i.e. CMake variable\\n`Horace_RELEASE_TYPE` is not equal to `RELEASE`), CMake will append a Git SHA\\nto the end of the version number. The Git SHA will be excluded from builds\\ncreated via the release pipeline; builds generated locally by developers and in\\npull request\/nightly Jenkins jobs will include the SHA.\\nIf CMake has not been run and has not generated the Matlab file containing the\\nversion, Matlab will read the VERSION file and append `.dev` to the version.\\nThis will signify this is an un-built developer version.\\n","tokens":34,"id":4241,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0004-cpp-tests-in-separate-projects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA large number of test files will be created  during the project. These should be stored in\\na consistent location across the C++ projects.\\nA number of options are available:\\n* filing alongside the source files\\n* in a `tests\/` folder within the source tree\\n* in a `.Tests` project created alongside the application source\\n","Decision":"We will create a parallel directory structure of test projects for each C++ project.\\n``` file listing\\nservice\\\\nfunctionOne\\\\nCMakeLists.txt\\n...\\nfunctionTwo\\\\nCMakeLists.txt\\n...\\ntests\\\\nfunctionOne.Tests\\\\nCMakeLists.txt\\n...\\nfunctionTwo.Tests\\\\nCMakeLists.txt\\n...\\n```\\nThe test projects will include the associated function project as an explicit reference e.g.\\n```cmake\\nadd_executable(\"functionOne.test\" ${TEST_SRC_FILES} ${TEST_HDR_FILES} ${SRC_FILES} ${HDR_FILES})\\ntarget_include_directories(\"functionOne.test\" PRIVATE \"${Horace_ROOT}\/_LowLevelCode\/cpp\")\\ntarget_link_libraries(\"functionOne.test\" gtest_main)\\n```\\nThis enables the test artifacts and dependencies to be simply excluded from system components that  will be deployed. The separation will also provide a clear cognitive separation between `function` and `function.Test` and support further extension to `function.IntegrationTest`.\\n","tokens":74,"id":4242,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0020-use-c-mex-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMatlab has two Mex APIs that can be used by C++ code:\\nthe [C API](https:\/\/www.mathworks.com\/help\/matlab\/cc-mx-matrix-library.html)\\nor the\\n[C++ API](https:\/\/www.mathworks.com\/help\/matlab\/cpp-mex-file-applications.html).\\nThe C++ API is relatively new and is therefore only compatible with Matlab releases\\n[R2018a and later](https:\/\/www.mathworks.com\/help\/matlab\/matlab_external\/choosing-mex-applications.html#mw_d3e64706-faf9-486f-ab58-1860c63564d8).\\nThroughout the Herbert and Horace codebases only the C API has been used.\\nThis is primarily due to the fact that Herbert and Horace pre-date the C++ API.\\nThe C++ API contains restrictions on directly accessing Matlab memory,\\nthis provided challenges when writing the Matlab object serializer in C++.\\nThe API does not allow access to the underlying memory of the Matlab objects,\\nand the C++ objects had varying sized headers.\\nThis caused problems when attempting to perform copies and casts:\\nthe underlying data needed to first be copied via the C++ API,\\nand only then could a `memcpy` be performed by the serializer.\\nThis meant there were three copies of the data present at one time,\\nwhich is not desirable for large objects (e.g. pixel data).\\n","Decision":"Herbert\/Horace will continue to use the C API for new Mex functions.\\n","tokens":294,"id":4243,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0010-package-dependencies-in-repo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are various C++ dependencies present in Horace. For example, the version\\nof MPICH used should match the version used by Matlab - this avoids incompatible\\nshared libraries being used when executing mex functions. These libraries may\\nbe old versions and may not be easily available for users to download.\\n","Decision":"Dependencies will be packaged within the repo to be built against. Matlab's own\\nshared libraries will be built against where possible, but the static libraries\\nand headers required will be within the repository. Libraries will be statically\\nlinked where possible so that the libraries do not need to be shipped to users.\\n","tokens":64,"id":4246,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0002-use-github-for-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to store project documentation (Developer Guides, System Architecture, System Maintenance, Test Plans) somewhere that it is easy to read and update.\\nDocumentation will directly reference Horace and Herbert projects.\\nThe GitHub Wiki repository does not support pull requests, or the display of any content not on the the `master` branch.\\n","Decision":"~We will store documentation in the `Horace\/documentation` folder and link to this from the [GitHub Wiki](https:\/\/github.com\/pace-neutrons\/pace-developers\/wiki) which is attached to the PACE project on GitHub.~\\nWe will store documentation in the `Horace\/documentation` folder which will be built regularly and deployed to the GitHub pages under the following scheme:\\nThe documentation will be supported and built by the Jenkins pipeline and automatically deployed on:\\n- nightly builds (unstable)\\n- deploy pipeline (stable & horace-version)\\nThe Jenkins will also build the documentation as a zipped artefact on other pipelines (PR, Branch, etc.) to ensure that any modifications do not affect the documentation. Following the implementation of documentation tests, will also ensure that scripts contained in the documentation remain valid and functional.\\n","tokens":69,"id":4247,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4248,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0003-use-cmake-for-build.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe project will need to be built for multiple platforms Windows (VStudio), MacOS, Linux (gcc\/Make) and include compilation of Matlab and C++ (with Matlab API wrapper).\\n","Decision":"We will use [CMake](https:\/\/cmake.org\/) to provide a platform agnostic build definition that can be configured for each target platform.\\n","tokens":41,"id":4249,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Horace\/0016-use-double-array-in-memory-for-pixel-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPixel data will be stored on disk as single precision (float32) data.\\nMATLAB's default numeric data type is `double` (float64).\\nMixing `single` and `double` data in calculations produces a `single` result.\\nOptions are to:\\n1. store data in `single` precision and extend the data to `double` precision before sequences of arithmetic operations are performed to reduce impact of rounding errors\\n2. store data in `double` precision and reduce the data to `single` precision before evaluation of performance intensive operations\\nLong chains of function calls are executed within some algorithms (e.g. TobyFit). If data is held in single precision, the accumulated rounding errors are likely to introduce qualitative changes in the results. The functions called in these chains are part of the larger Horace toolkit.\\nSignificant performance improvements can be seen in some algorithms when performed on `single` precision rather than `double` data..\\nPixel binning can be performed in both MATLAB and C++ code.\\nThe two routines can result in inconsistent binning due to rounding errors.\\n","Decision":"Pixel data will stored internally as an array of MATLAB doubles and converted on read or write.\\nAccess to the data through the PixelData class API will return `double` values.\\n","tokens":227,"id":4251,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0009-what-is-a-reference-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe original goal for the reference implementation refactor was to provide an\\nimplementation which is both an aid to understanding the specification and a\\ngood architecture for other implementations to mimic.\\nDuring refactoring efforts on the metadata API and ngclient, several friction\\npoints have arisen where a safe object-oriented API would result in a less\\ndirect mapping to the [Document formats] in the specification.\\nThe archetypal example friction point is that [Timestamp] lists snapshot _only_\\nin a `meta` dictionary of `METAPATH` -> attribute fields. The dictionary will\\nonly ever contain one value and creates an extra level of indirection for\\nimplementations which try to map to the file format.\\nWhen presented with such cases, we have considered multiple options:\\n* Strict mapping to the [Document formats]\\n* Simple and safe API in preference to mapping to the [Document formats]\\n* Strict mapping to the [Document formats] with additional convenience API\\nwhich is documented as the preferred interface for users\\nSo far implementation has tended towards the final option, but this is\\nunsatisfying because:\\n* the API contains traps for the unsuspecting users\\n* two code paths to achieve the same goal is likely to result in inconsistent\\nbehaviour and bugs\\nTherefore, we would like to define our primary purpose so that we can make\\nconsistent decisions.\\n[Document formats]: https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats\\n[Timestamp]: https:\/\/theupdateframework.github.io\/specification\/latest\/#file-formats-timestamp\\n## Decision Drivers\\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\n","Decision":"* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages\/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths\/API for the same feature is a common source of bugs\\nPrimary purpose of the reference implementation is as an exemplary reference:\\nproviding a safe, consistent API for users and a good architecture for other\\nimplementations to mimic.\\n","tokens":408,"id":4252,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0010-repository-library-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe Metadata API provides a modern Python API for accessing individual pieces\\nof metadata. It does not provide any wider context help to someone looking to\\nimplement a TUF repository.\\nThe legacy python-tuf implementation offers tools for this but suffers from\\nsome issues (as do many other implementations):\\n* There is a _very_ large amount of code to maintain: repo.py,\\nrepository_tool.py and repository_lib.py alone are almost 7000 lines of code.\\n* The \"library like\" parts of the implementation do not form a good coherent\\nAPI: methods routinely have a large number of arguments, code still depends\\non globals in a major way and application (repo.py) still implements a lot of\\n\"repository code\" itself\\n* The \"library like\" parts of the implementation make decisions that look like\\napplication decisions. As an example, repository_tool loads _every_ metadata\\nfile in the repository: this is fine for CLI that operates on a small\\nrepository but is unlikely to be a good choice for a large scale server.\\n## Decision Drivers\\n* There is a consensus on removing the legacy code from python-tuf due to\\nmaintainability issues\\n* Metadata API makes modifying metadata far easier than legacy code base: this\\nmakes significantly different designs possible\\n* Not providing a \"repository library\" (and leaving implementers on their own)\\nmay be a short term solution because of the previous point, but to make\\nadoption easier and to help adopters create safe implementations the project\\nwould benefit from some shared repository code and a shared repository design\\n* Maintainability of new library code must be a top concern\\n* Allowing a wide range of repository implementations (from CLI tools to\\nminimal in-memory implementations to large scale application servers)\\nwould be good: unfortunately these can have wildly differing requirements\\n","Decision":"* There is a consensus on removing the legacy code from python-tuf due to\\nmaintainability issues\\n* Metadata API makes modifying metadata far easier than legacy code base: this\\nmakes significantly different designs possible\\n* Not providing a \"repository library\" (and leaving implementers on their own)\\nmay be a short term solution because of the previous point, but to make\\nadoption easier and to help adopters create safe implementations the project\\nwould benefit from some shared repository code and a shared repository design\\n* Maintainability of new library code must be a top concern\\n* Allowing a wide range of repository implementations (from CLI tools to\\nminimal in-memory implementations to large scale application servers)\\nwould be good: unfortunately these can have wildly differing requirements\\nOption 3: Minimal repository abstraction\\nWhile option 1 might be used temporarily, the goal should be to implement a\\nminimal repository abstraction as soon as possible: this should give the\\nproject a path forward where the maintenance burden is reasonable and results\\nshould be usable very soon. The python-tuf repository functionality can be\\nlater extended as ideas are experimented with in upstream projects and in\\npython-tuf example code.\\nThe concept is still unproven but validating the design should be straight\\nforward: decision could be re-evaluated in a few months if not in weeks.\\n","tokens":379,"id":4253,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0006-where-to-implemenent-model-serialization.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n","Decision":"* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n","tokens":140,"id":4254,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0005-use-google-python-style-guide.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe Secure Systems Lab code style guide, which has been used for most of the\\ncode base, has become outdated. Through the upcoming rewrite, we have the\\nchance to ignore consistency considerations with existing code style and can\\nchoose a more standard and up-to-date style guide.\\n## Decision Drivers\\n* Flaws in original Secure Systems Lab style guide\\n* Curating a complete custom style guide is time consuming\\n* Well-established style rules lower contribution barrier\\n* Custom style is not supported by default in common tooling (i.e. editors\\nand linters)\\n","Decision":"* Flaws in original Secure Systems Lab style guide\\n* Curating a complete custom style guide is time consuming\\n* Well-established style rules lower contribution barrier\\n* Custom style is not supported by default in common tooling (i.e. editors\\nand linters)\\nChosen option: \"Use Google style guide with refinements\", because the Google\\nstyle guide is a comprehensive, well-established style guide that is mostly\\nbased on PEP-8 and was accepted by everyone on the TUF team. There is no need\\nto replicate these recommendations. However, we do provide a very slim document\\nwith additional refinements, in order to emphasize items the we consider\\nespecially important, want to be handled differently, or in one specific way,\\nwhere the Google guide would allow multiple.\\n**Course of Action:**\\n* Follow existing style when working on existing code (files)\\n* Follow new style in any new code (files)\\n* Consider providing linter and formatter configuration (e.g. pylint, flake8,\\nblack, yapf) to enforce and facilitate new style\\n","tokens":124,"id":4255,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0003-where-to-develop-TUF-1-0-0.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe plan is to implement a refactored TUF (1.0.0) alongside the current\\ncode base, in order to not disrupt existing usage and keep providing\\na Python 2.7 client.\\nWe need to decide on the best place to do this development.\\n## Decision Drivers\\n* Developing the new code piecemeal\\n* Continuing to make releases in the interim\\n* Avoiding maintenance overhead\\n","Decision":"* Developing the new code piecemeal\\n* Continuing to make releases in the interim\\n* Avoiding maintenance overhead\\nChosen option: \"Develop TUF 1.0.0 in a subdirectory of the current TUF\\nimplementation\", because we want to add the new TUF code gradually\\nwhile keep maintaining the current implementation given limited\\nmaintenance resources.\\nOnce development of the new version is complete, we will transition\\nfrom TUF 1.0.0 in a subdirectory to stand-alone TUF 1.0.0 by the following\\nprocedure:\\n* flesh out tuf\/api\/*\\n* implement tuf\/client\/new-updater.py\\n* implement tuf\/repository\/*\\n* \\<iterate\\>\\n* git mv tuf\/client\/new-updater.py tuf\/client\/updater.py\\n* git rm tuf\/\\*.py\\n* tag 1.0.0\\n","tokens":95,"id":4256,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0002-pre-1-0-deprecation-strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe plan to refactor the reference implementation significantly and, as part of\\nthat effort, drop support for no-longer maintained versions of Python\\n(see ADR 0001).\\nHowever, a major user of (and contributor to) the project has users of the\\nclient stuck on older Python versions.\\nWe would like to define a reasonable support policy for the current, Python 2.7\\nsupporting, codebase.\\n## Decision Drivers\\n* We have finite resources.\\n* A major adopter\/user of the project has a need to maintain support for\\nPython 2.7 clients.\\n","Decision":"* We have finite resources.\\n* A major adopter\/user of the project has a need to maintain support for\\nPython 2.7 clients.\\nChosen option: \"Support the old code on a best-effort basis once the refactored\\ncode is released\", because we only have finite resources and want to focus them\\non moving the project forward, including supporting PyPI\/pip integration and\\nproviding a solid implementation for developing specification enhancements in.\\nWe should document this outcome clearly in a governance document describing\\nthe release process with words along the lines of:\\n\"Support for older releases:\\nBugs reported with tuf versions prior to 1.0.0 will likely not be addressed\\ndirectly by tuf\u2019s maintainers. Pull Requests to fix bugs in the last release\\nprior to 1.0.0 will be considered, and merged (subject to normal review\\nprocesses). Note that there may be delays due to the lack of developer resources\\nfor reviewing such pull requests.\"\\n","tokens":131,"id":4257,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0004-extent-of-OOP-in-metadata-model.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCustom classes for the TUF signed metadata wrapper (Metadata) and metadata\\npayload containers (Root, Timestamp, Snapshot, Targets) were added recently.\\nComplex attributes on these classes are still represented as dictionaries.\\nShould we add classes for these attributes too?\\n## Decision Drivers\\n* Transition to class-based role metadata containers in progress (see *\"class\\nmodel\"* links below)\\n* Harden in-memory representation of metadata model\\n* Replace `securesystemslib` schema validation (see *\"schema checker\"* link\\nbelow)\\n","Decision":"* Transition to class-based role metadata containers in progress (see *\"class\\nmodel\"* links below)\\n* Harden in-memory representation of metadata model\\n* Replace `securesystemslib` schema validation (see *\"schema checker\"* link\\nbelow)\\nChosen option: \"Use custom classes for complex attributes\", to provide a\\nconsistently object-oriented, well-defined, single source of truth about the\\nTUF metadata model (not only its containers). In addition to convenience update\\nmethods, the model may be extended with self-validation behavior (see\\n*\"validation guidelines\"* link below) to replace `securesystemslib` schema\\nchecks.\\n### Negative Consequences\\n* Implementation overhead\\n* Less flexibility in usage and development (this is actually desired)\\n* Maybe less idiomatic than dictionaries\\n","tokens":114,"id":4258,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0001-python-version-3-6-plus.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe are planning a refactor of tuf where:\\n* We do not want to try and support end-of-life versions of the language.\\n* We want to use modern language features, such as typing.\\n* We want to ease maintainer burden, by reducing the major language versions supported.\\n## Decision Drivers\\n* Python 2.7 is end-of-life\\n* Python 3.5 is end-of-life\\n* Modern Python allows use of desirable features such as type hints\\n* Supporting end-of-life Python versions adds maintenance overhead\\n","Decision":"* Python 2.7 is end-of-life\\n* Python 3.5 is end-of-life\\n* Modern Python allows use of desirable features such as type hints\\n* Supporting end-of-life Python versions adds maintenance overhead\\nChosen option: \"Support only Python 3.6+\", because we want modern features and lower\\nmaintainer effort as we work to improve our codebase through the refactor effort.\\nNew modules should target Python 3.6+.\\nUsing modules to polyfill standard library features from Python 3.6+ feels\\nuntenable as more libraries are dropping support for EOL Python releases.\\n### Negative Consequences\\n* Leaves major adopter and contributor without an actively developed client for some of\\ntheir customers stuck on older Python versions.\\n","tokens":117,"id":4259,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"python-tuf\/0008-accept-unrecognised-fields.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe current reference implementation will ignore unrecognized fields in a\\nmetadata file when loading it.\\nThis leads to the side effect that if you read a metadata file with unrecognized\\nfields and immediately write it back to the disk, this file will be modified.\\nFurthermore, some TAPs like:\\n- [TAP 6](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap6.md)\\n- [TAP 10](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap10.md)\\n- [TAP 14](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap14.md)\\n- [TAP 15](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap15.md)\\n- [TAP 16](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap16.md)\\nare relying on that unrecognized fields will be accepted to introduce new fields\\nto the specification without making the metadata invalid for older clients who\\ndon't recognize the field.\\n## Decision Drivers\\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don't store unrecognized fields when it is not allowed by the specification.\\n","Decision":"- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don't store unrecognized fields when it is not allowed by the specification.\\nChosen option: \"Ignore, but store unrecognized fields as an additional attribute\\nexcept for a couple of places where it's not allowed by the specification.\"\\nThe motivation for this decision is that the TUF specification already implies\\nthat we should accept unrecognized fields for backward compatibility and easier\\nfuture extensibility.\\nAdditionally, it seems unacceptable to change a metadata file content just by\\nreading and writing it back.\\nThere are exceptions however for places in the metadata format when it is not\\nallowed by specification: keys, roles, meta, hashes, and targets are\\nactual dictionaries (vs JSON objects that most structures in the format are)\\nwhere `unrecognized field` is not a meaningful concept.\\n","tokens":346,"id":4260,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"laundromat\/0002-scoring-mechanism.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA confidence score is very useful when evaluating the output of any model. It enables the user to put varying degrees of faith in the model predictions and allows for certain performance metrics. Sadly the SpaCy NER model\u2019s architecture is such that it does not output confidence scores. There are ways around this, but they are unsatisfying and produce very low quality confidence scores. Therefore another method of gaining these confidence scores is required.\\n","Decision":"We have chosen to create a model that will take as input the SpaCy NER model\u2019s predicted entities, and output a confidence score. The architecture of this model has not been decided yet, but we will explore simple GLMs first before trying neural models.\\n","tokens":89,"id":4262,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"laundromat\/0001-create-own-python-package.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt the beginning of the project Presidio seemed like a promising framework to for anonymizing text. Part of our desire to use it was that Presidio is supported by Microsoft, meaning that significantly more resources than are available to us might be used to develop and maintain it. Additionally, Presidio supports image anonymization which might be a useful feature for NAV down the line.\\nOur initial idea was to modify Presidio to support Norwegian, and to include our own custom Named Entities. After some experimentation, we discovered the following:\\n* Presidio\u2019s documentation is sparse and lacks detail.\\n* Presidio adds only limited functionality to SpaCy for text.\\n* Much of Presidio is hard coded to use English, and so adapting it for Norwegian was difficult.\\n","Decision":"We will not be using the Presidio framework. Instead we will make our own python package implementing Spacy and RegEx.\\n","tokens":164,"id":4263,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"backdrop\/adr-003-web-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which web framework to use for the data\\nAPI for the Performance Platform.\\nWe are writing the API in Python. We need to support accepting unstructured\\ndata, and returning structured data for reads.\\nThe API will not be RESTful initially. We want to optimise for easy writing,\\nand do the hard work to return structured data at read time. We do not\\nunderstand the model or resources that we will be using, so for now, we think\\nwe will allow data sets to be created and written to, and do some\\ntransformations at query time on those data sets.\\nThis means that we think we need to allow simple POST\/PUT to a data set, and\\nthen a more complex read query to produce structured data.\\nOver time, we expect to see common data sets \/ types and we can then look to\\nuse a more structured approach, defining resources and relationships. At that\\npoint, we might also want to consider using RESTful features, or switching\\nto a framework that has more support for that approach.\\nOptions considered include Django and Flask, since we are familar with those.\\n# Decision\\nWe will write the data API using Flask.\\nWe don't need the ORM for Django, and Flask seems simpler in terms of starting\\nwith a small API.\\n# Status\\nAccepted.\\n# Consequences\\nDevs and operations need to be comfortable with Flask apps.\\n","Decision":"We will write the data API using Flask.\\nWe don't need the ORM for Django, and Flask seems simpler in terms of starting\\nwith a small API.\\n# Status\\nAccepted.\\n# Consequences\\nDevs and operations need to be comfortable with Flask apps.\\n","tokens":341,"id":4264,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"backdrop\/adr-001-implementation-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which language to write the data\\nAPI for the Performance Platform.\\nGDS has experience in running Ruby (Rails\/Sinatra) and Scala apps in\\nproduction. Choosing one of these (Ruby) would allow us to rotate people\\nacross GDS in and out of the Performance Platform team.\\nBut we have some excellent Python developers in GDS, who\\ndevelop in Ruby at work. There is a community here that we expect would be\\ninterested in working in their preferred language, so choosing Python\\nmight be an way of encouraging rotation (people would not have to leave\\nthe organisation to try a new thing) and diversity.\\nWe have:\\n- lots of people here that have operated Python applications in\\nproduction\\n- knowledge about how to architect and write Python applications\\n- an easy step to deploying Python in production\\n# Decision\\nWe will write the data API in Python.\\n# Status\\nAccepted.\\n# Consequences\\nWe will have to write (Capistrano or Fabric?) code to deploy a Python\\napplication.\\n","Decision":"We will write the data API in Python.\\n# Status\\nAccepted.\\n# Consequences\\nWe will have to write (Capistrano or Fabric?) code to deploy a Python\\napplication.\\n","tokens":263,"id":4265,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"continuouspipe\/0003-merge-the-builder-micro-service-within-river.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are running 12 micro-services, a lot of them are Symfony applications and we operate them differently.\\n","Decision":"We are going to merge some into this river application.\\n","tokens":25,"id":4267,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"continuouspipe\/0002-store-the-new-tide-projections-into-firebase.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to be able to store a projection of the tides and especially their tasks statuses. The aim is to display\\na per-task detailed pipeline-like UI, as much real time as possible.\\n","Decision":"Firebase helped us a lot with the logs and appear to be successful, it makes sense to extend its usage\\nto this projection.\\nIn order to ensure a permission control, the tides will be stored with the following hierarchy:\\n- `\/flows\/{flowUuid}\/tides\/{tideUuid}`\\n","tokens":44,"id":4268,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"continuouspipe\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4269,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cautious-waffle\/help_handlers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHow is the responsibility for providing help context defined.\\n","Decision":"All handlers defined for slash commands are responsible for exporting\\na 'handlerHelp' function that will be callable with the string[] supplied\\nby the help handler. This delegates the responsibility for help to the\\nauthor of the slash command handler and co-locates the help definition\\nwith the handler it self.\\n","tokens":14,"id":4270,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sexual-health-service-finder\/0001-record-architecture-decision-records.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis\\n[article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":4272,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sexual-health-service-finder\/0004-use-azure-search.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAzure Search is the organisation\/programme's strategic vision for serving both\\norganisation and service data. This removes the need to maintain an instance of\\nElasticsearch and related infrastructure.\\n","Decision":"The application will use Azure Search API to provide sexual health service\\ndata.\\n","tokens":41,"id":4273,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sexual-health-service-finder\/0003-use-elasticsearch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nElasticsearch is configured as a cluster for reliability and failover, and provides a single point for data updates.\\nElasticsearch is used for filtering for the different service data requirements of the application.\\n","Decision":"The application will use Elasticsearch for service data retrieval purposes.\\n","tokens":45,"id":4275,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"early-careers-framework\/0002-use-bigdecimal-for-currency-storage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA common pitfall in choice of data storage for currency for money is to use floating point which then results in rounding errors due to the inexact representation of base-10 (decimal) fractional numbers in base-2 (binary) floating point storage that can result in a loss of faith in a calculations system.\\nOptions available:\\n1. [BigDecimal](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html) as pounds.pence\\n2. [BigDecimal](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html) as pence\\n3. The [RubyMoney\/Money gem](https:\/\/github.com\/RubyMoney\/money)\\n4. [float](https:\/\/ruby-doc.org\/core-3.0.0\/Float.html) (default for numbers in ruby)\\nI'm glad to say rounding errors here are [unlikely to be fatal as has been seen before](https:\/\/www.gao.gov\/products\/imtec-92-26) but we'd still like to avoid them.\\n### Research\\n* <https:\/\/www.honeybadger.io\/blog\/ruby-currency\/>\\n","Decision":"### 1. BigDecimal as pounds.pence - yes\\n* Provides accurate rounding compared to float, producing the results that accountants would expect.\\n* Extra dependency, albeit commonly used.\\n* No mismatch between normal written representation of the value and the stored value (compared to storing as pence).\\nBigDecimal provides [`BigDecimal.round`](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html#method-i-round) that we can use to round up to pennies as needed.\\n### 2. BigDecimal as pence - no\\nSome developers like to store currency in pennies.\\nI've not seen a compelling reason to do this. I personally think it adds extra risk of confusion compared to using the decimal type in a way that matches the currency (i.e. pounds as the integer part of a decimal and pennies as the fractional part).\\n* <https:\/\/stackoverflow.com\/questions\/18934774\/what-are-the-downsides-to-storing-money-values-as-cents-minor-units>\\n### 3. RubyMoney - no (for now)\\nThe [RubyMoney\/Money gem](https:\/\/github.com\/RubyMoney\/money):\\nThis type provides extra information about the currency in use (e.g. GBP, USD) which is of no use to us as this is an entirely GBP system.\\nUsing a money type would explicitly shows that the numbers stored are financial which would be nice but isn't critical to understanding.\\nIt also provides formatting and truncation methods. We don't yet know if these will prove useful.\\nIt is judged that the tradeoff for introducing an additional relatively complex dependency for the possible benefits is not currently worthwhile.\\n### 4. Floats - no\\nDo not use for money. Ever.\\nStoring currency in floats causes rounding errors so that's out.\\n","tokens":249,"id":4278,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"early-careers-framework\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4279,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-stub-idp\/0003-deploying-metadata-to-paas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently we are migrating stub IDP from UKCloud to PaaS. Stub IDP uses hub metadata to validate signatures\\nand to achieve this, we need to whitelist the IPs that needs access to hub metadata. Inorder to whitelist we need\\nstatic IP address for stub IDP in PaaS for certain environments (`joint, integration, staging, perf`).\\nThis feature is currently not offered by PaaS.\\n","Decision":"We have decided to deploy hub metadata to PaaS as part of the current metadata release process.\\nWhenever metadata is being deployed to UKCloud there is a copy of it being pushed to\\nPaaS so that both (UKCloud and PaaS) use the same version of metadata.\\n","tokens":87,"id":4282,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-stub-idp\/0002-persisting-test-data-in-sql-rdbms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to be able to persist test data for users of Stub IDP.\\nCurrently we use Infinispan to store user data so that RPs can configure\\ntheir own users in the Stub IDPs we create for them and a YAML file for\\nstoring federation config for Stub IDP. We cannot deploy Stub IDP to PaaS\\nwith Infinispan so we need to move the user data into a persistent\\ndata store and eventually move the fed config into a database as well.\\n","Decision":"We decided to use PostgreSQL RDBMS because it is currently best\\nsupported RDBMS on PaaS.\\nWe also decided that the data about the users is saved as JSON in the DB.\\nThis might change in the future once we know what the eIDAS integration\\nrequirements will be.\\nAs a start, we are going to be using the following data model:\\n![Stub IDP ER Model](images\/ida-stub-idp-er-diagram.png?raw=true \"Stub IDP ER Model\")\\nThe \"data\" field in the \"users\" table is where the user data will be saved\\nin JSON format. The object structure will be the same as the object\\ncurrently saved to Infinispan.\\n","tokens":107,"id":4283,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-stub-idp\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4284,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bob\/0002-replace-compojure-with-contract-first-solution.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCompojure is a routing library that we are using right now. The downside is that one has to specify the routes\\nas code. We want an openapi-spec that defines our API and a router that creates routes from it. The specs should\\nbe openapi 3 conform.\\nBesides that we need to model complex flows, an easy way to model auth per resource, easy swagger docs\\nand we want good async performance.\\n","Decision":"Since there is no obvious Clojure implementation of a spec-first routing library and [juxt\/apex](https:\/\/github.com\/juxt\/apex)\\nis in its infancy we are stuck with Java options. [openapi.tools](https:\/\/openapi.tools\/#server) tells us there\\nis Spring and Vert.x so we chose to use [Vert.x](https:\/\/vertx.io\/) for our new API-server.\\n","tokens":92,"id":4286,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bob\/0003-introduce-queue.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe current design is based around a single Bob node written in Clojure responsible for:\\n- Exposing the REST API\\n- Implementing the step execution logic via a local Docker daemon\\n- Implementing the registration, creation and update of all the resources\\n- Controlling the lifecycle of a pipeline\\nThis node is expected to be replicated based on the scaling needs and they all would be simply load balanced behind a simple Application Load Balancer like NGINX.\\nThis brings forth the following issues:\\n- There is no back-pressure support: When the nodes are overwhelmed, there is no way to queue up builds and results in dropping of jobs and errors\\n- There is a shared state of where exactly the pipeline is really running and requests like stopping, pausing which need to be exactly delivered to the concerned node. There is no clean way of doing this\\n- The node being implemented in Clojure\/JVM and using docker to implement the steps has a opinionated view:\\n- Platforms which are either unsupported\/resource constrained by the JVM cannot be addressed in a simple manner\\n- Builds with special privileged needs aren't simple to implement\\n- There is no central place for errors and no ability to program\/orchestrate on errors. Use case: CD for machine learning\\n- The scale bottle neck is the runner but the scale unit is the whole of Bob which is quite suboptimal\\n- It's not simple to implement a declarative style of CI\/CD without queueing and back-pressure\\n","Decision":"Based on the above facts the following is decided:\\n- Break the core up into 3 services:\\n- API Server, implementing the spec-first API\\n- Entities, implementing the creation, registration of entities like Pipeline, Resource Provider and Artifact Store\\n- Runner, implementing the step execution logic based on Docker, streaming of logs to DB and pushing out the errors to the queue\\n- Use [RabbitMQ](https:\/\/www.rabbitmq.com\/) as the central queue and rendezvous point for all the services for the following reasons:\\n- It's quite ubiquitous and well battle tested\\n- The protocol and client ecosystem is quite diverse and mature\\n- It's quite resilient and independently scalable\\n- Use the fanout capabilities of the queue to broadcast the stop, pause requests to all connected runners\\n","tokens":305,"id":4287,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bob\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4288,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bob\/0005-introduce-unprivileged-runtime.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently we are using [Docker](https:\/\/www.docker.com\/) as the runtime and orchestration platform for implementing the pipeline steps.\\nGiven its maturity, ubiquitous deployment and tooling around it, it gave us great ease to use it specially via its REST API.\\nBob is what it is mainly due to the enablement Docker had.\\nHowever, it raises the following issues:\\n- Docker mainly runs as a daemon and moreover **needs root access** for the daemon function\\n- Bob is generally intended as a [Cloud Native](https:\/\/en.wikipedia.org\/wiki\/Cloud_native_computing) tool which means all components should be containerized.\\n- Given that docker needs root permissions to run, the runner needs to be privileged to function, causing a security risk to the cluster\\n- The alternative being to mount the host's docker socket into the runner container which is an even bigger security risk\\n- Docker running as a daemon in a container is not a very reliable setup and its own monitoring is still a concern\\n","Decision":"Based on the above facts the following is decided:\\n- Use [Podman](https:\/\/podman.io\/) as the container runtime and orchestration engine for the following reasons:\\n- It is rootless and daemonless\\n- Developed by [RedHat](https:\/\/www.redhat.com\/) and the [OCI](https:\/\/opencontainers.org\/) community\\n- Fully FOSS\\n- Exposes a REST API which is docker complaint too\\n- Brings in the possibilities of more things like pods and more\\n- Swap out [clj-docker-client](https:\/\/github.com\/into-docker\/clj-docker-client) in favor of [contajners](https:\/\/github.com\/lispyclouds\/contajners) as the Clojure interface to the engine\\n- Have a self contained image having the runner, the JVM and Podman and run it **unprivileged**.\\n","tokens":210,"id":4289,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"molgenis-js-auth\/0003-this-will-be-a-general-ui.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis project started out as a permission UI specifically for the Armadillo suite. Along the way it became clear that it could\\nbe used for other Fusion Auth applications as well.\\n","Decision":"The server and UI in this project are generalized in such a way that they can be used for any Fusion Auth application.\\n","tokens":40,"id":4290,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"disc-golf-statistics\/0003-use-node-js-and-express-js.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNode.js acts as the server\/backend for this project and Express.js is a simpler way to work with Node.js.\\n","Decision":"We will use Node.js and Express.js as the server and backend framework for this project.\\n","tokens":27,"id":4293,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"disc-golf-statistics\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4294,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"disc-golf-statistics\/0002-use-es2016-modules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.\\n","Decision":"We will use ES2016 modules to organize and share code. More information can be found here: http:\/\/exploringjs.com\/es6\/ch_modules.html#sec_modules-in-javascript\\n","tokens":36,"id":4295,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"PlaceCal\/0001-combine-places-and-partners.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nPartners and Places both contain a lot of shared functionality. They can also only currently be linked in very crude ways (Partners manage Places), which is resulting in some on-the-ground situations not being mappable into PlaceCal. This would roll both into one model with a variety of toggleable functionality. We need to find a way to simplify and improve the possibilities of relationships between Places, Partners and the rest of the site.\\n## Decision Drivers\\n- Places and Partners are not intuitive units and duplicate a lot of functionality.\\n- It's desirable to have multiple different relationship types between real-life assets such as multiple people managing one place.\\n- There's a lot of duplicated code that can be removed, adding to maintainability.\\n","Decision":"- Places and Partners are not intuitive units and duplicate a lot of functionality.\\n- It's desirable to have multiple different relationship types between real-life assets such as multiple people managing one place.\\n- There's a lot of duplicated code that can be removed, adding to maintainability.\\nChosen option 1.\\nPositive Consequences:\\n- Less code, making it easier to maintain\\n- A single relationship table will make for clearer and more deterministic relationships\\n- Allows more complex relationships between multiple partners.\\n- Events can be owned by multiple partners\\nNegative consequences:\\n- A lot of work to clean up messy forms and re-do relationships\\n- Could be difficult to decide what is shown on each partner's page\\n- Multiple relationship types can make permissions messy\\n","tokens":155,"id":4296,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0021-use-logback-for-logging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nEach application should log.\\n","Decision":"Chosen option: \"Logback\", because it\\n1) natively implements SLF4J and\\n2) SLF4J [offers bridging legacy APIs](https:\/\/www.slf4j.org\/legacy.html) and thus allows to unify logging.\\nPositive Consequences:\\n* Single point for logging configuration\\n* Logging over SLF4J API\\nNegative consequences:\\n* Developers have to include following dependencies in their `pom.xml`, because the concrete logging framework should be chosen at the project executed by a user (or on a server).\\nTests still require logging and thus the concrete logging framework has to be included for tests.\\n<dependency>\\n<groupId>org.slf4j<\/groupId>\\n<artifactId>slf4j-api<\/artifactId>\\n<version>1.7.25<\/version>\\n<scope>compile<\/scope>\\n<\/dependency>\\n<dependency>\\n<groupId>ch.qos.logback<\/groupId>\\n<artifactId>logback-classic<\/artifactId>\\n<version>1.2.3<\/version>\\n<scope>test<\/scope>\\n<\/dependency>\\n<dependency>\\n<groupId>org.slf4j<\/groupId>\\n<artifactId>jcl-over-slf4j<\/artifactId>\\n<version>1.7.25<\/version>\\n<scope>test<\/scope>\\n<\/dependency>\\n* [For easy logging, it is enough to include logback libraries at `WEB-INF\/lib`](https:\/\/logback.qos.ch\/manual\/loggingSeparation.html#easy).\\nThus, following dependency has to be included at REST services:\\n<dependency>\\n<groupId>ch.qos.logback<\/groupId>\\n<artifactId>logback-classic<\/artifactId>\\n<version>1.2.3<\/version>\\n<scope>compile<\/scope>\\n<\/dependency>\\n* Any logging wrapper has also has to be included at \"user-facing\" ends (CLI or WAR).\\nNote that it cannot be included at libraries, because it [leads to an infinite loop](https:\/\/www.slf4j.org\/legacy.html#jclRecursion).\\n<dependency>\\n<groupId>org.slf4j<\/groupId>\\n<artifactId>jcl-over-slf4j<\/artifactId>\\n<version>1.7.25<\/version>\\n<scope>compile<\/scope>\\n<\/dependency>\\n* Legacy\/old libraries depending on a certain logging framework have to be loaded with an exclusion of the logging framework.\\n* The documentation of Winery has to state that slf4j was chosen and that the logging has to be configured accordingly.\\n","tokens":12,"id":4297,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0026-store-license-and-readme-in-entity-root-folder-in-csar.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n`LICENSE` and `README.md` have to be stored in a standardized location when CSARs are exported.\\n## Decision Drivers\\n- Standardized CSAR structure\\n","Decision":"- Standardized CSAR structure\\nChosen option: \"Store these files in a root folder of a respective entity\", because\\n- Less visual clutter\\n- In repository, these files are not separated and stored together with the definition file\\n### Positive Consequences\\n- Standardized access to `LICENSE` and `README.md` files in any Winery-exported CSAR\\n","tokens":41,"id":4298,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0028-semantics-in-the-model.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn the problem detection and solving approach by Saatkamp et al., detected problems in a topology are solved by specific algorithms.\\nThese algorithms must know some semantics in order to perform correctly.\\nTherefore, collection of predefined and known elements which the algorithms can work on is required.\\n","Decision":"Chosen option: \"Predefined elements in constants\".\\nHowever, in near future, we could make this configurable by using the new configuration which is currently implemented by some students.\\n","tokens":63,"id":4300,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0028-use-hardcoded-namespaces-for-threat-modeling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe threat modeling approach relies on pairs of threats and mitigations.\\nEach \"threat\" should be referenced by one particular \"mitigation\".\\n","Decision":"Chosen option: hardcoded namespaces, due to ease of implementation and static nature of the problem\\n### Positive Consequences\\nIn the context of threat modeling multiple different types of threats\/mitigations are not necesaary so a minimal base type that carries the required properties (reference) can be used and extended\\n","tokens":36,"id":4301,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0030-deployable-docker-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe execution time of an online crawler of the DeployableComponents project can not be foreseen, because it depends on the accessed online service.\\nAn architecture, which considers this problem, is needed.\\n","Decision":"Chosen option: \"Asynchronous round-based background execution\", because best performance with best fitting execution procedure to the expected use case.\\n### Positive Consequences <!-- optional -->\\n* good performance\\n* use case of hugh source data set is fulfilled\\n### Negative consequences <!-- optional -->\\n* more complex architecture\\n","tokens":47,"id":4302,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0024-use-travis-for-continuous-integration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWinery needs to be build using continuous integration.\\n## Decision Drivers\\n* Easy to configure CI solution\\n* For free CI solution\\n* Only important output should be logged\\n","Decision":"* Easy to configure CI solution\\n* For free CI solution\\n* Only important output should be logged\\nChosen option: \"TravisCI\", because\\n- The build worklfow on Jenkins is hard to configure\\n- Even though CircleCI offers to use a Docker image as basis and then executing arbitrary commands\\n- Even though CircleCI offers to collect test results and presents them in aggregated form\\n- Even though CircleCI offers to collect build artifacts and offers them to logged in users\\n- Even though TravisCI sometimes throttles down builds (due to heavy load in the Eclipse organization)\\n- [CircleCI cannot be used at Eclipse projects](https:\/\/bugs.eclipse.org\/bugs\/show_bug.cgi?id=536180)\\n","tokens":42,"id":4303,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0029-IPSec-algorithm-application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn the problem detection and solving approach by Saatkamp et al., detected problems in a topology are solved by specific algorithms.\\nThese algorithms must know some semantics in order to perform correctly.\\nConcretely: The IPSec algorithm must know some kind of abstract Virtual Machine (VM) Node Type, since it replaces unsecure VMs with secure VMs that open a secure connection on the IP level.\\n","Decision":"Chosen option: \"Abstract VM Node Type\" since TOSCA allows inheritance and inheritance creates mor semantic meaning.\\nChosen option: \"Secure VMs collected in a special namspace\" since they are special kinds of the \"normal\" VMs, they should inherit from them (and consequently from the abstract VM type mentioned above) to create a meaningful semantics.\\nHowever, instead of creating a special namespace, this should be changed to \"Annotate Secure Types with a Tag\" in near future.\\n","tokens":89,"id":4304,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0023-use-maven-as-build-tool.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhich build tool should be used?\\n","Decision":"Chosen option: \"Maven\", because\\n* None of Gradle's customizability and the overhead in setup that comes with that is required.\\n* The structure the comes with Maven makes the build files easier to understand compared to ANT.\\n","tokens":14,"id":4306,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0025-use-same-logback-test-xml-for-each-sub-project.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen executing tests, logback is loaded as logging framework.\\nLogback in testing mode is configured using `logback-test.xml`.\\nIn case logback is not configured, it does not output anything.\\n## Decision Drivers\\n- Ease of use for developers\\n","Decision":"- Ease of use for developers\\nChosen option: \"Use same `logback-test.xml` for each sub project\", because\\n- During development, the \"local\" `logback-test.xml` file can be adjusted to the needs\\n- During continuous integration, the output should contain warnings and errors only and not any debug information\\n### Positive Consequences\\n- When modifying `logback-test.xml`, a developer just has to copy it over to the other sub projects without thinking which change to propagate to which sub project.\\n","tokens":61,"id":4309,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"winery\/0020-TOSCA-definitions-contain-extactly-one-element.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow should TOSCA data be stored?\\n","Decision":"Chosen option: \"Allow exactly one TOSCA Definitions child in a definition\", because\\n- Definitions are not modeled as explicit element. Only the nested elements are handled by Winery.\\n- That means, it is not possible to specify custom definitions bundling a customized subset of available elements.\\n","tokens":16,"id":4310,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-007-light-client-supervisor-ergonomics.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe initial approach to use callbacks in order to capture async responses\\nfrom requests issued by the `Handle` to its `Supervisor` introduces a couple of\\ndrawbacks, i.e. try semantics, scope and borrow complexities. As the\\ngeneral pattern seems to be that a bounded channel is constructed to transport\\nthe response it could be reworked to be the actual value send with the event to\\nthe Supervisor. Which in turn send back the response on the typed channel to the\\nHandle. As an added side-effect this could remove the need for the Callback\\nabstraction all together and fully embraces CSP style concurrency.\\nFurthermore we have async versions of most `Handle` methods, but no real\\nuse-case driving it. At this point we can't anticipate the limitations of the\\ncurrent design with regards to concurrent access.\\n","Decision":"Remove the callback abstraction in favour of passing channels in the events\\nexchanged directly as the pattern is present already in the current concrete\\nimplementation of the `supervisor::Handle` anyway.\\nRemove async versions of the public methods until a use-case drives their\\nimplementation. We will leave concurrency to be informed by emerging use-cases\\nand field reports. With the first real integration likely happen in the RPC\\nserver and ibc relayer.\\n","tokens":174,"id":4311,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-006-light-client-refactor.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe light client protocol provides a method for verifying application\\nstate without the execution of all preceding transactions. The\\nprotocol for the light client is described in\\n[English](https:\/\/github.com\/tendermint\/spec\/tree\/bucky\/light-reorg\/spec\/consensus\/light)\\nwhile the rust implementation is described in\\n[ADR-002](adr-002-light-client-adr-index.md). This ADR outlines the\\nnext iteration of the light client implementation in rust in which\\noutlines the runtime and concurrency concerns.\\nThe basis for this work comes from the learnings of the [Reactor\\nExperiments](https:\/\/github.com\/informalsystems\/reactor-experiments) as\\nwell as the [Blockchain Reactor\\nRefactor](https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/docs\/architecture\/adr-043-blockchain-riri-org.md). The key take always from that work are:\\n1. Separate concerns into independently verifiable (by humans as well as\\ncomputers) components.\\n2. Deterministic functions wherever possible\\n3. Use events to index parameters of component function executions to\\nmaintain alignment between specs and implementations.\\n","Decision":"The LightClient will be refactored to coordinate interactions with\\nmultiple peers. Two abstractions will be introduced; Instance\\nrepresenting API for interacting with a single Peer and Supervisor,\\nwhich provides an API for interacting with a set of peers known as a\\nPeerList. The PeerList will have a single peer as the Primary and\\nmultiple Peers available as Secondaries.\\nEach LightClient Instance will be configured to perform the light client\\nprotocol against a single peer. The Instance will be decomposed into\\nindependently verifiable components for:\\n* IO: Fetching LightBlocks from the peer\\n* Verifier: Verifying a LightBlock based on a TrustedState\\n* Scheduler: Schedule the next block based on the last TrustedState\\n* Clock: Provide the current time\\nIn addition, cryptographic operations are also decomposed into three \"sub-components\":\\n* Hasher: For hashing headers\\n* CommitValidator: For validating commit integrity\\n* VotingPowerCalculator: For tallying the voting power of a validator set in a commit and verifying signatures\\nEach component and operation will be represented as a trait allowing mock replacements\\nto be used during integration testing.\\nThe Supervisor will:\\n* Manage the lifecycle of Instances\\n* Coordinate the execution of the fork detection protocol\\n* Publishing evidence to peers\\n","tokens":239,"id":4312,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-009-transport-agnostic-peer-abstraction.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWith the opportunity to design and implement the peer-to-peer stack from\\nscratch in the context of the Tendermint implementation in Rust, a lot of the\\nlearnings of the shortcomings of the original Go implementation can be used to\\nprevent certain mistakes. Namely two:\\n* Leakage of physical concerns into the core domain\\n* Flexibility to adopt different wire protocols for transport of messages\\nFor that, the first set of newly introduced concepts will attempt to be generic\\nover which transport is used to connect and converse with other peers. Given\\nstrongly tailored abstract interfaces, concrete implementations will be easy to\\nspin up and plug into the machinery which lifts bytes from the wire into the\\ncore domain and transports messages into the rest of the system.\\n","Decision":"### Transport\\nWrapping the design is the `Transport`. Modelled with the properties of\\na physical network endpoint in mind, which can be bound and stopped. It should\\nstrongly correspond to the acquisition and lifecycle management of network\\nresources on the system.\\n``` rust\\npub trait Transport {\\ntype Connection: Connection;\\ntype Endpoint: Endpoint<Connection = <Self as Transport>::Connection>;\\ntype Incoming: Stream<Item = Result<<Self as Transport>::Connection>> + Send + Sync;\\nasync fn bind(self, bind_info: BindInfo) -> Result<(Self::Endpoint, Self::Incoming)>;\\n}\\n```\\nAfter the successful bind the caller holds an `Endpoint` as well as a stream of\\nincoming `Connection`s. Which is a standardised way to connect to new peers and\\nreact to newly connected ones respectively.\\n``` rust\\npub trait Endpoint: Send + Sync {\\ntype Connection;\\nasync fn connect(&self, info: ConnectInfo) -> Result<Self::Connection>;\\nfn listen_addrs(&self) -> Vec<SocketAddr>;\\n}\\n```\\nCenterpiece of the whole shebang is the `Connection`. It represents a connected\\npeer and provides the primitives to get data and send data from a peer. It is\\ndesigned with the outlook to support stream based transports down the road.\\nWhile being open to enable feature parity with current production installations\\nbased on tendermint-go's `MConn`.\\n``` rust\\npub trait StreamSend: Send + Sync {\\nasync fn send<B: AsRef<[u8]>>(msg: B) -> Result<()>;\\n}\\npub trait Connection: Send + Sync {\\ntype Error: std::error::Error + Send + Sync + 'static;\\ntype StreamRead: Stream<Item = Result<Vec<u8>>> + Send;\\ntype StreamSend: StreamSend;\\nfn advertised_addrs(&self) -> Vec<SocketAddr>;\\nasync fn close(&self) -> Result<()>;\\nfn local_addr(&self) -> SocketAddr;\\nasync fn open_bidirectional(\\n&self,\\nstream_id: StreamId,\\n) -> Result<(Self::StreamRead, Self::StreamSend), Self::Error>;\\nfn public_key(&self) -> PublicKey;\\nfn remote_addr(&self) -> SocketAddr;\\n}\\n```\\n### Peer\\nGiven a correct implementation of a `Transport` and its `Connection` newly\\nestablished ones will be wrapped with a `Peer`. Which is in charge of setting\\nup the correct streams on the `Connection` and multiplex messages - incoming\\nand outgoing alike - efficiently. It's also an attempt to enforce\\ncorrect-by-construction constraints on the state machine of the peer. To avoid\\nmisuse or unexpected transitions. The only way to construct is, is from an\\nexisting connection which gives the caller a connected peer. When invoking run\\non that one a fully function peer is \"returned\". Therefore the states look\\nlike: `Connected -> Running -> Stopped`.\\n``` rust\\nimpl<Conn> Peer<Connected<Conn>>\\nwhere\\nConn: Connection,\\n{\\npub async fn run(self, stream_ids: Vec<StreamId>) -> Result<Peer<Running<Conn>>> {\\n\/\/ ...\\n}\\nasync fn stop(self) -> Result<Peer<Stopped>> {\\n\/\/ ...\\n}\\n}\\nimpl<Conn> Peer<Running<Conn>>\\nwhere\\nConn: Connection,\\n{\\npub async fn send(&self, message: message::Send) -> Result<()> {\\n\/\/ ...\\n}\\npub async fn stop(self) -> Result<Peer<Stopped>> {\\n\/\/ ...\\n}\\n}\\n```\\nWhile sending messages is done through a method on a running peer, getting hold\\nof incoming messages can be achieved by draining the `Receiver` part of the\\nrunning state.\\n### Supervisor\\nThe `Supervisor` is the main entry point to the p2p package giving higher-level\\ncomponents access to a unified stream of peer events and messages as well as\\nthe ability to control peer lifecycle (connect, disconnect, etc.).\\n``` rust\\npub enum Command {\\nAccept,\\nConnect(SocketAddr),\\nDisconnect(node::Id),\\nMsg(node::Id, message::Send),\\n}\\npub enum Event {\\nConnected(node::Id, Direction),\\nDisconnected(node::Id, Report),\\nMessage(node::Id, message::Receive),\\nUpgraded(node::Id),\\nUpgradeFailed(node::Id, Report),\\n}\\nstruct CommandHandle;\\nimpl CommandHandle {\\nfn instruct(command: Command) -> Result<()> {\\n\/\/ ..\\n}\\n}\\nimpl Supervisor {\\npub fn new<T>(transport: T) -> Self\\nwhere\\nT: transport::Transport + Send + 'static,\\n{\\n\/\/ ..\\n}\\npub handle(&self) -> CommandHandle {\\n\/\/ ..\\n}\\npub subscribe(&self) -> Receiver<Event> {\\n\/\/ ..\\n}\\npub async fn run<T>(self) -> Result<()> {\\n\/\/ ...\\n}\\n}\\n```\\n","tokens":155,"id":4313,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-008-event-subscription.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe [Tendermint Light Client](..\/..\/light-client\/) is one of many important\\napplications that will make use of the [RPC client](..\/..\/rpc\/) to query\\nTendermint full nodes for information relating to the blockchain.\\nTendermint servers (e.g. full nodes) provide an [event\\nsubscription][tm-event-subs] RPC endpoint to allow clients to receive\\nnotifications of specific events as they happen (currently via a WebSockets\\nconnection). We need to expose this subscription mechanism through the RPC client.\\nIn order to achieve this, we need:\\n1. An ergonomic interface for the RPC client that allows developers to subscribe\\nto events produced by specific queries. Specifically, this interface must:\\n1. Offer **subscription** functionality, where:\\n1. A single **subscription** takes place to the **events** produced by a\\n**query** (the [PEG] for which, at the time of this writing, is located\\n[here][query-peg]).\\n2. Callers must be able to create multiple distinct subscriptions.\\n3. A subscription must offer an interface to allow for iteration over\\nincoming events only relevant to that subscription (i.e. it should\\nproduce an **event stream**).\\n4. It must be possible to, from outside of the RPC client, receive events\\nfrom multiple subscriptions' event streams concurrently without\\ninterfering with\/blocking each other.\\n5. It must be possible to, from outside of the RPC client, handle\\nsubscriptions' event streams without blocking other RPC operations.\\n2. Offer the ability to **unsubscribe** from specific queries (i.e. to\\nterminate specific subscriptions).\\n2. An appropriate concurrency model for drivers of the transport layer that\\nallows the transport layer to operate independently of consumers of\\nsubscriptions. This is so that consumers don't block transport layer\\nactivities and vice-versa.\\n","Decision":"### Assumptions\\n* All blocking operations that deal with I\/O must be `async`.\\n* We will not be [\"de-asyncifying\" the RPC][issue-318] and will rather, in a\\nfuture ADR, propose a synchronous architecture as well should we need one.\\n### Proposed Entities and Relationships\\nThe entities in the diagram below are described in the following subsections.\\n![RPC client ERD](assets\/rpc-client-erd.png)\\n### `Event`\\nIn terms of the subscription interface, this is ultimately what the end user is\\nmost interested in obtaining. The `Event` type's structure is dictated by the\\nTendermint RPC:\\n```rust\\npub struct Event {\\n\/\/\/ The query that produced the event.\\npub query: String,\\n\/\/\/ The data associated with the event (determines its `EventType`).\\npub data: EventData,\\n\/\/\/ Event type and attributes map.\\npub events: Option<HashMap<String, Vec<String>>>,\\n}\\npub enum EventData {\\nNewBlock {\\nblock: Option<Block>,\\nresult_begin_block: Option<BeginBlock>,\\nresult_end_block: Option<EndBlock>,\\n},\\nTx {\\ntx_result: TxResult,\\n},\\n\/\/ ...\\n}\\n```\\n### `Subscription`\\nA `Subscription` here is envisaged as an entity that implements the\\n[Stream][futures-stream] trait, allowing its owner to asynchronously iterate\\nthrough all of the relevant incoming events. Use of such a subscription should\\nbe as simple as:\\n```rust\\nwhile let Some(result_event) = subscription.next().await {\\nmatch result_event {\\nOk(event) => { \/* handle event *\/ },\\nErr(e) => { \/* terminate subscription and report error *\/ },\\n}\\n}\\n\/\/ Terminate the subscription (i.e. unsubscribe and consume it).\\n\/\/ Since a `Subscription` could be moved to and consumed in any asynchronous\\n\/\/ context (and a distinct context to the original client entity that created\\n\/\/ it), it would make sense that **unsubscribing** should be accessible from\\n\/\/ that same context.\\nsubscription.terminate().await.unwrap();\\n```\\nOnce [asynchronous destructors][async-drop] are available in Rust, the\\n`terminate` method should no longer be necessary.\\nFor efficient routing of events to `Subscription`s, each `Subscription` should\\nhave some kind of unique identifier associated with it (a `SubscriptionId`).\\nEach `Subscription` relates only to a single [`Query`](#query). Therefore, its\\npublicly accessible fields may resemble the following:\\n```rust\\npub struct Subscription {\\npub id: SubscriptionId,\\npub query: Query,\\n\/\/ ... other fields to help facilitate inter-task comms ...\\n}\\n```\\n#### Buffering of Events\\nSince the rate at which events could be produced by the remote RPC endpoint may\\ndiffer from the rate at which the client process them, we need to buffer\\nincoming events in a `Subscription`.\\nUnder the hood, a `Subscription` is envisaged to make use of some kind of\\n**unbounded channel** to buffer incoming `Event`s, such as that provided by\\n[Tokio's `mpsc`][tokio-mpsc]. We don't propose the use of bounded channels yet\\nsince they complicate the concurrency model significantly: we would need to\\ncater for cases where we encounter full channels and provide for conventional or\\napplication-specific ways of dealing with those full channels (e.g. back-off, or\\nback-pressure).\\n#### Managing Multiple Simultaneous Subscriptions\\nThere may come instances where clients would want to initiate multiple\\nsubscriptions to different event types and consume them all from the same\\ncontext. Since the `Subscription` struct implements the\\n[`Stream`][futures-stream] trait, all of the [stream-related\\nfunctionality][futures-stream-mod] should enhance the ergonomics of working with\\n`Subscription`s.\\nFor example, if you wanted to iterate through two subscriptions at the same\\ntime, processing events in the order in which they are received by the client:\\n```rust\\nuse futures::stream::select_all;\\n\/\/ `subs1` and `subs2` are `Subscription`s:\\nwhile let Some(res) = select_all(vec![subs1, subs2]).next().await {\\nmatch res {\\nOk(event) => { \/* handle event *\/ },\\nErr(e) => { \/* handle error *\/ },\\n}\\n}\\n```\\n### Client Model\\nUsers of the Tendermint RPC library may or may not want access to subscription\\nfunctionality. Since such functionality comes with additional overhead in terms\\nof resource usage and asynchronous task management, it would be optimal to\\nprovide two separate client traits: one that only implements non-subscription\\nfunctionality, and one that only implements subscription functionality (where\\nclients could either implement one or both traits).\\nThe interfaces of the two types of clients are envisaged as follows.\\n#### `Client`\\nThis type of client would allow for interaction with all RPC endpoints except\\nthose pertaining to subscription management. In our current implementation, this\\nclient would only interact via the HTTP RPC endpoints (the `HttpClient` in the\\nentity diagram above).\\n**Note**: All `async` traits are facilitated by the use of [async-trait].\\n```rust\\npub type Result<R> = std::result::Result<R, Error>;\\n#[async_trait]\\npub trait Client {\\n\/\/\/ `\/abci_info`: get information about the ABCI application.\\nasync fn abci_info(&self) -> Result<abci_info::AbciInfo>;\\n\/\/\/ `\/abci_query`: query the ABCI application\\nasync fn abci_query<V>(\\n&self,\\npath: Option<abci::Path>,\\ndata: V,\\nheight: Option<Height>,\\nprove: bool,\\n) -> Result<abci_query::AbciQuery>\\nwhere\\nV: Into<Vec<u8>> + Send;\\n\/\/\/ ...\\n\/\/\/ Perform a general request against the RPC endpoint\\nasync fn perform<R>(&self, request: R) -> Result<R::Response>\\nwhere\\nR: Request;\\n}\\n```\\n#### `SubscriptionClient`\\nA `SubscriptionClient` would be one that only provides access to subscription\\nfunctionality. In our current implementation, this client would interact with a\\nWebSocket connection to provide subscription functionality (the\\n`WebSocketClient` in the entity diagram above).\\n```rust\\n#[async_trait]\\npub trait SubscriptionClient {\\n\/\/\/ `\/subscribe`: subscribe to receive events produced by the given query.\\nasync fn subscribe(&mut self, query: Query) -> Result<Subscription>;\\n}\\n```\\n### Client Implementations\\nWe envisage 2 distinct client implementations at this point:\\n* `HttpClient`, which only implements [`Client`](#client) (over HTTP).\\n* `WebSocketClient`, which will implement [`Client`](#client) and\\n[`SubscriptionClient`](#subscriptionclient) (over a WebSocket connection).\\n#### Handle-Driver Concurrency Model\\nDepending on the underlying transport, a client may need a **transport driver**\\nrunning in an asynchronous context. As in the example of a WebSocket connection,\\nthe rate at which one interacts with the WebSocket connection may differ to the\\nrate at which one interacts with `Event`s being produced by a `Subscription`, so\\nthey necessarily need to run concurrently in different contexts.\\nImplementation of such a transport driver is transport-specific. Short-lived\\nrequest\/response interactions (such as HTTP) would not require such a transport\\ndriver, whereas a WebSocket connection would.\\nIn cases where a driver is necessary, the client implementation would have to\\nbecome a **handle** to the driver, facilitating communication with it across\\nasynchronous tasks.\\nA rough sketch of the interaction model between the different components\\nenvisaged to make up the subscription subsystem is shown in the following\\nsequence diagram.\\n![RPC client concurrency model](.\/assets\/rpc-client-concurrency.png)\\n### `Query`\\nIt is proposed that, using a *builder pattern*, we implement a subscription\\n`Query` interface that implements the full [query PEG][query-peg] provided by\\nthe Go implementation of the RPC client. This would allow for compile-time\\nvalidation of queries.\\nThe desired interface for constructing a query would look as follows:\\n```rust\\n\/\/ tm.event='NewBlock'\\nlet query = Query::from(EventType::NewBlock);\\n\/\/ tm.event='Tx' AND tx.hash='XYZ'\\nlet query = Query::from(EventType::Tx).and_eq(\"tx.hash\", \"XYZ\");\\n\/\/ tm.event='Tx' AND tx.height=5\\nlet query = Query::from(EventType::Tx).and_eq(\"tx.height\", 5);\\n```\\nThis interface could be implemented along the following lines.\\n```rust\\n\/\/ Query would only have constructors that either specified an event type\\n\/\/ (corresponding to a `tm.event='eventtype'` query) or a condition. There must\\n\/\/ be no constructor that allows for construction of an empty query.\\npub struct Query {\\n\/\/ A query can only match zero or one type of event.\\nevent_type: Option<EventType>,\\n\/\/ A query can contain zero or more conditions.\\nconditions: Vec<Condition>,\\n}\\nimpl From<EventType> for Query {\\nfn from(event_type: EventType) -> Self {\\nSelf {\\nevent_type: Some(event_type),\\nconditions: Vec::new(),\\n}\\n}\\n}\\nimpl Query {\\n\/\/ An example of a constructor for `Operation::Eq`.\\npub fn eq(key: &str, value: impl Into<Operand>) -> Self {\\nSelf {\\nevent_type: None,\\nconditions: vec![Condition::new(key, Operation::Eq(value.into()))],\\n}\\n}\\n\/\/ ...\\n\/\/ Allows for a simple builder pattern.\\npub fn and_eq(mut self, key: &str, value: impl Into<Operand>) -> Self {\\nself.conditions.push(Condition::new(key, Operation::Eq(value.into())));\\nself\\n}\\n\/\/ ...\\n}\\n\/\/ Derived from https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/types\/events.go\\npub enum EventType {\\nNewBlock,\\nNewBlockHeader,\\nNewEvidence,\\nTx,\\nValidatorSetUpdates,\\n}\\n\/\/ A condition specifies a key (first parameter) and, depending on the\\n\/\/ operation, an value which is an operand of some kind.\\npub enum Condition {\\n\/\/ Equals\\nEq(String, Operand),\\n\/\/ Less than\\nLt(String, Operand),\\n\/\/ Less than or equal to\\nLte(String, Operand),\\n\/\/ Greater than\\nGt(String, Operand),\\n\/\/ Greater than or equal to\\nGte(String, Operand),\\n\/\/ Contains (to check if a key contains a certain sub-string)\\nContains(String, String),\\n\/\/ Exists (to check if a key exists)\\nExists(String),\\n}\\n\/\/ According to https:\/\/docs.tendermint.com\/v0.34\/rpc\/#\/Websocket\/subscribe,\\n\/\/ an operand can be a string, number, date or time. We differentiate here\\n\/\/ between integer and floating point numbers.\\n\/\/\\n\/\/ It would be most useful to implement `From` traits for each of the different\\n\/\/ operand types to the `Operand` enum, as this would improve ergonomics.\\npub enum Operand {\\nString(String),\\nSigned(i64),\\nUnsigned(u64),\\nFloat(f64),\\nDate(chrono::Date),\\nDateTime(chrono::DateTime),\\n}\\n```\\n### Subscription Tracking and Routing\\nInternally, a `SubscriptionRouter` is proposed that uses a `HashMap` to keep\\ntrack of all of the queries relating to a particular client.\\n```rust\\npub struct SubscriptionRouter {\\n\/\/ A map of queries -> (map of subscription IDs -> result event tx channels)\\nsubscriptions: HashMap<String, HashMap<String, SubscriptionTx>>,\\n}\\n```\\nAt present, Tendermint includes the ID of the subscription relating to a\\nparticular event in the JSON-RPC message, and so this data structure is optimal\\nfor such a configuration. This will necessarily change if Tendermint's WebSocket\\nserver [drops subscription IDs from events][tendermint-2949], which is likely if\\nwe want to conform more strictly to the [JSON-RPC standard for\\nnotifications][jsonrpc-notifications].\\n### Handling Mixed Events and Responses\\nSince a full client needs to implement both the `Client` and\\n`SubscriptionClient` traits, for certain transports (like a WebSocket\\nconnection) we could end up receiving a mixture of events from subscriptions\\nand responses to RPC requests. To disambiguate these different types of\\nincoming messages, a simple mechanism is proposed for the\\n`WebSocketClientDriver` that keeps track of pending requests and only matures\\nthem once it receives its corresponding response.\\n```rust\\npub struct WebSocketClientDriver {\\n\/\/ ...\\n\/\/ Commands we've received but have not yet completed, indexed by their ID.\\n\/\/ A Terminate command is executed immediately.\\npending_commands: HashMap<String, DriverCommand>,\\n}\\n\/\/ The different types of requests that the WebSocketClient can send to its\\n\/\/ driver.\\n\/\/\\n\/\/ Each of SubscribeCommand, UnsubscribeCommand and SimpleRequestCommand keep\\n\/\/ a response channel that allows for the driver to send a response later on\\n\/\/ when it receives a relevant one.\\nenum DriverCommand {\\n\/\/ Initiate a subscription request.\\nSubscribe(SubscribeCommand),\\n\/\/ Initiate an unsubscribe request.\\nUnsubscribe(UnsubscribeCommand),\\n\/\/ For non-subscription-related requests.\\nSimpleRequest(SimpleRequestCommand),\\nTerminate,\\n}\\n```\\nIDs of outgoing requests are randomly generated [UUIDv4] strings.\\nThe logic here is as follows:\\n1. A call is made to `WebSocketClient::subscribe` or\\n`WebSocketClient::perform`.\\n2. The client sends the relevant `DriverCommand` to its driver via its internal\\ncommunication channel.\\n3. The driver receives the command, sends the relevant simple or subscription\\nrequest, and keeps track of the command in its `pending_commands` member\\nalong with its ID. This allows the driver to continue handling outgoing\\nrequests and incoming responses in the meantime.\\n4. If the driver receives a JSON-RPC message whose ID corresponds to an ID in\\nits `pending_commands` member, it assumes that response is relevant to that\\ncommand and sends back to the original caller by way of a channel stored in\\none of the `SubscribeCommand`, `UnsubscribeCommand` or\\n`SimpleRequestCommand` structs. Failures are also communicated through this\\nsame mechanism.\\n5. The pending command is evicted from the `pending_commands` member.\\n","tokens":396,"id":4314,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-001-repo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis crate began its life in the\\n[tendermint\/kms](http:\/\/github.com\/tendermint\/kms)\\nas a library of components necessary to build the Tendermint KMS.\\nOther features were added to support additional tooling, like a Tendermint config parser and RPC client.\\nWe would now like to develop it further to support lite clients and eventually\\nfull nodes. It has thus been split into its own repo to evolve independently of\\nthe KMS. Here we propose an initial plan for the evolution of the repo towards\\nthese goals.\\n","Decision":"At this stage, the repository must address three concerns:\\n1) Maintain existing tendermint-rs roughly as is for the KMS and rpc client\\nusers\\n2) Establish a Lite Client\\n3) Establish a Reactor Framework and initial implementations of Tendermint\\nreactor components\\nFor now, we can consider that each of these will proceed as their own crate.\\n`tendermint-rs` already exists as the repo namesake, while `lite` and `reactor`\\ncrates can be created in the root.\\n### Maintaining Tendermint-RS\\nWe should support the current tendermint-rs as necessary, and not hasten to make\\nany sweeping changes to the structure until we better understand consumers.\\nAny changes must be well co-ordinated with at least the KMS. Also, the\\n`secret_connection` code should be\\n[moved back to the KMS](https:\/\/github.com\/interchainio\/tendermint-rs\/pull\/21#issuecomment-529061992)\\nWe may ultimately want to consider further breaking it up into crates. At\\npresent, that might consist of the following:\\n- crypto\\n- tendermint-rs (depends on crypto)\\n- config (depends on tendermint-rs)\\n- rpc (depends on tendermint-rs)\\nSee some prior discussion about this\\n[here](https:\/\/github.com\/interchainio\/tendermint-rs\/issues\/7).\\n### Lite Client\\nMost of the necessary data types already exist (pubkey\/signature, blockid,\\nvote), with support for serialization\/deserialization. The crux of the lite\\nclient is verifying validator sets by computing their merkle root, and verifying\\ncommits by checking validator signatures. We have recently completed first\\npasses at these verifications - they need to be further reviewed, better\\nstructured, and better tested.\\nIdeally, as much of the lite client code as possible is independent of the\\nparticulars of pubkey\/signature\/blockid\/vote\/etc. The lite client should be\\nwritten generically, with its own traits, and the existing types made to\\nimplement them.\\nWe should follow the [lite client spec closely](https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/docs\/spec\/consensus\/light-client.md), and we should work in parallel on a TLA+ implementation.\\nNote the spec assumes a function `signers(commit)`, which returns the validators\\nfor a given commit. In practice, the validator set itself is not in the commit,\\nso this requires fetching the validator set from a full node, computing the merkle root,\\nand comparing it against the ValidatorsHash in the header. We must also ensure\\nthe hash of the header is included in the BlockID. This kind of stuff can be\\nabstracted from the core lite client code (ie. `signers(commit)`), but needs to\\nbe supported in the actual implementation.\\nNote the\\n[structure of commits is about to change in\\nTendermint](https:\/\/github.com\/tendermint\/tendermint\/issues\/1648),\\na big breaking change that will make blocks much smaller by eliminating\\nredundant information from the included votes (ie. just including the\\nsignatures). The generic lite client should be able to abstract over such a\\nchange just fine, but we'd of course have to update how the commit types\\nimplement the lite client traits.\\nWe should try to surface the validity of data as much as possible in the type\\nsystem, so we can clarify the levels of validity of our data. For instance, the\\ndifference between a random validator set that's just been loaded and a\\nvalidator set that has been verified against some commits should have different\\ntypes.\\nWe should also get started with a `lite` binary that reads from the rpc and performs a lite client sync.\\nMost of the RPC client is fleshed out, but we'll have to add support for a few\\nmore fields and write tests.\\n### Reactors\\nThe primary goals of the new reactor design are [deterministic simulation](https:\/\/www.youtube.com\/watch?v=4fFDFbi3toc)\\nand a tight mapping between code and formal specification.\\nDeterministic simulation should allow us to simulate networks with hundreds (thousands?!)\\nof nodes in a single process, and to deterministically replay simulations, allowing complex\\nexecutions to be studied and debugged. The framework will require further\\ninvestigations of Rust's concurrency model and existing frameworks.\\nTight mapping between code and formal specification will allow us to more easily reason\\nabout the correctness of the code, and, with some R&D work, automatically generate tests\\nfrom the formal specification. It believes this will be helped via ideas like session types,\\nwhere as much as possible about the system's state and transitions is expressed\\nin the type system.\\nWork on both of these goals can begin independently. On the one hand, we should\\nbe exploring Rust frameworks for deterministic simulation, and on the other we\\nshould be writing the core reactor state machines, eventually to be plugged into\\nthe simulation framework.\\n","tokens":117,"id":4315,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-004-light-client-cli.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe high level context for the light client is described in\\n[ADR-002](adr-002-light-client-adr-index.md).\\nFor reference, a schematic of the light node is below:\\n![Light Node Diagram](assets\/light-node.png).\\nHere we focus on how the Light Node process itself is composed.\\nThe light node process must consider the following features:\\n- command line UX and flags\\n- config file\\n- logging\\n- error handling\\n- state management\\n- exposing RPC servers\\nIdeally, it can support all of this with a minimum of dependencies.\\nWe'd like to be able to start a light node process and have it sync to the\\nlatest height and stay synced while it runs.\\n","Decision":"### Abscissa\\n[Abscissa](https:\/\/github.com\/iqlusioninc\/abscissa) is a framework for building CLI\\ntools in Rust by Tony Arcieri of Iqlusion.\\nIt's focus is on security and minimizing dependencies.\\nThe full list of dependencies can be found [here](https:\/\/github.com\/iqlusioninc\/abscissa#dependencies).\\nFor instance, while it includes functionality for command-line option parsing like that\\nprovided by `structopt` + `clap`, it does so with far less dependencies.\\n[Users](https:\/\/github.com\/iqlusioninc\/abscissa#projects-using-abscissa)\\nof note include the [Tendermint KMS](https:\/\/github.com\/tendermint\/kms)\\nfor validators and the new\\n[Zebra ZCash full node](https:\/\/github.com\/ZcashFoundation\/zebra).\\nSee the [introductory blog\\npost](https:\/\/iqlusion.blog\/introducing-abscissa-rust-application-framework)\\nfor more details.\\n### Config\\nConfig includes:\\n- trusting period\\n- initial list of full nodes\\n- method (sequential or skipping)\\n- trust level (if method==skipping)\\nThe configuration contains an initial list of full nodes (peers).\\nFor the sake of simplicity, one of the peers is selected as the \"primary\", while the\\nrest are considered \"backups\". Most of the data is downloaded from the primary,\\nand double checked against the backups.\\nThe state is considered \"expired\" if the difference between the current time and\\nthe time from the trusted header is greater than a configurable \"trusting\\nperiod\". If at any point the state is expired, the node should log an error and\\nexit - it's needs to be manually reset.\\n### Initialization\\nThe node is initialized with a trusted header for some height and a validator set for the next height.\\nThe node may be initialized by the user with only a height and header hash, and\\nproceed to request the full header and validator set from a full node. This\\nreduces the initialization burden on the user, and simplifies passing this\\ninformation into the process, but for the state to be properly initialized it\\nwill need to get the correct header and validator set before starting the light\\nclient syncing protocol.\\n### State\\nThe light node will need to maintain state including the current height, the\\nlast verified and trusted header, and the current set of trusted validators.\\n","tokens":151,"id":4316,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tendermint-rs\/adr-003-light-client-core-verification.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe high level context for the light client is described in\\n[ADR-002](adr-002-light-client-adr-index.md).\\nFor reference, a schematic of the light node is below:\\n![Light Node Diagram](assets\/light-node.png).\\nHere we focus on the core verification library, which is reflected in the\\ndiagram as the \"Light Client Verifier\" and \"Bisector\".\\nThe light node is subjectively initialized, and then attempts to sync to a most\\nrecent height by skipping straight to it. The verifier is the core logic\\nto check if a header can be trusted and if we can skip to it.\\nIf the validator set has changed too much, the header can't be trusted,\\nand we'll have to request intermediate headers as necessary to sync through the validator set changes.\\nNote the verifier should also work for IBC, though in IBC bisection can't be performed directly\\nsince blockchains can't make HTTP requests. There, bisection is expected to be done by an external relayer\\nprocess that can make requests to full nodes and submit the results to an IBC enabled chain.\\nThe library should lend itself smoothly to both use cases and should be difficult to use incorrectly.\\nThe core verification operates primarily on data types that are already implemented\\nin tendermint-rs (headers, public keys, signatures, votes, etc.). The crux of it\\nis verifying validator sets by computing their merkle root, and verifying\\ncommits by checking validator signatures. The particular data structures used by\\nTendermint have considerably more features\/functionality than needed for this,\\nhence the core verification library should abstract over it.\\nHere we describe the core verification library, including:\\n- traits\\n- public functions for IBC and light nodes\\n- implementations of traits for the existing Tendermint data-structures\\n","Decision":"The implementation of the core verification involves two components:\\n- a new `light` crate, containing traits and functions defining the core verification\\nlogic\\n- a `light-impl` module within the `tendermint` crate, containing implementations of\\nthe traits for the tendermint specific data structures\\nThe `light` crate should have minimal dependencies and not depend on\\n`tendermint`. This way it will be kept clean, easy to test, and easily used for\\nvariations on tendermint with different header and vote data types.\\nThe light crate exposes traits for the block header, validator set, and commit\\nwith the minimum information necessary to support general light client logic.\\nAccording to the specification, the key functionality the light client\\nverification requires is to determine the set of signers in a commit, and\\nto determine the voting power of those signers in another validator set.\\nHence we can abstract over the lower-level detail of how signatures are\\nvalidated.\\n### Header\\nA Header must contain a height, time, and the hashes of the current and next validator sets.\\nIt can be uniquely identified by its hash:\\n```rust\\npub trait Header {\\nfn height(&self) -> Height;\\nfn bft_time(&self) -> Time;\\nfn validators_hash(&self) -> Hash;\\nfn next_validators_hash(&self) -> Hash;\\nfn hash(&self) -> Hash;\\n}\\n```\\n### Commit\\nA commit in the blockchain contains the underlying signatures from validators, typically in\\nthe form of votes. From the perspective of the light client,\\nwe don't care about the signatures themselves.\\nWe're only interested in knowing the total voting power from a given validator\\nset that signed for the given block. Hence we can abstract over underlying votes and\\nsignatures and just expose a `voting_power_in`, as per the spec:\\n```rust\\npub trait Commit {\\ntype ValidatorSet: ValidatorSet;\\nfn header_hash(&self) -> Hash;\\nfn validate(&self, &Self::ValidatorSet) -> Result<(), Error>;\\nfn voting_power_in(&self, vals: &Self::ValidatorSet) -> Result<u64, Error>;\\n}\\n```\\nThe `header_hash` is the header being committed to,\\nand `validate` performs Commit-structure-specific validations,\\nfor instance checking the number of votes is correct, or that they are\\nall for the right block ID.\\nComputing `voting_power_in` require access to a validator set\\nto get the public keys to verify signatures. But this specific relationship\\nbetween validators and commit structure isn't the business of the `light` crate;\\nit's rather how a kind of Tendermint Commit structure implements the `light` traits.\\nBy making ValidatorSet an associated type of Commit, the light crate doesn't\\nhave to know much about validators or how they relate to commits, so the ValidatorSet\\ntrait can be much smaller, and we never even define the concept of an individual\\nvalidator.\\nNote this means `Commit` is expected to have access to the ids of the validators that\\nsigned, which can then be used to look them up in their ValidatorSet implementation.\\nThis is presently true, since Commit's contain the validator addresses\\nalong with the signatures. But if the addresses were removed, for instance to\\nsave space, the Commit trait would need access to the relevant validator set to\\nget the Ids, and this validator set may be different than the one being passed\\nin `voting_power_in`.\\nBy abstracting over the underlying vote type, this trait can support\\noptimizations like batch verification of signatures, and the use of\\naggregate signatures instead of individual votes.\\nSo long as it can be determined what voting power of a given validator set\\nsigned correctly for the commit.\\nThe method `voting_power_in` performs the underlying signature verifications.\\nIt should return an error if any of them fail or are for the wrong header hash.\\nNote the specification introduces a `signers(commit) -> validators` method that\\nreturns the list of validators that signed a commit. However, such a method\\nwould require access to the underlying validator set in order to verify the\\ncommits, and it is only ever used in computing `voting_power_in`. Hence, we\\ndispense with it here in favour of a `voting_power_in` that operates on a\\n`Commit` and `ValidatorSet`. However, this also means that ValidatorSet will\\nneed to expose facilities for determining whether a validator signed correctly in\\norder for implementations to make use of it to compute `voting_power_in`.\\nNote also that in Tendermint, commits are for a particular block ID, which\\nincludes both a header hash and a \"parts set hash\". The latter is completely\\nirrelevant to the light client, and can only be verified by downloading the full\\nblock. Hence it is effectively ignored here. It would be great if Tendermint\\ncould disentangle commits to the proposal block parts for gossip (ie. the parts\\nset hash) from commits to the header itself (ie. the header hash), but that's\\nleft for the future.\\nFor more background on implementation of Tendermint commits and votes, see:\\n- [ADR-025](https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/docs\/architecture\/adr-025-commit.md)\\n- [Validator Signing Spec](https:\/\/github.com\/tendermint\/tendermint\/blob\/main\/docs\/spec\/consensus\/signing.md)\\n- [Tendermint consensus specification](https:\/\/arxiv.org\/abs\/1807.04938)\\n### Validator Set\\nA validator set has a unique hash which must match what's in the header.\\nIt also has a total power used for determining if the result of `voting_power_in` is greater\\nthan a fraction of the total power.\\nValidatorSet is implemented as an associated type of Commit, where it's\\nnecessary to compute `voting_power_in`, so the underlying implementation must\\nhave some way to determine the voting power of the validators that signed,\\nsince voting power is not found in the commit itself.\\nNote we don't need to define individual validators since all the details of how validators relates to commits\\nis encapsulated in the Commit.\\n```rust\\npub trait ValidatorSet {\\nfn hash(&self) -> Hash;\\nfn total_power(&self) -> u64;\\n}\\n```\\n### State\\nAccording to the spec, the light client is expected to have a store that it can\\npersist trusted headers and validators to. This is necessary to fetch the last trusted\\nvalidators to be used in verifying a new header, but it's also needed in case\\nany conflicting commits are discovered and they need to be published to the\\nblockchain. That said, it's not needed for the core verification, so we don't\\ninclude it here. Users of the `light` crate like a light node decide how to\\nmanage the state. We do include some convenience structs:\\n```rust\\npub struct SignedHeader<C, H>\\nwhere\\nC: Commit,\\nH: Header,\\n{\\ncommit: C,\\nheader: H,\\n}\\npub struct TrustedState<C, H>{\\nwhere\\nC: Commit,\\nH: Header,\\n{\\nlast_header: SignedHeader<C, H>,\\nvalidators: C::ValidatorSet,\\n}\\n```\\nHere the trusted state combines both the signed header and the validator set,\\nready to be persisted to some external store.\\n### TrustThreshold\\nThe amount of validator set change that occur when skipping to a higher height depends on the\\ntrust threshold, as per the spec. Here we define it as a trait that encapsulated the math of what percent\\nof validators need to sign:\\n```rust\\npub trait TrustThreshold: Copy + Clone {\\nfn is_enough_power(&self, signed_voting_power: u64, total_voting_power: u64) -> bool;\\n}\\n```\\nWe provide a conenvient implementation that takes a numerator and a denominator. The default is of course 1\/3.\\n### Requester\\nThe light node needs to make requests to full nodes during bisection for intermediate signed headers and validator sets:\\n```rust\\npub trait Requester<C, H>\\nwhere\\nC: Commit,\\nH: Header,\\n{\\nfn signed_header(&self, h: Height) -> Result<SignedHeader<C, H>, Error>;\\nfn validator_set(&self, h: Height) -> Result<C::ValidatorSet, Error>;\\n}\\n```\\nIn practice, this can be implemented as a Tendermint RPC client making requests\\nto the `\/commit` and `\/validators` endpoints of full nodes.\\nFor testing, the Requester can be implemented by JSON files.\\n### Verification\\nBoth IBC and full node syncing have to perform a common set of checks:\\n- validate the hashes\\n- if the header is sequential, validate the next validator set\\n- if the header is not sequential, check if the trust threshold is reached\\n- this uses `voting_power_in` with a validator set that may be different from\\nthe one that actually created the commit.\\n- check that +2\/3 of the validators signed\\n- this uses `voting_power_in` with the actual validator set\\nThese are implemented in a common function, `verify_single_inner`:\\n```rust\\nfn verify_single_inner<H, C, L>(\\ntrusted_state: &TrustedState<C, H>,\\nuntrusted_sh: &SignedHeader<C, H>,\\nuntrusted_vals: &C::ValidatorSet,\\nuntrusted_next_vals: &C::ValidatorSet,\\ntrust_threshold: L,\\n) -> Result<(), Error>\\n```\\nNote however that light client security model is highly sensitive to time, so the public functions\\nexposed for IBC and bisection, which will call `verify_single_inner`, must take a current time\\nand check we haven't expired.\\nFor IBC, since it can't make its own requests, the public function just takes the untrusted state\\nin full, and return it as a TrustedState if it verifies:\\n```rust\\npub fn verify_single<H, C, T>(\\ntrusted_state: TrustedState<C, H>,\\nuntrusted_sh: &SignedHeader<C, H>,\\nuntrusted_vals: &C::ValidatorSet,\\nuntrusted_next_vals: &C::ValidatorSet,\\ntrust_threshold: T,\\ntrusting_period: &Duration,\\nnow: &SystemTime,\\n) -> Result<TrustedState<C, H>, Error>\\n```\\nFor the light node, we pass in a Requester, and specify a height we want to sync to.\\nIt will fetch that header and try to verify it using the skipping method,\\nand will run a bisection algorithm to recursively request headers of lower height\\nas needed. It returns a list of headers it verified along the way:\\n```rust\\npub fn verify_bisection<C, H, L, R>(\\ntrusted_state: TrustedState<C, H>,\\nuntrusted_height: Height,\\ntrust_threshold: L,\\ntrusting_period: &Duration,\\nnow: &SystemTime,\\nreq: &R,\\n) -> Result<Vec<TrustedState<C, H>>, Error>\\n```\\n### Implementing Traits\\nThe core `light` traits can be implemented by the Tendermint data structures and\\ntheir variations. For instance, v0.33 of Tendermint Core introduced a breaking\\nchange to the Commit structure to make it much smaller. We can implement the\\n`light` traits for both versions of the Commit structure.\\nThe `light` abstractions also facilitate testing, as complete Tendermint data structures\\nare not required to test the light client logic, only\\nthe elements it cares about. This means we can implement mock commits and validators,\\nwhere validators are just numbered 0,1,2... and both commits and validators are\\nsimply lists of integers representing the validators that signed or are in the\\nvalidator set. This aligns closely with how these structures are represented in\\nthe [TLA+\\nspec](https:\/\/github.com\/interchainio\/verification\/blob\/develop\/spec\/light-client\/Blockchain.tla).\\nWhile this provides a lot of flexibility in mocking out\\nthe types, we must be careful to ensure they match the semantics of the actual\\nTendermint types, and that we still test the verification logic sufficiently for\\nthe actual types.\\n### Other Validation\\nSome fields in the header are left explicitly unvalidated as they have minimal bearing on the correctness of the light client.\\nThese include:\\n- LastCommitHash\\n- In the skipping case, it's not possible to verify the header refers to the correct previous block without reverting to the sequential case. So in the sequential case, we don't validate this either. If it's incorrect, in indicates the validators are misbehaving, though we can only detect it as a light client if there's a fork.\\n- BlockID\\n- As mentioned, this includes a merkle root of the entire block and is not verifiable without downloading the whole block, which would defeat the purpose of the light client!\\n- Time\\n- Verifying the time would require us to download the commit for the previous block, and to take the median of the timestamps from that commit. This would add significant overhead to the light client (an extra commit to validate for every block!). If the time is incorrect, in indicates that the validators are explicitly violating the protocol rules in a detectable way which full nodes should detect in the first place and shouldn't forward to light clients, so there would probably be bigger issues at foot.\\nThere are likely a few other instances of things the light client is not validating that it in theory could but likely indicate some larger problem afoot that the client can't do anything about anyways. Hence we really only focus on the correctness of commits and validator sets and detecting forks!\\n","tokens":386,"id":4317,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0003-integration-tests-and-ports.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are two ways I can currently see of doing this, one is building stubs into bundles, the other is serialization of port functions\\nin the test:\\n```js\\nasync supplyPorts(ports = {}) {\\nthis._page = await this.page();\\nawait this._page.evaluate(fun => {\\ncore.queryWith(() => eval(fun)());\\ncore.onSaved(e => console.log(`[ACTION.SAVED]`));\\nwindow.console.log(`Reset hacker news port and added action listener`);\\ncore.news();\\n}, ports.top.toString());\\n}\\n```\\nNote the `ports.top.toString()`, this is serializing a function.\\nThis still leaves us disconnected, though -- we can't then use it as a mock because it does not share memory with the test.\\nThe function has been serialized and sent across the wire.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":182,"id":4320,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0012-can-t-use-esbuild.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHad a go with `esbuild`, and it *is* very fast, but we can't use yet due to runtime failures in the browser.\\n> 'Dynamic require of \"http\" is not supported'\\nWhich stems from our usage of `rss-parser` in `src\/adapters\/rss\/feed.ts`.\\n### Tips\\nOur current `esbuild` fails at runtime, but we alse needed to resolve a couple of problems to get the build itself to work.\\nWe ended up with:\\n```\\n.\/node_modules\/.bin\/esbuild .\/src\/adapters\/application\/real-application.js\\\\n--external:http --external:https --external:timers\\\\n--bundle --global-name=real --outfile=.\/src\/adapters\/web\/gui\/assets\/dist\/real.bundle.js\\n```\\nWhich resolves the following issues.\\n#### How to allow node libraries like `http`\\nSolution is to use `--external`.\\nWithout `--external:http --external:https --external:timers` you get these errors:\\n```\\n> node_modules\/rss-parser\/lib\/parser.js:2:21: error: Could not resolve \"http\" (use \"--platform=node\" when building for node)\\n2 \u2502 const http = require('http');\\n\u2575                      ~~~~~~\\n> node_modules\/rss-parser\/lib\/parser.js:3:22: error: Could not resolve \"https\" (use \"--platform=node\" when building for node)\\n3 \u2502 const https = require('https');\\n\u2575                       ~~~~~~~\\n> node_modules\/xml2js\/lib\/parser.js:17:25: error: Could not resolve \"timers\" (use \"--platform=node\" when building for node)\\n17 \u2502   setImmediate = require('timers').setImmediate;\\n\u2575                          ~~~~~~~~\\n```\\n#### How to make `real.*` to work\\nFor example, in the html page we do this:\\n```js\\nvar toggles = new real.QueryStringToggles(document.location.search);\\n```\\nTo make `real.QueryStringToggles` available, use [`--global-name`](https:\/\/esbuild.github.io\/api\/#global-name).\\n```\\n--global-name=real\\n```\\nThis is like webpack's `library` field.\\n","Decision":"wait for later version.\\n","tokens":479,"id":4321,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0010-polymer-and-web-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur `polymer.js` implementation is read-only.\\nhttp:\/\/localhost:8080\/home.html?use-polymer\\nThe `lit-html` tepmplating we're using does not support data binding, and I can't understand `lit-element`, which wants to create [custom web components](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/Web_Components).\\nI tried it briefly but could not get a data feed in.\\n","Decision":"Leave this one read only -- it does show the bookmarks list at least.\\n","tokens":92,"id":4322,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0006-a-state-requirement.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0008-objects-function-refs-and-this.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIf you make a log like this:\\n```js\\nclass ConsoleLog {\\nconstructor(opts = {}) {\\nthis._opts = { allowTrace: false, ...opts };\\n};\\ninfo(message) {\\nconsole.log(message);\\n}\\ntrace(message) {\\nconsole.log(`ths: ${this}`);\\nif (this._opts.allowTrace) {\\nconsole.log(message);\\n}\\n}\\n}\\n```\\nand then pass a reference to `trace` to another function, `trace` fails because `this` is undefined.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":121,"id":4324,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0009-on-testing-with-events.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have had some flaky tests that are due to timing of events.\\nBecause the initial render is non-deterministic, firing test events may give the wrong results.\\n```js\\n\/\/ test\/integration\/web\/ui\/bookmarks-examples.js\\ndescribe(\"list items are added in response to 'bookmark-added' notification\", () => {\\nlet interactor, page, consoleMessages = null;\\nconst itemsSelector = 'div#application div#bookmarks .items';\\nbefore(async ()  =>\\n{\\ninteractor      = new WebInteractor({ headless: true });\\npage            = await interactor.page();\\nconsoleMessages = new ConsoleListener(page);\\n});\\nbeforeEach(async () => {\\nawait page.goto(`${baseUrl}?unplug=1&${feature}`, { waitUntil: 'domcontentloaded' });\\n});\\nafter(async () => await interactor.close());\\nit('for example', async () => {\\nawait page.once('load', async () => {\\nawait page.evaluate(() => {\\napplication.notify(\\n'bookmark-added',\\n{ id: 'id-1337', title: 'Title 1', url: 'http:\/\/abc\/def', source: '' }\\n);\\n});\\n});\\nawait page.waitForSelector(`${itemsSelector} li`);\\nconst listIds = await page.$$eval(`${itemsSelector} li`, items => items.map(it => ( it.id )));\\nexpect(listIds).to.eql([ 'bookmark-id-1337' ]);\\n});\\n});\\n```\\nHere if the `bookmark-added` notification is sent before the page has loaded, then the test will fail because\\n`div#application div#bookmarks .items` would have been overwritten.\\n","Decision":"Now trying introducing the idea that there is something in the html page called `view` that monitors load events:\\n```js\\nclass UIEvents {\\nconstructor(opts = { }) {\\nopts = { idlePeriod: 200, ...opts }\\nthis._idlePeriod = opts.idlePeriod;\\nthis._lastRender = this.now();\\n}\\nnotifyRendering() {\\nthis._lastRender = this.now();\\n}\\nasync waitUntilIdle(opts = { }) {\\nopts = { timeout: 500, ...opts };\\nconst startTime = new Date();\\nconst timedOut = () => new Date() - startTime >= opts.timeout;\\nconst delay = ms => new Promise(res => setTimeout(res, ms));\\nwhile(false == this.isIdle()) {\\nif (timedOut())\\nthrow new Error(`Timed out after <${opts.timeout}ms>`);\\nawait delay(10);\\n}\\n}\\nisIdle() {\\nreturn (this.now() - this._lastRender) > this._idlePeriod;\\n}\\nnow() { return new Date(); }\\n}\\nmodule.exports = { UIEvents }\\n```\\nThis *does* require views to add notifications like:\\n```js\\nimport { afterUpdate } from 'svelte';\\nafterUpdate(() => {\\nwindow.view.notifyRendering();\\n});\\n```\\n@todo: I guess we should add tests for that: make sure each view type raise these.\\n### Attempt 1\\nTried using [`page.once`](https:\/\/github.com\/puppeteer\/puppeteer\/blob\/main\/src\/common\/Page.ts), which is supposed to be a callback for pad load.\\nIt did work for a start but only by fluke I think.\\n### Decided against\\n* forcing views to implement a finished notification\\n* changing `Application` to notify when finished\\n","tokens":377,"id":4325,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0004-application-use-cases-ports-and-mocking.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSkewering the UI requires a different kind of isolation to the rest of the application.\\nThis is what we have in the integration tests:\\n```js\\nconst { MockLobsters }       = require('..\/..\/..\/..\/test\/support\/mock-lobsters');\\nconst { Application, Ports } = require('..\/..\/..\/core\/application');\\nmodule.exports.application = () => {\\nreturn new Application(new Ports(new MockLobsters()));\\n};\\n```\\nThis is *not* what we want. It has set up the application with its ports stubbed.\\nWe want to isolate the web view part entirely from the applications port requirements.\\nWe want to be able to make this kind of statement:\\n```\\nWhen the UI executes some action it calls this use case\\n```\\nWe don't want the view to know *anything at all* about the application's dependencies.\\nIt's only when unit testing `Application` that we want to know about `Ports`.\\nThe difficulty is arranging both applications so that they have the same shape. Prior to making the last change, we had both\\nsets of tests passing but the application was no plugged in correctly.\\n","Decision":"Keep a separate mock application apart from the real one for a while.\\n","tokens":245,"id":4326,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0002-displaying-lobste-rs-news.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4327,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0013-visualising-layout.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTrying out `https:\/\/github.com\/githubocto\/repo-visualizer` command line version.\\n<img src=\".\/assets\/0013\/news.svg\" \/>\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":36,"id":4328,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0007-svelte-and-events.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSvelte does support our existing events:\\n```html\\n<script>\\napplication.on('rnz-news-item-deleted', e => {\\nconsole.log(e);\\n});\\n<\/script>\\n```\\nThe tricky part is going to be editing the list to remove the item.\\nIn the interim we can use the identical handlers to vanilla as it just works:\\n```js\\napplication.on('rnz-news-item-deleted', e => {\\nconst list = document.querySelector(`div#rnzNews .items`);\\nconst elementId = `news-${e.id}`;\\nconst selector = `div#rnzNews li#${elementId}`;\\nconst listItem = document.querySelector(selector);\\ntry {\\nlist.removeChild(listItem);\\nconsole.log(`Item deleted <${e.id}>`);\\n} catch (error) {\\nconsole.error(\\n`Failed to delete <${e.id}> with selector <${selector}>` +\\n`The error is: ${error}`);\\n}\\n});\\n```\\nWe could deduplicate these I guess.\\n","Decision":"Look at [bindings](https:\/\/svelte.dev\/docs#bind_group) perhaps as a better way of adding list items. We have\\n","tokens":227,"id":4329,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"news\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4331,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/006-kata-dependencies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nKnowledge to solve a certain kata depends on the knowledge one should have gathered from other katas.\\nWithin one kata-bundle (e.g. \"es6\/language\") this is well defined and can be done already.\\nHow can this be done across all katas in this repo?\\nE.g. a ES10 kata might require knowledge of something learned in an ES6 kata. To be able to inform the\\nuser about such a learning path the katas should be able to reflect their dependencies throughout the\\nentire repo.\\n","Decision":"See ES1 kata, where this was done. All katas export the available katas like so:\\n```js\\nconst buildReferenceForId = id => ({bundle: 'es1\/language', id});\\nexport const es1 = {\\n'ARRAY_SORT_BASICS': buildReferenceForId(1),\\n'ARRAY_SORT_WITH_FUNCTION': buildReferenceForId(2),\\n'GLOBAL_PARSEINT': buildReferenceForId(3),\\n\/\/ ...\\n};\\n```\\nInside the ES6 katas any kata can be refered to by importing and refering to this exact kata.\\n```js\\n\/\/ inside es6\/language\/__raw-metadata__.js\\nimport {es1} from '..\/..\/es1\/language\/__raw-metadata__.js';\\nconst requiresKnowledgeFrom = [es1.GLOBAL_PARSEINT];\\n```\\n","tokens":112,"id":4332,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/000-use-adrs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/001-formatting-katas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI come back to the katas and my formatting style I currently use has\\nchanged, but it makes no sense to adapt all katas in here every time.\\nThese katas are a long-term thingy, they live for a long time, so this\\nshould be defined and settled once and for all.\\n","Decision":"A kata looks like the following:\\n```\\ndescribe('`Reflect` basics', function() {     \/\/ The test description is a proper English sentence, so it starts upper case.\\ndescribe('Reflect is special, it is different to e.g. `Object`', function() {\\nit('it`s of type object', function() {\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n});\\n});\\n});\\n\/\/ the file ends with an empty line\\n```\\nThere are no unnecessary spaces and new lines, also not before the assert.\\nIn some rare cases where a new line really helps readability it's ok, but it should\\nbe the exception.\\nAll lines should be as short as possible.\\nOverall, most important be consistent!\\n","tokens":68,"id":4334,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/004-in-depth-katas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI (Wolfram) am writing most of the katas and slowly I see a pattern arising in how I write them and also of which depth\\nI want them to be. Lately some things made me think about in-depth versions of katas.\\nOne was writing the ES10 `Object.fromEntries()` kata. Writing this kata took me more than a week, every evening\\nfiguring out certain details of the spec and how to transport those into a kata. This made me realize that\\nthis kata has become too complex to get just a feeling for how `Object.fromEntries()` works, and it also made\\nme realize that I want this depth because I don't want to write any kata that states wrong things.\\nBut this kata has simply become too big and too detailed.\\nDiscussing this at #jscc19 and also at the #jslang meetup, I realized that people would appreciate a simple\\nintroductory kata, which allows one to learn the feature (superficically) and have an in-depth or expert kata,\\nwhich covers all specifics that might be described in the spec.\\n","Decision":"Katas might come in two flavours, the simple kata and the in-depth kata.\\nIn case of the above described `Object.fromEntries()` kata these two katas will be:\\n- stored in `es10\/language\/object-api\/fromEntries.js` and `es10\/language\/object-api\/fromEntries-in-depth.js`\\n- they are two separate katas, with separate metadata, etc.\\nThe simple kata serves the purpose of understanding and being able to use the functionality\/API.\\nThe in-depth kata covers things like how does this functionality behave with edge cases, type coercion, using\\nthe function directly from it's prototype and alike things.\\n","tokens":230,"id":4335,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/002-no-eslint.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/005-kata-naming.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe more katas come to life and the more overlap between es6, es7, es8, etc. versions exist the more a structure is\\nneeded on how to allow grouping the katas in any other form.\\nOne example. ES6 has katas in the group \"Array API\", \"Object API\", \"String API\", etc. ES1, ES7, ES10 and other also\\nhave katas that have the same group names and that is logically correct.\\nIn order to make these easy to use and allow grouping, for example all Array-API-katas into one group and present\\nthem on the website (or somewhere else), the groups must be used consistently.\\n","Decision":"Many katas can be put into the following X groups and this is done across the board (not only for language katas,\\nalso for libraries, etc.) in order to have a coherent structure and make it easier for the user's to understand\\nwhere to look for katas.\\n### The \"Xxx API\" katas\\nThe ES6 Promise katas are a very good example (not perfect at this point in time, but very good). The standards\\nare like this:\\n1. The group name is \"Xxx API\", for example \"Promise API\", \"String API\", ...\\n1. The directory name is `...\/xxx-api\/`, for example `es6\/language\/array-api`\\n1. The group contains API katas, such as the `Xxx.method()`, `Xxx.property` katas\\n1. Two additional katas should be provided:\\n1. The \"Basics\" kata, this kata describes basic knowledge that is needed for understanding this topic,\\nfor example promises need some explaination, see the [ES6 Promise \"Basics\" kata][1], which does that very well.\\n1. \"The API\" kata gives a quick overview of the API to understand what can be done with it. Just pick the\\nmost common use case for every method\/property and show it briefly, the explicit kata like \"Promise.all()\" will\\ncover the feature more detailed, and if needed there might even be an [in-depth kata][2] for it.\\nkatas for good examples\\n### Syntax katas\\nKatas that help to learn syntax of JavaScript are in this group.\\nThese are for example \"Block scope\", \"Template string\", \"Rest operator\", \"Destructuring\" and others.\\nTBD: should they adhere to a naming scheme?\\n### TBD: Other katas???\\nAre there other kata types? If so, must they be described or even standardized?\\n","tokens":144,"id":4337,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"katas\/003-katafication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"imageclass-uplift\/0002-use-the-design-system.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe uplift front-end work should be based on the [U-M Library Design System](https:\/\/design-system.lib.umich.edu\/):\\n* The Digital Collections Platform long-term goal is that \"DLXS collections\" should blend with the Library (unless contrained by stakeholder requirements, e.g. the Bentley Library).\\n* Building templates against the Design System today should mean more of these templates can be re-purposed for however the DCP evolves.\\n","Decision":"Use the Design System components and CSS to build out the uplift front-end.\\nSet a milestone to evaluate which (if any) uplift patterns can be extracted up to the Design System.\\n","tokens":96,"id":4339,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"imageclass-uplift\/0005-configuring-dlxs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDLXS has machinery to assemble an XSLT template via a path fallback mechanism.\\nBecause the uplift will not be immediately applied across all collections, DLXS will need to be extended to support an alternative assembly process.\\n","Decision":"Update DLXS:\\n* Configure a `collmgr` setting to flag a collection as being \"uplifted\"\\n* If detected, the fallback machinery will look in an uplift-specific path fallback to avoid any existing XSLT templates\\n","tokens":48,"id":4340,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"imageclass-uplift\/0004-transformation-pipeline.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe timeline for how long the uplift will be in operation is vague and undetermined.\\n* DLXS has a concept of collection-specific templates to extend\/modify the default transformations\\n* It is likely that DCC staff will need to continue creating local templates in the future\\n* Some of the proposed Design System technologies can be verbose; updating component definitions across collections would be tedious\\n","Decision":"We will adopt a **transformation pipeline**.\\n* DLXS XML will be transformed into **UI XML**, which will write to pre-defined component \"blocks\"\\n* The UI XML will then be transformed into **Design System HTML**, which is served to the client\\nDCC staff will _ideally_ work against the UI XML.\\nUpdates to the Design System transformation can be centrally managed.\\n","tokens":79,"id":4341,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"imageclass-uplift\/0003-develop-locally.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDeveloping in DLXS can be tedious, especially experiments:\\n* DLXS uses CVS for version control (no easy branches!)\\n* the CVS repository is *everything* (no easy branches!)\\n* Mostly developers work against production data\\n* Configuring modern front-end tooling will be its own project\\nBut:\\n* DLXS can trivially generate the XML that is processed as source by the page templates\\n","Decision":"The uplift front-end will be developed locally using modern front-end tooling. Experiments can be deployed to Netlify (etc) for sharing\/comments.\\n","tokens":90,"id":4342,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"imageclass-uplift\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4343,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0010-vrf-elections.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhile functional, the current PVSS-based random beacon is neither all that\\nperformant, nor all that scalable.  To address both concerns, this ADR\\nproposes transitioning the election procedure to one that is based on\\ncryptographic sortition of Verifiable Random Function (VRF) outputs.\\n","Decision":"### Cryptographic Primitives\\nLet the VRF to be used across the system be ECVRF-EDWARDS25519-SHA512-ELL2\\nfrom the [Verifiable Random Functions (VRFs) draft (v10)][1], with the\\nfollowing additions and extra clarifications:\\n- All public keys MUST be validated via the \"ECVRF Validate Key\" procedure\\nas specified in section 5.6.1 (Small order public keys MUST be\\nrejected).\\n- The string_to_point routine MUST reject non-canonically encoded points\\nas specified in RFC 8032.  Many ed25519 implementations are lax about\\nenforcing this when decoding.\\n- When decoding s in the ECVRF_verify routine, the s scalar MUST fall\\nwithin the range 0 <= i < L.  This change will make proofs\\nnon-malleable.  Note that this check is unneeded for the c scalar\\nas it is 128-bits, and thus will always lie within the valid range.\\nThis check was not present in the IETF draft prior to version 10.\\n- Implementations MAY choose to incorporate additional randomness into\\nthe ECVRF_nonce_generation_RFC8032 function.  Note that proofs (pi_string)\\nare not guaranteed to be unique or deterministic even without this\\nextension (the signer can use any arbitrary value for the nonce and\\nproduce a valid proof, without altering beta_string).\\nLet the tuple oriented cryptographic hash function be TupleHash256 from\\n[NIST SP 800-185][2].\\n### Node Descriptor Changes\\nThe node descriptor of each node will be extended to include the following\\ndatastructure.\\n```golang\\ntype Node struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ VRF is the public key used by the node to generate VRF proofs.\\nVRF *VRFInfo `json:\"vrf,omitempty\"`\\n}\\ntype VRFInfo struct {\\n\/\/ ID is the unique identifier of the node used to generate VRF proofs.\\nID signature.PublicKey `json:\"id\"`\\n}\\n```\\nThe VRF public key shall be a long-term Ed25519 public key, that is distinct\\nfrom every other key used by the node.  The key MUST not be small order.\\nThe existing `Beacon` member of the node descriptor is considered deprecated\\nand will first be ignored by the consensus layer, and then removed in a\\nsubsequent version following a transitionary period.\\n### Consensus Parameters\\nThe scheduler module will have the following additional consensus parameters\\nthat control behavior.\\n```golang\\ntype ConsensusParameters struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ VRFParameters is the paramenters for the VRF-based cryptographic\\n\/\/ sortition based election system.\\nVRFParameters *VRFParameters `json:\"vrf_params\"`\\n}\\n\/\/ VRFParameters are the VRF scheduler parameters.\\ntype VRFParameters struct {\\n\/\/ AlphaHighQualityThreshold is the minimum number of proofs (Pi)\\n\/\/ that must be received for the next input (Alpha) to be considered\\n\/\/ high quality.  If the VRF input is not high quality, runtimes will\\n\/\/ be disabled for the next epoch.\\nAlphaHighQualityThreshold uint64 `json:\"alpha_hq_threshold,omitempty\"`\\n\/\/ Interval is the epoch interval (in blocks).\\nInterval int64 `json:\"interval,omitempty\"`\\n\/\/ ProofSubmissionDelay is the wait peroid in blocks after an epoch\\n\/\/ transition that nodes MUST wait before attempting to submit a\\n\/\/ VRF proof for the next epoch's elections.\\nProofSubmissionDelay int64 `json:\"proof_delay,omitempty\"`\\n\/\/ PrevState is the VRF state from the previous epoch, for the\\n\/\/ current epoch's elections.\\nPrevState *PrevVRFState `json:\"prev_state,omitempty\"`\\n}\\n\/\/ PrevVRFState is the previous epoch's VRF state that is to be used for\\n\/\/ elections.\\ntype PrevVRFState struct {\\n\/\/ Pi is the accumulated pi_string (VRF proof) outputs for the\\n\/\/ previous epoch.\\nPi map[signature.PublicKey]*signature.Proof `json:\"pi.omitempty\"`\\n\/\/ CanElectCommittees is true iff the previous alpha was generated\\n\/\/ from high quality input such that committee elections are possible.\\nCanElectCommittees bool `json:\"can_elect,omitempty\"`\\n}\\n```\\n### Consensus State, Events, and Transactions\\nThe scheduler component will maintain and make available the following additonal\\nconsensus state.\\n```golang\\n\/\/ VRFState is the VRF scheduler state.\\ntype VRFState struct {\\n\/\/ Epoch is the epoch for which this alpha is valid.\\nEpoch EpochTime `json:\"epoch\"`\\n\/\/ Alpha is the active VRF alpha_string input.\\nAlpha []byte `json:\"alpha\"`\\n\/\/ Pi is the accumulated pi_string (VRF proof) outputs.\\nPi map[signature.PublicKey]*signature.Proof `json:\"pi,omitempty\"`\\n\/\/ AlphaIsHighQuality is true iff the alpha was generated from\\n\/\/ high quality input such that elections will be possible.\\nAlphaIsHighQuality bool `json:\"alpha_hq\"`\\n\/\/ SubmitAfter is the block height after which nodes may submit\\n\/\/ VRF proofs for the current epoch.\\nSubmitAfter int64 `json:\"submit_after\"`\\n}\\n```\\nImplementations MAY cache the beta_string values that are generated from valid\\npi_strings for performance reasons, however as this is trivial to recalculate,\\nit does not need to be explicitly exposed.\\nUpon epoch transition, the scheduler will emit the following event.\\n```golang\\n\/\/ VRFEvent is the VRF scheduler event.\\ntype VRFEvent struct {\\n\/\/ Epoch is the epoch that Alpha is valid for.\\nEpoch EpochTime `json:\"epoch,omitempty\"`\\n\/\/ Alpha is the active VRF alpha_string input.\\nAlpha []byte `json:\"alpha,omitempty\"`\\n\/\/ SubmitAfter is the block height after which nodes may submit\\n\/\/ VRF proofs for the current epoch.\\nSubmitAfter int64 `json:\"submit_after\"`\\n}\\n```\\n```golang\\ntype VRFProve struct {\\n\/\/ Epoch is the epoch that this VRF proof is for.\\nEpoch epochtime.EpochTime `json:\"epoch\"`\\n\/\/ Pi is the VRF proof for the current epoch.\\nPi     []byte              `json:\"pi\"`\\n}\\n```\\n### VRF Operation\\nFor the genesis epoch, let the VRF alpha_string input be derived as:\\n`TupleHash256((chain_context, I2OSP(epoch,8)), 256, \"oasis-core:vrf\/alpha\")`\\nFor every subsequent epoch, let alpha_string be derived as:\\n<!-- markdownlint-disable line-length -->\\n`TupleHash256((chain_context, I2OSP(epoch, 8), beta_0, ... beta_n), 256, \"oasis-core:vrf\/alpha\")`\\nwhere beta_0 through beta_n are the beta_string outputs gathered from\\nall valid pi_strings submitted during the previous epoch (after the\\non-transition culling is complete), in ascending lexographic order by\\nVRF key.  If the number of beta values incorporated into the TupleHash\\ncomputation is greater than or equal to AlphaHighQuality threshold,\\nthe alpha is considered \"strong\", and committee elections are allowed\\nbased on the proofs generated with this alpha.  If the alpha value is\\nweak (insufficient nodes submitted proofs), only validator elections\\nare allowed.\\nUpon receiving a VRFEvent, all eligible nodes MUST wait a minimum of\\nProofSubmissionDelay blocks, and then submit a VRFProve transaction,\\nwith the Proof field set to the output of\\n`ECVRF_prove(VRFKey_private, alpha_string)`.\\nUpon receiving a VRFProve transaction, the scheduler does the following:\\n1. Rejects the transaction if less than ProofSubmissionDelay blocks\\nhave elapsed since the transition into the current epoch.\\n2. Checks to see if the node tentatively eligible to be included in\\nthe next election according to the following criteria:\\n- Not frozen.\\n- Has registered the VRF.ID used to generate the proof prior\\nto the transition into the current epoch (May slash).\\n- Has not already submitted a proof for the current epoch\\n(May slash if proof is different).\\n3. Validates the proof, and if valid, stores the VRF.ID + pi_string\\nin the consensus state.\\n### VRF Committee Elections\\nThe following changes are made to the committee election process.\\nOn epoch transition, as long as the alpha used to generate the proofs\\nis considered strong re-validate node eligibility for all nodes that\\nsubmitted a VRF proof (Not frozen, VRF.ID has not changed), and cull\\nproofs from nodes that are now ineligible.\\nIf the alpha value is considered weak, no commitee elections are allowed.\\nFor each committee:\\n1. Filter the node list based on the current stake\/eligibility criteria,\\nand additionally filter out nodes that have not submitted a valid\\nVRF proof.\\n2. For each eligible (node, commitee kind, committe role) tuple, derive\\na sortition string as:\\n<!-- markdownlint-disable line-length code-block-style ol-prefix -->\\n`s_n = TupleHash256((chain_context, I2OSP(epoch, 8), runtime_id, I2OSP(kind, 1), I2OSP(role, 1), beta_n), 256, \"oasis-core:vrf\/committee\")`\\n3. Sort s_0 ... s_n in ascending lexographical order.\\n4. Select the requisite nodes that produced the sortition strings\\nstarting from the head of the sorted list as the committee.\\nCommittee elections MUST be skipped for the genesis and subsequent epoch,\\nas the genesis epoch has no VRF proofs, and proofs submitted during the\\ngenesis epoch are based on the bootstrap alpha_string.\\n### VRF Validator Elections\\nThe only place where the beacon is currently used in the validator selection\\nprocess is to pick a single node out of multiple eligible nodes controlled by\\nthe same entity to become a validator.\\nWhen this situation occurs the validator is selected as follows:\\n1. For all validator-eligible nodes controlled by the given entity,\\nderive a sortition string as:\\n<!-- markdownlint-disable line-length code-block-style ol-prefix -->\\n`s_n = TupleHash256((chain_context, I2OSP(epoch, 8), beta_n), 256, \"oasis-core:vrf\/validator\")`\\n2. Sort s_0 ... s_n, in ascending lexographic order.\\n3. Select the node that produced the 0th sortition string in the sorted\\nlist as the validator.\\nThis is safe to do with beta values generated via the bootstrap alpha string\\nas it is up to the entity running the nodes in question as to which ones\\nare a validator anyway.\\nAs a concession for the transition process, if the number of validators\\nthat submit proofs is less than the minimum number of validators configured\\nin the scheduler, validator tie-breaks (and only validator tie-breaks)\\nwill be done by permuting the node list (as in the current PVSS beacon),\\nusing entropy from the block hash.\\nAs nodes are required to submit a VRF public key as part of non-genesis\\nregistrations, and each node will attempt to submit a VRF proof, this\\nbackward compatibility hack should only be triggered on the genesis\\nepoch, and can be removed on the next major upgrade.\\n### Timekeeping Changes\\nTimekeeping will go back to a fixed-interval epoch transition mechanism, with\\nall of the beacon related facilities removed.  As this is primarily a module\\nrename and code removal, the exact details are left unspecified.\\n","tokens":66,"id":4344,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0002-go-modules-compatible-git-tags.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProjects that depend on [Oasis Core's Go module], i.e.\\n`github.com\/oasisprotocol\/oasis-core\/go`, need a way to depend on its particular\\nversion.\\nGo Modules only allow [Semantic Versioning 2.0.0] for\\n[versioning of the modules][go-mod-ver] which makes it hard to work\\nwith [Oasis Core's CalVer (calendar versioning) scheme].\\nThe currently used scheme for Go Modules compatible Git tags is:\\n```\\ngo\/v0.YY.MINOR[.MICRO]\\n```\\nwhere:\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\n- `MINOR` represents the minor version starting with zero (e.g. `0`, `1`, `2`,\\n`3`, ...),\\n- `MICRO` represents the final number in the version (sometimes referred to as\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\nIf the `MICRO` version is `0`, it is omitted.\\nIt turns out this only works for Oasis Core versions with the `MICRO` version\\nof `0` since the Go Modules compatible Git tag omits the `.MICRO` part and is\\nthus compatible with [Go Modules versioning requirements][go-mod-ver].\\n[Oasis Core's Go module]:\\nhttps:\/\/pkg.go.dev\/mod\/github.com\/oasisprotocol\/oasis-core\/go\\n[Semantic Versioning 2.0.0]:\\nhttps:\/\/semver.org\/spec\/v2.0.0.html\\n[go-mod-ver]:\\nhttps:\/\/golang.org\/ref\/mod#versions\\n[Oasis Core's CalVer (calendar versioning) scheme]: ..\/versioning.md\\n","Decision":"The proposed design is to tag Oasis Core releases with the following Go Modules\\ncompatible Git tags (in addition to the ordinary Git tags):\\n```\\ngo\/v0.YY0MINOR.MICRO\\n```\\nwhere:\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\n- `0MINOR` represents the zero-padded minor version starting with zero (e.g.\\n`00`, `01`, `02`, ..., `10`, `11`, ...),\\n- `MICRO` represents the final number in the version (sometimes referred to as\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\nHere are some examples of how the ordinary and the corresponding Go Modules\\ncompatible Git tags would look like:\\n| Version       | Ordinary Git tag | Go Modules compatible Git tag  |\\n|:-------------:|:----------------:|:------------------------------:|\\n| 20.9          | `v20.9`          | `go\/v0.2009.0`                 |\\n| 20.9.1        | `v20.9.1`        | `go\/v0.2009.1`                 |\\n| 20.9.2        | `v20.9.2`        | `go\/v0.2009.2`                 |\\n| 20.10         | `v20.10`         | `go\/v0.2010.0`                 |\\n| 20.10.1       | `v20.10.1`       | `go\/v0.2010.1`                 |\\n| 20.10.2       | `v20.10.2`       | `go\/v0.2010.2`                 |\\n| ...           | ...              | ...                            |\\n| 21.0          | `v21.0`          | `go\/v0.2100.0`                 |\\n| 21.0.1        | `v21.0.1`        | `go\/v0.2100.1`                 |\\n| 21.0.2        | `v21.0.2`        | `go\/v0.2100.2`                 |\\n| 21.1          | `v21.1`          | `go\/v0.2101.0`                 |\\n| 21.1.1        | `v21.1.1`        | `go\/v0.2101.1`                 |\\n| 21.1.2        | `v21.1.2`        | `go\/v0.2101.2`                 |\\n| ...           | ...              | ...                            |\\nUsing such a scheme makes the version of the Oasis Core Go module fully\\ncompatible with the [Go Modules versioning requirements][go-mod-ver] and thus\\nenables users to use the familiar Go tools to check for new module versions,\\ni.e. `go list -m -u all`, or to obtain and require a module, i.e.\\n`go get github.com\/oasisprotocol\/oasis-core\/go@latest`.\\n","tokens":390,"id":4345,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0006-consensus-governance.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently the consensus layer does not contain any on-chain governance\\nmechanism so any network upgrades need to be carefully coordinated off-chain.\\nAn on-chain governance mechanism would allow upgrades to be handled in a more\\ncontrolled (and automatable) manner without introducing the risk of corrupting\\nstate.\\n","Decision":"This proposal introduces a minimal on-chain governance mechanism where anyone\\ncan submit governance proposals and the validators can vote where one base unit\\nof delegated stake counts as one vote.\\nThe high-level overview is as follows:\\n- **A new governance API** is added to the consensus layer and its Tendermint\\nbased implementation. It supports transactions for submitting proposals and\\nvoting on proposals. It supports queries for listing current proposals and\\nvotes for any given proposal.\\n- **Two governance proposal kinds are supported**, a consensus layer upgrade\\nproposal (where the content is basically the existing upgrade descriptor) and\\nthe cancellation of a pending upgrade.\\nA proposal is created through a _submit proposal_ transaction and requires a\\nminimum deposit (which is later refunded in case the proposal passes). Once a\\nproposal is successfully submitted the voting period starts. Entities that are\\npart of the validator set may cast votes for the proposal. After the voting\\nperiod completes, the votes are tallied and the proposal either passes or is\\nrejected.\\nIn case the proposal passes, the actions specified in the content of the propsal\\nare executed. Currently the only actions are scheduling of an upgrade by\\npublishing an upgrade descriptor or cancelling a previously passed upgrade.\\n### State\\n#### Staking\\nThis proposal adds the following consensus layer state in the staking module:\\n- **Governance deposits account balance (`0x59`)**, similar to the common pool.\\n#### Governance\\nThis proposal adds the following consensus layer state in the governance module:\\n- **Next proposal identifier (`0x80`)**\\nThe next proposal identifier is stored as a CBOR-serialized `uint64`.\\n- **List of proposals (`0x81`)**\\nEach proposal is stored under a separate storage key with the following key\\nformat:\\n```\\n0x81 <proposal-id (uint64)>\\n```\\nAnd CBOR-serialized value:\\n```golang\\n\/\/ ProposalState is the state of the proposal.\\ntype ProposalState uint8\\nconst (\\nStateActive   ProposalState = 1\\nStatePassed   ProposalState = 2\\nStateRejected ProposalState = 3\\nStateFailed   ProposalState = 4\\n)\\n\/\/ Proposal is a consensus upgrade proposal.\\ntype Proposal struct {\\n\/\/ ID is the unique identifier of the proposal.\\nID uint64 `json:\"id\"`\\n\/\/ Submitter is the address of the proposal submitter.\\nSubmitter staking.Address `json:\"submitter\"`\\n\/\/ State is the state of the proposal.\\nState ProposalState `json:\"state\"`\\n\/\/ Deposit is the deposit attached to the proposal.\\nDeposit quantity.Quantity `json:\"deposit\"`\\n\/\/ Content is the content of the proposal.\\nContent ProposalContent `json:\"content\"`\\n\/\/ CreatedAt is the epoch at which the proposal was created.\\nCreatedAt beacon.EpochTime `json:\"created_at\"`\\n\/\/ ClosesAt is the epoch at which the proposal will close and votes will\\n\/\/ be tallied.\\nClosesAt beacon.EpochTime `json:\"closes_at\"`\\n\/\/ Results are the final tallied results after the voting period has\\n\/\/ ended.\\nResults map[Vote]quantity.Quantity `json:\"results,omitempty\"`\\n\/\/ InvalidVotes is the number of invalid votes after tallying.\\nInvalidVotes uint64 `json:\"invalid_votes,omitempty\"`\\n}\\n```\\n- **List of active proposals (`0x82`)**\\nEach active proposal (one that has not yet closed) is stored under a separate\\nstorage key with the following key format:\\n```\\n0x82 <closes-at-epoch (uint64)> <proposal-id (uint64)>\\n```\\nThe value is empty as the proposal ID can be inferred from the key.\\n- **List of votes (`0x83`)**\\nEach vote is stored under a separate storage key with the following key\\nformat:\\n```\\n0x83 <proposal-id (uint64)> <voter-address (staking.Address)>\\n```\\nAnd CBOR-serialized value:\\n```golang\\n\/\/ Vote is a governance vote.\\ntype Vote uint8\\nconst (\\nVoteYes     Vote = 1\\nVoteNo      Vote = 2\\nVoteAbstain Vote = 3\\n)\\n```\\n- **List of pending upgrades (`0x84`)**\\nEach pending upgrade is stored under a separate storage key with the following\\nkey format:\\n```\\n0x84 <upgrade-epoch (uint64)> <proposal-id (uint64)>\\n```\\nThe value is empty as the proposal upgrade descriptor can be obtained via\\nproposal that can be inferred from the key.\\n- **Parameters (`0x85`)**\\nGovernance consensus parameters.\\nWith CBOR-serialized value:\\n```golang\\n\/\/ ConsensusParameters are the governance consensus parameters.\\ntype ConsensusParameters struct {\\n\/\/ GasCosts are the governance transaction gas costs.\\nGasCosts transaction.Costs `json:\"gas_costs,omitempty\"`\\n\/\/ MinProposalDeposit is the number of base units that are deposited when\\n\/\/ creating a new proposal.\\nMinProposalDeposit quantity.Quantity `json:\"min_proposal_deposit,omitempty\"`\\n\/\/ VotingPeriod is the number of epochs after which the voting for a proposal\\n\/\/ is closed and the votes are tallied.\\nVotingPeriod beacon.EpochTime `json:\"voting_period,omitempty\"`\\n\/\/ Quorum is he minimum percentage of voting power that needs to be cast on\\n\/\/ a proposal for the result to be valid.\\nQuorum uint8 `json:\"quorum,omitempty\"`\\n\/\/ Threshold is the minimum percentage of VoteYes votes in order for a\\n\/\/ proposal to be accepted.\\nThreshold uint8 `json:\"threshold,omitempty\"`\\n\/\/ UpgradeMinEpochDiff is the minimum number of epochs between the current\\n\/\/ epoch and the proposed upgrade epoch for the upgrade proposal to be valid.\\n\/\/ This is also the minimum number of epochs between two pending upgrades.\\nUpgradeMinEpochDiff beacon.EpochTime `json:\"upgrade_min_epoch_diff,omitempty\"`\\n\/\/ UpgradeCancelMinEpochDiff is the minimum number of epochs between the current\\n\/\/ epoch and the proposed upgrade epoch for the upgrade cancellation proposal to be valid.\\nUpgradeCancelMinEpochDiff beacon.EpochTime `json:\"upgrade_cancel_min_epoch_diff,omitempty\"`\\n}\\n```\\n### Genesis Document\\nThe genesis document needs to be updated to include a `governance` field with\\nany initial state (see [_State_]) and consensus parameters (see [_Consensus\\nParameters_]) for the governance service.\\n[_State_]: #state\\n[_Consensus Parameters_]: #consensus-parameters\\n### Transaction Methods\\nThis proposal adds the following transaction methods in the governance module:\\n#### Submit Proposal\\nProposal submission enables a new consensus layer governance proposal to be\\ncreated.\\n**Method name:**\\n```\\ngovernance.SubmitProposal\\n```\\n**Body:**\\n```golang\\n\/\/ ProposalContent is a consensus layer governance proposal content.\\ntype ProposalContent struct {\\nUpgrade       *UpgradeProposal       `json:\"upgrade,omitempty\"`\\nCancelUpgrade *CancelUpgradeProposal `json:\"cancel_upgrade,omitempty\"`\\n}\\n\/\/ UpgradeProposal is an upgrade proposal.\\ntype UpgradeProposal struct {\\nupgrade.Descriptor\\n}\\n\/\/ CancelUpgradeProposal is an upgrade cancellation proposal.\\ntype CancelUpgradeProposal struct {\\n\/\/ ProposalID is the identifier of the pending upgrade proposal.\\nProposalID uint64 `json:\"proposal_id\"`\\n}\\n```\\n**Fields:**\\n- `upgrade` (optional) specifies an upgrade proposal.\\n- `cancel_upgrade` (optional) specifies an upgrade cancellation proposal.\\nExactly one of the proposal kind fields needs to be non-nil, otherwise the\\nproposal is considered malformed.\\nUpon processing any proposal the following steps are first performed:\\n- The account indicated by the signer is loaded.\\n- If the account balance is less than `min_proposal_deposit`, the method call\\nfails with `ErrInsufficientBalance`.\\nUpon processing an **`UpgradeProposal`** the following steps are then performed:\\n- The upgrade descriptor is checked for basic internal validity. If the check\\nfails, the method call fails with `ErrInvalidArgument`.\\n- The upgrade descriptor's `epoch` field is compared with the current epoch. If\\nthe specified epoch is not at least `upgrade_min_epoch_diff` epochs ahead of\\nthe current epoch, the method call fails with `ErrUpgradeTooSoon`.\\n- The set of pending upgrades is checked to make sure that no upgrades are\\ncurrently pending within `upgrade_min_epoch_diff` epochs of the upgrade\\ndescriptor's `epoch` field. If there is such an existing upgrade pending, the\\nmethod call fails with `ErrUpgradeAlreadyPending`.\\nUpon processing a **`CancelUpgradeProposal`** the following steps are then\\nperformed:\\n- The set of pending upgrades is checked to make sure that the given upgrade\\nproposal is currently pending to be executed. If there is no such upgrade, the\\nmethod call fails with `ErrNoSuchUpgrade`.\\n- The upgrade descriptor's `epoch` field is compared with the current epoch. If\\nthe specified epoch is not at least `upgrade_cancel_min_epoch_diff` epochs\\nahead of the current epoch, the method call fails with `ErrUpgradeTooSoon`.\\nUpon processing any proposal the following steps are then performed:\\n- The `min_proposal_deposit` base units are transferred from the signer's\\naccount to the governance service's _proposal deposit account_.\\n- The signer's account is saved.\\n- A new proposal is created and assigned an identifier.\\n- The corresponding `ProposalSubmittedEvent` is emitted with the following\\nstructure:\\n```golang\\ntype ProposalSubmittedEvent struct {\\n\/\/ ID is the unique identifier of a proposal.\\nID uint64 `json:\"id\"`\\n\/\/ Submitter is the staking account address of the submitter.\\nSubmitter staking.Address `json:\"submitter\"`\\n}\\n```\\n- The corresponding `staking.TransferEvent` is emitted, indicating transfer from\\nthe submitter's account to the _proposal deposit account_.\\n#### Vote\\nVoting for submitted consensus layer governance proposals.\\n**Method name:**\\n```\\ngovernance.CastVote\\n```\\n**Body:**\\n```golang\\ntype ProposalVote struct {\\n\/\/ ID is the unique identifier of a proposal.\\nID uint64 `json:\"id\"`\\n\/\/ Vote is the vote.\\nVote Vote `json:\"vote\"`\\n}\\n```\\nUpon processing a vote the following steps are performed:\\n- The entity descriptor corresponding to the transaction signer is fetched. In\\ncase no such entity exists, the method call fails with `ErrNotEligible`.\\n- It is checked whether any entity's nodes are in the current validator set. In\\ncase they are not, the method call fails with `ErrNotEligible`.\\n- The proposal identified by `id` is loaded. If the proposal does not exist,\\nthe method call fails with `ErrNoSuchProposal`.\\n- If the proposal's state is not `StateActive`, the method call fails with\\n`ErrVotingIsClosed`.\\n- The vote is added to the list of votes. If the vote already exists, it is\\noverwritten.\\n- The corresponding `VoteEvent` is emitted with the following structure:\\n```golang\\ntype VoteEvent struct {\\n\/\/ ID is the unique identifier of a proposal.\\nID uint64 `json:\"id\"`\\n\/\/ Submitter is the staking account address of the submitter.\\nSubmitter staking.Address `json:\"submitter\"`\\n\/\/ Vote is the cast vote.\\nVote Vote `json:\"vote\"`\\n}\\n```\\n### Queries\\nThis proposal introduces the following query methods in the governance module:\\n```golang\\ntype Backend interface {\\n\/\/ ActiveProposals returns a list of all proposals that have not yet closed.\\nActiveProposals(ctx context.Context, height int64) ([]*Proposal, error)\\n\/\/ Proposals returns a list of all proposals.\\nProposals(ctx context.Context, height int64) ([]*Proposal, error)\\n\/\/ Proposal looks up a specific proposal.\\nProposal(ctx context.Context, query *ProposalQuery) (*Proposal, error)\\n\/\/ Votes looks up votes for a specific proposal.\\nVotes(ctx context.Context, query *ProposalQuery) ([]*VoteEntry, error)\\n\/\/ PendingUpgrades returns a list of all pending upgrades.\\nPendingUpgrades(ctx context.Context, height int64) ([]*upgrade.Descriptor, error)\\n\/\/ StateToGenesis returns the genesis state at specified block height.\\nStateToGenesis(ctx context.Context, height int64) (*Genesis, error)\\n\/\/ ConsensusParameters returns the governance consensus parameters.\\nConsensusParameters(ctx context.Context, height int64) (*ConsensusParameters, error)\\n\/\/ GetEvents returns the events at specified block height.\\nGetEvents(ctx context.Context, height int64) ([]*Event, error)\\n\/\/ WatchEvents returns a channel that produces a stream of Events.\\nWatchEvents(ctx context.Context) (<-chan *Event, pubsub.ClosableSubscription, error)\\n}\\n\/\/ ProposalQuery is a proposal query.\\ntype ProposalQuery struct {\\nHeight int64  `json:\"height\"`\\nID     uint64 `json:\"id\"`\\n}\\n\/\/ VoteEntry contains data about a cast vote.\\ntype VoteEntry struct {\\nVoter staking.Address `json:\"voter\"`\\nVote  Vote            `json:\"vote\"`\\n}\\n\/\/ Event signifies a governance event, returned via GetEvents.\\ntype Event struct {\\nHeight int64     `json:\"height,omitempty\"`\\nTxHash hash.Hash `json:\"tx_hash,omitempty\"`\\nProposalSubmitted *ProposalSubmittedEvent `json:\"proposal_submitted,omitempty\"`\\nProposalExecuted  *ProposalExecutedEvent  `json:\"proposal_executed,omitempty\"`\\nProposalFinalized *ProposalFinalizedEvent `json:\"proposal_finalized,omitempty\"`\\nVote              *VoteEvent              `json:\"vote,omitempty\"`\\n}\\n```\\n### Tallying\\nIn `EndBlock` the list of active proposals is checked to see if there was an\\nepoch transition in this block. If there was, the following steps are performed\\nfor each proposal that should be closed at the current epoch:\\n- A mapping of current validator entity addresses to their respective active\\nescrow balances is prepared.\\n- A results mapping from `Vote` to number of votes is initialized in the\\nproposal's `results` field.\\n- Votes from the list of votes for the given proposal are iterated and the\\naddress of each vote is looked up in the prepared entity address mapping. The\\ncorresponding number of votes (on the principle of 1 base unit equals one\\nvote) are added to the results mapping based on the voted option. Any votes\\nthat are not from the current validator set are ignored and the\\n`invalid_votes` field is incremented for each such vote.\\n- In case the percentage of votes relative to the total voting power is less\\nthan `quorum`, the proposal is rejected.\\n- In case the percentage of `VoteYes` votes relative to all valid votes is less\\nthan `threshold`, the proposal is rejected.\\n- Otherwise the proposal is passed.\\n- The proposal's status is changed to either `StatePassed` or `StateRejected`\\nand the proposal is saved.\\n- The proposal is removed from the list of active proposals.\\n- In case the proposal has been passed, the proposal content is executed. If\\nproposal execution fails, the proposal's state is changed to `StateFailed`.\\n- The corresponding `ProposalFinalizedEvent` is emitted with the following\\nstructure:\\n```golang\\ntype ProposalFinalizedEvent struct {\\n\/\/ ID is the unique identifier of a proposal.\\nID uint64 `json:\"id\"`\\n\/\/ State is the new proposal state.\\nState ProposalState `json:\"state\"`\\n}\\n```\\n- In case the proposal has been passed, the deposit is transferred back to the\\nproposal submitter and a corresponding `staking.TransferEvent` is emitted,\\nindicating transfer from the _proposal deposit account_ to the submitter's\\naccount.\\n- In case the proposal has been rejected, the deposit is transferred to the\\ncommon pool and a corresponding `staking.TransferEvent` is emitted,\\nindicating transfer from the _proposal deposit account_ to the common pool\\naccount.\\n### Proposal Content Execution\\nAfter any proposal is successfully executed the corresponding\\n`ProposalExecutedEvent` is emitted with the following structure:\\n```golang\\ntype ProposalExecutedEvent struct {\\n\/\/ ID is the unique identifier of a proposal.\\nID uint64 `json:\"id\"`\\n}\\n```\\n#### Upgrade Proposal\\nThe set of pending upgrades is checked to make sure that no upgrades are\\ncurrently pending within `upgrade_min_epoch_diff` of the upgrade descriptor's\\n`epoch` field. If there is such an existing pending upgrade the upgrade proposal\\nexecution fails.\\nWhen an upgrade proposal is executed, a new entry is added to the list of\\npending upgrades using `epoch` as `<upgrade-epoch>`.\\nOn each epoch transition (as part of `BeginBlock`) it is checked whether a\\npending upgrade is scheduled for that epoch. In case it is and we are not\\nrunning the new version, the consensus layer will panic. Otherwise, the pending\\nupgrade proposal is removed.\\n#### Cancel Upgrade Proposal\\nWhen a cancel upgrade proposal is executed, the proposal identified by\\n`proposal_id` is looked up and removed from the list of pending upgrades. In\\ncase the pending upgrade does not exist anymore, no action is performed.\\n### Consensus Parameters\\nThis proposal introduces the following new consensus parameters in the\\ngovernance module:\\n- `gas_costs` (transaction.Costs) are the governance transaction gas costs.\\n- `min_proposal_deposit` (base units) specifies the number of base units that\\nare deposited when creating a new proposal.\\n- `voting_period` (epochs) specifies the number of epochs after which the voting\\nfor a proposal is closed and the votes are tallied.\\n- `quorum` (uint8: \\[0,100\\]) specifies the minimum percentage of voting power\\nthat needs to be cast on a proposal for the result to be valid.\\n- `threshold` (uint8: \\[0,100\\]) specifies the minimum percentage of `VoteYes`\\nvotes in order for a proposal to be accepted.\\n- `upgrade_min_epoch_diff` (epochs) specifies the minimum number of epochs\\nbetween the current epoch and the proposed upgrade epoch for the upgrade\\nproposal to be valid. Additionally specifies the minimum number of epochs\\nbetween two consecutive pending upgrades.\\n- `upgrade_cancel_min_epoch_diff` (epochs) specifies the minimum number of\\nepochs between the current epoch and the proposed upgrade epoch for the\\nupgrade cancellation proposal to be valid.\\nThe following parameter sanity checks are introduced:\\n- Product of `quorum` and `threshold` must be 2\/3+.\\n- `voting_period` must be less than `upgrade_min_epoch_diff` and\\n`upgrade_cancel_min_epoch_diff`.\\n","tokens":64,"id":4347,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0011-incoming-runtime-messages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere is currently a single mechanism through which the consensus layer and a\\nruntime may interact in a consistent and secure manner. This is the mechanism\\nof runtime messages that can be emitted by runtimes (see [ADR 3]) and allows\\nthe consensus layer to act on a runtime's behalf. This mechanism is currently\\nused for _pulling_ tokens from consensus layer accounts that have previously\\nset proper allowances and for updating the runtime descriptor when the runtime\\ngovernance model (see [ADR 4]) is in effect.\\nThis ADR proposes to implement the reverse mechanism where anyone issuing a\\ntransaction at the consensus layer can queue arbitrary messages for processing\\nby the runtime in its next round.\\n[ADR 3]: 0003-consensus-runtime-token-transfer.md\\n[ADR 4]: 0004-runtime-governance.md\\n","Decision":"On a high level this proposal affects the following components:\\n- A new transaction method `roothash.SubmitMsg` is added to the roothash\\nconsensus service to queue a new message for the specific runtime.\\n- Additional per-runtime state is added to the roothash service containing the\\ncurrently queued messages, sorted by arrival time.\\n- During processing of a round the proposer may propose to pop any number of\\nmessages and process them by pushing them to the runtime, similar as it does\\nfor transaction batches. This is of course subject to discrepancy detection.\\n- The runtime host protocol is updated to allow the host to push arbitrary\\nincoming messages in addition to the transaction batch.\\n- The runtime descriptor is updated to include a field that specifies the\\nmaximum size of the incoming message queue.\\n### Incoming Message\\nEach incoming message is represented as follows:\\n```golang\\ntype IncomingMessage struct {\\n\/\/ ID is the unique identifier of the message.\\nID uint64 `json:\"id\"`\\n\/\/ Caller is the address of the caller authenticated by the consensus layer.\\nCaller staking.Address `json:\"caller\"`\\n\/\/ Tag is an optional tag provided by the caller which is ignored and can be used to match\\n\/\/ processed incoming message events later.\\nTag uint64 `json:\"tag,omitempty\"`\\n\/\/ Fee is the fee sent into the runtime as part of the message being sent.\\n\/\/ The fee is transferred before the message is processed by the runtime.\\nFee quantity.Quantity `json:\"fee,omitempty\"`\\n\/\/ Tokens are any tokens sent into the runtime as part of the message being\\n\/\/ sent. The tokens are transferred before the message is processed by the\\n\/\/ runtime.\\nTokens quantity.Quantity `json:\"tokens,omitempty\"`\\n\/\/ Data is arbitrary runtime-dependent data.\\nData []byte `json:\"data,omitempty\"`\\n}\\n```\\n### Executor Commitments\\nThe compute results header structure is updated to include two fields that\\nspecify the number and hash of incoming messages included in a batch as follows:\\n```golang\\ntype ComputeResultsHeader struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ InMessagesHash is the hash of processed incoming messages.\\nInMessagesHash *hash.Hash `json:\"in_msgs_hash,omitempty\"`\\n\/\/ InMessagesCount is the number of processed incoming messages.\\nInMessagesCount uint32 `json:\"in_msgs_count,omitempty\"`\\n}\\n```\\nWhere the hash of included incoming messages is computed as follows:\\n```golang\\n\/\/ InMessagesHash returns a hash of provided incoming runtime messages.\\nfunc InMessagesHash(msgs []IncomingMessage) (h hash.Hash) {\\nif len(msgs) == 0 {\\n\/\/ Special case if there are no messages.\\nh.Empty()\\nreturn\\n}\\nreturn hash.NewFrom(msgs)\\n}\\n```\\nNote that this also requires the enclave RAK signature (for runtimes requiring\\nthe use of TEEs) to be computed over this updated new header.\\n### Runtime Block Header\\nThe runtime block header is updated to include the `InMessagesHash` field as\\nfollows:\\n```golang\\ntype Header struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ InMessagesHash is the hash of processed incoming messages.\\nInMessagesHash hash.Hash `json:\"in_msgs_hash\"`\\n}\\n```\\n### Runtime Descriptor\\nThis proposal updates the runtime transaction scheduler parameters (stored under\\nthe `txn_scheduler` field of the runtime descriptor) as follows:\\n```golang\\ntype TxnSchedulerParameters struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ MaxInMessages specifies the maximum size of the incoming message queue\\n\/\/ for this runtime.\\nMaxInMessages uint32 `json:\"max_in_messages,omitempty\"`\\n}\\n```\\nIt also updates the runtime staking parameters (stored under the `staking` field\\nof the runtime descriptor) as follows:\\n```golang\\ntype RuntimeStakingParameters struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ MinInMessageFee specifies the minimum fee that the incoming message must\\n\/\/ include for the message to be queued.\\nMinInMessageFee quantity.Quantity `json:\"min_in_msg_fee,omitempty\"`\\n}\\n```\\n### State\\nThis proposal introduces\/updates the following consensus state in the roothash\\nmodule:\\n- **Incoming message queue metadata (`0x28`)**\\nMetadata for the incoming message queue.\\n```\\n0x28 <H(runtime-id) (hash.Hash)>\\n```\\nThe value is the following CBOR-serialized structure:\\n```golang\\ntype IncomingMessageQueue struct {\\n\/\/ Size contains the current size of the queue.\\nSize uint32 `json:\"size,omitempty\"`\\n\/\/ NextSequenceNumber contains the sequence number that should be used for\\n\/\/ the next queued message.\\nNextSequenceNumber uint64 `json:\"next_sequence_number,omitempty\"`\\n}\\n```\\n- **Incoming message queue item (`0x29`)**\\nA queue of incoming messages pending to be delivered to the runtime in the\\nnext round.\\n```\\n0x29 <H(runtime-id) (hash.Hash)> <sequence-no (uint64)>\\n```\\nThe value is a CBOR-serialized `IncomingMessage` structure.\\n### Transaction Methods\\nThis proposal updates the following transaction methods in the roothash module:\\n#### Submit Message\\nThe submit message method allows anyone to submit incoming runtime messages to\\nbe queued for delivery to the given runtime.\\n**Method name:**\\n```\\nroothash.SubmitMsg\\n```\\n**Body:**\\n```golang\\ntype SubmitMsg struct {\\nID     common.Namespace  `json:\"id\"`\\nFee    quantity.Quantity `json:\"fee,omitempty\"`\\nTokens quantity.Quantity `json:\"tokens,omitempty\"`\\nData   []byte            `json:\"data,omitempty\"`\\n}\\n```\\n**Fields:**\\n- `id` specifies the destination runtime's identifier.\\n- `fee` specifies the fee that should be sent into the runtime as part of the\\nmessage being sent. The fee is transferred before the message is processed by\\nthe runtime.\\n- `tokens` specifies any tokens to be sent into the runtime as part of the\\nmessage being sent. The tokens are transferred before the message is processed\\nby the runtime.\\n- `data` arbitrary data to be sent to the runtime for processing.\\nThe transaction signer implicitly specifies the caller. Upon executing the\\nsubmit message method the following actions are performed:\\n- Gas is accounted for (new `submitmsg` gas operation).\\n- The runtime descriptor for runtime `id` is retrieved. If the runtime does not\\nexist or is currently suspended the method fails with `ErrInvalidRuntime`.\\n- The `txn_scheduler.max_in_messages` field in the runtime descriptor is\\nchecked. If it is equal to zero the method fails with\\n`ErrIncomingMessageQueueFull`.\\n- If the value of the `fee` field is smaller than the value of the\\n`staking.min_in_msg_fee` field in the runtime descriptor the method fails with\\n`ErrIncomingMessageInsufficientFee`.\\n- The number of tokens corresponding to `fee + tokens` are moved from the\\ncaller's account into the runtime account. If there is insufficient balance to\\ndo so the method fails with `ErrInsufficientBalance`.\\n- The incoming queue metadata structure is fetched. If it doesn't yet exist it\\nis populated with zero values.\\n- If the value of the `size` field in the metadata structure is equal to or\\nlarger than the value of the `txn_scheduler.max_in_messages` field in the\\nruntime descriptor the method fails with `ErrIncomingMessageQueueFull`.\\n- An `IncomingMessage` structure is generated based on the caller and method\\nbody and the value of the `next_sequence_number` metadata field is used to\\ngenerate a proper key for storing it in the queue. The structure is inserted\\ninto the queue.\\n- The `size` and `next_sequence_number` fields are incremented and the updated\\nmetadata is saved.\\n### Queries\\nThis proposal adds the following new query methods in the roothash module by\\nupdating the `roothash.Backend` interface as follows:\\n<!-- markdownlint-disable line-length -->\\n```golang\\ntype Backend interface {\\n\/\/ ... existing methods omitted ...\\n\/\/ GetIncomingMessageQueueMeta returns the given runtime's incoming message queue metadata.\\nGetIncomingMessageQueueMeta(ctx context.Context, request *RuntimeRequest) (*message.IncomingMessageQueueMeta, error)\\n\/\/ GetIncomingMessageQueue returns the given runtime's queued incoming messages.\\nGetIncomingMessageQueue(ctx context.Context, request *InMessageQueueRequest) ([]*message.IncomingMessage, error)\\n}\\n\/\/ IncomingMessageQueueMeta is the incoming message queue metadata.\\ntype IncomingMessageQueueMeta struct {\\n\/\/ Size contains the current size of the queue.\\nSize uint32 `json:\"size,omitempty\"`\\n\/\/ NextSequenceNumber contains the sequence number that should be used for the next queued\\n\/\/ message.\\nNextSequenceNumber uint64 `json:\"next_sequence_number,omitempty\"`\\n}\\n\/\/ InMessageQueueRequest is a request for queued incoming messages.\\ntype InMessageQueueRequest struct {\\nRuntimeID common.Namespace `json:\"runtime_id\"`\\nHeight    int64            `json:\"height\"`\\nOffset uint64 `json:\"offset,omitempty\"`\\nLimit  uint32 `json:\"limit,omitempty\"`\\n}\\n```\\n<!-- markdownlint-enable line-length -->\\n### Runtime Host Protocol\\nThis proposal updates the existing host to runtime requests in the runtime host\\nprotocol as follows:\\n```golang\\ntype RuntimeExecuteTxBatchRequest struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ IncomingMessages are the incoming messages from the consensus layer that\\n\/\/ should be processed by the runtime in this round.\\nIncomingMessages []*IncomingMessage `json:\"in_messages,omitempty\"`\\n}\\n```\\n### Rust Runtime Support Library\\nThis proposal updates the `transaction::Dispatcher` trait as follows:\\n```rust\\npub trait Dispatcher: Send + Sync {\\n\/\/ ... existing unchanged methods omitted ...\\n\/\/\/ Execute the transactions in the given batch.\\nfn execute_batch(\\n&self,\\nctx: Context,\\nbatch: &TxnBatch,\\nin_msgs: Vec<IncomingMessage>, \/\/ Added argument.\\n) -> Result<ExecuteBatchResult, RuntimeError>;\\n}\\n```\\n### Executor Processing\\nThe executor processing pipeline is changed such that pending incoming messages\\nare queried before the next round starts and are then passed to the runtime via\\nthe runtime host protocol.\\nThe executor may perform checks to estimate resource use early, similarly to how\\nchecks are performed for transactions as they arrive.\\n### Runtime Processing\\nThe proposal requires that messages are processed by the runtime in queue order\\n(e.g. on each round `InMessagesCount` messages are poped from the queue). This\\nsimplifies the design but the runtimes need to carefully consider how much\\nresources to allocate for executing messages (vs. regular transactions) in a\\nround.\\nThe runtime has full autonomy in choosing how many messages to execute as it\\nis given the complete message batch. It should first compute how many messages\\nto process by running them in \"check\" mode and computing how much gas (or other\\nresources) they take and then choosing as many as fits.\\nSpecifying these details is left to the runtime implementation although the SDK\\nis expected to adopt an approach with separate `max_inmsg_gas` and\\n`max_inmsg_slots` parameters which limits how resources are allocated for\\nincoming message processing in each round. If a single message exceeds either of\\nthese limits it will result in execution failure of that message.\\n### Root Hash Commitment Processing\\nThe processing of executor commitments is modified as follows:\\n- No changes are made to the discrepancy detection and resolution protocols\\nbesides the newly added fields being taken into account in discrepancy\\ndetermination.\\n- After a successful round, the `InMessagesCount` field of the compute body is\\nchecked and the corresponding number of messages are popped from the queue in\\nincreasing order of their sequence numbers. The queue metadata is updated\\naccoordingly by decrementing the value of the `size` field and the\\n`InMessagesHash` is added to the newly emitted block header.\\n","tokens":176,"id":4348,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0003-consensus-runtime-token-transfer.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently each runtime can define its own token (or none at all) and there is no\\nmechanism that would support transfer of consensus layer tokens into a runtime\\nand back out.\\nIntroducing such a mechanism would allow the consensus layer tokens to be used\\ninside runtimes for various functions. This ADR proposes such a mechanism.\\n","Decision":"On a high level, this proposal adds support for consensus\/runtime token\\ntransfers as follows:\\n- **Each staking account can set an allowance for beneficiaries.** Each staking\\naccount can set an allowance, a maximum amount a beneficiary can withdraw from\\nthe given account. Beneficiaries are identified by their address. This is\\nsimilar to approve\/transferFrom calls defined by the [ERC-20 Token Standard].\\nPreviously such functionality was already present but was removed in\\n[oasis-core#2021].\\n- **Each runtime itself has an account in the consensus layer.** This account\\ncontains the balance of tokens which are managed exclusively by the runtime\\nand do not belong to any specific regular account in the consensus layer.\\nIt is not possible to transfer directly into a runtime account and doing so\\nmay result in funds to be locked without a way to reclaim them.\\nThe only way to perform any operations on runtime accounts is through the use\\nof messages emitted by the runtime during each round. These messages are\\nsubject to discrepancy detection and instruct the consensus layer what to do.\\nCombined, the two mechanisms enable account holders to set an allowance in the\\nbenefit of runtimes so that the runtimes can withdraw up to the allowed amount\\nfrom the account holder's address.\\n### Addresses\\nThis proposal introduces the following new address context for the runtime\\naccounts:\\n```\\noasis-core\/address: runtime\\n```\\nInitial version for the address context is `0`. To derive the address, the\\nstandard address derivation scheme is used, with the runtime's 32-byte\\nidentifier used as the `data` part.\\n### State\\nThis proposal introduces\/updates the following consensus state in the staking\\nmodule:\\n#### General Accounts\\nThe general account data structure is modified to include an additional field\\nstoring the allowances as follows:\\n```golang\\ntype GeneralAccount struct {\\n\/\/ ... existing fields omitted ...\\nAllowances map[Address]quantity.Quantity `json:\"allowances,omitempty\"`\\n}\\n```\\n### Transaction Methods\\nThis proposal adds the following new transaction methods in the staking module:\\n#### Allow\\nAllow enables an account holder to set an allowance for a beneficiary.\\n**Method name:**\\n```\\nstaking.Allow\\n```\\n**Body:**\\n```golang\\ntype Allow struct {\\nBeneficiary  Address           `json:\"beneficiary\"`\\nNegative     bool              `json:\"negative,omitempty\"`\\nAmountChange quantity.Quantity `json:\"amount_change\"`\\n}\\n```\\n**Fields:**\\n- `beneficiary` specifies the beneficiary account address.\\n- `amount_change` specifies the absolute value of the amount of base units to\\nchange the allowance for.\\n- `negative` specifies whether the `amount_change` should be subtracted instead\\nof added.\\nThe transaction signer implicitly specifies the general account. Upon executing\\nthe allow the following actions are performed:\\n- If either the `disable_transfers` staking consensus parameter is set to `true`\\nor the `max_allowances` staking consensus parameter is set to zero, the method\\nfails with `ErrForbidden`.\\n- It is checked whether either the transaction signer address or the\\n`beneficiary` address are reserved. If any are reserved, the method fails with\\n`ErrForbidden`.\\n- Address specified by `beneficiary` is compared with the transaction signer\\naddress. If the addresses are the same, the method fails with\\n`ErrInvalidArgument`.\\n- The account indicated by the signer is loaded.\\n- If the allow would create a new allowance and the maximum number of allowances\\nfor an account has been reached, the method fails with `ErrTooManyAllowances`.\\n- The set of allowances is updated so that the allowance is updated as specified\\nby `amount_change`\/`negative`. In case the change would cause the allowance to\\nbe equal to zero or negative, the allowance is removed.\\n- The account is saved.\\n- The corresponding `AllowanceChangeEvent` is emitted with the following\\nstructure:\\n```golang\\ntype AllowanceChangeEvent struct {\\nOwner        Address           `json:\"owner\"`\\nBeneficiary  Address           `json:\"beneficiary\"`\\nAllowance    quantity.Quantity `json:\"allowance\"`\\nNegative     bool              `json:\"negative,omitempty\"`\\nAmountChange quantity.Quantity `json:\"amount_change\"`\\n}\\n```\\nWhere `allowance` contains the new total allowance, the `amount_change`\\ncontains the absolute amount the allowance has changed for and `negative`\\nspecifies whether the allowance has been reduced rather than increased. The\\nevent is emitted even if the new allowance is zero.\\n#### Withdraw\\nWithdraw enables a beneficiary to withdraw from the given account.\\n**Method name:**\\n```\\nstaking.Withdraw\\n```\\n**Body:**\\n```golang\\ntype Withdraw struct {\\nFrom   Address           `json:\"from\"`\\nAmount quantity.Quantity `json:\"amount\"`\\n}\\n```\\n**Fields:**\\n- `from` specifies the account address to withdraw from.\\n- `amount` specifies the amount of base units to withdraw.\\nThe transaction signer implicitly specifies the destination general account.\\nUpon executing the withdrawal the following actions are performed:\\n- If either the `disable_transfers` staking consensus parameter is set to `true`\\nor the `max_allowances` staking consensus parameter is set to zero, the method\\nfails with `ErrForbidden`.\\n- It is checked whether either the transaction signer address or the\\n`from` address are reserved. If any are reserved, the method fails with\\n`ErrForbidden`.\\n- Address specified by `from` is compared with the transaction signer address.\\nIf the addresses are the same, the method fails with `ErrInvalidArgument`.\\n- The source account indicated by `from` is loaded.\\n- The destination account indicated by the transaction signer is loaded.\\n- `amount` is deducted from the corresponding allowance in the source account.\\nIf this would cause the allowance to go negative, the method fails with\\n`ErrForbidden`.\\n- `amount` is deducted from the source general account balance. If this would\\ncause the balance to go negative, the method fails with\\n`ErrInsufficientBalance`.\\n- `amount` is added to the destination general account balance.\\n- Both source and destination accounts are saved.\\n- The corresponding `TransferEvent` is emitted.\\n- The corresponding `AllowanceChangeEvent` is emitted with the updated\\nallowance.\\n### Queries\\nThis proposal adds the following new query methods in the staking module by\\nupdating the `staking.Backend` interface as follows:\\n```golang\\ntype Backend interface {\\n\/\/ ... existing methods omitted ...\\n\/\/ Allowance looks up the allowance for the given owner\/beneficiary combination.\\nAllowance(ctx context.Context, query *AllowanceQuery) (*quantity.Quantity, error)\\n}\\n\/\/ AllowanceQuery is an allowance query.\\ntype AllowanceQuery struct {\\nHeight      int64   `json:\"height\"`\\nOwner       Address `json:\"owner\"`\\nBeneficiary Address `json:\"beneficiary\"`\\n}\\n```\\n### Messages\\nSince this is the first proposal that introduces a new runtime message type that\\ncan be emitted from a runtime during a round, it also defines some general\\nproperties of runtime messages and the dispatch mechanism:\\n- Each message has an associated gas cost that needs to be paid by the\\nsubmitter (e.g. as part of the `roothash.ExecutorCommit` method call). The gas\\ncost is split among the committee members.\\n- There is a maximum number of messages that can be emitted by a runtime during\\na given round. The limit is defined both globally (e.g. a roothash consensus\\nparameter) and per-runtime (which needs to be equal to or lower than the\\nglobal limit).\\n- Messages are serialized using a sum type describing all possible messages,\\nwhere each message type is assigned a _field name_:\\n```golang\\ntype Message struct {\\nMessage1 *Message1 `json:\"message1,omitempty\"`\\nMessage2 *Message2 `json:\"message2,omitempty\"`\\n\/\/ ...\\n}\\n```\\n- All messages are versioned by embeding the `cbor.Versioned` structure which\\nprovides a single `uint16` field `v`.\\n- A change is made to how messages are included in commitments, to reduce the\\nsize of submitted transactions.\\nThe `ComputeResultsHeader` is changed so that the `Messages` field is replaced\\nwith a `MessagesHash` field containing a hash of the CBOR-encoded messages\\nemitted by the runtime.\\nAt the same time `ComputeBody` is changed to include an additional field\\n`Messages` as follows:\\n```golang\\ntype ComputeBody struct {\\n\/\/ ... existing fields omitted ...\\nMessages []*block.Message `json:\"messages,omitempty\"`\\n}\\n```\\nThe `Messages` field must only be populated in the commitment by the\\ntransaction scheduler and must match the `MessagesHash`.\\n- If any of the included messages is deemed _malformed_, the round fails and the\\nruntime state is not updated.\\n- In order to support messages that fail to execute, a new roothash event is\\nemitted for each executed message:\\n```golang\\ntype MessageEvent struct {\\nIndex  uint32 `json:\"index,omitempty\"`\\nModule string `json:\"module,omitempty\"`\\nCode   uint32 `json:\"code,omitempty\"`\\n}\\n```\\nWhere the `index` specifies the index of the executed message and the `module`\\nand `code` specify the module and error code accoording to Oasis Core error\\nencoding convention (note that the usual human readable message field is not\\nincluded).\\nThis proposal introduces the following runtime messages:\\n#### Staking Method Call\\nThe staking method call message enables a runtime to call one of the supported\\nstaking module methods.\\n**Field name:**\\n```\\nstaking\\n```\\n**Body:**\\n```golang\\ntype StakingMessage struct {\\ncbor.Versioned\\nTransfer *staking.Transfer `json:\"transfer,omitempty\"`\\nWithdraw *staking.Withdraw `json:\"withdraw,omitempty\"`\\n}\\n```\\n**Fields:**\\n- `v` must be set to `0`.\\n- `transfer` indicates that the `staking.Transfer` method should be executed.\\n- `withdraw` indicates that the `staking.Withdraw` method should be executed.\\nExactly one of the supported method fields needs to be non-nil, otherwise the\\nmessage is considered malformed.\\n### Consensus Parameters\\n#### Staking\\nThis proposal introduces the following new consensus parameters in the staking\\nmodule:\\n- `max_allowances` (uint32) specifies the maximum number of allowances an\\naccount can store. Zero means that allowance functionality is disabled.\\n#### Roothash\\nThis proposal introduces the following new consensus parameters in the roothash\\nmodule:\\n- `max_runtime_messages` (uint32) specifies the global limit on the number of\\nmessages that can be emitted in each round by the runtime. The default value\\nof `0` disables the use of runtime messages.\\n### Runtime Host Protocol\\nThis proposal modifies the runtime host protocol as follows:\\n#### Host to Runtime: Initialization\\nThe existing `RuntimeInfoRequest` message body is updated to contain a field\\ndenoting the consensus backend used by the host and its consensus protocol\\nversion as follows:\\n```golang\\ntype RuntimeInfoRequest struct {\\nConsensusBackend         string `json:\"consensus_backend\"`\\nConsensusProtocolVersion uint64 `json:\"consensus_protocol_version\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThis information can be used by the runtime to ensure that it supports the\\nconsensus layer used by the host. In case the backend and\/or protocol version is\\nnot supported, the runtime should return an error and terminate. In case the\\nruntime does not interact with the consensus layer it may ignore the consensus\\nlayer information.\\n#### Host to Runtime: Transaction Batch Dispatch\\nThe existing `RuntimeExecuteTxBatchRequest` and `RuntimeCheckTxBatchRequest`\\nmessage bodies are updated to include the consensus layer light block at the\\nlast finalized round height (specified in `.Block.Header.Round`) and the list of\\n`MessageEvent`s emitted while processing the runtime messages emitted in the\\nprevious round as follows:\\n```golang\\ntype RuntimeExecuteTxBatchRequest struct {\\n\/\/ ConsensusBlock is the consensus light block at the last finalized round\\n\/\/ height (e.g., corresponding to .Block.Header.Round).\\nConsensusBlock consensus.LightBlock `json:\"consensus_block\"`\\n\/\/ MessageResults are the results of executing messages emitted by the\\n\/\/ runtime in the previous round (sorted by .Index).\\nMessageResults []roothash.MessageEvent `json:\"message_results,omitempty\"`\\n\/\/ ... existing fields omitted ...\\n}\\ntype RuntimeCheckTxBatchRequest struct {\\n\/\/ ConsensusBlock is the consensus light block at the last finalized round\\n\/\/ height (e.g., corresponding to .Block.Header.Round).\\nConsensusBlock consensus.LightBlock `json:\"consensus_block\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThe information from the light block can be used to access consensus layer\\nstate.\\n#### Runtime to Host: Read-only Storage Access\\nThe existing `HostStorageSyncRequest` message body is updated to include an\\nendpoint identifier as follows:\\n```golang\\ntype HostStorageSyncRequest struct {\\n\/\/ Endpoint is the storage endpoint to which this request should be routed.\\nEndpoint string `json:\"endpoint,omitempty\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThe newly introduced `endpoint` field can take the following values:\\n- `runtime` (or empty string) denotes the runtime state endpoint. The empty\\nvalue is allowed for backwards compatibility as this was the only endpoint\\navailable before this proposal.\\n- `consensus` denotes the consensus state endpoint, providing access to\\nconsensus state.\\n### Rust Runtime Support Library\\nThe Rust runtime support library (`oasis-core-runtime`) must be updated to\\nsupport the updated message structures. Additionally, there needs to be basic\\nsupport for interpreting the data from the Tendermint consensus layer backend:\\n- Decoding light blocks.\\n- Decoding staking-related state structures.\\nThe Tendermint-specific functionality should be part of a separate crate.\\n### Expected User\/Consensus\/Runtime Flow\\n**Scenario:**\\nAccount holder has 100 tokens in her account in the consensus layer staking\\nledger and would like to spend 50 tokens to execute an action in runtime X.\\n**Flow:**\\n- Account holder sets an allowance of 50 tokens for runtime X by submitting an\\nallow transaction to the consensus layer.\\n- Account holder submits a runtime transaction that performs some action costing\\n50 tokens.\\n- Account holder's runtime transaction is executed in runtime X round R:\\n- Runtime X emits a message to transfer 50 tokens from the user's account to\\nthe runtime's own account.\\n_As an optimization runtime X can verify current consensus layer state and\\nreject the transaction early to prevent paying for needless consensus layer\\nmessage processing._\\n- Runtime X updates its state to indicate a pending transfer of 50 tokens from\\nthe user. It uses the index of the emitted message to be able to match the\\nmessage execution result once it arrives.\\n- Runtime X submits commitments to the consensus layer.\\n- When finalizing round R for runtime X, the consensus layer transfers 50 tokens\\nfrom the account holder's account to the runtime X account.\\n- Corresponding message result event is emitted, indicating success.\\n- When runtime X processes round R+1, the runtime receives the set of emitted\\nmessage result events.\\n- Runtime X processes message result events, using the index field to match the\\ncorresponding pending action and executes whatever action it queued.\\n- In case the message result event would indicate failure, the pending action\\ncan be pruned.\\n","tokens":73,"id":4349,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0004-runtime-governance.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently all runtimes can only be governed by a single entity -- the runtime\\nowner. In this regard governance means being able to update certain fields in\\nthe runtime descriptor stored by the consensus layer registry service. On one\\nhand the runtime descriptor contains security-critical parameters and on the\\nother there needs to be a mechanism through which the runtimes can be upgraded\\n(especially so for TEE-based runtimes where a specific runtime binary is\\nenforced via remote attestation mechanisms).\\nThis proposal extends runtime governance options and enables a path towards\\nruntimes that can define their own governance mechanisms. This proposal assumes\\nthat [ADR 0003] has been adopted and runtimes can have their own accounts in the\\nstaking module.\\n","Decision":"This proposal takes a simplistic but powerful approach which allows each runtime\\nto choose its governance model upon its first registration. It does so through\\na newly introduced field in the runtime descriptor which indicates how the\\nruntime descriptor can be updated in the future.\\n### Runtime Descriptor\\nThe runtime descriptor version is bumped to `2`. Version `1` descriptors are\\naccepted at genesis and are converted to the new format by assuming the entity\\ngovernance model as that is the only option in v1. All new runtime registrations\\nmust use the v2 descriptor.\\n#### Governance Model\\nThis proposal updates the runtime descriptor by adding fields as follows:\\n```golang\\ntype Runtime struct {\\n\/\/ GovernanceModel specifies the runtime governance model.\\nGovernanceModel RuntimeGovernanceModel `json:\"governance_model\"`\\n\/\/ ... existing fields omitted ...\\n}\\n\/\/ RuntimeGovernanceModel specifies the runtime governance model.\\ntype RuntimeGovernanceModel uint8\\nconst (\\nGovernanceEntity    RuntimeGovernanceModel = 1\\nGovernanceRuntime   RuntimeGovernanceModel = 2\\nGovernanceConsensus RuntimeGovernanceModel = 3\\n)\\n\/\/ ... some text serialization methods omitted ...\\n```\\nThe `governance_model` field can specifiy one of the following governance\\nmodels:\\n- **Entity governance (`GovernanceEntity`).** This causes the runtime to behave\\nexactly as before, the runtime owner (indicated by `entity_id` in the runtime\\ndescriptor) is the only one who can update the runtime descriptor via\\n`registry.RegisterRuntime` method calls.\\nThe runtime owner is also the one that needs to provide the required stake\\nin escrow in order to avoid the runtime from being suspended. As before note\\nthat anyone can delegate the required stake to the runtime owner in order to\\nenable runtime operation (but the owner can always prevent the runtime from\\noperating by performing actions which would cause the stake claims to no\\nlonger be satisfied).\\n- **Runtime-defined governance (`GovernanceRuntime`).** In this case the runtime\\nitself is the only one who can update the runtime descriptor by emitting a\\nruntime message. The runtime owner (indicated by `entity_id`) is not able to\\nperform any updates after the initial registration and such attempts must\\nreturn `ErrForbidden`.\\nThe runtime itself is the one that needs to provide the required stake in\\nescrow in order to avoid the runtime from being suspended. This assumes that\\nruntimes can have accounts in the staking module as specified by [ADR 0003].\\nNote that anyone can delegate the required stake to a runtime in order to\\nenable its operation.\\n- **Consensus layer governance (`GovernanceConsensus`).** In this case only the\\nconsensus layer itself can update the runtime descriptor either through a\\nnetwork upgrade or via a consensus layer governance mechanism not specified by\\nthis proposal.\\nRuntimes using this governance model are never suspended and do not need to\\nprovide stake in escrow.\\nRuntimes using this governance model cannot be registered\/updated via regular\\nregistry method calls or runtime messages (doing so must return\\n`ErrForbidden`). Instead such a runtime can only be registered at genesis,\\nthrough a network upgrade or via a consensus layer governance mechanism not\\nspecified by this proposal.\\n#### Entity Whitelist Admission Policy\\nThe entity whitelist admission policy configuration structure is changed to\\nallow specifying the maximum number of nodes that each entity can register under\\nthe given runtime for each role.\\n```golang\\ntype EntityWhitelistConfig struct {\\n\/\/ MaxNodes is the maximum number of nodes that an entity can register under\\n\/\/ the given runtime for a specific role. If the map is empty or absent, the\\n\/\/ number of nodes is unlimited. If the map is present and non-empty, the\\n\/\/ the number of nodes is restricted to the specified maximum (where zero\\n\/\/ means no nodes allowed), any missing roles imply zero nodes.\\nMaxNodes map[node.RolesMask]uint16 `json:\"max_nodes,omitempty\"`\\n}\\ntype EntityWhitelistRuntimeAdmissionPolicy struct {\\nEntities map[signature.PublicKey]EntityWhitelistConfig `json:\"entities\"`\\n}\\n```\\nThe new `max_nodes` field specifies the maximum number of nodes an entity can\\nregister for the given runtime for each role. If the map is empty or absent, the\\nnumber of nodes is unlimited. If the map is present and non-empty, the number of\\nnodes is restricted to the specified number (where zero means no nodes are\\nallowed). Any missing roles imply zero nodes.\\nEach key (roles mask) in the `max_nodes` map must specify a single role,\\notherwise the runtime descriptor is rejected with `ErrInvalidArgument`.\\nWhen transforming runtime descriptors from version 1, an entry in the `entities`\\nfield maps to an `EntityWhitelistConfig` structure with `max_nodes` absent,\\ndenoting that an unlimited number of nodes is allowed (as before).\\n#### Minimum Required Committee Election Pool Size\\nThe executor and storage runtime parameters are updated to add a new field\\ndefining the minimum required committee election pool size. The committee\\nscheduler is updated to refuse election for a given runtime committee in case\\nthe number of candidate nodes is less than the configured minimum pool size.\\n```golang\\ntype ExecutorParameters struct {\\n\/\/ MinPoolSize is the minimum required candidate compute node pool size.\\nMinPoolSize uint64 `json:\"min_pool_size\"`\\n\/\/ ... existing fields omitted ...\\n}\\ntype StorageParameters struct {\\n\/\/ MinPoolSize is the minimum required candidate storage node pool size.\\nMinPoolSize uint64 `json:\"min_pool_size\"`\\n\/\/ ... existing fields omitted ...\\n}\\n```\\nThe value of `min_pool_size` must be non-zero and must be equal to or greater\\nthan the corresponding sum of `group_size` and `group_backup_size`. Otherwise\\nthe runtime descriptor is rejected with `ErrInvalidArgument`.\\nWhen transforming runtime descriptors from version 1, `min_pool_size` for the\\nexecutor committee is computed as `group_size + group_backup_size` while the\\n`min_pool_size` for the storage committee is equal to `group_size`.\\n### State\\nThis proposal introduces\/updates the following consensus state in the registry\\nmodule:\\n#### Stored Runtime Descriptors\\nSince the runtime descriptors can now be updated by actors other than the\\ninitial registering entity, it does not make sense to store signed runtime\\ndescriptors. The value of storage key prefixed with `0x13` which previously\\ncontained signed runtime descriptors is modified to store plain runtime\\ndescriptors.\\n### Genesis Document\\nThis proposal updates the registry part of the genesis document as follows:\\n- The type of the `runtimes` field is changed to a list of runtime descriptors\\n(was a list of _signed_ runtime descriptors before).\\n- The type of the `suspended_runtimes` field is changed to a list of runtime\\ndescriptors (was a list of _signed_ runtime descriptors before).\\nRuntime descriptors must be transformed to support the new fields.\\n### Transaction Methods\\nThis proposal updates the following transaction methods in the registry module:\\n#### Register Runtime\\nRuntime registration enables a new runtime to be created or an existing runtime\\nto be updated (in case the governance model allows it).\\n**Method name:**\\n```\\nregistry.RegisterRuntime\\n```\\nThe body of a register runtime transaction must be a `Runtime` descriptor.\\nThe signer of the transaction must be the owning entity key.\\nRegistering a runtime may require sufficient stake in either the owning entity's\\n(when entity governance is used) or the runtime's (when runtime governance is\\nused) escrow account.\\nChanging the governance model from `GovernanceEntity` to `GovernanceRuntime` is\\nallowed. Any other governance model changes are not allowed and must fail with\\n`ErrForbidden`. Support for other changes is deferred to a consensus layer\\ngovernance mechanism not specified by this proposal.\\nUsing the `GovernanceRuntime` governance model for a runtime of any kind other\\nthan `KindCompute` must return `ErrInvalidArgument`.\\n### Messages\\nThis proposal introduces the following runtime messages:\\n#### Update Runtime Descriptor\\nThe update runtime descriptor message enables a runtime to update its own\\ndescriptor when the current governance model allows it.\\n**Field name:**\\n```\\nupdate_runtime\\n```\\n**Body:**\\n```golang\\ntype UpdateRuntimeMessage struct {\\nregistry.Runtime\\n}\\n```\\nThe body of the update runtime descriptor message is a new runtime descriptor\\nthat must be for the runtime emitting this message. Otherwise the message is\\nconsidered malformed.\\nThe actions performed when processing the message are the same as those\\nperformed when processing the `registry.RegisterRuntime` method call, just made\\non the runtime's (instead of an entity's) behalf.\\n### Consensus Parameters\\n#### Registry\\nThis proposal introduces the following new consensus parameters in the registry\\nmodule:\\n- `enable_runtime_governance_models` (set of `RuntimeGovernanceModel`) specifies\\nthe set of runtime governance models that are allowed to be used when\\ncreating\/updating registrations (either via method calls or via runtime\\nmessages). In case a runtime is using a governance model not specified in this\\nlist, an update to such a runtime must fail with `ErrForbidden`.\\n### Rust Runtime Support Library\\nThe Rust runtime support library (`oasis-core-runtime`) must be updated to\\nsupport the updated and newly needed message structures (the runtime descriptor\\nand the update runtime message).\\n","tokens":154,"id":4350,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0007-improved-random-beacon.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n> Any one who considers arithmetical methods of producing random digits\\n> is, of course, in a state of sin.\\n>\\n> --Dr. John von  Neumann\\nThe existing random beacon used by Oasis Core, is largely a placeholder\\nimplementation that naively uses the previous block's commit hash as the\\nentropy input.  As such it is clearly insecure as it is subject to\\nmanipulation.\\nA better random beacon which is harder for an adversary to manipulate\\nis required to provide entropy for secure committee elections.\\n","Decision":"At a high level, this ADR proposes implementing an on-chain random beacon\\nbased on \"SCRAPE: Scalabe Randomness Attested by Public Entities\" by\\nCascudo and David.  The new random beacon will use a commit-reveal scheme\\nbacked by a PVSS scheme so that as long as the threshold of participants\\nis met, and one participant is honest, secure entropy will be generated.\\nNote: This document assumes the reader understands SCRAPE. Details\\nregarding the underlying SCRAPE implementation are omitted for brevity.\\n### Node Descriptor\\nThe node descriptor of each node will be extended to include the following\\ndatastructure.\\n```golang\\ntype Node struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ Beacon contains information for this node's participation\\n\/\/ in the random beacon protocol.\\n\/\/\\n\/\/ TODO: This is optional for now, make mandatory once enough\\n\/\/ nodes provide this field.\\nBeacon *BeaconInfo `json:\"beacon,omitempty\"`\\n}\\n\/\/ BeaconInfo contains information for this node's participation in\\n\/\/ the random beacon protocol.\\ntype BeaconInfo struct {\\n\/\/ Point is the elliptic curve point used for the PVSS algorithm.\\nPoint scrape.Point `json:\"point\"`\\n}\\n```\\nEach node will generate and maintain a long term elliptic curve point\\nand scalar pair (public\/private key pair), the point (public key) of\\nwhich will be included in the node descriptor.\\nFor the purposes of the initial implementation, the curve will be P-256.\\n### Consensus Parameters\\nThe beacon module will have the following consensus parameters that\\ncontrol behavior.\\n```golang\\ntype SCRAPEParameters struct {\\nParticipants  uint64 `json:\"participants\"`\\nThreshold     uint64 `json:\"threshold\"`\\nPVSSThreshold uint64 `json:\"pvss_threshold\"`\\nCommitInterval  int64 `json:\"commit_interval\"`\\nRevealInterval  int64 `json:\"reveal_interval\"`\\nTransitionDelay int64 `json:\"transition_delay\"`\\n}\\n```\\nFields:\\n- `Participants` - The number of participants to be selected for each\\nbeacon generation protocol round.\\n- `Threshold` - The minimum number of participants which must\\nsuccessfully contribute entropy for the final output to be\\nconsidered valid.\\n- `PVSSThreshold` - The minimum number of participants that are\\nrequired to reconstruct a PVSS secret from the corresponding\\ndecrypted shares (Note: This usually should just be set to\\n`Threshold`).\\n- `CommitInterval` - The duration of the Commit phase, in blocks.\\n- `RevealInterval` - The duration of the Reveal phase, in blocks.\\n- `TransitionDelay` - The duration of the post Reveal phase delay, in blocks.\\n### Consensus State and Events\\nThe on-chain beacon will maintain and make available the following consensus\\nstate.\\n```golang\\n\/\/ RoundState is a SCRAPE round state.\\ntype RoundState uint8\\nconst (\\nStateInvalid  RoundState = 0\\nStateCommit   RoundState = 1\\nStateReveal   RoundState = 2\\nStateComplete RoundState = 3\\n)\\n\/\/ SCRAPEState is the SCRAPE backend state.\\ntype SCRAPEState struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nInstance     *scrape.Instance      `json:\"instance,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\nEntropy      []byte                `json:\"entropy,omitempty\"`\\nBadParticipants map[signature.PublicKey]bool `json:\"bad_participants,omitempty\"`\\nCommitDeadline   int64 `json:\"commit_deadline,omitempty\"`\\nRevealDeadline   int64 `json:\"reveal_deadline,omitempty\"`\\nTransitionHeight int64 `json:\"transition_height,omitempty\"`\\nRuntimeDisableHeight int64 `json:\"runtime_disable_height,omitempty\"`\\n}\\n```\\nFields:\\n- `Height` - The block height at which the last event was emitted.\\n- `Epoch` - The epoch in which this beacon is being generated.\\n- `Round` - The epoch beacon generation round.\\n- `State` - The beacon generation step (commit\/reveal\/complete).\\n- `Instance` - The SCRAPE protocol state (encrypted\/decrypted shares of\\nall participants).\\n- `Participants` - The node IDs of the nodes selected to participate\\nin this beacon generation round.\\n- `Entropy` - The final raw entropy, if any.\\n- `BadParticipants` - A map of nodes that were selected, but have failed\\nto execute the protocol correctly.\\n- `CommitDeadline` - The height in blocks by which participants must\\nsubmit their encrypted shares.\\n- `RevealDeadline` - The height in blocks by which participants must\\nsubmit their decrypted shares.\\n- `TransitionHeight` - The height at which the epoch will transition\\nassuming this round completes successfully.\\n- `RuntimeDisableHeight` - The height at which, upon protocol failure,\\nruntime transactions will be disabled.  This height will be set to\\nthe transition height of the 0th round.\\nUpon transition to a next step of the protocol, the on-chain beacon will\\nemit the following event.\\n```golang\\n\/\/ SCRAPEEvent is a SCRAPE backend event.\\ntype SCRAPEEvent struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\n}\\n```\\nField definitions are identical to that of those in the `SCRAPEState`\\ndatastructure.\\n","tokens":116,"id":4351,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0005-runtime-compute-slashing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe runtime compute nodes make updates to the runtime state by submitting\\ncommitment messages to the roothash service in the consensus layer where\\ndiscrepancy detection and resolution are performed.\\nCurrently, the compute nodes are never slashed even if they commit incorrect\\nresults. While integrity is guarded by discrepancy detection and resolution,\\ncompute nodes should be disincentivized to behave incorrectly.\\n","Decision":"This proposal introduces a slashing mechanism for punishing misbehaving compute\\nnodes as follows:\\n- **Per-runtime configurable slashing parameters** are added to the runtime\\ndescriptor similar to the global slashing configuration that currently exists\\nin the staking service.\\n- **New runtime-specific slashing reasons** are introduced: (i) submitting\\nincorrect compute results and (ii) signing two different executor commits or\\nproposed batches for the same round.\\n- **Failure-indicating executor commits** are introduced in order to give the\\ncompute nodes a possibility to vote for failure when they cannot execute the\\ngiven batch (e.g., due to unavailability of storage or key manager) without\\ngetting slashed. Such commits will always trigger a discrepancy during\\ndiscrepancy detection and will vote for failing the round in discrepancy\\nresolution phase.\\n### Runtime Descriptor\\nThis proposal updates the runtime staking parameters (stored under the `staking`\\nfield of the runtime descriptor) as follows:\\n```golang\\ntype RuntimeStakingParameters struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ Slashing are the per-runtime misbehavior slashing parameters.\\nSlashing map[staking.SlashReason]staking.Slash `json:\"slashing,omitempty\"`\\n\/\/ RewardSlashEquvocationRuntimePercent is the percentage of the reward obtained when slashing\\n\/\/ for equivocation that is transferred to the runtime's account.\\nRewardSlashEquvocationRuntimePercent uint8 `json:\"reward_equivocation,omitempty\"`\\n\/\/ RewardSlashBadResultsRuntimePercent is the percentage of the reward obtained when slashing\\n\/\/ for incorrect results that is transferred to the runtime's account.\\nRewardSlashBadResultsRuntimePercent uint8 `json:\"reward_bad_results,omitempty\"`\\n}\\n```\\n### Slashing Parameters\\nThe slash reason type in the staking module is changed from `int` to `uint8`.\\nThe slash reason definitions are updated as follows:\\n```golang\\nconst (\\n\/\/ SlashConsensusEquivocation is slashing due to equivocation in the\\n\/\/ consensus layer.\\nSlashConsensusEquivocation SlashReason = 0x00\\n\/\/ SlashRuntimeIncorrectResults is slashing due to submission of incorrect\\n\/\/ results in runtime executor commitments.\\nSlashRuntimeIncorrectResults SlashReason = 0x80\\n\/\/ SlashRuntimeEquivocation is slashing due to signing two different\\n\/\/ executor commits or proposed batches for the same round.\\nSlashRuntimeEquivocation SlashReason = 0x81\\n)\\n```\\n### Executor Commitments\\nThe executor commitment body structures are updated to make certain fields\\noptional and to introduce the `failure` field as follows:\\n```golang\\ntype ExecutorCommitmentFailure uint8\\nconst (\\n\/\/ FailureNone indicates that no failure has occurred.\\nFailureNone ExecutorCommitmentFailure = 0\\n\/\/ FailureUnknown indicates a generic failure.\\nFailureUnknown ExecutorCommitmentFailure = 1\\n\/\/ FailureStorageUnavailable indicates that batch processing failed due to\\n\/\/ storage being unavailable.\\nFailureStorageUnavailable ExecutorCommitmentFailure = 2\\n\/\/ FailureKeyManagerUnavailable indicates that batch processing failed due\\n\/\/ to key manager being unavailable.\\nFailureKeyManagerUnavailable ExecutorCommitmentFailure = 3\\n)\\ntype ExecutorCommitmentHeader struct {\\n\/\/ Required fields.\\nRound        uint64    `json:\"round\"`\\nPreviousHash hash.Hash `json:\"previous_hash\"`\\n\/\/ Optional fields (may be absent for failure indication).\\nIORoot       *hash.Hash `json:\"io_root,omitempty\"`\\nStateRoot    *hash.Hash `json:\"state_root,omitempty\"`\\nMessageHash  *hash.Hash `json:\"messages_hash,omitempty\"`\\n}\\ntype ExecutorCommitmentBody struct {\\nHeader  ExecutorCommitmentHeader  `json:\"header\"`\\nFailure ExecutorCommitmentFailure `json:\"failure,omitempty\"`\\nTxnSchedSig      signature.Signature   `json:\"txn_sched_sig\"`\\nInputRoot        hash.Hash             `json:\"input_root\"`\\nInputStorageSigs []signature.Signature `json:\"input_storage_sigs\"`\\n\/\/ Optional fields (may be absent for failure indication).\\nStorageSignatures []signature.Signature   `json:\"storage_signatures,omitempty\"`\\nRakSig            *signature.RawSignature `json:\"rak_sig,omitempty\"`\\n}\\n```\\nThe notion of an _failure-indicating_ executor commitment is introduced as being\\nan executor commitment with the following field values:\\n- The `failure` field must be present and non-zero. The code can indicate a\\nreason for the failure but currently the reason is ignored during processing.\\n- `header.round`, `header.previous_hash`, `txn_sched_sig`, `input_root` and\\n`input_storage_sigs` are set as for usual commitments (e.g., they must be\\nvalid).\\n- All other fields must be omitted or set to nil.\\n### Root Hash Commitment Processing\\nThe processing of executor commitments by the commitment pool is modified as\\nfollows:\\n- **Adding new commitments (`AddExecutorCommitment`)**\\n- If a commitment for a node already exists the existing commitment is\\nchecked for evidence of equivocation. Any evidence of misbehavior is\\nprocessed as described in the _Evidence_ subsection below.\\n- **Discrepancy detection (`DetectDiscrepancy`)**\\n- If any executor commitment indicates failure, the discrepancy detection\\nprocess signals a discrepancy (which implies that discrepancy resolution is\\ntriggered).\\n- **Discrepancy resolution (`ResolveDiscrepancy`)**\\n- When tallying votes, any executor commitments indicating failure are tallied\\ninto its own bucket. If the failure bucket receives 1\/2+ votes, the round\\nfails.\\n- If after discrepancy resolution a non-failure option receives 1\/2+ votes,\\nthis is considered the correct result. Executor commitments for any other\\nresult (excluding failure indication) are considered incorrect and are\\nsubject to slashing (based on the configured slashing instructions for the\\n`SlashRuntimeIncorrectResults` reason).\\nA portion of slashed funds is disbursed equally to the compute nodes which\\nparticipated in discrepancy resolution for the round. The remainder of slashed\\nfunds is transferred to the runtime account.\\nAny slashing instructions related to freezing nodes are currently ignored.\\n### State\\nThis proposal introduces\/updates the following consensus state in the roothash\\nmodule:\\n- **List of past valid evidence (`0x24`)**\\nA hash uniquely identifying the evidence is stored for each successfully\\nprocessed evidence that has not yet expired using the following key format:\\n```\\n0x24 <H(runtime-id) (hash.Hash)> <round (uint64)> <evidence-hash (hash.Hash)>\\n```\\nThe value is empty as we only need to detect duplicate evidence.\\n### Transaction Methods\\nThis proposal updates the following transaction methods in the roothash module:\\n#### Evidence\\nThe evidence method allows anyone to submit evidence of runtime node\\nmisbehavior.\\n**Method name:**\\n```\\nroothash.Evidence\\n```\\n**Body:**\\n```golang\\ntype EvidenceKind uint8\\nconst (\\n\/\/ EvidenceKindEquivocation is the evidence kind for equivocation.\\nEvidenceKindEquivocation = 1\\n)\\ntype Evidence struct {\\nID   common.Namespace `json:\"id\"`\\nEquivocationExecutor *EquivocationExecutorEvidence `json:\"equivocation_executor,omitempty\"`\\nEquivocationBatch    *EquivocationBatchEvidence    `json:\"equivocation_batch,omitempty\"`\\n}\\ntype EquivocationExecutorEvidence struct {\\nCommitA commitment.ExecutorCommitment `json:\"commit_a\"`\\nCommitB commitment.ExecutorCommitment `json:\"commit_b\"`\\n}\\ntype EquivocationBatchEvidence struct {\\nBatchA commitment.SignedProposedBatch `json:\"batch_a\"`\\nBatchB commitment.SignedProposedBatch `json:\"batch_b\"`\\n}\\n```\\n**Fields:**\\n- `id` specifies the runtime identifier of a runtime this evidence is for.\\n- `equivocation_executor` (optional) specifies evidence of an executor node\\nequivocating when signing commitments.\\n- `equivocation_batch` (optional) specifies evidence of an executor node\\nequivocating when signing proposed batches.\\nIf no evidence is specified (e.g., all evidence fields are `nil`) the method\\ncall is invalid and must fail with `ErrInvalidArgument`.\\nFor all kinds of evidence, the following steps are performed to verify evidence\\nvalidity:\\n- Current state for the runtime identified by `id` is fetched. If the runtime\\ndoes not exist, the evidence is invalid.\\n- If no slashing instructions for `SlashRuntimeEquivocation` are configured for\\nthe given runtime, there is no point in collecting evidence so the method call\\nmust fail with `ErrRuntimeDoesNotSlash`.\\nWhen processing **`EquivocationExecutor`** evidence, the following steps are\\nperformed to verify evidence validity:\\n- `header.round` fields of both commitments are compared. If they are not the\\nsame, the evidence is invalid.\\n- Both executor commitments are checked for basic validity. If either is\\ninvalid, the evidence is invalid.\\n- The `header.previous_hash`, `header.io_root`, `header.state_root` and\\n`header.messages_hash` fields of both commitments are compared. If they are\\nthe same, the evidence is invalid.\\n- The failure indication fields of both commitments are compared. If they are\\nthe same, the evidence is invalid.\\n- `header.round` field is compared with the runtime's current state. If it is\\nmore than `max_evidence_age` (consensus parameter) rounds behind, the evidence\\nis invalid.\\n- Public keys of signers of both commitments are compared. If they are not the\\nsame, the evidence is invalid.\\n- Signatures of both commitments are verified. If either is invalid, the\\nevidence is invalid.\\n- Otherwise the evidence is valid.\\nWhen processing **`EquivocationBatch`** evidence, the following steps are\\nperformed to verify evidence validity:\\n- The `header.round` fields of both proposed batches are compared. If they are\\nnot the same, the evidence is invalid.\\n- The `header` fields of both proposed batches are checked for basic validity.\\nIf any is invalid, the evidence is invalid.\\n- The `io_root` fields of both proposed batches are compared. If they are the\\nsame, the evidence is invalid.\\n- Public keys of signers of both commitments are compared. If they are not the\\nsame, the evidence is invalid.\\n- Signatures of both proposed batches are validated. If either is invalid, the\\nevidence is invalid.\\n- Otherwise the evidence is valid.\\nFor all kinds of valid evidence, the following steps are performed after\\nvalidation:\\n- The evidence hash is derived by hashing the evidence kind and the public key\\nof the signer and the evidence is looked up in the _list of past valid\\nevidence_. If evidence already exists there, the method fails with\\n`ErrDuplicateEvidence`.\\n- The valid evidence hash is stored in the _list of past valid evidence_.\\nIf the evidence is deemed valid by the above procedure, the misbehaving compute\\nnode is slashed based on the runtime slashing parameters for the\\n`SlashRuntimeEquivocation` reason.\\nAny slashing instructions related to freezing nodes are currently ignored.\\nThe node submitting the evidence may be rewarded from part of the slashed\\namount to incentivize evidence submission. The remainder of slashed funds is\\ntransferred to the runtime account.\\n### Evidence Expiry\\nOn each epoch transition, for each runtime, expired evidence (as defined by the\\n`max_evidence_age` and the current runtime's round) must be pruned from the\\n_list of past valid evidence_.\\n### Evidence Collection\\nNodes collect commitment messages distributed via the P2P gossip network and\\ncheck for any signs of misbehavior. In case valid evidence can be constructed,\\nit is submitted to the consensus layer. Any evidence parts that have expired\\nshould be discarded.\\n### Consensus Parameters\\n#### Roothash\\nThis proposal introduces the following new consensus parameters in the roothash\\nmodule:\\n- `max_evidence_age` (uint64) specifies the maximum age of submitted evidence in\\nthe number of rounds.\\n","tokens":83,"id":4352,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0012-runtime-message-results.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, the results of emitted runtime messages are `MessageEvent`s, which\\nonly provide information whether the message execution was successful or not.\\nFor various use-cases additional information about message results would be\\nuseful.\\nOne of such is supporting staking by runtimes. Currently, a runtime can emit an\\n`AddEscrow` message, but is unaware of the actual amount of shares it obtained\\nas a result of the added escrow. For some use-cases (e.g. runtime staking user\\ndeposited funds) this information is crucial for accounting.\\nSimilarly, for `ReclaimEscrow`, the runtime doesn't have the direct information\\nat which epoch the stake gets debonded.\\nThe only way to currently obtain this data is to subscribe to consensus events,\\nsomething which runtime doesn't have access to.\\nAdding results to `MessageEvent` solves both of the mentioned use cases:\\n- for `AddEscrow` the result should contain amount of shares obtained with the\\nescrow\\n- for `ReclaimEscrow` the result should contain the amount of shares and epoch\\nat which the stake gets debonded\\n","Decision":"Implement support for arbitrary result data in `MessageEvent` runtime message\\nresults.\\n","tokens":243,"id":4353,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"oasis-core\/0009-ed25519-semantics.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n> In programming, it's often the buts in the specification that kill you.\\n>\\n> -- Boris Beizer\\nFor a large host of reasons, mostly historical, there are numerous definitions\\nof \"Ed25519 signature validation\" in the wild, which have the potential to\\nbe mutually incompatible.  This ADR serves to provide a rough high-level\\noverview of the issue, and to document the current definition of \"Ed25519\\nsignature verification\" as used by Oasis Core.\\n","Decision":"The Oasis Core consensus layer (and all of the Go components) currently uses\\nthe following Ed25519 verification semantics.\\n- Non-canonical s is rejected (MUST enforce `s < L`)\\n- Small order A\/R are rejected\\n- Non-canonical A\/R are accepted\\n- The cofactored verification equation MUST be used (`[8][S]B = [8]R + [8][k]A`)\\n- A\/R may have a non-zero torsion component.\\n### Reject Non-canonical s\\nEd25519 signatures are trivially malleable unless the scalar component is\\nconstrained to `0 <= s < L`, as is possible to create valid signatures\\nfrom an existing public key\/message\/signature tuple by adding L to s.\\nThis check is mandated in all recent formulations of Ed25519 including\\nbut not limited to RFC 8032 and FIPS 186-5, and most modern implementations\\nwill include this check.\\nNote: Only asserting that `s[31] & 224 == 0` as done in older implementations\\nis insufficient.\\n### Reject Small Order A\/R\\nRejecting small order A is required to make the signature scheme strongly\\nbinding (resilience to key\/message substitution attacks).\\nRejecting (or accepting) small order R is not believed to have a security\\nimpact.\\n### Accept Non-canonical A\/R\\nThe discrete logarithm of the Ed25519 points that have a valid non-canonical\\nencoding and are not small order is unknown, and accepting them is not\\nbelieved to have a security impact.\\nNote: RFC 8032 and FIPS 186-5 require rejecting non-canonically encoded\\npoints.\\n### Cofactored Verification Equation\\nThere are two forms of the Ed25519 verification equation commonly in use,\\n`[S]B = R + [k]A` (cofactor-less), and `[8][S]B = [8]R + [8][k]A`\\n(cofactored), which are mutually incompatible in that it is possible\\nto produce signatures that pass with one and fail with the other.\\nThe cofactored verification equation is explicitly required by FIPS 186-5,\\nand is the only equation that is compatible with batch signature verification.\\nAdditionally, the more modern lattice-reduction based technique for fast\\nsignature verification is incompatible with existing implementations unless\\ncofactored.\\n### Accept A\/R With Non-zero Torsion\\nNo other library enforces this, the check is extremely expensive, and\\nwith how Oasis Core currently uses Ed25519 signatures, this has no security\\nimpact.  In the event that Oasis Core does exotic things that, for example,\\nrequire that the public key is in the prime-order subgroup, this must be\\nchanged.\\n","tokens":105,"id":4355,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/007-secrets.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way to store secrets that are used by our application.\\n","Decision":"We are going to use AWS SSM. We will store our secrets as Secure Strings.\\n","tokens":18,"id":4356,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/003-runtime.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to build a serverless application with a function that\\nreturns a `\"Hello World!\"` message. We need to pick a programming\\nlanguage.\\n","Decision":"We will use [Python3.8]\\n","tokens":35,"id":4357,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/008-deployment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way to deploy our application to AWS.\\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/005-test-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/004-serverless-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are building a `\"Hello World\"` application with AWS Lambda. What\\ntools or frameworks can we leverage?\\n","Decision":"We will use the [Chalice] framework to buiild our serverless\\napplication.\\n","tokens":27,"id":4360,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/001-record-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/006-access-control.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way to protect our app. Only a small number of people\\nshould be able to access the application. This includes the\\ndevelopers and the intended end users (i.e. you).\\n","Decision":"To protect our application, we will require all requests to include\\nan `Authorization` header containing a JWT. Any request that is missing\\nthis header will be rejected. Futhermore, the JWT will include an\\nexpiry so we can control the time period in which users can access\\nthe application.\\nThe authentication process will be implemented as an additional AWS\\nLambda function. In [Chalice], this is referred to as a Custom Authorizer\\n","tokens":46,"id":4362,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hello\/002-compute.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to build a `\"Hello World\"` application using one of the\\nfollowing approaches:\\n* **Containers** (i.e. K8S, ECS, EC2, etc.)\\n* **Serverless** (i.e. AWS Lambda)\\n","Decision":"We will use AWS Lambda for this particular assignment.\\n","tokens":56,"id":4363,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"firefox-tv\/adr-0003-mvvm-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe initial version of the codebase was built without a specific architecture in mind, and this has led to a few problems:\\n1. New UX flow proposals were incompatible with previous assumptions of how different views were bound together, and those were fragile and difficult to change.\\n1. Integrating large pieces of the android-components library caused many regressions, partly because state management is distributed throughout different parts of the UI.\\n1. Testing app state (especially the BrowserOverlay, or homescreen) was difficult because side effects of actions were spread out throughout the app, rather than being consolidated.\\n1. A formal architecture was needed to provide clarity for where to change\/add code as the number of people working with the codebase grows.\\nFormal architectures address these problems by explicitly decoupling the state and UI. The common ones are MVC\/MVP, MVVM, MVI.\\n","Decision":"We decided to use [Google's MVVM with Architecture Components](https:\/\/developer.android.com\/jetpack\/docs\/guide) because in addition to satisfying the other requirements of separating state and views, it also allowed for incremental change, which was important because switching to a new architecture would extend past a single sprint, and there was the possibility that we\u2019d need to respond to critical issues during the refactor. It\u2019s also a familiar and well-documented architecture, and integrates well with Android\u2019s `ViewModel` and `LiveData`.\\nWe also considered MVI, which would make testing easier because state is immutable, but MVI didn\u2019t allow for incremental change, and has a steeper learning curve. We also decided against MVP because it doesn\u2019t enforce very clear separations between model and presenter, which often leads to bloated presenters, and could complicate Android lifecycle management.\\n### Model\\n- Different Repositories hold state for components of the app old state\\n- Logic to handle actions that change the model\\n### ViewModel\\n- Observes changes in Repositories and updates views\\n- Calls actions exposed by Model from UI interactions\\n### Fragments\\n- Connect VM and Model during `Fragment#onViewCreated` by setting VM observers on the Model\\nStatus: Accepted\\n","tokens":183,"id":4366,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"commcare-hq\/0003-remove-warehouse-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe data warehouse was intended to house data for all CommCare HQ reports.\\nThe warehouse would replace Elasticsearch in almost all contexts that it is currently used.\\nThe migration began in 2017 with the Application Status report and the effort\\nto move the report to the warehouse and ensure it is stable, performs well and\\nprovides the same features as the ES-backed reports was much higher than\\nanticipated.\\n","Decision":"To reduce our infrastructure dependencies and focus our efforts on existing databases,\\nwe have decided to remove the warehouse and stop any efforts to iterate on it.\\nThis decision is not because we believe that the warehouse is a worse implementation than Elasticsearch.\\nThis decision is because we believe that with our current priorities, we will\\nnot be able to spend the appropriate amount of time to make the warehouse a\\nrobust solution for generic reports in the near future.\\nBecause no current reports are backed by the warehouse, it is an important time\\nto reconsider our approach and decide on what will be appropriate long term.\\nWhen there are more dedicated resources for generic reports, we believe that\\na warehouse-style approach should be considered when implementing.\\n","tokens":85,"id":4367,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"commcare-hq\/0002-keep-static-ucr-configurations-in-memory.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs part of the UCR framework configurations for data sources and reports\\nmay be stored in the database or as static files shipped with the code.\\nThese static files can apply to many different domains and even different\\nserver environments.\\nWhen a data source or report configuraiton is requested the static configuration\\nis read from disk and converted into the appropriate JsonObject class.\\nDuring some performance testing on ICDS it was noted that the process or\\nreadying the static configuration files from disk and converting them\\nto the JsonObject classes was taking up significant time (14% of restore\\ntime for ICDS).\\n","Decision":"To improve the performance (primarily of restores) it was decided to maintain\\nthe list of configurations in memeory rather than re-read them from disk\\nfor each request.\\nIn order to keep the memory footprint to a minimum only the static configurations\\nare kept in memory and not the generated classes. This also serves to ensure\\nthat any modifications that may get made to the classes do not persist.\\nThere are still some places that re-read the configurations from disk\\neach time but these not called in places that require high performance. An\\nexample of this is the UCR pillow bootstrapping.\\n","tokens":128,"id":4368,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"commcare-hq\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4369,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0006_using_buildpacks_on_paas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe chose GOV.UK PaaS to simplify our infrastructure and reduce our time to deploy and our ongoing maintenance overheads.\\nIn order to [maximize the support we receive from GOV.UK\\nPaaS](https:\/\/docs.cloud.service.gov.uk\/responsibility_model.html#standard-buildpack-responsibilities) during our\\ntransition, and ensure we do not engage in unnecessary clean-up work on our existing-poorly configured-docker\\ncontainers, we have decided to use\\n[buildpacks](https:\/\/docs.cloud.service.gov.uk\/deploying_apps.html#deploy-an-app-to-production) initially.\\n","Decision":"TVS will be run using buildpacks when it goes live on GOV.UK PaaS. In order to bring the infrastructure in-line with\\ndepartmental standards, this will be reviewed and changed to docker containers when resources and time permit.\\n","tokens":129,"id":4371,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0001_get_postcode_from_coordinates.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to get a postcode from the coordinates we get from the browser.\\n","Decision":"To use postcodes.io instead of geocoder gem and just make a simple AJAX call from the browser.\\n# Consequences\\nWe avoid creating an endpoint on the server, therefore reducing the load we have to manage. On the other side we rely on a service that is less trusted than Google Maps, but open source and based on Open Data.\\n","tokens":19,"id":4372,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0005_elasticsearch_to_algolia_migration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nElasticSearch (ES) is used as the search engine for:\\n- UI search for vacancies\\n- Querying vacancies to construct email job alerts based on a users criteria.\\nThey both currently use a ruby ES client library to submit queries. Both use a base client class(es) that search and job alerts functionality extend from.\\nCurrent search has been identified as not giving optimal search results and job alerts are based on very simplistic search rules currently. Is there a better tool to use that is more easily configurable?\\nCurrently indexing and maintaining ranking of search terms is an engineering task.\\n","Decision":"Algolia was identified an alternative as it:\\n- has easier\/improved 'out of the box' functionality yielding better search results\\n- is up to 200x faster than ES\\n- has a comprehensive dashboard UI that means rankings\/weightings could potentially be managed by wider range of people\\n- has a comprehensive toolset including UI components and libraries\\n","tokens":122,"id":4373,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0008_continue_to_use_Algolia_as_search_engine.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe current technical implementation of the Teaching Vacancies web application adopts Algolia as Search Engine to serve job search results to teachers. The development team migrated from Elastic Search to Algolia in April 2020. The reasons behind the migration are outlined in this document:\\nhttps:\/\/docs.google.com\/document\/d\/1SjmXcnjJyAuAE8WRPrGX7MRrn6UQAfOQyb9GjGsa43c\/edit?usp=sharing\\nA deeper analysis of TVS search requirements can be found in Search Analysis document:\\nhttps:\/\/docs.google.com\/document\/d\/1fEQINBHTjI-TqcDfZ6IEDpNAghRGsnQg0EGJoD4E5wg\/edit?usp=sharing\\nTeaching Vacancies conducted a review into which search engine option to take forward in April 2020 and Algolia was compared to Elastic Search. Despite limited technical justification, Algolia was selected as the preferred option to take forward, and the switch to Algolia standard plan was completed on the 18th of May 2020. Since that time a number of features closely linked to search have been introduced and improved.\\n","Decision":"Based on a technical and financial review of the search engines assessed, the recommendation put forward is that Teaching Vacancies should continue to use the Algolia standard plan, and not revert back to using Elastic Search.\\n","tokens":247,"id":4374,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0004_define_environments_after_gov_uk_paas_migration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe're migrating the Teacher Vacancy application to GOV&#46;UK PaaS and we want\\nto rethink and simplify our delivery process and what environments we're using\\nto aid development, testing and deployment.\\nThis is especially relevant as we are moving away from\\n[Gitflow](https:\/\/datasift.github.io\/gitflow\/IntroducingGitFlow.html) toward\\na simple usage of Git (refer to ADR003). Not having long-living branches we can utilise a simpler environments setup.\\nThe current setup is this:\\n* **testing** - user researchers to conduct usability testing sessions\\n* **edge** - devs to test or debug features, code and integrations in isolation\\n* **staging** - final quality assurance step and demos\\n* **production** - the live service\\n","Decision":"We will reduce the number of environment to:\\n* **staging** - used for User Acceptance testing, quality assurance testing and demos\\n* **production** - the live service, runs the latest version of the main branch\\nDevelopers will use local environments to test or debug features, code and integrations.\\nThe same applies to user researchers when conducting usability testing sessions.\\nHaving a staging environment is also handy to test builds and deployments before\\ngoing to production. Keeping a staging environment close to production reduces\\nthe risk of having unexpected issues when deploying a release.\\n","tokens":168,"id":4375,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0009_build_job_applications_rather_than_buy_cots.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTeaching Vacancies is evolving the set of features available to its users.\\nStarting from next year, jobseekers will be able to apply for jobs directly through the service. Delivering this new functionality requires a significant amount of change and opens up the question of whether we should opt for a COTS (commercial off the shelf) Application Tracking solution, or take on the development effort and build it ourselves.\\n","Decision":"Teaching Vacancies will build Job Application functionality rather than purchasing an off the shelf solution.\\n","tokens":86,"id":4376,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0007_use_devise_for_authentication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTeaching Vacancies will soon start to develop a new set of features which ultimately will allow job seekers to apply for jobs directly on the TVS website.\\nThis feature constitutes an important milestone for the service and will require job seekers users to create accounts to manage their job applications. Account functionality will permit teachers to initiate web sessions on TVS website as well as manage their account (password resets, email verification and account closing).\\n","Decision":"TVS will adopt Devise as authentication library.\\n","tokens":89,"id":4377,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"molgenis-ops-helm\/0001-use-adr-to-describe-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4379,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"molgenis-ops-helm\/0002-manage-user-permissions-in-fusion-auth.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4380,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"occurrent\/0004-mongodb-datetime-representation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRepresenting RFC 3339 in MongoDB when using the CloudEvent Jackson SDK _and_ allow for queries is problematic.\\nThis is because MongoDB internally represents a Date with only millisecond resolution (see [here](https:\/\/docs.mongodb.com\/manual\/reference\/method\/Date\/#behavior)):\\n> Internally, Date objects are stored as a signed 64-bit integer representing the number of milliseconds since the Unix epoch (Jan 1, 1970).\\nSince the Java CloudEvent SDK uses `OffsetDateTime` we'll lose nanosecond precision and the timezone if converting the `OffsetDateTime`\\nto `Date`. This is really bad since it goes against the \"understandability\" and \"transparent\" goals of Occurrent. I.e. if you create\\na CloudEvent with a OffsetDateTime containing nanoseconds in timezone \"Europe\/Stockholm\" you would expect to get the same value back on read\\nwhich is not possible if we convert it to a date.\\n","Decision":"We've thus decided to just store the `ZoneDateTime` as an RFC 3339 string in MongoDB. This means that range queries on \"time\" will be\\nhorrendously slow and probably not work as expected (string comparision instead of date comparision). The `EventStoreQueries` API\\ncurrently supports sorting events by natural order ascending\/descending which will be much faster (since it's using the timestamp of the\\ngenerated mongodb object id).\\nNote that sort options `TIME_ASC` and `TIME_DESC` are still retained in the API. The reason for this that we may allow customizations\\nto the serialization mechanism _if_ nanosecond resolution and is not required and timezone is always `UTC` in the future.\\n","tokens":201,"id":4381,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n","Decision":"None yet\\n","tokens":59,"id":4382,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"occurrent\/0006-mongodb-stream-version.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen doing a code review with [Per \u00d6kvist](https:\/\/github.com\/perokvist) he pointed out that the stream consistency\\nwas not implemented the way it normally seems to be in most event stores. Typically, each event would contain a version.\\nWhile this is probably not crucial for persisting the event (and maintaining consistency) a unique version or sequence number per\\nevent can be really beneficial when doing projections. For example if you want to create an integration event when event X is\\nreceived then you may way to read all events less than the version number of X. This also allows for different strategies\\nwhen it comes to waiting for all projections to update on write before returning to a client (using a lock that waits until\\nall projections have reached a certain version). A stream\/sequence version might also be required when implementing with\\ncertain event stores\/databases.\\n","Decision":"In light of this I've re-written the consistency guarantee to use compare the WriteCondition to the latest \"stream version\"\\nfor a particular stream. Only if that version fulfills the WriteCondition the new events are written. This greatly simplifies\\nboth the implementation, user configuration (no need for \"StreamConsistency\") and understandability.\\n","tokens":184,"id":4383,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"occurrent\/0003-add-streamid-to-cloudevent-when-writing-to-eventstore.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPreviously my thoughts regarding a \"streamid\" in the `CloudEvent` has been that since it's not visible to the user.\\nThe \"streamid\" extension property was added on write and removed on read to avoid surprises to the user.\\nHowever this leads to problems implementing snapshotting and sagas since then it's highly likely that we want to use the streamId\\nin these cases (and the user probably wants to know the stream id in a saga).\\n","Decision":"For this reason the implementors of the `WriteEventStream` api will _add_ a \"streamid\" to each `CloudEvent`.\\n","tokens":98,"id":4384,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"occurrent\/0005-mongodb-subscription-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA ResumeToken for MongoDB subscriptions is only available once the first document has been written to MongoDB. This is a huge drawback\\nin the following scenario:\\n1. The application is started with a subscription that listens to events.\\n1. This subscription contains a bug which prevents it from being executed correctly (i.e. it throws an exception).\\n1. Since the ResumeToken will not be persisted in this case it's lost! When the application is restarted the subscription will start at the current position and thus losing the event.\\n","Decision":"To avoid this the subscription must be a bit more complex. When a new subscription is started it reads the operation time from MongoDB and persists it\\nas the \"subscription position document\" for the subscription. This means that if the scenario described above where to happen the stream will be resumed from the\\noperation time and _not_ a resume token. Once the first resume token is persisted the stream will use the resume token position instead.\\nAlso see [stackoverflow](https:\/\/stackoverflow.com\/questions\/63323190\/get-resume-token-with-mongodb-java-driver-before-first-document-received-in-chan).\\n","tokens":110,"id":4386,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openfido-workflow-service\/0003-authentication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSeveral applications for OpenFIDO will require APIs with application level\\nauthentication and authorization. These include the pipeline API itself, its\\nworkers, the OpenFIDO services and web client, and the Blob service.\\nThese other services will want to import and use the application roles tables\\nand functions defined in this project for their own API endpoints, but might\\nnot the pipeline endpoints.\\nDocs:\\n* [OpenFIDO Db\/Service Model](https:\/\/app.lucidchart.com\/documents\/edit\/5dcaf4fa-7cad-4ce1-9275-ab86110fc2a6\/0_0?shared=true)\\n","Decision":"Create a separate 'application roles' package and set of models within this\\nproject that can be imported separately from the pipelines API.\\n- [x] Ensure that both logic to enforce a permission (a decorator enforcing a\\nrequirement of a specific set of SystemPermissions) is included in this\\npackage.\\n- [x] Models are included in such a way that they can be included in an existing\\nAlembic database schema (have one central 'db' that is configured by the\\nimporting app)\\n- [x] Create a setup.py file and reference flask project that imports this project\\n(basis example for other projects).\\n","tokens":135,"id":4387,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openfido-workflow-service\/0004-deployment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- AWS_ACCESS_KEY_ID\\n- AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":4389,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openfido-workflow-service\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4390,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nnts2\/001_choose_directory_schema.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nNNTS2 supports nested directories for every organization. What is the best schema\/database to represent this nested structure?\\n## Decision Drivers\\nThe ease of these operations\\n* View all directories for an org\\n* Get immediate parent\/child directories\\n* Add a directory in a another directory (parent)\\n* Move\/delete directories\\n* Using inherited permissions for directories\\n* Future migrations of data\\nNNTS2 use case is read-heavy than write-heavy. In most cases we are looking at immediate parent\/child relationships. But in case of inherited permissions, the queries become more complicated.\\n","Decision":"The ease of these operations\\n* View all directories for an org\\n* Get immediate parent\/child directories\\n* Add a directory in a another directory (parent)\\n* Move\/delete directories\\n* Using inherited permissions for directories\\n* Future migrations of data\\nNNTS2 use case is read-heavy than write-heavy. In most cases we are looking at immediate parent\/child relationships. But in case of inherited permissions, the queries become more complicated.\\nChosen __Adjacency trees__ with calculation of tree json in application layer. This is the best option for read\/write queries on immediate parent\/child directories.\\n### Positive Consequences\\n* Queries for only one level directories quite fast.\\n* Can be used in any type of relational database.\\n### Negative Consequences\\n* Will have to make sure that going from parent child rows --> tree json is efficient and fast.\\n","tokens":126,"id":4391,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"connaisseur\/ADR-4_modular.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWith the upcoming of [notaryv2](https:\/\/github.com\/notaryproject\/nv2) and similar projects like [Cosign](https:\/\/github.com\/sigstore\/cosign) the opportunity for Connaisseur arises to support multiple signing mechanisms, and combine all into a single validation tool. For that to work, the internal validation mechanism of connaisseur needs to be more modular, so we can easily swap in and out different methods.\\n","Decision":"We are going with this structure (**option 1.1.1.1**) due to the lack of other alternatives. It provides all needed information and the flexibility to use multiple validation methods, as needed.\\n##### 1.1.2 Sensitive values\\nIf we allow multiple validators that may contain different forms of sensitive values, i.e. notary credentials, symmetric keys, service principals, ..., they need to be properly handled within the Helm chart with respect to ConfigMaps and Secrets. Currently, the distinction is hard-coded.\\n##### Option 1.1.2.1\\nAdd an optional `sensitive([-_]fields)` field at the validator config top level. Any sensitive values go in there and will be handled by the Helm chart to go into a secret. Any other values are treated as public and go into the ConfigMap.\\nAdvantages:\\n- Generic configuration\\n- Could be used by potential plugin validators to have their data properly handled (potential future)\\n- Hard to forget the configuration for newly implemented validators\\nDisadvantage: If implemented in a `config = merge(secret, configmap)` way, might allow sensitive values in configmap and Connaisseur still working\\n##### Option 1.1.2.2\\nHard-code sensitive values based on validator type\\nAdvantages: Can do very strict validation on fields without extra work\\nDisadvantages:\\n- Helm chart change might be forgotten for new validator\\n- Helm chart release required for new validator\\n- Does not \"natively\" allow plugins\\n##### Decision\\nWe are going with **option 1.1.2.2** and hard code the sensitive fields, to prevent users from misconfigure and accidentally but sensitive parts into configmaps.\\n#### Image policy (1.2)\\nFor the image policy similar changes to the notary configuration have to be made.\\n##### Proposition\\nThe previous `notary` field in the image policy will be changed to `validator`, referencing a `name` field of one item in the validators list. Any additional fields, e.g. required delegation roles for a notaryv1 validator will be given in a `with` field. This will look similar to this:\\n```yaml\\npolicy:\\n- pattern: \"docker.harbor.io\/*:*\"\\nvalidator: \"harbor-nv1\"\\nwith:\\nkey: \"default\"\\ndelegations:\\n- lou\\n- max\\n- pattern: \"docker.io\/*:*\"\\nvalidator: \"dockerhub-nv2\"\\n```\\n##### Option 1.2.1.1\\nBesides the self configured validator, two additional validators will be available: _allow_ and _deny_. The allow validator will allow any image and the deny validator will deny anything.\\nAdvantages: More powerful than `verify` flag, i.e. has explicit deny option.\\nDisadvantages: More config changes for users\\n##### Option 1.2.1.2\\nStick with current `verify` flag.\\nAdvantages: Config known for current users\\nDisadvantages: No explicit deny option\\n##### Decision\\nWe are going with **option 1.2.1.1**, as we don't have to use additional fields and offer more powerful configuration options.\\n##### Option 1.2.2.1\\nWhen no validator given, default to deny validator.\\nAdvantages: Easy\\nDisadvantages: Not explicit\\n##### Option 1.2.2.2\\nRequire validator in policy config.\\nAdvantages: Explicit configuration, no accidental denying images\\nDisadvantages: ?\\n#### Decision\\nWe are going with **option 1.2.2.1** as it reduces configurational effort and is consistent with the key selection behavior.\\n#### Option 1.2.3.1\\nThe validators from option 1.2.1.1 (_allow_ and _deny_) will be purely internal, and additional validator can not be named \"allow\" or \"deny\".\\nAdvantages: Less configurational effort\\nDisadvantage: A bit obscure for users\\n#### Option 1.2.3.2\\nThe _allow_ and _deny_ validator will be added to the default configuration as `type: static` with an extra argument (name up for discussion) that specifies whether everything should be denied or allowed. E.g.:\\n```yaml\\nvalidators:\\n- name: allow\\ntype: static\\napprove: true\\n- name: deny\\ntype: static\\napprove: false\\n- ...\\n```\\nAdvantages: No obscurity, if user don't need these they can delete them.\\nDisadvantage: Bigger config file ...?\\n#### Decision\\nWe are going with **option 1.2.3.2** as we favor less obscurity over the \"bigger\" configurational \"effort\".\\n### Validator interface (Choice 2)\\nSee [validator interface](https:\/\/github.com\/sse-secure-systems\/connaisseur\/blob\/master\/connaisseur\/validators\/validator.py)\\nShould validation return JSON patch or digest?\\n#### Option 2.1.1\\n`Validator.validate` creates a JSON patch for the k8s request. Hence, different validators might make changes in addition to transforming tag to digest.\\nAdvantages: More flexibility in the future\\nDisadvantages: We open the door to changes that are not core to Connaisseur functionality\\n#### Option 2.1.2\\n`Validator.validate` returns a digest and Connaisseur uses the digest in a \"standardized\" way to create a JSON patch for the k8s request.\\nAdvantage: No code duplication and we stay with core feature of translating input data to trusted digest\\nDisadvantages: Allowing additional changes would require additional work if we wanted to allow them in the future\\n#### Decision\\nWe are going with **option 2.1.2** as all current and upcoming validation methods return a digest.\\n","tokens":98,"id":4394,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"connaisseur\/ADR-7_wsgi-server.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe were running the Flask WSGI application with the built-in Flask server, which is not meant for production. Problems are mainly due to potential debug shell on the server and single thread in default configuration. Both were mitigated in our setup, but we decided to test a proper WSGI server at some point. Especially the log entry\\n```text\\n* Serving Flask app 'connaisseur.flask_server' (lazy loading)\\n* Environment: production\\nWARNING: This is a development server. Do not use it in a production deployment.\\nUse a production WSGI server instead.\\n```\\ndid cause anguish among users, see e.g. [issue 11](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/11).\\n","Decision":"We chose option 1.2 and will for now go forward with Cheroot as the WSGI server. The decision was based on the server performing best in the relevant parts of the stress and load tests.\\n","tokens":164,"id":4396,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"connaisseur\/ADR-6_dynamic-config.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe configuration of validators are mounted into Connaisseur as a configmap, as it is common practice in the Kubernetes ecosystem. When this configmap is upgraded, say with a `helm upgrade`, the resource itself in Kubernetes is updated accordingly, but that doesn't mean it's automatically updated inside the pods which mounted it. That only occurs once the pods are restarted and until they are the pods still have an old version of the configuration lingering around. This is a fairly unintuitive behavior and the reason why Connaisseur doesn't mount the image policy into its pods. Instead, the pods have access to the kube API and get the image policy dynamically from there. The same could be done for the validator configuration, but there is also another solution.\\n## Problem 1 - Access to configuration\\nHow should Connaisseur get access to its configuration files?\\n### Solution 1.1 - Dynamic access\\nThis is the same solution as currently employed for the image policy configuration. The validators will get their own CustomResourceDefinition and Connaisseur gets access to this resource via RBAC so it can use the kube API to read the configuration.\\n**Pros:** Pods don't need to be restarted and the configuration can be changed \"on the fly\", without using Helm.\\n**Cons:** Not a very Kubernetes native approach and Connaisseur must always do some network requests to access its config.\\n### Solution 1.2 - Restart pods\\nThe other solution would be to use ConfigMaps for validators and image policy and then restart the pods, once there were changes in the configurations. This can be achieved by setting the hash of the config files as annotations into the deployment. If there are changes in the configuration, the hash will change and thus a new deployment will be rolled out as it has a new annotation. This corresponds to the [suggestion](https:\/\/helm.sh\/docs\/howto\/charts_tips_and_tricks\/#automatically-roll-deployments) made by Helm.\\n**Pros:** Kubernetes native and no more CustomResourceDefinitions!\\n**Cons:** No more \"on the fly\" changes.\\n### Decision Outcome (1)\\nSolution 1.2 was chosen, going with the more Kubernetes native way.\\n","Decision":"Solution 1.2 was chosen, going with the more Kubernetes native way.\\nSolution 2.2 was chosen as it is the more simpler of the two.\\n","tokens":458,"id":4398,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gp-redirect\/0002-use-referer-header-for-id-acquisition.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe mechanism for directing people to the new GP Profile pages is to use an\\nexisting system where a banner is displayed on the current page. From this\\nbanner the user is able to confirm they wish to see the new page by clicking on\\na button. The button is a link, able to be configured for a single static URL.\\nThe user needs to experience a seamless transition to the new page after they\\nhave clicked on the button. In order to do this the ID of the previous page\\nis required. Given the constraints of the existing system the `referer` header\\nis the only way that can be determined.\\n","Decision":"We will use the `referer` header to identify the ID of the GP profile page the\\nuser was previously viewing in order to redirect them to the new page.\\n","tokens":130,"id":4399,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gp-redirect\/0003-use-prometheus-for-exposing-metrics.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":4400,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rtk-companion\/0001-create-an-anki-addon.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nRather than using a user uploaded deck for RTK, I decided to create cards as\\nI go through the book. The user decks I went through sometimes would have cards in an\\nodd order or would just contain fields I care not about.\\nAlso, I wanted to make an Anki addon to exercise my rusty python.\\nCreating own cards though is quite involved. It usually goes something like this:\\n1. Lookup the kanji number in [Koohi](https:\/\/kanji.koohii.com\/).\\n1. Copy paste into Anki. Write the number and keyword in Anki.\\n1. Browse Koohi stories for inspiration.\\n1. Look up radicals in Jisho for inspiration.\\n1. Write the story in Anki.\\n1. Find stroke order image\/gif (usually done by taking a screenshot on [Jisho](https:\/\/jisho.org\/))\\n","Decision":"Chose to experiment with making an Anki addon housing all the information I was using to create\\nRTK notes displayed in a way that allows easy browsing between Kanji.\\nThe addon should create a window that displays a Kanji along with:\\n- Heisig number (Edition 6)\\n- Keyword\\n- List of radicals\\n- Stroke order gif and\/or image\\n- Shared stories on Koohi for kanji\\nLastly, it should have a button to add a new note to an existing deck for the currently viewed\\nKanji.\\n","tokens":197,"id":4403,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rtk-companion\/0002-define-goal-for-an-mvp.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhat will an MVP of this plugin look like and do.\\n","Decision":"Mvp will have:\\n- Data for 10 kanji\\n- User to click to view next and previous\\n- Has an input text field for the user story\\n- Has a button that once clicked will add a note to an existing deck\\nA Tool menu button must open a window to display the kanji data.\\n","tokens":19,"id":4404,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Conduit\/0002-solve-the-xunit-msbuild-problem-with-assembly-merging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have encountered probelms with the `xunit` task when run with `msbuild` (`xbuild works`) in that it is unable to locate required xunit assemblies.\\n","Decision":"This can be solved by copying the missing files to the same directory as `msbuild.exe` -- but this way may be easier to consume.\\nMerge the required assemblies into `Conduit.Adapters.Build.dll` or  `Conduit.Adapters.Targets.dll`\\n","tokens":41,"id":4405,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Conduit\/0003-try-the-xunit-target.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have created our own target, but it appears there is already one that might be worth trying. If this works then we don't need to reimplment unless we want fancy formatting.o\\n","Decision":"Give it a go.\\n","tokens":42,"id":4406,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Conduit\/0004-postpone-xunit-msbuild.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe xunit msbuild target does not work under xbuild and also we would like to be able to select files by glob pattern. The xunit target requires you know the name up front. In a CI environment these may differ, for example you may be unpacking an artifact archive.\\n","Decision":"Try implementing our own one that allows globbing but uses the internal from the xunit version\\n","tokens":62,"id":4407,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-digideps\/0004-user-emails-and-roles-are-immutable.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDue to the freedom given to users and admins in changing a user's email address or role, we've had issues identifying why people have certain permissions in DigiDeps. As well as being confusing, we believe this has led to security issues where a high-authority user has had their email changed in an attempt to reuse the account, thereby granting unreasonable access to the owner of the new email address.\\nThere are rarely good reasons to change a user's email address or role. Because very little in DigiDeps belongs directly to a user, these situtations can be resolved by deleting the original account and creating a new one with the correct permissions.\\n","Decision":"User email addresses and roles will henceforth be immutable: they are set when a new user is created and cannot subsequently be changed.\\nThere is one exception to this: organisation administrators will be able to switch users between team member and admin roles. This role switch is entirely internal to the organisation and doesn't affect any view\/edit permissions to clients and reports.\\n","tokens":136,"id":4409,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-digideps\/0003-use-aws-parameter-store-for-feature-flags.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to use feature flags in the service to allow us to easily enable and disable functionality. Feature flags should be easy to change, audited and able to take effect immediately.\\n","Decision":"We will use AWS Parameter Store to store our feature flags in a standardised format: `\/{environment}\/flag\/{flagName}`. This will make flags easy to identify, change and debug. Operators will have access to change the value of these flags.\\nResources inside our service can access feature flags either by having them passed in as environment variables, or by directly querying Parameter Store on-demand.\\nParameter Store values must be strings, so we will consistently use the values `0` (off) and `1` (on).\\n","tokens":40,"id":4410,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-digideps\/0002-use-amazon-aurora-for-application-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24\/7 for each environment.\\nWe have several environments which do not need to be highly available. This includes \"ephemeral\" development environments, the \"main\" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.\\n","Decision":"We will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.\\n","tokens":128,"id":4411,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-digideps\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4412,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"faculty-planner-django\/0001-get-current-month-course-date.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way of getting all courses for the current month, and year.\\n","Decision":"For May(2018) E.g:\\n```\\n[{\\ndate:\"2018-05-01\", course_dates:{...}\\n},{\\ndate:\"2018-05-02\", course_dates:{...}\\n},{\\ndate:\"2018-05-03\", course_dates:{...}\\n}, ...]\\n```\\n","tokens":20,"id":4413,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0008-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0002-use-aws-bare-metal-rig-approach.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n","Decision":"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: CodePipeline, with Jenkins as an eventual target\\n","tokens":24,"id":4417,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0004-security.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBookit needs the concept of a user so it can identify who booked what bookable and allow certain users to overrule a booking.\\nThis involves both authentication (who you are) and authorization (what you're allowed to do).  For the MVP we are primarily focused on Authentication.\\nEmployees from different domains (Wipro, Designit, Cooper, etc) need to log into the system.\\nProtect all endpoints, optionally allow \/ping but, if authentication provided, return profile information as well.\\n","Decision":"* We will use Azure AD v2 OpenID Connect to exchange an id_token for an opaque Bookit API opaque access_token\\n","tokens":106,"id":4418,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0010-jpa-manages-schema.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":4419,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":4420,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0009-use-jpa.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOriginally, we utilized a Spring's JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.\\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\\n","Decision":"* Use JPA to map objects into database tables\\n* Use Hibernate as the JPA implementation - Spring Boot's default\\n* Leverage Spring Data's JPA support to implement queries via Repository interface patterns\\n","tokens":166,"id":4421,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":4423,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":4425,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0003-use-junit-for-tests-instead-of-spek.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are a number of unit testing frameworks available for the JVM.  There are also some newer unit testing frameworks that are specific to Kotlin.  Spring currently (4.x) only supports JUnit 4 and TestNG.  JUnit 5 can be made to work however.\\n","Decision":"Use JUnit 5 for all unit and e2e tests.  This will simplify thing and has better integration currently with IntelliJ IDE.\\n","tokens":61,"id":4426,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\n","Decision":"We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\n","tokens":27,"id":4427,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Shelter\/0002-using-proptypes-over-typescript.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe use of TypeScript would in turn provide a more stable codebase.\\nFor this particular project since the vast majority of components will receive either strings, objects or arrays of strings as props and have no state due to the fact they just render externally provided content.\\nWe feel that in this use-case adding the complexity, additional dependencies, increased on-boarding and higher risk of incompatibilities the benefits do not out weigh the overhead, with PropTypes being sufficient given the static nature of the website.\\nTypeScript would increase the complexity of the codebase and the time needed to on-board a developer.\\n","Decision":"We will use PropTypes\\n","tokens":124,"id":4429,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Shelter\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4430,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Shelter\/0003-using-styled-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing either plain CSS or CSS Modules instead of using Styled Components.\\nOne main driver for not using styled components was that it could be more portable in the future, but there was no real use case for this.\\nA limitation of CSS modules is the inability to pass values to CSS. CSS modules would potentially have to use a mix of inline styles and classes to achieve this.\\n","Decision":"We will use Styled Components\\n","tokens":80,"id":4431,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0020-we-will-put-verified-status-inside-json-object.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen receiving a User Account Creation response from the MSA, the response contains the attributes required for account creation.\\nA User Account Creation SAML object contains *_VERIFIED attributes which are siblings of the attribute they reference.\\n","Decision":"The Verify Service Provider will require the service to have requested both an attribute and its\\ncorresponding verified attribute or it will error. This is already enforced by the integration environment form.\\nThe JSON the VSP returns will group the attribute value and verified status together.\\n","tokens":48,"id":4432,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0010-we-will-keep-the-config-as-simple-as-possible.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nExisting verify configuration for things like metadata can be quite\\nintimidatingly complex. We'd like the application to be as simple as\\npossible to configure.\\n","Decision":"We'll try to make the config have sane defaults wherever possible\\nso that the user doesn't have to specify things they shouldn't care\\nabout.\\nThe user shouldn't have to specify things we don't use (e.g. truststorePath).\\n","tokens":36,"id":4433,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0009-we-will-not-use-a-dependency-injection-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe're writing a really small project and we don't think that a\\nframework like Guice will provide enough benefit to outweigh the cost.\\n","Decision":"We will use constructor injection, but we won't use a DI framework. We'll use a\\nfactory object instead.  We'll avoid calling new inside constructors where\\npractical.\\n","tokens":32,"id":4434,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0022-we-will-not-validate-the-recipient.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe subject confirmation data on assertions has a recipient attribute, which the MSA sets to the assertion consumer service url. The assertions also have an audience restriction, which is set to the service entity id.\\nThe verify service provider, conceptually, should not need to know the assertion consumer service url - as this ties somewhat to the relying party's infrastructure, and makes it difficult to change. Knowing it would also lead to more complicated configuration for relying parties as we introduce multi tenancy to the verify service provider.\\nIn usual usages of SAML, the recipient should be validated, as should the audience restriction (if present), and these might be different and have slightly different meanings. However, since we control the production of this SAML (via the MSA), we know that the audience restriction will always be present and set to the service entity id, so validating this will mitigate any potential issues from not validating the recipient.\\n","Decision":"The verify service provider will not validate the assertion consumer service url.\\n","tokens":189,"id":4435,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0014-html-will-not-be-generated-by-the-verify-service-provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs well as the SAML AuthnRequest something needs to render an HTML form to handle\\nthe \"SAML Post Binding\". This form contains the SAML request, the relay state, and\\noptionally any \"hints\" the relying party needs to send the hub.\\nThis form could either be generated by the verify-service-provider, the client library\\n(e.g. passport-verify), or the service itself.\\n","Decision":"The HTML form will be generated by the client library.\\n","tokens":88,"id":4437,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0019-we-will-validate-assertion-details.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAssertions are generated by the MSA, and they look like:\\n```\\n<saml2:Assertion ID=\"_95dc6950-68ff-4ad7-b062-c7ff44998386\" IssueInstant=\"2017-08-01T15:21:20.087Z\" Version=\"2.0\">\\n<saml2:Issuer Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:entity\">http:\/\/www.test-rp-ms.gov.uk\/SAML2\/MD<\/saml2:Issuer>\\n<ds:Signature>...<\/ds:Signature>\\n<saml2:Subject>\\n<saml2:NameID Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\">expected-pid<\/saml2:NameID>\\n<saml2:SubjectConfirmation Method=\"urn:oasis:names:tc:SAML:2.0:cm:bearer\">\\n<saml2:SubjectConfirmationData InResponseTo=\"request-id-386f467d-85c1-4c71-b3fc-cdc3739682b1\" NotOnOrAfter=\"2017-08-01T16:21:20.087Z\" Recipient=\"http:\/\/rp-endpoint.com\"\/>\\n<\/saml2:SubjectConfirmation>\\n<\/saml2:Subject>\\n<saml2:Conditions>\\n<saml2:AudienceRestriction>\\n<saml2:Audience>rp-entity<\/saml2:Audience>\\n<\/saml2:AudienceRestriction>\\n<\/saml2:Conditions>\\n<saml2:AuthnStatement AuthnInstant=\"2017-08-01T15:21:20.092Z\">\\n<saml2:AuthnContext>\\n<saml2:AuthnContextClassRef>urn:uk:gov:cabinet-office:tc:saml:authn-context:level2<\/saml2:AuthnContextClassRef>\\n<\/saml2:AuthnContext>\\n<\/saml2:AuthnStatement>\\n<\/saml2:Assertion>\\n```\\nElements such as `Conditions` specify restrictions on where the assertion\\nshould be treated as valid. Relying parties SHOULD validate these things.\\n","Decision":"We will validate any details that the MSA currently provides. If we see\\nelements we're not expecting we'll throw.\\nWe'll validate:\\n* There must be exactly one `SubjectConfirmation`\\n* There must be exactly one `NameID` inside `Subject`\\n* The `SubjectConfirmationData` method MUST be `...bearer`\\n* If there's a `NotBefore` on the `SubjectConfirmationData` then it MUST be valid\\n* There MUST be a `NotOnOrAfter` on the `SubjectConfirmationData` and it MUST be valid\\n* `InResponseTo` MUST be present on the `SubjectConfirmationData` and it MUST be valid\\n* `Conditions` MUST be present\\n* If `Conditions` has `NotBefore` or `NotOnOrAfter` they MUST be valid\\n* `Conditions` MUST NOT contain any `OneTimeUse` or `ProxyRestriction` elements\\n* There MUST be exactly one `AudienceRestriction` and it must match our `entityID`\\nWe'll ignore:\\n* Recipient - this concept is already checked by AudienceRestriction, and it's hard for the\\nVSP to check whether it's correct because we don't know what URL we're running behind.\\n","tokens":484,"id":4438,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0007-we-will-document-a-strawman-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n","Decision":"We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\\n","tokens":32,"id":4439,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0018-we-will-have-hubssolocation-as-a-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVerify Service Provider needs to know the SSO location for Verify Hub in order\\nto generate a valid AuthnRequest.\\nHub Metadata does not provide SSOLocation because if it would, it would have to\\ndescribe Hub as an IDP and therefore provide signing certificates. Having signing\\ncertificates in the Hub metadata makes Relying Parties trust assertions signed by\\nhub, which would break the security contract between the Matching Service Adapter\\nand the Hub.\\nMatching Service Adapter does provide Hub SSOLocation in its metadata. However,\\nthe value is directly copied in from the Matching Service Adapter configuration.\\nMatching Service Adapter itself does not use the value, therefore we do not\\ntrust this feature to remain as part of the Matching Service Adapter.\\n","Decision":"We will provide the hub sso-location as a configuration option instead of parsing\\nit from the msa-metadata.\\n","tokens":159,"id":4440,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0012-we-will-use-the-full-profile.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVerify's SAML profile specifies that Responses and Assertions should be signed.\\nResponses should be signed by the Verify Hub and Assertions should be signed by\\nthe Matching Service Adapter.\\nThis profile causes problems with some off-the-shelf SAML service providers,\\nwhich can't handle multiple signatures from different keys in the same message.\\nAs a workaround, Verify introduced a \"simple\" profile where we do not sign Responses.\\n","Decision":"We will use the \"full\" profile, not the \"simple\" profile. The hub will sign responses\\nand the service provider will validate them against the hub's metadata.\\n","tokens":89,"id":4441,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0026-development-mode-for-the-verify-service-provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nInteracting with the Compliance Tool (CT) is not a straightforward process and the API could be better.\\nWhen a user is using the VSP for the first time it can be frustrating having to make sense of the CT API in order to make progress.\\nIn order to get the VSP running with the CT so that can start development they need to:\\n1. Create a set of private keys and certificates\\n2. Configure the VSP to interact with the CT and use the newly generated keys\\nand certificates\\n3. Create a script that can initiate a session with the CT\\n4. Create a client that can interact with the VSP\\n5. Test the interaction between the client, VSP, and CT\\n","Decision":"The VSP will provide a development mode initialized by a Dropwizard command.\\nWhen starting in this mode the VSP will:\\n- create in-memory keys and self-sign certificates\\n- use a random UUID for an entity ID\\n- configure itself to read the CT's metadata\\n- configure itself to run in non-matching mode\\n- initialize a new session with the compliance tool.\\nA user of the VSP will be able start development mode by running the following\\ncommand from the shell:\\n```\\n.\/bin\/verify-service-provider development\\n```\\nThe user should also be able to run this mode on Windows.\\nThe command will provide additional options to the user so that they can\\ncontrol:\\n- the host the VSP will bind to\\n- the port the VSP will run on\\n- the location where the CT will send SAML responses to\\n- the contents of the matching dataset that the CT will use in SAML responses\\nIn order to simplify the implementation of this mode the behaviour will only be\\navailable to users of the 'non-matching' journey.\\n","tokens":151,"id":4444,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0011-we-will-use-a-secure-token-to-validate-inresponseto.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA SAML relying party must check that responses it receives have an InResponseTo\\nattribute that matches the ID of a request that the relying party sent for this\\nuser.\\nSince we're trying to keep the verify service provider as stateless as possible\\nwe don't want it to have to remember which IDs belong to which users. Instead\\nwe'd like to delegate this behaviour to the client of the service provider\\n(i.e. the service itself), which will have its own session store.\\nIf a relying party's session store was insecure (for example and unsigned\\nsession cookie) then an attacker could circumvent the InResponseTo check by\\nsetting the value in session to something else. To mitigate against this attack\\nthe verify service provider could pass the service back a HMAC of the ID\\n(either alongside the ID or instead of it).\\n","Decision":"We won't implement this for now because initially we're happy that our relying\\nparties will store the session securely. We may want to implement it later as\\nan extra level of security.\\n","tokens":176,"id":4445,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0002-how-do-we-secure-the-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":4446,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0004-users-will-be-able-to-provide-relay-state.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn SAML RPs can provide some extra data along with the request. This is\\ncalled RelayState. Some existing RPs use this, but we're not sure what\\nthey use it for.\\nWe're not aware of any need for the service-provider to use relay state itself.\\n","Decision":"Users will be able to specify whatever relay state they want to and it will be\\nprovided in the response.\\n","tokens":64,"id":4447,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0017-we-will-pass-the-request-id-to-the-client.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVerify will send the same request ID that it sees in the AuthnRequest to the\\nMSA when it does matching.  If the service is able to see the request ID in a\\nconvenient form then they are able to correlate MSA attribute query requests\\nwith AuthnRequests they've sent.\\nWe also need to be able to validate that the InResponseTo attribute on the\\nResponse matches the ID we sent in the AuthnRequest. So long as the service is\\nable to store the ID in a secure way (i.e. without the possibility of a\\nmalicious user changing it in their session) it should be fine for them to send\\nthis back to the service to validate directly.\\nThere was also discussion about sending an HMAC of the ID back to the client be\\nstored in session - this would mitigate against attacks due a relying party\\nstoring the ID insecurely.\\n","Decision":"We'll return the request ID alongside the AuthnRequest when we generate it and\\nask for it when translating Responses.\\n","tokens":188,"id":4449,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0021-we-will-use-http-200-for-valid-saml.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen communicating with the Verify Service Provider API, we need to decide what status code to respond with\\nfor correctly formatted SAML that represents some kind of authentication failure (eg. NO_MATCH).\\n","Decision":"Any valid SAML will return a 200 OK response and can be deserialized as a <code>TranslatedRepsonseBody<\/code>.\\nWe will have to define an enum of possible SAML outcomes (<code>Scenario<\/code>) as we can't use HTTP codes\\nInvalid JSON\/SAML or internal errors will use a relevant, different HTTP status code.\\n","tokens":42,"id":4450,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0015-we-will-depend-on-a-minimal-set-of-verify-saml-libs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVerify have a number of pre-existing (closed source) libraries at various\\nlevels of abstraction for handling SAML behaviour. By depending on these\\nwe can benefit from all the work that's already been done making these easy\\nto use and secure by default.\\nThere's a plan in the medium term to open source a lot of these libraries.\\nBy depending on as few of them as possible we should be able to call the\\nverify-service-provider \"fully open\" sooner.\\nSome of the libraries at higher levels of abstraction are of questionable\\nvalue - although they make it easy to be consistent with the rest of verify\\nthey abstract away the exact nature of the SAML which makes the code hard to\\nread.\\n","Decision":"Good libraries that we should use for now:\\n* saml-serialisers\\n* ida-saml-extensions\\n* saml-security\\n* saml-metadata-bindings\\nLibraries we think we should ignore\\n* hub-saml\\n","tokens":151,"id":4451,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0024-release-process-for-verify-service-provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe verify-service-provider will be used by number of relying parties (RPs).\\nCurrently to distribute the artifact to RPs we upload them manually to some server which is not a secure way of doing it.\\nSo we wanted to automate the release process for verify service provider and make it more secure by signing the artifact.\\nThis will make RPs confident that they are not running any compromised software in their infrastructure.\\n","Decision":"We decided to follow the below release process for verify service provider:\\n* During development, maintain #NEXT release notes :\\n- Update the release notes with any breaking changes \/ new feature \/ anything important that RPs should know about\\n* When we decide to release the following steps have to be followed :\\n- Finalise the release notes\\n- Decide appropriate release number\\n- Include diff link in release notes\\n- Commit, tag it with release number and push release notes to master\\n- The release jenkins job should be triggered and if it passes, it should produce an artifact\\n- Ask for approval (in Google \/ Slack)\\n- Release approver should check the artifact to ensure its proper one and sign it using the certificate\\n- Approver should also upload the signed artifact to github releases\\n- Jenkins job to check if release artifact has been signed. If not, should send email to appropriate people\\n- If release artifact is proper it should send an email to the RP's regarding new VSP.\\n","tokens":88,"id":4452,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0005-sp-will-generate-request-id.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAuthnRequests contain an ID attribute the value of which will be sent back in\\nthe Response as an \"InResponseTo\" attribute.\\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\\n","Decision":"The service provider will generate a random GUID to use as the AuthnRequest ID.\\n","tokens":62,"id":4453,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0016-we-will-have-a-healthcheck-endpoint.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn various user research sessions we've observed users start the MSA or the verify service provider\\nand then want to check whether it's working correctly. There's also a need for users to be able to\\nmonitor the health of the system once it's deployed to their environment.\\nDropwizard allows you to configure an HTTP endpoint as a healthcheck. This can perform some arbitrary\\nactions that check the health of the system.\\n","Decision":"We will have a healthcheck endpoint that will check the verify-service-provider can read metadata from\\nthe hub and the MSA.\\n","tokens":90,"id":4455,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0003-use-files-to-store-private-keys.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\\ndecrypt Response Assertions.\\nThey will need to provide these to the verify-service-provider in some, reasonably\\nsecure way. Different users may have different opinions on how best to do this.\\n","Decision":"Initially we'll use files for this.\\nWe chose not to use environment variables because they're visible to other processes.\\nWe chose not to use a more complicated solution because it would be more complicated.\\n","tokens":62,"id":4456,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-service-provider\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4457,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0010-queryservice-tarball.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFor the building of the queryservice docker image we are currently using the versioned release tarballs [published by the WMF](https:\/\/archiva.wikimedia.org\/repository\/releases\/org\/wikidata\/query\/rdf\/service).\\nThis package already contains all the required components to build the docker image and we don't see any reason to alter or publish these releases further.\\n","Decision":"Do not produce a queryservice tarball to be published.\\n","tokens":77,"id":4458,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0002-tarball-hosting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to determine a place to host our new release artifacts (tarballs). Currently releases are being served by the Extension Distributor and the release branches of the git repositories.\\n","Decision":"- Wikibase release artifacts will be hosted on the WMF-controlled domain https:\/\/releases.wikimedia.org\/.\\n","tokens":39,"id":4459,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0015-security-fixes-non-wmde-software.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWMDE needs to decide on a process for including security fixes of non-WMDE maintained software (e.g. Mediawiki, Elasticearch, WDQS) as well as WMDE maintained software (Wikibase, WDQS-frontend) and how to publish security fix releases of Wikibase suite in such cases.\\nSecurity fixes for WMDE maintained software will fall under the maintenance category and will most likely be worked on by campsite, however as the running of the pipeline and publishing of docker images requires special privileges it could make sense that this step is tasked to a certain group of people or that all engineers of WMDE are given the rights to publish releases.\\nApplying security fixes to the running Wikibase instance (or other Wikibase related software system) is a time-sensitive operation. Therefore WMDE will pre-announce the upcoming security release a bit in advance before making the fixes (hence also vulnerabilities) public, to allow maintainers of Wikibase instances to plan for their updates.\\n","Decision":"- ANY component included in the wikibase suite (docker images, tarballs) that has a security update available can generate a new release. It is up to the team to determine it's severity and if a new release is required.\\n- ALL MediaWiki maintenance releases should generate a new Wikibase release.\\n- Add some kind of mechanism that regularly checks what the latest stable MediaWiki version is. If the version has changed we should do a release.\\n- Add a link to the [bug reporting documentation](https:\/\/www.mediawiki.org\/wiki\/Reporting_security_bugs) on the [Wikibase landing page on mediawiki.org](https:\/\/www.mediawiki.org\/wiki\/Wikibase)\\n","tokens":207,"id":4463,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0017-using-mediawiki-docker-image.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDuring the first iteration of development on the wikibase release pipeline, one of the goals was to build and run it against the master branches of MediaWiki, Wikibase and other bundled extensions ([T270133](https:\/\/phabricator.wikimedia.org\/T270133)).\\nBecause of the lack of such docker images at the time the team decided to create our own, inspired by the work of the official docker images. The benefits of this decision was seen when previously untested parts of Wikibase (Multi Wiki testing client\/repo ) now had some coverage on the master branch. During the development of the pipeline several issues were detected by using these custom docker images, sometimes the pipeline would breakdown days before a bug report would appear on phabricator.\\nThis can be useful but also comes with some additional drawbacks that can affect the maintainability and the quality of the releases WMDE will produce.\\n- To offer the same quality and security as the official Mediawiki docker images we now also have to maintain our own rather than building upon what already exists.\\n- Any updates or security fixes to these images are probably also more likely to be identified and patched in the official MediaWiki docker images quicker than in any image maintained by WMDE.\\n- The MediaWiki docker images are battle proven with 10+ Million downloads, our custom images are not.\\nAs the priority of the release pipeline should be to provide stable and secure releases it could make sense to revert this initial decision of building our own image.\\nThe decision to adopt parts of the testing done in the release pipeline for Wikibase CI is still pending. Depending on the outcome of [T282476](https:\/\/phabricator.wikimedia.org\/T282476), custom images could then be required again and could serve as a base when used for testing in CI where the requirements for security or performance aren't as high ([T282479](https:\/\/phabricator.wikimedia.org\/T282479)).\\n","Decision":"- Use the official MediaWiki docker images as a base for the Wikibase base and bundle images.\\n","tokens":399,"id":4464,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0016-updating-non-wmde-software.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWikibase Suite includes a number of software components that are not maintained by Wikimedia Deutschland (WMDE). Those include (examples as of May 2021):\\n- Software which is maintained by the Wikimedia Foundation (WMF), e.g. Wikidata Query Service (WDQS); in case of docker images also MediaWiki software.\\n- Broadly-used software which is maintained by non-Wikimedia parties, e.g. Elastic Search.\\n- Wikibase\/Wikidata-specific software which is maintained by Wikimedia community (volunteer) developers, e.g. Quick Statements.\\nAll these software components receive updates from their maintainers in a way which is not necessarily synchronized with the release cycle of Wikibase Suite. Users of Wikibase Suite might benefit from those changes, in particular from the updates which fix the incorrect functionality (bugs), and issues related to the software security.\\nOffering updates to software components not maintained by the WMDE that are known to compatible with the rest of the Wikibase Suite would require applying updates to versions of the components included in the Wikibase Suite, possible changes to Wikibase Suite structure (e.g. configuration) when applicable, and necessary testing procedures to ensure continued functionality of the Wikibase Suite with the updated components.\\nThere might also be changes desired by the users of Wikibase Suite to be made to these software components (e.g. new functionality, bug fixes) that might be not be provided by the maintainers in subjectively-perceived timely manner.\\nMaking changes to the software not maintained by WMDE would mean additional effort for WMDE's software developer teams, and likely require WMDE development teams gather additional expertise to be able to make good contributions to those software components.\\n","Decision":"To ensure the security and integrity of systems running Wikibase Suite, WMDE will be releasing updated versions of the Wikibase Suite that will include fixes to significant security issues discovered in the software components not maintained by WMDE, once those fixes have been published by respective maintainers.\\nOccassionally, WMDE might also publish releases of Wikibase Suite that will include updates to the components not maintained by WMDE that bring important additional functionality.\\nIn order to maximize the impact WMDE software development teams can bring for the users of Wikibase Suite, WMDE will generally not be making changes to the software components of the Wikibase Suite that are not maintained by WMDE.\\n","tokens":349,"id":4465,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0005-release-notes-process.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs we adopt the new release strategy we also want to determine a process for writing and maintaining release notes.\\nOur release process will still be closely bound to the release branches WMF are using it makes sense to inspect their process for producing and maintaining release notes within the source control system.\\nFor mediawiki\/core the release notes are maintained and worked on within the source repository. Each release branch contains a RELEASE_NOTES-N.NN document describing changes that was made to the software up until the point the branch was cut from master. Any backports to these branches also comes with an update to the release notes document.\\nAs a new release branch is cut\/created a new [template] release document is added to the master branch and any previous release notes are merged into a [HISTORY] document within the repository containing all previous release notes.\\n","Decision":"Release notes within the Wikibase MediaWiki extension repository will adopt a similar process to the one being used by the MediaWiki\/core developers.\\n","tokens":174,"id":4468,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0011-wikibase-bundle.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA key part of release pipeline is to produce a bundled wikibase docker image prepared with the extensions and components known as the \"wikibase suite\". This bundle image will consist of the build artifacts and as described in [ADR0001](0001-docker-image-repository.md) these images should be published to dockerhub.\\nIn previous wikibase docker artifacts WMDE has offered a \"base\" and a \"bundled\" version of Wikibase where the base version only contain mediawiki and Wikibase. The new pipeline should still produce and publish these artifacts.\\nAs we publish our releases we also need to make a decision if we want this bundle version to be available in a tarball for the user to install manually. In a daily today this topic was discussed and the team decided against it.\\n","Decision":"The wikibase release pipeline will not produce a bundled tarball to be published.\\n","tokens":167,"id":4469,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0009-non-WMDE-release-notes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe intend to package and release software that is not maintained by WMDE. For example, the Wikidata Query Service (WDQS).\\nThis software comes from a variety of sources including software that is used and maintained by the WMF in Wikimedia production Wikis but also some from complete third parties.\\nSome of this software including ElasticSearch and WikibaseLocalMedia already have curated release notes for versions.\\nOther software such as WDQS and Mediawiki extensions do not have release notes. They may have notable changes documented either in git commit messages or in phabricator tickets linked to those commits. It could be possible to computationally extract a compile these with some effort.\\nSoftware such as QuickStatements may prove difficult to build release note from git. It may require inspecting the code changes by eye.\\nCompiling release notes for software we do not maintain would add a significant maintenance burden.\\n","Decision":"We will not write custom release notes for software that we do not maintain.\\nWe will attempt to forward already curated release notes from upstream maintainers.\\n","tokens":185,"id":4470,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0001-docker-image-repository.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently the Wikibase docker images are built on travis and deployed to Dockerhub.\\nThere are a number of existing docker repositories we could consider to host our images. We need not pick only one repository but for the purposes of documentaion and clarity we probably want a single canonical one then other repositories could then be \"syndicated\" from the canonical one.\\nSome key properties of existing registries were considered in the table below:\\n| Repository | Free to WMDE | Self-Service (1) | Tooling provided for a built-in pipeline | Visibility of built images (2) | Possibility of replication to repository | Restrictions on use of non-Wikimedia images |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Dockerhub | Currently (3) | Yes | Static builds from dockerfile | High | Yes | no |\\n| Github Container registry | Currently | Yes | Github Actions | Medium | Yes | no |\\n| Google Container Registry | No (pay for storage and egress) | Yes | Google Cloud | Medium | Yes | no |\\n| AWS Container registry | No (pay for storage and egress) | Yes | Amazon Cloud | Medium | Yes | no |\\n| Azure Container Registry | No (some complex structure) | Yes | Azure Container Registry Tasks | Medium | Yes | no |\\n| WMF Docker Registry | Yes | No (negotiation needed) | [https:\/\/wikitech.wikimedia.org\/wiki\/PipelineLib](https:\/\/wikitech.wikimedia.org\/wiki\/PipelineLib) | Low | No (probably not) | Yes (only audited versions of specific images are allowed) (4) |\\n1. We can create new images and names without filing a ticket for speaking to people\\n2. Approximate fraction of the market of docker pulls that happens here\\n3. Dockerhub introduces limitations in use - but WMDE will likely be entitled to a free unlimited \"open source\" plan\\n4. Potentially WMDE could get their space of the registry with more loose restrictions (discussion ongoing)\\n","Decision":"We will publish Docker release images to Dockerhub. Dockerhub is the goto location for publishing Docker images. We do not consider publishing Wikibase Docker images to WMF Docker Registry yet, as its purpose is to hold images to be used in Wikimedia production infrastructure. This might change once Wikibase (i.e. MediaWiki and extensions) will be deployed using container images - not the case as of December 2020.\\n","tokens":430,"id":4471,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikibase-release-pipeline\/0004-wdqs-tarball-content.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUnlike in the case of Mediawiki extensions, where the extension code is packaged as a tarball to be installed in end user's environment, there is no immediate idea of how to package the Query Service (in particular as it actually involves a number of software components). Components included [Query Service backend] (including the WMF Blazegraph fork) and [Query Service UI]. Should these be bundled as separate tarballs? Or as a single one?\\n[Query Service backend] releases is currently [built and published](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/+\/refs\/heads\/master\/dist\/) as a [service zip archive] which contains the necessary components including the built-in UI of blazegraph.\\nThe [Query Service UI] has at one point been part of this build process but the two components have recently been [separated](https:\/\/phabricator.wikimedia.org\/T241291).\\n","Decision":"As the [Query Service backend] does not depend on the [Query Service UI] and they are currently two separate components there is no need to alter the structure of the [service zip archive] as no obvious benefit of doing so can be identified.\\n","tokens":199,"id":4473,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mbed-tools\/0002-move-mbed-config-header-defines-to-cmake.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n`mbed_config.h` was created because the command line length limit on older versions of Windows prevented passing many \"-D\" defines to the compiler and linker.\\nAdditionally, passing many \"-D\" defines globally is a bad idea in itself. Therefore, `mbed_config.h` was a workaround to make a bad practice work.\\nOn modern versions of Windows, the command line limit isn't an issue. CMake can also do what's necessary to prevent exceeding the command line length limit where needed.\\n","Decision":"We will remove the generation of `mbed_config.h` and simply pass the \"-D\" defines using CMake directives. This is acheived by moving the \"-D\" definitions to the tools generated `mbed_config.cmake`.\\n","tokens":106,"id":4475,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mbed-tools\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4476,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0004-transformers-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nWe can develop event transformers either using the \u201cBackend\u201d architecture\\nor \u201cProcessor\u201d architecture. Making the transformers \u201cbackends\u201d will result\\nin relatively more nesting in the configurations as this \u201cbackend\u201d will have\\nits own configurations.\\nIf we decide to develop the event transformers as \u201cprocessors\u201d, it will result\\nin less complexity in the code since the transformer can be easily appended in\\nany backend\u2019s (router or logger) processors\u2019 pipeline.\\nDecision\\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","Decision":"--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","tokens":188,"id":4478,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0001-purpose-of-this-repo.rst","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\n`OEP-26 <https:\/\/open-edx-proposals.readthedocs.io\/en\/latest\/oep-0026-arch-realtime-events.html>`__\\nconsists of the following components:\\n-  Asynchronous Routing Backend\\n-  Regular-expressions based filter processor\\n-  IMS Caliper transformer\\n-  xAPI transformer\\n-  Router to forward events\\nKeeping all of these components in one repo will make the repository\\nunnecessarily tangled since these additional components are not required\\nby the core app for its functionality.\\nDecision\\n--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https:\/\/github.com\/openedx\/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n","Decision":"--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https:\/\/github.com\/openedx\/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n","tokens":306,"id":4479,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0002-binding-app-for-caliper.rst","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nWe need to decide some approach to encapsulate and structure the Caliper\\nevents. We have the following options for binding caliper events:\\n-  Make our own data structure to store a caliper event (e.g. a python dict)\\n-  Use IMS\u2019 `caliper-python <https:\/\/github.com\/IMSGlobal\/caliper-python>`__ library\\nDecision\\n--------\\nWe will be using python dictionaries for processing and encapsulation of\\ntransformed events to keep things simple.\\nConsequences\\n------------\\nPython dictionaries will be used for binding Caliper events. These\\nevents stored as python dictionaries can be converted into JSON later\\nand sent to interested consumers.\\nRejected Alternatives\\n---------------------\\n**Use IMS\u2019 caliper-python library**\\nThe library under discussion is not published on\\n`PyPI <https:\/\/pypi.org\/search\/?q=caliper>`__. We\u2019d have to fork the\\nrepo and add its dependency on our app. Therefore we won\u2019t be using this\\nlibrary.\\n","Decision":"--------\\nWe will be using python dictionaries for processing and encapsulation of\\ntransformed events to keep things simple.\\nConsequences\\n------------\\nPython dictionaries will be used for binding Caliper events. These\\nevents stored as python dictionaries can be converted into JSON later\\nand sent to interested consumers.\\nRejected Alternatives\\n---------------------\\n**Use IMS\u2019 caliper-python library**\\nThe library under discussion is not published on\\n`PyPI <https:\/\/pypi.org\/search\/?q=caliper>`__. We\u2019d have to fork the\\nrepo and add its dependency on our app. Therefore we won\u2019t be using this\\nlibrary.\\n","tokens":218,"id":4480,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0009-persistence-and-retries-for-events.rst","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\n`event-routing-backends` transmits events to configured recipients (Learning Record Stores) via http protocol in near real-time. A strategy needs to be implemented to handle the case when an LRS's link is down.\\nDecision\\n--------\\n1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.\\n2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.\\n3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.\\n4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.\\n5. An interface shall be provided for admin to view the list of recipient(s) whose events are persisting in the database. The admin may choose to contact the recipient(s) to try and resolve the communication issue.\\nConsequences\\n------------\\n1. All but *business critical events*, will be lost after the time and number of retry attempts in decision # 2 expire.\\n2. Decision # 1 is necessary to enable decision # 3 but will also increase the number of celery workers in use.\\n3. The admin will need to respond to alert discussed in decision # 4 to avoid unnecessary utilisation of storage space.\\n","Decision":"--------\\n1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.\\n2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.\\n3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.\\n4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.\\n5. An interface shall be provided for admin to view the list of recipient(s) whose events are persisting in the database. The admin may choose to contact the recipient(s) to try and resolve the communication issue.\\nConsequences\\n------------\\n1. All but *business critical events*, will be lost after the time and number of retry attempts in decision # 2 expire.\\n2. Decision # 1 is necessary to enable decision # 3 but will also increase the number of celery workers in use.\\n3. The admin will need to respond to alert discussed in decision # 4 to avoid unnecessary utilisation of storage space.\\n","tokens":434,"id":4481,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0003-router-configurations-storage.rst","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nThe router will use some sort of configuration to decide what events are\\nto be sent to what consumers and how. These configurations can be\\ndefined for each partner\/enterprise so we could end up having 20-50\\nconfigurations.\\nWe have these two options for storing these configurations:\\n-  Store configurations in Django settings (YAML configuration files).\\n-  Store configurations in some Django model (database).\\nDecision\\n--------\\nKeeping in mind a large number of enterprise clients, we will store\\nrouter configurations in `Django configuration models\\n<https:\/\/github.com\/openedx\/django-config-models>`__.\\nConsequences\\n------------\\nStoring configurations in `Django configuration\\nmodels <https:\/\/github.com\/openedx\/django-config-models>`__ will keep the\\nsettings files cleaner.\\nDjango configuration models have built-in caching support which would\\nhelp us address performance-related challenges.\\nSince enterprise customer management might be done by teams other than\\noperators, putting these configurations in database will allow us to let\\ncustomer support teams manage the settings or could also make make it self\\nservice in future.\\nRejected Alternatives\\n---------------------\\n**Store configurations in Django settings**\\nHaving too many settings can clutter app settings and would be difficult\\nto manage.\\n","Decision":"--------\\nKeeping in mind a large number of enterprise clients, we will store\\nrouter configurations in `Django configuration models\\n<https:\/\/github.com\/openedx\/django-config-models>`__.\\nConsequences\\n------------\\nStoring configurations in `Django configuration\\nmodels <https:\/\/github.com\/openedx\/django-config-models>`__ will keep the\\nsettings files cleaner.\\nDjango configuration models have built-in caching support which would\\nhelp us address performance-related challenges.\\nSince enterprise customer management might be done by teams other than\\noperators, putting these configurations in database will allow us to let\\ncustomer support teams manage the settings or could also make make it self\\nservice in future.\\nRejected Alternatives\\n---------------------\\n**Store configurations in Django settings**\\nHaving too many settings can clutter app settings and would be difficult\\nto manage.\\n","tokens":275,"id":4482,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gatsby-template-adr\/1-Choosing-a-frontend-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProvide reasons and context of this ADR.\\n","Decision":"Provide the decision taken by the team.\\n","tokens":13,"id":4485,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"gatsby-template-adr\/2-Switching-frontend-technology.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProvide reasons and context of this ADR.\\n","Decision":"Provide the decision taken by the team.\\n","tokens":13,"id":4486,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hoard\/0004-use-airflow-and-fargate.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe ingest process will be a scheduled task and we have an Airflow instance designed for just this sort of thing.\\n","Decision":"We will use Airflow to handle scheduling the ingest. The ingest process itself will be run inside a Fargate container.\\n","tokens":27,"id":4487,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hoard\/0002-use-python.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe data ingest process will be custom software. We need to choose a language.\\n","Decision":"We will use Python.\\n","tokens":20,"id":4488,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hoard\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4489,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hoard\/0003-use-type-hinting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPython 3 added support for static type checking (see: https:\/\/docs.python.org\/3\/library\/typing.html). Type hinting is not an all or nothing thing, and can be applied in a progressive manner. It's also worth noting that type checks are not applied at runtime.\\n","Decision":"We will use type hinting. Our focus should be on type hints for function arguments and return values, and not aim for full coverage.\\n","tokens":62,"id":4490,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-09-08-CustomField-label-loading-in-storefront.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to provide the labels of custom fields in the storefront to third party developers.\\nOn one hand we could add the labels to every loaded entity, but this will cause a heavy leak of performance and the labels\\nare often not used in the template.\\n","Decision":"We implemented a subscriber, which listen on the `custom_field.written` event to add also snippets to all snippet sets with\\nthe given label translations of the custom field. The `translationKey` of the snippets are prefixed with `customFields.`,\\nfollowed by the technical name of the custom field. Thus the snippets can be used in the storefront.\\n","tokens":56,"id":4491,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-11-19-dal-join-filter.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, there are various difficulties with the current implementation of the `anti-join-filter`.\\nSometimes this does not lead to the correct results or the query cannot be executed due to a PHP exception.\\nFurthermore the counterpart to the `anti-join-filter`, the `join-filter`, is missing.\\nCurrently the `anti-join-filter` is automatically assembled in the entity searcher if a `not-filter` exists that points to a field of an association.\\n### Anti join filter concept\\nThe `anti-join-filter` should make sure that a `to-many` association can be queried negated on multiple values, here is an example:\\n**Give me all products which do not have \"red\" or \"yellow\" as property, but also not \"XL\" or \"L\".**\\nOn the SQL side, the following query must be generated for this purpose:\\n```sql\\nSELECT product.id\\nFROM product\\nLEFT JOIN property_properties color_filter\\nON color_filter.product_id = product.id\\nAND color_filter.id IN (\"red\", \"yellow\")\\nLEFT JOIN property_properties size_filter\\nON size_filter.product_id = product.id\\nAND size_filter.id IN (\"XL\", \"L\")\\nWHERE size_filter.product_id IS NULL\\nAND color_filter.product_id IS NULL\\n```\\n### Join filter concept\\nThe `join-filter` should make sure that a `to-many` association can be queried on multiple values, here is an example:\\n**Give me all products which do have \"red\" or \"yellow\" as property, but also \"XL\" or \"L\".**\\nOn the SQL side, the following query must be generated for this purpose:\\n```sql\\nSELECT product.id\\nFROM product\\nLEFT JOIN property_properties color_filter\\nON color_filter.product_id = product.id\\nAND color_filter.id IN (\"red\", \"yellow\")\\nLEFT JOIN property_properties size_filter\\nON size_filter.product_id = product.id\\nAND size_filter.id IN (\"XL\", \"L\")\\nWHERE size_filter.product_id IS NOT NULL\\nAND color_filter.product_id IS NOT NULL\\n```\\n","Decision":"Whether several joins must be made on an association must be recognized by the DBAL implementation itself. The user of the DAL does not have to pass an extra parameter for this.\\nHowever, since it is difficult to interpret what exactly is to be determined by the criteria, the algorithm for determination is based on certain rules.\\nWe will form so called `join-groups` in the DAL. These are created per `multi-filter` layer. So a join to an association is only possible once per `multi-filter` layer. So we allow to query several fields within one join.\\nBut if an already filtered field is filtered in another or nested `multi-filter`, a separate join is created for this field.\\nIt is only necessary to resolve the `to-many` association several times.\\nAfter the `join-groups` have been formed, the field to be resolved is passed to the `FieldResolver` (which forms the SQL JOIN) and the filter in which this field is located.\\nResolved filters in the JOIN are then marked and later in the WHERE they are linked with the corresponding AND\/OR\/NOT logic.\\n","tokens":438,"id":4492,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-11-09-increment-pattern.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe current `message_queue_stats` table records which messages are in the message queue and how often. This\\nis done by a subscriber, which then writes a record to the database via the mysql connection. If this record already exists,\\na duplicate key is triggered and the `count` value of the record will be updated.\\nSystems with many write operations generate a lot of traffic on the message queue. This in turn\\ngenerates a lot of traffic on the `message_queue_stats` table, resulting in bottlenecks and\/or deadlocks.\\nOn another issue, we also provide a feature called `Frequently Used modules` to increase the count everytime an admin module is visited which faces a similar issue.\\n","Decision":"We introduce a new table `increment` to store these countable information and deprecated `message_queue_stats` table.\\nWe create a possibility to control the access to the `increment` table via Redis or another storage, which is optimized for such scenarios.\\nTo do this, however, we need to prevent \/ extend access via the DAL.\\nSo we implement a new gateway with the following methods:\\n* `increment(string $cluster, string $key): void`\\n* `decrement(string $cluster, string $key): void`\\n* `list(string $cluster, int $limit = 5, int $offset = 0): array`\\n* `reset(string $cluster, ?string $key = null): array`\\n* `getPool(): string`\\n* `getConfig(): array`\\n* `getDecorated(): self`\\nThis then enables the following functional flow:\\n![](.\/assets\/message_queue_stats.png \"Message queue stats gateway\")\\nFurthermore, it should also be possible to completely disable the message queue stats or any pool via config file.\\nYou can easily tweak or define new increment pools in config file with your own pool's configuration. For e.g:\\n```yaml\\nshopware:\\nincrement:\\nuser_activity:\\ntype: 'mysql'\\nmessage_queue:\\ntype: 'redis'\\nconfig:\\nurl: 'redis:\/\/localhost'\\ncustom_pool:\\ntype: 'array'\\n```\\nBy default, we ship a Redis, MySQL and array adapter for the gateway. It should be possible to easily switch the adapter via config.\\nIf you want to override the default mysql or redis adapter, you need to register your own incrementer gateway in DI container with the id `shopware.increment.<custom_pool>.gateway.<adapter>`\\n","tokens":148,"id":4494,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-08-11-make-platform-stand-alone.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe platform requires some additional config, a console and web entrypoint and additional development tooling for development, tests and\\nrunning the application. In practice this is provided by one of the templates: `shopware\/development` or `shopware\/production`.\\nThis creates a cyclic dependency, which brings some problems:\\n- `shopware\/development` and `shopware\/platform` need to be updated in lockstep, which makes updating them individually sometimes impossible\\n- some IDEs have trouble with multi repository projects\\n- updating development tooling breaks everything\\n- auto-detection of git revision and diff is broken, because the development template is the root\\n- for each release branch an additional branch needs to be maintained\\n","Decision":"- use shopware\/platform directly in the pipeline\\n- allow development without a template, by moving the development tooling into platform\\n- only advertise this as `shopware\/platform` development setup. Projects should still start with `shopware\/production` as a template\\n- `shopware\/development` should continue to work\\n- allow testing by adding entrypoints for cli and web\\n- add scripts to composer to ease common tasks\\n* these scripts should be kept small and simple\\n* essential functionality should be implemented as npm scripts or symfony commands\\n* we should improve the symfony commands or npm scripts if they are too complicated\\n* if possible the scripts should allow adding arguments\\n- use standard convention\\n* `.env.dist` provides default environment variables\\n* `.env` can be used to define a custom environment (for example, if you use a native setup)\\n* `docker-compose.yml` provides a working environment\\n* `docker-compose.override.yml` can be used for local overrides to expose ports for example\\n- use defaults that work out of the box in most cases\\n* don't expose hard coded ports in docker-compose.yml. It's not possible to undo it and may prevent startup of the app service\\n","tokens":150,"id":4495,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-07-20-unified-notification-titles.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* Creating notifications messages in the administration caused the effort of making up not only a title but a message too.\\nThis has led to inconsistent notification appearances. In some cases the notification message simply duplicated the title,\\nothers wore the module's name as a title and so on.\\n* Now, since it is a set design decision to use the following four types of notification as titles at the same time,\\nit is just logical to make use of the global namespace and manage notification titles centrally.\\n* `Success` (green outline)\\n* `Error` (red outline)\\n* `Info` (blue outline)\\n* `Warning` (orange outline)\\n","Decision":"* Implement a global default title for all notifications types in\\n`\/platform\/src\/Administration\/Resources\/app\/administration\/src\/app\/mixin\/notification.mixin.js`\\n* Remove the superfluous title definitions and snippets\\n","tokens":139,"id":4496,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDefine a strategy for deprecations.\\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-08-31-refactor-admin-build-process-to-webpack-multi-compiler-mode.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPreviously the plugins are not completely independent from the core and other plugins. This has sometimes caused built plugin files to be incompatible with the core. Unless they were rebuilt again with the core.\\nThe reason for this was that dependencies between plugins and the Core were optimized by Webpack. This was because Webpack saw the combination of Core and plugins as one big program. So using tree-shaking, sometimes dependencies were removed or added depending on which plugins were installed.\\nAlso, a custom Webpack configuration in plugins resulted in it unavoidably being applied in core as well. This could sometimes result in the plugin only being compatible with the core if both were built together. If the plugin was then installed on other systems with only the built files, it could cause it not to work.\\n","Decision":"Webpack is known by many users and already in use. A switch to another builder needs to be deeply analyzed at first and then all plugin devs need to learn this bundler too, which can be frustrating, when you want to write a great plugin but has to learn a new bundler for no reason.\\nSo the isolated compiling and production bundling will be realized with webpack. Webpack also provides a good way how to solve the problem. With the webpack-multi-compiler we can build several independent configurations which do not affect each other. The watch mode also works with this setup so that no developer needs to relearn something.\\n","tokens":159,"id":4499,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-09-14-technical-concept-custom-entities.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt should be possible for apps to define their entities. Furthermore, it should be possible, if desired, that these entities are available via Store API.\\nLater, it should also be possible for store operators to create such entities. The concept is to consider that Apps can not add PHP code into the system under current circumstances. Also, a store operator is, seen from our point of view, not able to write PHP code himself to guarantee logic for his custom entities.\\nTherefore, purely through the definition of a custom entity, certain business logic should be automatically guaranteed.\\n","Decision":"### Schema\\n* Definition\\n* An app can include a `config\/custom_entity.xml` file.\\n* Multiple custom entities can be defined in the XML file.\\n* Each custom entity, is registered with the prefix `custom_entity_` or the `ce_` shorthand.\\n* App developers can then define that they would like to have `custom_entity_swag_blog` as an entity\\n* To prevent naming collisions, app developers should always add their developer prefix to the entity name\\n* We then create the `custom_entity_swag_blog` table\\n* Tables \/ Properties \/ Columns:\\n* A proper MySQL table is created for each custom entity.\\n* For each custom entity field we create a real MySQL table column.\\n* We support the following field data types:\\n* All scalar fields (int, string, text, float, date, boolean)\\n* All JSON fields (JSON, list, price, etc.)\\n* All \"linking\" associations (many-to-one and many-to-many)\\n* A bi-directional association will be left out for now.\\n* one-to-one and one-to-many will be not supported for now.\\n* Install & Update\\n* When installing and updating an app, the core automatically performs a schema update.\\n* Consider running a `dal:validate` on the schema when installing and updating an app.\\n* New fields on a custom entity must always be nullable or have a default\\n* Changing a field\/property data type is not allowed\\n* If a field is no longer defined in the .xml file, it will be deleted from the database.\\n* Identification and representation\\n* Each custom entity gets a `IdField(id)`, which serves as primary key\\n* Each custom entity gets a field `TranslatedField(label)`, which is required and serves as display name for the admin\\n### Bootstrapping\\n* At kernel boot we load all custom entities from the database and register them in the registry and di-container.\\n* For each custom entity, an entity definition is registered\\n* A generic entity definition is used, which gets the property\/column schema injected\\n* It must be checked how performant this is in case of bad performance we must put a cache in front of it (serialized properties\/columns e.g.)\\n* If no database connection exists, a kernel boot should still be possible\\n* The loading of the custom entities for the kernel boot should be outsourced to a CustomEntityKernelLoader\\n### Api availability\\nFor routing, we have to trick a bit, because currently for each entity in the system the routes defined exactly. This is not possible because the API route loader is triggered before the custom entities are registered. Therefore...\\n* We always register `\/api\/custom-entity-{entity}` as an API route and point to a custom controller that derives from ApiController.\\n* A request `\/api\/custom-entity-swag-blog`, then runs into our controller, and we get for the parameter `entity` the value `swag-blog`. We then pass this value to the parent method and prefetch it\\n* If the entity was defined with the `ce_` shorthand the API endpoints also use that shorthand, which means the route would be `\/api\/ce-{entity}`.\\n### Store api integration\\n* On the schema of the entity, the developer can define if this is `store_api_aware`.\\n* Entities which are not marked as `store_api_aware` will be removed from the response\\n* We will provide no automatic generated endpoint for the entities.\\n* Store api logics will be realized with the app-scripting epic\\n","tokens":118,"id":4501,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-03-24-nested-line-items.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to handle nested order line items.\\nCurrently, the line items are available nested, but all cart processors only consider the first level of line items.\\nOn one hand, we could implement all cart processors, that they process all levels of line items, but on the other hand,\\nall nested line items are added via plugins, which would implement their own processing logic.\\n","Decision":"The core cart processors will continue to work with `getFlat()` in enrich. This way the required data for all items in the\\ncart will be fenced and each item could also be processed by its processor.\\nThe `process` method on other hand will still not work with `getFlat()`, but will only take care of line items that are on the first level.\\nThis way there will be no collisions in the processing of these line items. A plugin that reuses core line items can\\neasily call the other processors to handle the nested line items themselves.\\nExample:\\n```\\n<?php declare(strict_types=1);\\nnamespace Shopware\\Core\\Checkout\\Cart;\\nuse Shopware\\Core\\Checkout\\Cart\\Error\\IncompleteLineItemError;\\nuse Shopware\\Core\\Checkout\\Cart\\LineItem\\CartDataCollection;\\nuse Shopware\\Core\\Checkout\\Cart\\LineItem\\LineItem;\\nuse Shopware\\Core\\Content\\Product\\Cart\\ProductCartProcessor;\\nuse Shopware\\Core\\System\\SalesChannel\\SalesChannelContext;\\nclass PluginCartProcessor implements CartProcessorInterface\\n{\\n\/**\\n* @var CreditCartProcessor\\n*\/\\nprivate $creditCartProcessor;\\n\/**\\n* @var ProductCartProcessor\\n*\/\\nprivate $productCartProcessor;\\npublic function __construct(CreditCartProcessor $creditCartProcessor, ProductCartProcessor $productCartProcessor)\\n{\\n$this->creditCartProcessor = $creditCartProcessor;\\n$this->productCartProcessor = $productCartProcessor;\\n}\\npublic function process(\\nCartDataCollection $data,\\nCart $original,\\nCart $toCalculate,\\nSalesChannelContext $context,\\nCartBehavior $behavior\\n): void {\\n$lineItems = $original->getLineItems()->filterType('plugin-line-item-type');\\n\/*\\n* Structure of the plugin line item:\\n* - plugin line item\\n*      - product line item(s)\\n*      - credit line item(s)\\n*\/\\nforeach ($lineItems as $lineItem) {\\n$this->calculate($lineItem, $original, $context, $behavior, $data);\\n$toCalculate->add($lineItem);\\n}\\n}\\nprivate function calculate(LineItem $lineItem, Cart $original, SalesChannelContext $context, CartBehavior $behavior, CartDataCollection $data): void\\n{\\nif (!$lineItem->hasChildren()) {\\n$original->remove($lineItem->getId());\\n$original->addErrors(new IncompleteLineItemError($lineItem->getId(), 'children'));\\nreturn;\\n}\\n$tempOriginalCart = new Cart('temp-original', $original->getToken());\\n$tempCalculateCart = new Cart('temp-calculate', $original->getToken());\\n\/\/ only provide the nested products and credit items\\n$tempOriginalCart->setLineItems(\\n$lineItem->getChildren()\\n);\\n\/\/ first start product calculation - all required data for the product processor is already loaded and stored in the CartDataCollection\\n$this->productCartProcessor->process($data, $tempOriginalCart, $tempCalculateCart, $context, $behavior);\\n\/\/ now calculate the credit, the credit is scoped to the already calculated products - all required data for the credit processor is already loaded and stored in the CartDataCollection\\n$this->creditCartProcessor->process($data, $tempOriginalCart, $tempCalculateCart, $context, $behavior);\\n\/\/ after all line items calculated - use them as new children\\n$lineItem->setChildren(\\n$tempCalculateCart->getLineItems()\\n);\\n}\\n}\\n```\\n","tokens":81,"id":4502,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-06-25-Implement-architecture-decision-records.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe should document architecture and technical decisions for the shopware platform. The documentation should be easy to understand and easy to follow. The workflow for new decisions should add to our existing workflows and should not block the whole development process. One solution could be the form of architecture decision records (ADR) as described in the following articles:\\n*  [Documenting Architecture Decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n*  [A Simple but Powerful Tool to Record Your Architectural Decisions](https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da)\\n","Decision":"*  [A Simple but Powerful Tool to Record Your Architectural Decisions](https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da)\\nWe will record architecture decisions (ADR) in markdown files directly in the platform repository. The workflow for ADRs will be integrated in the existing merge request workflow. This has the following advantages:\\n*  Decision records are an integral part of the development process\\n*  Decisions remain in sync with the code itself\\n*  The Git history is also the decision history\\n*  Decisions are public available and accessible for every developer\\n*  Also external developers can add new ADRs via GitHub pull requests\\n*  Decision finding can be asynchronous via comments in the corresponding merge request\\n**How can you create new ADRs?**\\nThe ADRs are markdown files inside the platform repository, located in the \"adr\" directory in the root of the repository. So new ADRs can simply be created via merge requests. The merge request is also the approval process for the ADR. Along with the ADR, all necessary code changes have to be added to the merge request, which are needed to implement the new decision. Add the \"ADR\" label to your merge request, so everyone can identify merge requests containing an ADR.\\n**How many people have to approve an ADR?**\\n*  Two additional developers have to review the ADR\\n*  One developer must be a member of the core development team\\n*  One developer must be a member of a team, other than the team of the creator\\n*  One product owner or higher role has to approve an ADR\\n**Should counter decisions also be documented?**\\nNot specific, but if there is more than one possible solution, all options should be outlined.\\n**How does an ADR look like?**\\nYou can use this first ADR as an orientation. The filename of the ADR should contain the date and a meaningful title. The content of the ADR should always use the following template:\\n```\\n# [Date] - [Title]\\n","tokens":155,"id":4503,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-11-20-add-login-required-annotation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome routes for the `sales-channel-api` and the `store-api\/storefront` depend on `SalesChannelContext` to identify whether the Customer is logged or not.\\nFor keeping clean code, consistency, and more easy to readable API. We create a new annotation for routing `\\Core\\Framework\\Routing\\Annotation\\LoginRequired`.\\n","Decision":"With the `store-api\/storefront` routing needs requiring logged in for access, developers need to define annotation `@LoginRequired `for API.\\nThis annotation to the following:\\n* `@LoginRequired`\\n* This annotation is validating the `SalesChannelContext` has Customer return success, otherwise throw `CustomerNotLoggedInException`\\n* `@LoginRequired(allowGuest=true)`\\n* This annotation is validating the `SalesChannelContext` has Customer and allow Guest admits, otherwise throw `CustomerNotLoggedInException`\\nAn example looks like the following:\\n```php\\n\/**\\n* @Since(\"6.0.0.0\")\\n* @LoginRequired()\\n* @Route(path=\"\/store-api\/v{version}\/account\/logout\", name=\"store-api.account.logout\", methods={\"POST\"})\\n*\/\\n\/**\\n* @Since(\"6.2.0.0\")\\n* @LoginRequired(allowGuest=true)\\n* @Route(\"\/account\/order\/edit\/{orderId}\", name=\"frontend.account.edit-order.page\", methods={\"GET\"})\\n*\/\\n```\\n","tokens":70,"id":4504,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-07-02-Implement-sales-channel-context-token-requirement.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome routes for the sales-channel-api and the store-api depend on a sales-channel-context-token to identify the correct context.\\nTo ensure these routes cannot be called accidentally or intentionally without a token, a route parameter is in need to distinguish open routes and those that need a token.\\n","Decision":"Every route that depends on a sales-channel-token will only be callable with such a token provided.\\nTo decide whether a route depends on a token or not the following questions should help:\\n* Will the automatic generation of the token be a security Issue?\\n* Will the automatic generation of the token lead to an abandoned entity? (e.g. the cart)\\n* Can every possible caller create or know the needed token beforehand? (e.g. the asynchronous payment provider cannot)\\n","tokens":60,"id":4505,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-11-02-preparing-data-for-rule-evaluation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen we are creating new `Rule` definitions, we need to consider how we are retrieving the data necessary for the evaluation.\\nSince rules could possibly be evaluated on any given request, the data for the evaluation must also be present at all times.\\nThis circumstance causes us to carefully consider the performance with regards to additional database queries or storing the data\\nbeforehand.\\n","Decision":"Instances of `Rule` should always be able to evaluate based on data retrieved from an instance of `RuleScope`. This instance\\nprovides getters for `Context`, `SalesChannelContext` and an instance of `\\DateTimeImmutable` as the current time. Everything\\nthat needs to be evaluated should be derived from methods of these instances.\\nWhen the data necessary for evaluation **can't** already be retrieved by the methods of `Context` and `SalesChannelContext`,\\nthe **least** favorable option should be to add additional associations to the `Criteria` of DAL searches, e.g. in the `SalesChannelContextFactory`.\\nUnless an additional association is needed anyways and in a much wider scope, the preferred option should always be to use indexers and updater services.\\nUsing the indexers, only the data absolutely necessary for evaluation can be stored as part of the target entity's definition.\\nAs the data is persisted asynchronously within the message queue, it should be kept up to date by background processes and we can avoid any additional database queries\\nduring storefront requests.\\n","tokens":78,"id":4506,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-08-14-merchant-registration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have to provide a registration for merchant.\\nThe definition of a customer, which is defined as a merchant, we want to realize via customer groups.\\nHowever, this is not \"merchant\" specific, because we do not react to \"merchant customer groups\" in any way in the core. In other words, we implement a customer group registration system.\\nThe process should work as follows:\\n* The shop owner enables customer group registration for a customer group and generates an url\\n* This url must be shared by the shop owner to customers (footer, social media, mails, etc.)\\n* Customer registers on an individual registration page on an individual url\\n* The customer will be created with the default customer group\\n* The shop operator can accept \/ decline the \"merchant\" registration in the admin\\nFor this I would suggest the following:\\n* At the customer we store another Foreign Key (desired customer group)\\n* This is then considered in the StoreApiRoute and stored at the customer\\n* In Administration we extend the current customer module with an accept \/ decline button\\n* Upon activation, we switch the customer group of the customer and set \"desired customer group\" back to zero.\\n","Decision":"### Headless Frontend\\n* Headless sales channel can resolve the url to get the foreign key using the seo-url store api route\\n* Call the customer-group-registration config endpoint with the foreign key to get the form configuration\\n* Sends a registration to customer registration endpoint with the `requestedGroupId`\\n","tokens":245,"id":4509,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-08-10-storefront-coding-standards.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* The current coding standards are not put into an ADR yet.\\n* This ADR is to determine the current standards to have a start from where to enhance the storefront concept further more.\\n","Decision":"### Controller\\n* Each controller action has to be declared with a @Since tag\\n* Each controller action requires a @Route annotation\\n* The name of the route should be starting with \"frontend\"\\n* Each route should define the corresponding HTTP Method (GET, POST, DELETE, PATCH)\\n* Routes which renders pages for the storefront (GET calls) are calling a respective pageloader to get the data it needs.\\n* The function name should be concise\\n* Each function should define a return type hint\\n* A route should have a single purpose\\n* Use Symfony flash bags for error reporting\\n* Each storefront functionality has to be available inside the store-api too\\n* A storefront controller should never contain business logic\\n* The controller class requires the annotation: @RouteScope(scopes={\"storefront\"})\\n* Depending services has to be injected over the class constructor\\n* Depending services has to be defined in the DI-Container service definition\\n* Depending services has to be assigned to a private class property\\n* A storefront controller has to extend the \\Shopware\\Storefront\\Controller\\StorefrontController\\n* Using LoginRequired annotation to identify whether the Customer is logged in or not.\\n* Each storefront functionality needs to make use of a store-api route service. to make sure, this functionality is also available via API\\n### Operations inside Storefront controllers\\nA storefront controller should never use a repository directly, It should be injected inside a Route.\\nRoutes which should load a full storefront page, should use a PageLoader class to load all corresponding data that returns a Page-Object.\\nPages which contains data which are the same for all customers, should have the @HttpCache annotation\\n#### Write operations inside Storefront controllers\\nWrite operations should create their response with the createActionResponse function to allow different forwards and redirects.\\nEach write operation has to call a corresponding store-api route.\\n### Page-\/PageletLoader\\n* A PageLoader is a class which creates a page-object with the data for the called whole page.\\n* A PageletLoader is a class which creates a pagelet-object with the data for a part of a page.\\nThe pageLoaders are a specific class to load the data for a given page.\\nThe controller calls the pageloader, which collects the needed data for that page via the Store-api.\\nThe pageloader can call other pageletloaders to get the data for pagelets(subcontent for a page).\\nThe pageloader always returns a page-object.\\n","tokens":43,"id":4510,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-05-14-when-to-use-plain-sql-or-dal.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt is often discussed whether to work with plain SQL or with the Data Abstraction Layer.\\n","Decision":"In the following application layers, the DAL should be used for the following reasons:\\n* In the Store API\\n* Data selected and returned via Store API must be extensible by third party developers.\\n* Requests against the Store API should always allow additional data to be loaded.\\n* Data retrieval and encoding must be secured by ACL.\\n* Storefront page loader and controller\\n* Data passed from the storefront to the Twig templates must be extensible by third party developers.\\n* Since our templates are customized by many developers, we cannot provide only a minimal offset of the actual data.\\n* On admin API level\\n* Data selected and returned via admin API must be extensible by third party developers.\\n* Requests that go against the Store API should always allow additional data to be loaded.\\n* Data retrieval and encoding must be secured by ACL.\\n* When writing data\\n* The DAL has a validation, event and indexing system which is used for the write process. Therefore, it is mandatory to ensure data integrity, that write processes take place exclusively via the DAL.\\n* The entity indexers are an exception here, see below.\\nIn the following application layers you should work with plain SQL because of the following reasons:\\n* In the entity indexers\\n* The entity indexers are located behind the entity repository layer, so it only makes sense that they do not work with the repositories but with the database connection directly.\\n* the entity indexers must be able to re-index all data after a versions update. To avoid as much hydration and event overhead as possible, they should work directly with the connection.\\n* The entity indexers are not an extension point of shopware. The queries that are executed there are only used for internal processing of data and should never be rewritten.\\n* In Core Components\\n* Core components like the theme compiler, request transformer, etc. are not places where a third party developer should be able to load additional data. The data loaded here is for pure processing only and should never be rewritten.\\n* Deep processes like theme compiling should not be affected by plugin entity schemas, because plugins are an optional part of the system and might be in an unstable state during an update process.\\n","tokens":22,"id":4511,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-11-22-merge-e2e-projects-into-a-single-project.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt's hard to test components in isolation. Other components are almost always also tested, which is intended because it's the nature of end-to-end tests being workflow-based.\\nThere are currently three E2E projects that are maintained separately. There are a lot of duplicated commands and different variations of them.\\n","Decision":"We'll merge all cypress e2e projects of platform into a single project.\\nThe projects will be merged by\\n- creating new project `E2E` in `tests\/E2E`\\n- moving storefront tests to `tests\/E2E\/cypress\/integration\/storefront`\\n- moving administration tests to `tests\/E2E\/cypress\/integration\/administration`\\n- moving recovery tests to `tests\/E2E\/cypress\/integration\/recovery`\\n- moving the new package test scenarios to `tests\/E2E\/cypress\/integration\/scenarios`\\n- merging the commands.js files and removing duplicate code\\n- merging the setup code\\n- merging fixtures\\n- use automatic cleanup in global setup instead of manual calls to `cleanUpPreviousState` in admin tests\\n","tokens":65,"id":4512,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-06-14-introduce-jest-fail-on-console.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA jest pipeline run produced previously hundreds of errors and warnings, which made it hard to see why a test failed and if a passing test isn\u2019t just a false positive.\\n","Decision":"To combat this, we decided to introduce the npm package [jest-fail-on-console](https:\/\/github.com\/ricardo-ch\/jest-fail-on-console#readme), which causes individual unit tests to fail if they log an error or a warning to the console.\\n","tokens":38,"id":4514,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-10-21-app-scripting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo improve the abilities of Apps, they should be able to execute code synchronously and hook into familiar places like:\\n- rules\\n- cart\\n- storefront page loading\\n- shipping method calculation\\n- flow builder extensions\\nThe app system requires that this code is in some way sandboxed, with no direct access to the database or filesystem, and the code is not saved on the server.\\nAdditionally, such a Scripting feature generally improves the capabilities of the AppSystem, this feature is not bound to the AppSystem exclusively, it should be possible to add standalone scripts.\\n","Decision":"We use Twig as it brings a secure PHP sandbox and allows interacting directly with objects. The scripts will be saved in the database and mapped to a specific scripting event.\\nScripting events are placed in many sections in Shopware to be able to adjust them. Apps can subscribe to the scripting events by placing their scripts into the correspondingly named folders.\\n### Scripting Events\\nThe data passed to the scripts always has to be an object so that the manipulation from Twig can affect the given value.\\nGiven objects must be wrapped into custom objects for app scripting to provide easier access to certain functionality and limit the scripting scope.\\nThe twig environment will provide additional functions like `dal_search` globally to all events to fetch other data in a consequent way\\n#### Which objects can be injected into the hooks and which have to be wrapped\\nIn general, it ok to inject `Struct` classes directly into the hooks, as long as those are rather \"dumb\" data containers (e.g. our DAL entity classes or the storefront page classes).\\nA notable Exception to this rule are `Struct` classes that provide business logic, besides simple getters and setters (e.g. the Cart struct).\\nThose `Structs` and all other `Services` that provide business logic or function that can lead to side effects (DB access, etc.) need to be wrapped into a facade.\\nThis will allow us to closely control the interface we want to provide inside the app scripts, to firstly improve developer experience by tailoring the API to the needs of app developers and secondly to ensure that we don't introduce any security issues with the app scripts.\\n### Scripting execution\\nEach script has its twig environments to improve execution stability. In failure cases, we will throw our exception.\\nThe twig environment is reduced to the only set of functionality that is needed; features like block and many template features are disabled.\\nScript loading can happen in multiple implementations, the default implementation will use the object cache to load the scripts and if missing loading it from the database.\\nThe compiled scripts will be cached on the filesystem in a separate folder per app and per appVersion.\\nFor development purposes, the scripts can be loaded from the filesystem to allow easier development. The default Twig cache will be used for faster code execution.\\n### Example pseudo-code of the ScriptEventRegistry\\n```php\\nclass ScriptEventRegistry\\n{\\npublic const EVENT_PRODUCT_PAGE_LOADED = 'product-page-loaded';\\nprivate $scripts = [];\\nprivate LoggerInterface $logger;\\npublic function execute(string $hook, array $context)\\n{\\n$scripts = $this->scripts[$hook] ?? [];\\nforeach ($scripts as $script) {\\n$this->executeScript($script, $context);\\n}\\n}\\nprivate function executeScript(array $script, array $context)\\n{\\n$twig = $this->initEnv($script);\\ntry {\\n$twig->render($script['name'], $context);\\n} catch (\\Throwable $e) {\\nthrow new ScriptExecutionFailed('Script execution failed', $e);\\n$this->logger->error('Execution of script failed', ['context' => $context, 'error' => $e]));\\n}\\n}\\nprivate function initEnv(array $script)\\n{\\n$cache = new ConfigurableFilesystemCache($this->cachePath . '\/twig\/scripts');\\n$cache->setConfigHash($script['appName'] . $script['appVersion']);\\n$twig = new Environment(\\nnew ScriptLoader([$script['name'] => $script['source']]),\\n[\\n'cache' => $cache,\\n]\\n);\\n\/\/ Setup some custom twig functions\\nreturn $twig;\\n}\\n}\\n```\\n### Example pseudo-code\\n#### Getting discount for high value order\\n```twig\\n{% if cart.price.totalPrice > 500 %}\\n{# get discount for high value orders #}\\n{% do cart.discount('percentage', 10, 'my_discount_snippet', cart.lineItems) %}\\n{% endif %}\\n```\\n#### Block cart\\n```twig\\n{% if cart.price.totalPrice < 500 %}\\n{# allow only carts with high values #}\\n{% do cart.block('you have to pay at least 500\u20ac for this cart') %}\\n{% endif %}\\n```\\n### Data Loading\\nTo allow apps to fetch additional data for the storefront, we will introduce PageLoaded-Hooks.\\nThose hooks will orient themself on the Page and PageLoadedEvents already present in the storefront. So for each PageType and PageLoadedEvent we will create a separate Hook class.\\nWe will create separate HookClasses and not just one generic class, so we are able to type hint all the dynamic data that is available for that hook. That will improve the developer experience as it allows for autocompletion in the scripts and allows us to generate documentation for the hooks.\\nThe hooks will be instantiated and passed to the HookExecutor from the Controllers where the pages are loaded, so we are able to pass additional data if it is needed or makes sense.\\nAdditionally, we explicitly decided to not provide CriteriaEvent-Hooks, as that idea is contrary to the direction we may want to go with a separate and specialized data view for the storefront.\\n### Documentation\\nTo ensure app developers can use the full potential of the app scripts, we need to ensure that we document the features of app scripts extensively and make sure that the documentation is always up-to-date.\\nFor this reason we decided to generate as much of the documentation as possible, so it never gets outdated, and it's easier to generate full reference (e.g. all hook points that exist with the associated data and available services).\\n","tokens":120,"id":4515,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-11-06-creating-events.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEvents throughout Shopware are quite inconsistent.\\nIt is not defined which data it must or can contain.\\nThis mainly depends on the domain where the events are thrown.\\n","Decision":"Developers should always have access to the right context of the current request,\\nat least the `Shopware\\Core\\Framework\\Context` should be present as property in events.\\nIf the event is thrown in a SalesChannel context,\\nthe `Shopware\\Core\\System\\SalesChannel\\SalesChannelContext` should also be present as property.\\n","tokens":38,"id":4517,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-08-10-Feature-flag-system.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo provide a way to toggle code from incomplete features, the feature flag system was implemented.\\nBecause of the high turnover on code for features in development, the system needs to be robust and easy to use.\\nAlso it is recommended that the system is safe to not breaking things when a flag is not completely provided.\\n","Decision":"* A feature flag will be added to the system by adding it to the file ```\/Core\/Framework\/Resources\/config\/packages\/shopware.yaml```\\n```yaml\\nshopware:\\n....\\nfeature:\\nflags:\\n- NEXT-733\\n- FEATURE-NEXT-1797\\n- FEATURE_NEXT_1797\\n```\\n* The flag should have a reference to an issue.\\n* The full flag name will always be ```FEATURE_XXX_XXX``` while the ```FEATURE_``` prefix is hard coded and can be provided in the configuration, but doesn't have to (see examples above).\\n* Inside the code the flag will always be noted with its full name ```FEATURE_XXX_XXX```\\n* After the feature is completed, the config of the flag and every note in the code have to be deleted with the merge request that also deletes possible old code which was replaced by the new feature.\\n","tokens":68,"id":4518,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-11-23-add-possibility-for-plugin-to-add-a-html-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe new ExtensionAPI is based on a iFrame communication architecture. The old App system for the admin relies on the XML\\nfile. And the normal plugin architecture in the admin is based on component overriding. The ideal way for developing\\nadmin extensions will be the ExtensionAPI.\\n","Decision":"To provide a smooth transition for plugin developer to the new ExtensionAPI which will be introduced soon we need to make sure that plugin can also\\nbehave like Apps in the administration. To fulfill this we need to provide a solution to show their own iFrame views.\\nThis is now directly possible when the plugin developer adds a `index.html` file to the plugin in the administration folder.\\nThis file will automatically be used by webpack and can be used like a normal web application.\\n","tokens":59,"id":4519,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-10-01-payment-flow.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have to provide a standardized way for Shopware extensions to implement custom payments.\\n","Decision":"We implement two possible handlers **Synchronous Payment** and **Asynchronous Payment**. Both handlers can optionally implement [Accepting-pre-created-payments](#accepting-pre-created-payments). If a [payment transaction fails](#after-order-payment-error-case), the user can choose an alternative payment method and trigger the flow again.\\n","tokens":20,"id":4520,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-07-22-move-storefront-scripts-to-head.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* Currently, our main storefront scripts (inline scripts as well as `all.js`) are located at the bottom of the page near the body end tag.\\n* The `async` attribute is used for the `all.js` but it isn't really suitable because our own JavaScript plugins depend on DOM elements in order to be initialized, and we have to wait for the document to be finished anyway.\\n* Additionally, the `DOMContentLoaded` is not compatible with `async` scripts because they might run before this particular event. That's why `document.readystatechange --> complete` is being used at the moment.\\n* This has the downside, that none of our JavaScript plugins initializes before the entire document is fully loaded including all resources like images.\\n","Decision":"* In order to improve the script loading all default `<script>` tags are moved to the `<head>` and get a `defer` attribute in favor of `async`.\\n* To initialize the JavaScript plugins, the `DOMContentLoaded` is being used instead of `document.readystatechange --> complete`.\\n* This ensures that the JavaScript plugins initialization must only wait for the document itself but not for additional resources like images.\\n* This change allows the browser to download\/fetch the scripts right away when the `<head>` is parsed instead of when almost the entire document is already parsed.\\n* Because of `defer` the script execution will wait until the document is parsed (Just right before the `DOMContentLoaded` event).\\n* `defer` also ensures that the different `<script>`'s are executed in the order in which they are declared.\\n","tokens":153,"id":4521,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-08-03-Implement-New-Changelog.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:\\n*  Unnecessary merge conflicts in the changelog file\\n*  Uncertainty about the correct release version of the change\\n*  Differences between the issue state in Jira and the code state\\nTo tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.\\n","Decision":"The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files.\\n","tokens":118,"id":4522,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-05-28-introduce-eslint-on-vue-admin.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to support ESLint of the administration vue app to keep and increase code quality.\\nBy adding ESLint to the administration app, every developer will get instant feedback and best practices on his code by writing.\\n","Decision":"For the `*.js` linting, we want to get pretty close to a standard vue2 app linting. In the `*.js` linting chapter, we explain where and why we choose to leave the common way.\\nFor the `*.html.twig` linting, we need to create a custom solution, which can handle or circumstance the twig syntax in our templates. We decided to convert all twig syntax to HTML comments within the linting process. This way, the linter ignores the twig parts and can handle the twig files like typical vue templates. The most significant tradeoff with this solution is that the linter cannot take the twig blocks into account on computing indentation levels.\\n### `*.js` linting\\nFor the `*.js` files we try to follow a standard vue cli linting way, with this adjustments:\\n* [`'vue\/require-prop-types': 'error'`](https:\/\/eslint.vuejs.org\/rules\/require-prop-types.html) - always use proper types definitions for `props`\\n* [`'vue\/require-default-prop': 'error'`](https:\/\/eslint.vuejs.org\/rules\/require-default-prop.html) - always provide a default value for optional `props`\\n* [`'vue\/no-mutating-props': ['off']`](https:\/\/eslint.vuejs.org\/rules\/no-mutating-props.html) - this is a tradeoff to allow mutating properties because it is already heavily used\\n* [`'vue\/component-definition-name-casing': ['error', 'kebab-case']`](https:\/\/eslint.vuejs.org\/rules\/component-definition-name-casing.html) - write component names in kebab-casing\\n### `*.spec.js` linting\\nDuring writing unit test files, we do not want to get a `max-len` warning.\\nA `max-len` rule may lead to hard understandable output in test names only to suit the `max-len` rules.\\nIn a test itself, you sometimes have `selector` phrases or something else where you exceed the `max-len` rule without a chance to solve it.\\n### `*.html.twig` linting\\nBesides the _twig-to-html-comment_ tradeoff, these exceptions are also made:\\n* `'vue\/component-name-in-template-casing': ['error', 'kebab-case']` - write vue component names in kebab-case in templates\\n* `'vue\/no-multiple-template-root': 'off',` - due to external template files and component inheritance\\n* `'vue\/attribute-hyphenation': 'error'` - write `hello-word=\"\"` attributes instead of `helloWorld=\"\"`\\n* `'vue\/no-parsing-error': ['error', {'nested-comment': false}]` - ignore nested html comments, which may be a result of the twig-to-html-comment workflow\\n* `'vue\/valid-template-root': 'off'` - @see `vue\/no-multiple-template-root`\\n* `'vue\/valid-v-slot': ['error', { allowModifiers: true }]` - allow `.`s in template slot names\\n* `'vue\/no-unused-vars': 'off'` - the twig parser cannot understand if a scoped slot value is used or not used properly\\n* `'vue\/no-template-shadow': 'off'` - for providing scoped values into another template scope\\n* `'vue\/no-lone-template': 'off'` - in some composition cases lone template tags are used\\n* `'vue\/no-v-html': 'off'` - for i18n and other reasons v-html is often used\\n### twig block indentation\\nTo accomplish the twig syntax being able to be linted, we needed to create a custom [`eslint-twig-vue-plugin`](..\/src\/Administration\/Resources\/app\/administration\/twigVuePlugin\/lib\/processors\/twig-vue-processor.js) and to accept the following changes in template writing:\\n_before_\\n``` html\\n\u2026\\n<div>\\n{% block block_name %}\\n<div>\\n\u2026\\n\u2026\\n```\\n_now_\\n``` html\\n\u2026\\n<div>\\n{% block block_name %}\\n<div>\\n\u2026\\n```\\nTo be able to lint the twig templates, we replace the twig syntax with HTML comments during the lint process, and thus every `twig` syntax is treated as an HTML comment and not recognised for indentation.\\n### self-closing components\\n_before_\\n``` html\\n\u2026\\n<sw-language-switcher><\/sw-language-switcher>\\n\u2026\\n```\\n_now_\\n``` html\\n\u2026\\n<sw-language-switcher \/>\\n\u2026\\n```\\n### attribute alignment\\nAs soon as more than 1 attribute exists, every attribute gets its own line:\\n_before_\\n``` html\\n\u2026\\n<div v-for=\"strategy in strategies\" class=\"sw-app-app-url-changed-modal__content-choices\">\\n\u2026\\n<sw-icon small color=\"#189eff\" name=\"default-basic-shape-circle-filled\"><\/sw-icon>\\n\u2026\\n```\\n_now_\\n``` html\\n\u2026\\n<div\\nv-for=\"strategy in strategies\"\\nclass=\"sw-app-app-url-changed-modal__content-choices\"\\n>\\n\u2026\\n<sw-icon\\nsmall\\ncolor=\"#189eff\"\\nname=\"default-basic-shape-circle-filled\"\\n\/>\\n\u2026\\n```\\n","tokens":47,"id":4523,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-07-02-control-clone-behavior.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe developer should be able to define, if an association has to be considered or skipped during the cloning of an entity.\\n","Decision":"The current clone behavior is controlled by the `Shopware\\Core\\Framework\\DataAbstractionLayer\\Field\\Flag\\CascadeDelete` flag.\\nAll associations which are marked with this flag are considered in the clone process.\\nWe will add an optional parameter to the flag constructor to disable this behavior.\\nI already added this flag to the following associations:\\n* `product.productReviews`\\n* This association is already overwritten by the administration\\n* `product.searchKeywords`\\n* Will be indexed by the product indexer, so we can skip this association in the clone process\\n* `product.categoriesRo`\\n* Will be indexed by the product indexer, so we can skip this association in the clone process\\nAn example looks like the following:\\n```\\n(new OneToManyAssociationField('searchKeywords', ProductSearchKeywordDefinition::class, 'product_id'))\\n->addFlags(new CascadeDelete(false)),\\n```\\n","tokens":28,"id":4524,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-11-05-adjust-ADR-approval-rules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen we decided to introduce ADRs we also decided that there need to be special approval rules for those ADRs.\\nThe approval rules are now outdated after the reorg, so to continuously ensure that the decisions documented in ADRs work in the long run for us, we want to adapt the approval rules.\\n","Decision":"The old approval rules for reference:\\n```\\n*  Two additional developers have to review the ADR\\n*  One developer must be a member of the core development team\\n*  One developer must be a member of a team, other than the team of the creator\\n*  One product owner or higher role has to approve an ADR\\n```\\nThe new approval rule is the following:\\n* At least one member of each of the Component Teams for the Core, Admin and Storefront area have to review the ADR.\\n","tokens":68,"id":4526,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2021-10-13-refund-handling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nShopware offers no way of unified refund handling. This results in every payment extension either implementing it themselves or not at all.\\n","Decision":"We want to implement the following structure to offer a unified refund handling for all extension types.\\n","tokens":29,"id":4527,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-12-02-removing-api-version.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDue to the new deprecation strategy and the 6-8 months major cycle, API versioning is no longer  reasonable or even possible.\\nDeprecated fields and routes are currently tagged in a minor version and will be removed with the next major version.\\nThe API version is currently not increased in a minor version, which would not make sense, because with every second minor version deprecations would have to be removed.\\n","Decision":"By removing the API versioning within the URL we want to simplify usage and the deprecation strategy of our API.\\nThe deprecated fields and routes are shown in the OpenAPI scheme as well as the API changelog and will be removed with the next major version (`6.x`).\\n","tokens":88,"id":4529,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"platform\/2020-12-03-The-best-practice-to-always-re-fetch-the-data-after-saving.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe should always re-fetch the entity data after saving within admin pages.\\n","Decision":"Reload the data after each saving progress to ensure the user will work only the latest data.\\nWhen you save data without reloading the entity, then you need to re-assign the values. But you can't be sure, that these values are the latest ones, because of possible data inconsistency during the saving process. That's why re-fetching data is always important for further CRUD operations.\\nFor example:\\n```html\\n<!-- we change the status by click to switch for example -->\\n<sw-switch-field\\nv-model=\"data.status\"\\n:label=\"$tc('sw-review.detail.labelStatus')\">\\n<\/sw-switch-field>\\n<!-- we will save data with onSave method -->\\n<sw-button-process @click=\"onSave\">\\n{{ $tc('global.default.save') }}\\n<\/sw-button-process>\\n```\\n```javascript\\n\/\/ This method for button save\\nonSave() {\\nthis.repository.save(this.data, Shopware.Context.api).then(() => {\\n\/\/ We should add the method to re-fetch the entity data after save success here\\nthis.loadEntityData();\\n});\\n},\\n\/\/ This method to re-fetch the data\\nloadEntityData() {\\nconst criteria = new Criteria();\\nconst context = { ...Shopware.Context.api, inheritance: true };\\nthis.repository.get(this.data.id, context, criteria).then((data) => {\\nthis.data = data;\\n});\\n},\\n```\\n","tokens":18,"id":4530,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tove\/adr-01.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Zooniverse maintains numerous Rails APIs, mostly built between three and five years ago. These vary wildly in the amount of attention they've received and all use gems that are deprecated, unmaintained, or otherwise generally not a good idea in TYOOL 2019. As such, research was necessary into what was more modern, standardized API functionality.\\nAny solution would need to meet the following requirements:\\n* CRUD and serialization\\n* Pagination\\n* Filtration\\n* Authentication and Authorization\\n","Decision":"At the end of the day it was difficult to not choose the JSON:API spec for a greenfield project. It was a balance of standardization vs. gadgetiness, basic non-DRY controllers on one end, GraphQL on the other. It is a spec that our front end devs are sort-of familiar with on account of our existing services' sort-of implementations of it. It gave me the opportunity to go down the rabbit hole a bit to learn what's out there, and some of those discoveries were surprising (ActiveModelSerializers is fully deprecated, who'd have guessed?)\\nWith that in mind, I test drove a few of the suboptions. JSONAPI::Resources was more a framework, and while I'm sure that this relatively simple use case could be handled by it, I found myself getting hung up on straightforward questions because I wasn't sure I was using all of the magic correctly. This definitely seemed like overkill for what I wanted to accomplish. Furthermore, all of that magic and extra (required) documentation reading would make it that much more difficult to onboard someone new to work on this app.\\nIn the end, I decided to go with jsonapi.rb. It leans on existing, popular tech to do the hard stuff (fast_jsonapi and ransack for serialization and filter\/sort, respectively). And pretty much everything else it does is stuff that I'd have to implement manually. This way, though, I can see exactly what it's doing, how it's being done, and override it if necessary. There's no sorcery or empty controllers, just simple classes\/methods that are clear and extendable.\\nI even fixed an example in the README and my PR was merged a couple hours later, so it's certainly active. That said, it would be a relatively straightforward process to disentangle this gem entirely from the rest of the app by just grabbing a few of the necessary classes out of its \/lib and throwing them into the app's.\\n","tokens":105,"id":4531,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tove\/adr-02.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRails apps need access to environment-based credentials (API keys, db URLs and passwords, etc). We do this a few different ways across all of our Rails apps. This ADR is a chance to take everyone's temperature on using a neat but new bit of Rails 6 and inform similar decisions later.\\n","Decision":"Decided to go with Option 2, Rails credentials. It's the most forward-looking option and isn't terribly different from existing setups. There's even a precedent in the graphql stats API. Also, as it's already being done in the aforementioned API, we're going to store encoded credentials in the repo.\\n### Links\\n* rails docs: https:\/\/edgeguides.rubyonrails.org\/security.html#custom-credentials\\n* PR that added environment specificity: https:\/\/github.com\/rails\/rails\/issues\/31349\\n* quick blog post on use: https:\/\/blog.saeloun.com\/2019\/10\/10\/rails-6-adds-support-for-multi-environment-credentials.html\\n","tokens":65,"id":4532,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tove\/adr-03.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ockam\/0002-profile-event-signing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProfile events can include multiple keys so the profile can prove the keys are valid. The question centers on\\nwhen adding new keys, should a proof of possession be required? This involves the new key signing the current\\nevent data, then the current profile root key signing the event data with the proof into the event.\\n","Decision":"It has been decided that a proof of possession will be required for now when adding a new key to the profile.\\n","tokens":68,"id":4534,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ockam\/0006-publishing-rust-crates.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPublishing a crate is a multi-step process requiring a variety of changes. A publishing checklist helps maintain consistency and quality in releases.\\n","Decision":"### Checklist\\n- [ ] **New crate?** - Crate has an entry in `directories` in `implementations\/rust\/build.gradle`\\n- [ ] Cargo.toml - Correct\/Next release version according to semver.\\n- [ ] Cargo.toml - Categories and keywords are correct.\\n- [ ] Cargo.lock - Present and updated. (Running `cargo build` does not generate changes to `Cargo.lock`)\\n- [ ] Build - `cargo test` succeeds.\\n- [ ] Examples - Any examples in `examples` run via `cargo run --example $name`.\\n- [ ] Docs - `cargo doc --open` - Docs are correct, clear and useful.\\n- [ ] README - Version matches release version.\\n- [ ] README - Header is correct.\\n- [ ] README - Have any features changed? Verify any updates.\\n- [ ] CHANGELOG - Entry present for release version. Date is correct.\\n- [ ] CHANGELOG - Added section is correct.\\n- [ ] CHANGELOG - Changed section is correct.\\n- [ ] CHANGELOG - Deleted section is correct.\\n- [ ] Publish - `cargo publish --dry-run` succeeds.\\nCorrectness of the Added, Changed and Deleted sections can be verified by running `git log $crate_name_v$current_vers..HEAD .` from the crate root directory. The commit\\ntitles should all be accounted for in the CHANGELOG in some manner. Commits containing the `feat` or `fix` tag must be accounted for.\\nFor example, `git log ockam_node_v0.5.0..HEAD .` will show the difference between that node release tag and the current develop branch, restricted to the crate directory.\\n","tokens":30,"id":4537,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"send-letter-service\/0001-increase-pod-memory-for-oom-error.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have received a couple of Out of Memory errors in relation to large pdfs which cause a problem themselves or when they are combined through the `count` parameter.\\n","Decision":"Given the low number of occurances it has been decided to simply increase the memory available to the JVM in the pod.\\nShould this problem continue to occur we will need to look at options such as:\\n* Streaming the zip file data to the SFTP server directly\\n* Storing binary data in Blob Storage instead of Postgresql and potentially shifting processing to a tmpfs volume\\n","tokens":37,"id":4540,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"forkhandles\/0002-monorepo-and-bom.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\\nThe former decouples release cadences.\\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\\n","Decision":"We will have a monorepo.\\n","tokens":86,"id":4542,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"forkhandles\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4543,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/007-package-repository.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Github Packages](https:\/\/github.com\/features\/packages). It is straightforward to set up and use, and we're already using other Github tools.\\nAlso, our team has a good experience with it, and that can boost our development speed.\\n","tokens":53,"id":4545,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/003-theming-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nAmong most frameworks, [ThemeUI](https:\/\/theme-ui.com\/) stands out for the following reasons:\\n* It is incredibly light (only 19kb unpacked)\\n* It is highly customizable (we can use the `sx` props to use theme tokens without hooks)\\n* It is slightly opinionated (more than [styled-system](https:\/\/styled-system.com\/) but less than [rebass](https:\/\/rebassjs.org\/))\\n* It contains some common structures\\nWe've not decided to use Rebass, since most of the community is already changing to ThemeUI, and also it allows us to do a more robust structure in our themes.\\n","tokens":53,"id":4546,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/005-lerna.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Lerna](https:\/\/lerna.js.org\/) as the tool for our deployment. It has incredible features (such as automatic versioning), and it helped us a lot.\\nOur team has already used it in previous projects, so it will easy to set up and keep that tool.\\n","tokens":53,"id":4548,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0006-public-discovery-access.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMetadata for works and collections should be publicly viewable. Only the binary content, i.e. the files, within the\\nwork needs to be restricted based on visibility or other permissions.\\n","Decision":"Grant discovery access to the public on all works and collections by creating the appropriate ACL for each work and\\ncollection. Discovery access stipulates that all metadata is viewable, but that binary content is not downloadable.\\n","tokens":41,"id":4551,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0004-blacklight-for-search-only.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are two ways we can display works and work versions in Scholarsphere: 1) using the record that is in the\\nPostgres database; or, 2) using the record that is in Solr.\\n","Decision":"We going to use the Postgres record for displaying individual records, leaving Blacklight's Solr record for displaying\\nsearch results only. The Solr record, or SolrDocument, will not be used when displaying the detailed record for a\\nwork or work version. It will only be used within the context of a list of search results.\\n","tokens":48,"id":4552,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0007-published-date-field.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe field for _Published Date_ is a free-text field, but also needs to be expressed as a date. Scholarsphere 3 has\\nentries that cannot be mapped to actual dates, so we need a way to store non-parseable dates but also have some kind of\\nvalidation as well.\\n","Decision":"Published date will only be validate at the UI and API level. The database will not validate any of the entries. This\\nallows us to store anything when migrating, or creating records through means other that the API or UI. When values are\\nentered through the API or UI, an EDTF date must be used.\\n","tokens":64,"id":4555,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0005-order-files-alphanumerically.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFiles within a work should be displayed in a certain order. The ordering can be automatic or arbitrary.\\n","Decision":"Files will be ordered alphanumerically, according to their names. The application can now render them in the same order\\neverytime, without additional metadata.\\n","tokens":24,"id":4558,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"scholarsphere\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4559,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"email-alert-api\/adr-006-email-delivery-responsibilities.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEmail, as an asynchronous communication medium, inherently has two boolean\\nscenarios that need to be true for an email to be received. An email has to be\\n_sent_ to a mail server and the mail server then needs to _deliver_ the email to the\\nrecipient's mail server. Resolving whether an email is sent is a rather simple\\nquestion that can be resolved synchronously - \"did the mail server accept the\\nemail?\" - however, determining whether an email is received is more complex\\nas delivery is not synchronous and may be subject to automated retrying should\\nany problems occur. Typically, in the medium of email, clients only consider\\nthe first scenario in reporting an email's status, this is the point an email\\nbecomes sent. The latter scenario, delivery, isn't typically reflected in\\nemail clients, it is normal to assume the email was received\\nsuccessfully unless later you receive an email indicating a bounce has\\noccurred.\\nThe [initial Email Alert API Notify integration][adr-1] was designed to\\nconsider both the sending and delivery of email. It had systems to monitor\\nwhether Notify had managed to deliver an email to a recipient and an automatic\\nretry mechanism for when Notify had problems delivering an email.\\nIn September 2020 we decided to re-evaluate this concept conflation as we were\\nconcerned that Notify functionality was duplicated in Email Alert API and\\nthat this had an adverse effect on the complexity of the system.\\n[adr-1]: adr-001-notify-integration.md\\n","Decision":"We decided that Email Alert API has a responsibility to send email, but does\\nnot have a responsibility to deliver it. That is the responsibility of Notify.\\nWe defined that, in the context of Email Alert API, the process of sending\\nemail was the ability to successfully send a request to Notify to perform\\nthis action - we consider this the equivalent of a mail client successfully\\nsending an email to an [SMTP][] server. We then consider the\\n[Notify callback][notify-callback] as the mechanism to learn if an email was\\ndelivered or not.\\nTo reflect this we have [switched][switched-to-sent-success] the\\nmeaning of an [email's status][email-status]. A status of \"sent\" now means that\\nthe email was accepted by Notify and not that the email was delivered. A\\nstatus of \"failed\" now means we weren't able to send the email to Notify,\\ninstead of the previous meaning where it meant that it may not have been sent\\nto Notify or Notify failed to deliver the email.\\n[SMTP]: https:\/\/en.wikipedia.org\/wiki\/Simple_Mail_Transfer_Protocol\\n[notify-callback]: https:\/\/docs.notifications.service.gov.uk\/ruby.html#delivery-receipts\\n[switched-to-sent-success]: https:\/\/github.com\/alphagov\/email-alert-api\/commit\/c457f62c3b6f1eaadf47e6596223cc0fdcffa853\\n[email-status]: https:\/\/github.com\/alphagov\/email-alert-api\/blob\/0b87e62288ddb1653451f84e1f36e17ce4e8e9dc\/app\/models\/email.rb#L7\\n","tokens":317,"id":4561,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"email-alert-api\/adr-005-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhile we have already recorded architectural decisions for this project, there is no guidance to follow for writing new ones. Recently we found that creating a new ADR was [hindered](https:\/\/github.com\/alphagov\/email-alert-api\/pull\/1441#discussion_r508729384) by wanting to be consistent with the structure of previous ADRs.\\n","Decision":"We will continue to use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\nWe will adopt a structure for future ADRs that matches this document - the exemplar in [our Rails app conventions](https:\/\/docs.publishing.service.gov.uk\/manual\/conventions-for-rails-applications.html#documenting-your-decisions). We will not change how the files are named, to avoid breaking links that were never expected to change.\\n","tokens":77,"id":4562,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-ecs\/0002-switch-to-ecs-optimized-amazon-linux-2-ami.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAmazon have introduced a new ECS-Optimized instance based on the new Amazon\\nLinux 2. This\\n[ECS-Optimized Amazon Linux\\n2](https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/developerguide\/al2ami.html)\\ninstance has also changed the Storage Driver from `devicemapper` to `OverlayFS`.\\nThe old ECS-Optimized instances will be phased out and not supported beyond June\\n2020.\\nStorage configuration has also been changed as the [Amazon 2 Storage\\nConfiguration](https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/developerguide\/al2ami-storage-config.html)\\nconsists of a single 30GB root disk. The old [Storage\\nConfiguration](https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/developerguide\/ecs-ami-storage-config.html)\\nwhich had a 30GB drive split into 8GB for the OS and 22GB for storage.\\n","Decision":"We will roll out the new ECS-Optimized instances into the environments on a\\nrolling basis starting with DEV to ensure there are no issues with our current\\ncontainer workloads with the change in Storage Drivers.\\nThis will also allow us to determine if we need to increase the size of the\\nstorage for use in our Kafka cluster which has specific storage requirements.\\n","tokens":201,"id":4565,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-ecs\/0003-ecs-iam-and-security-groups.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe ECS IAM roles are currently in the global IAM module and the security groups\\nare in the VPC modules. These made sense when first developed and based on the\\nexisting multi environment in one account set up. They don't make sense when\\ndoing a new single environment account - which is the way we are transitioning\\nto.\\n","Decision":"Refactor the IAM roles and security groups into the ECS Cluster in a way that\\nmaintains backward compatability and supports the future approach of single\\nenvironment accounts.\\nToggles can be used to support the backwards compatability and future\\napproaches.\\n","tokens":73,"id":4566,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"terraform-aws-ecs\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4567,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"arduino-printf\/0002-library-source-reorganization-to-support-avr-default-settings.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to provide default settings for AVR chips that reduces the library size *without modifying the original `printf` library files*. We can do this by including the `.c` file directly after we specify the default settings. However, this results in duplicate symbols when the `printf` library files are contained in the `src\/` directory, because the Arduino IDE automatically builds them. We need a way to prevent the IDE from automatically building the files so we do not end up with duplicate symbols.\\n","Decision":"* The `src\/` folder contains `LibPrintf.cpp` and `LibPrintf.h`\\n* The `extras\/printf` folder contains `printf.c` and `printf.h` as files directly within the repository, enabling the library to work without submodule\\n* `LibPrintf.h` will include `..\/extras\/printf\/printf.h` to access the `printf` definitions\\n* `LibPrintf.cpp` will include `..\/extras\/printf\/printf.c` to add the symbols for the `printf` library. We will also add our definition defaults for `__AVR__`, ensuring that the settings are applied automatically without modifying build rules\\n","tokens":103,"id":4568,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"arduino-printf\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4569,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0010-use-nhsuk-frontend-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe service uses an old version of the nhsuk-frontend library. When the service moves from beta to\\nlive we want it to look consistent with other nhs.uk services.\\n","Decision":"Use the Nunjucks macros, SCSS and JS available in the nhsuk-frontend NPM package. Consuming these\\ndirectly from the package allow us to take advantage of any updates to the library.\\n","tokens":41,"id":4570,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0002-use-express-web-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4571,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0007-use-service-specific-contact-details.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe search index contains two sets of contact details, one for the\\norganisations and one for the psychological therapies services provided by the\\norganisations. The service contact details are usually more specific, e.g. a\\ndirect telephone number rather than the hospital switchboard, so likely to be\\nmore useful to someone searching for a psychological therapies service.\\n","Decision":"Use the service specific contact details stored as metrics instead of the\\norganisation contact details in the Contacts field.\\n","tokens":75,"id":4572,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0001-record-architecture-decision-records.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\nthis\\n[article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":4573,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0006-use-local-data-for-west-hampshire-ccg.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe data synchronisation between front end and back end is broken and is taking\\ntime to fix. In the interim the data can be stored locally within the\\napplication.\\n","Decision":"The data for West Hampshire will be stored locally within the application until\\nsuch a time as the data synchronisation has been fixed. Once fixed, the local\\ndata store will be removed so the centrally held data can be requested and used\\nwithin the application.\\n","tokens":39,"id":4574,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0008-do-not-use-local-data-for-ccgs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAll current IAPT services should have an ODS code. Any future services must\\nhave an ODS code assigned. There for it is no longer necessary to use local\\ndata to relate CCGs to IAPT services.\\n","Decision":"Remove local data for all CCGs. The source for CCG to IAPT service relationship\\ndata will only be the central data store.\\n","tokens":50,"id":4575,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0005-use-local-data-for-tower-hamlets-ccg.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTower Hamlets CCG's IAPT service does not have an ODS code. As a consequence it\\nis not possible to relate the CCG to the IAPT service. When a GP is providing\\nservices for Tower Hamlets CCG this will allow data to be displayed.\\n","Decision":"Rather than display no result we know the contact information for the IAPT\\nservice. We have decided to store this information locally and display it for\\nthe user.\\nThis is only a temporary measure and the change will be reverted once the data\\nhas been loaded into the central system.\\n","tokens":61,"id":4576,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0009-add-service-search-to-site-root.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSitting directly on the `\/find-a-psychological-therapies-service` path is not in keeping with the organisations plans for\\ninformation architecture going forward. When the service moves from beta.nhs.uk to nhs.uk we want\\nit to sit on path consistent with other finder services.\\n","Decision":"The site root will be changed to be `\/service-search\/find-a-psychological-therapies-service`\\n","tokens":64,"id":4577,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0003-do-not-use-azure-search-service-highlighting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[Azure Search Service](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/searchservice\/)\\nhas the ability to return results with the search term hits\\n[highlighted](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/searchservice\/search-documents#highlightpretagstring-optional).\\nThe searches submitted by the application add a wildcard character (`*`) to\\neach term. The reason for this is to have results returned where the term\\nmatches the beginning of the word e.g. `prac` with return results with the word\\n`practice` in (and any other word starting with `prac`). A consequence of this\\nis the highlighting returns full word matches e.g. searches for `prac` will\\nreturn highlights as `<span class\"highlight\">practice<\/span>` whereas only the\\nexact term entered should be highlighted. There is no option within the search\\nservice to have only the exact term entered highlighted.\\n","Decision":"Given the search service is not able to fulfil the requirement of exact term\\nhighlighting the decision is to highlight the results by the application\\nserver, once the results have been returned from the search service.\\n","tokens":193,"id":4578,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0004-use-local-data-for-redbridge-and-north-cumbria-ccgs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNorth Cumbria CCG and Redbridge CCG do not have an ODS code. As a consequence it is not possible to relate them to the IAPT services. When a GP is serving under either of these CCGs we can display the information for the CCG.\\n","Decision":"Rather than display no result we know the contact information for each of these CCGs. We have decided to store this information locally and display it for the user.\\nThis is only a temporary measure and the change will be reverted once the data has been loaded into the central system.\\n","tokens":61,"id":4579,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mental-health-service-finder\/0011-use-nhsuk-header-and-footer-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA API has been created which can be used to build header and footer links for NHS.UK.\\nWe want to use this across all NHS.UK applications.\\n","Decision":"Build a middleware function which can get the header and footer links from the API and\\nmake them available within the Nunjucks templates. We will also cache these to prevent\\na massive amount of calls to the API\\n","tokens":37,"id":4580,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"TechChallengeApp\/0004-config-only-in-config-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nApplication configuration can be overridden by command line flags and environment variables. Is this something we want to take advantage of?\\n","Decision":"No, configuration will be limited to the configuration file for sake of simplicity and having a single way to configure the application.\\n","tokens":27,"id":4581,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"TechChallengeApp\/0005-skip-create-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDatabase upgrade script contains everything required to deploy and seed a test database. Some PaaS services like azure db have requirements that means it is difficult to have a generic way to create the database.\\n","Decision":"To make it easier, we've decided to add an option to skip the database creation and allow for an external process to create the database while still creating tables and seeding it with data.\\n","tokens":42,"id":4582,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"TechChallengeApp\/0003-removed-scaffolding.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nShould we provide scaffolding for the test takers, or should we expect them to be able to set that up themselves?\\n","Decision":"We decided to remove the scaffolding and rather suggest that the test taker uses the default VPC to deploy their application.\\n","tokens":29,"id":4583,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"TechChallengeApp\/0006-environment-variables-overrides-config-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn some environments the port and other variables are only available on startup, and will have to be overriden. See https:\/\/github.com\/servian\/TechChallengeApp\/issues\/21\\n","Decision":"Add environment variables overrides back\\n","tokens":42,"id":4584,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"TechChallengeApp\/0002-use-viper-for-config.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe solution was built using a custom toml configuration solution, should we standardise on a library for less maintnance overhead?\\n","Decision":"Decided to use viper as the configuration library, as it tightly integrates with cobra which we already use for helping with command line integration.\\n","tokens":30,"id":4585,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"TechChallengeApp\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4586,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"wikidata-query-builder\/0000-use-toolforge-kubernetes-for-test-system.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis project is new and its exact technological future is unknown. For example, we still don't know if this will have to be a nodejs app in the future, or if it will be only a client-side app served as static assets using Apache, Nginx, etc.\\nWe also need a publicly accessible test setup to have its features tested by UX and PM once they land on the main branch. We also need a development environment, need a CI environment and so on. Each environment has its own unique requirements. For production we are not sure yet whether we will use [the Blubber deployment pipeline](https:\/\/wikitech.wikimedia.org\/wiki\/Blubber) and deploy the Query Builder as a node app or whether it will be part of the standard WDQS GUI assets.\\nToolforge seems like a good setup for small projects due to its baked-in support of serving static assets using lighttpd or serving node10 apps. It has been changing and growing more mature. One of its most recent features is support for dockerized webservices and cronjobs using kubernetes. You can read more about it in [here](https:\/\/wikitech.wikimedia.org\/wiki\/Help:Toolforge\/Kubernetes). We can't use our built Dockerfile in Toolforge and if we want to go in that direction, we would have to have a dedicated VM just for this service in CloudVPS instead and make it automatically updated using puppet or ansible which would be too much work for too little gain.\\n","Decision":"We are going to have the test setup in Toolforge, served as static assets but built using a Kubernetes cronjob that builds these assets using the node10 docker image in Toolforge automatically and copies them to the public endpoint. You can see the exact configuration and steps in [PR#9](https:\/\/github.com\/wmde\/query-builder\/pull\/9).\\n","tokens":307,"id":4587,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"open-bounty\/0004-use-tachyons.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA primary component of OpenBounty is a web application. As part of our work on this web application we regularly need to implement new UI elements or flows to support overall product development. This frontend work requires usage of CSS to specify positioning, text styles and many more variables.\\nA common problem with CSS is that developers try to generalize CSS classes so that they can be reused (see e.g. [BEM](http:\/\/getbem.com\/)). Arguably the intention is great but inevitably the time will come when constraints change and so the component's CSS is modified. By that time other people may have used that component in other places relying on the current implementation.\\nIn programming languages breaking a collaborator's expectation like this can be mitigated using assertions or automatic tests but this is less easily done when working with CSS.\\n","Decision":"In order to avoid the problems outlined above we will adopt the approach of using atomic, immutable utility classes as promoted by the [Tachyons](http:\/\/tachyons.io\/) library.\\nTachyons provides safe-to-reuse, single-purpose classes that help with achieving consistent scales of whitespace and font-sizes.\\nBy not modifying the definition of CSS classes anymore we can safely build out UI components using those classes without needing to worry if we're breaking someone else's expectations.\\n","tokens":171,"id":4588,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"open-bounty\/0002-sign-commits-with-gpg.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOpenBounty is a system which has value flowing through it.\\nNaturally security is a concern that should be taken into consideration.\\nCurrently an attacker might get access to an account of a team member\\nand pose as that developer, merging PRs and pushing changes.\\nStatus.im as a company is also encouraging the use of GPG signing and\\nhas a Pull Request check in place on Github. This check will mark PRs\\nas failing if the commits come from an organization member and have not\\nbeen GPG-signed.\\n","Decision":"In order to verify that commits in the repository are actually authored by the specified\\nauthor we adopt [GPG signing of Git commits](https:\/\/git-scm.com\/book\/id\/v2\/Git-Tools-Signing-Your-Work).\\nThis will allow us to verify authenticity of the author information saved in\\na Git commit and make workflows like deploying on push safer.\\nIt also introduces some complexity because contributors who want to sign\\ntheir commits need to set up the appropriate tooling. Due to that we will\\nnot require outside contributors to sign their commits for now.\\nAdopting GPG signing for contributors will also make our PR checks pass\\nallowing us to more easily discern actually broken and working PRs.\\n","tokens":112,"id":4589,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"open-bounty\/0003-include-compiled-web3j-contracts-in-git.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe OpenBounty project utilizes smart contracts to make certain core\\naspects of it's product work without centralized trust.\\nIn order to develop the platform a Java interface to these contracts needs to\\nbe built beforehand. To create those interfaces various tools are required\\n(`web3j` & `solc`), often in specific versions that are not easily available\\nvia widespread package managers.\\nThis hurdle also applies to any other situations where the application is set\\nup from scratch, e.g. continuous integration.\\n","Decision":"Instead of forcing every contributor to install those tools we will include\\nthe compiled Java interfaces in our Git repository. This removes a significant\\nsetup cost and hopefully allows people to get going much faster.\\nInstalling `web3j` and `solc` will only be required when hacking on the\\ncontracts itself which are much more stable than the majority of the code.\\nAn alternative would be implementing scripts that install those tools in a\\nplatform independent manner but this would require more work. Once we have\\nthe time or someone wants to work on creating those scripts we can easily\\nrevert the decision outlined in this document.\\n","tokens":108,"id":4590,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"elm-spec\/001_animation_frames.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Elm runtime uses `requestAnimationFrame` to keep the view updated. In order to\\nrun faster specs, elm-spec uses [@simon\/fake-timers](https:\/\/github.com\/sinonjs\/fake-timers)\\nto fake the `requestAnimationFrame` function (along with other timers). This provides\\nelm-spec with control over when to run this function, and thus when to update the view.\\nHowever, things are a little more complicated than they might seem. It turns out that the\\nElm runtime also uses `requestAnimationFrame` during some commands that involve the DOM,\\nin particular those like `Browser.Dom.getElement` in the `Browser.Dom` module. This means\\nthat sometimes a step might actually need multiple animation frames to complete and update\\nthe view as expected. This gets complicated when a program uses the `Browser.Events.onAnimationFrame`\\nsubscription, which sends an update message on each animation frame.\\nPreviously, elm-spec would run the next animation frame before any step or observer that\\ntouched the DOM (so that the view was updated for that step), and attempt to continue to\\nrun animation frame tasks until none remained at the end of that step. This was a little weird,\\nand had to try and only run *newly added* tasks so that the spec would not go into an\\ninfinite loop -- since the task that updates the view and the task that listens for\\nanimation frames each add a new request for the next animation frame when they are run.\\nOne major problem with this approach is that sometimes a program might need to run the next\\nanimation frame even if the step itself doesn't touch the DOM. For example, if `Browser.Dom.getElement`\\nis called when a port message is received, and the spec wants to see that the size of the\\nelement is retrieved and sent out on another port or something. The step and observer just\\ninvolves Ports but an animation frame needs to run for the command to complete.\\nThere also exists `Spec.Time.nextAnimationFrame` for manually running the next animation frame.\\nHowever, in the case described, something would happen where even that wouldn't trigger the\\nexpected behavior. And in any case, this would actually continue to run animation frames if\\nthey were added in the process of execiting a command, and so probably should have been called\\n'runAnimationFramesUntilComplete' or something.\\n","Decision":"Elm-spec will run an animation frame (or try to) at the *end* of each spec step, no matter what,\\nincluding the step that runs the initial command. And it will *only* run one animation frame.\\nIf any extra animation frame tasks are detected (that is, if there is more than one remaining\\nanimation frame task, which is the view animator), then the scenario will be rejected with a\\nmessage explaining that one of the steps triggers extra animation frames.\\nIn order to prevent the scenario from being rejected, one needs to add `Spec.Time.allowExtraAnimationFrames`\\nto the setup section of the scenario. The point of this really is just to let people know that\\nsomething funny could be going on. One can then use `Spec.Time.nextAnimationFrame` to\\nrun single animation frames until one triggers the behavior one wants.\\nThis seems to work much better. First, many common scenarios will just work as expected. If,\\nfor example, one triggers a `Browser.Dom.getElement` command based on some step that just\\nsimulates receiving a port message, then that will just work as expected since the animation frame\\nwill be run at the end of the step -- this will update the view and also run the `getElement`\\ncommand.\\nSecond, in the case of more complicated scenarios involving DOM commands that trigger other\\nDOM commands or `Browser.Events.onAnimationFrame` the spec writer will be alerted to the fact\\nthat things are complicated, and will have full control over how many animation frames to\\nexecute.\\n","tokens":492,"id":4591,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"grafana-simple-grpc-datasource\/0002-add-provide-listmetrics-with-targetquerytype.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSome metrics do not support all query types. The list of metrics, however, does not have any context about the query for which query type the metrics are selected.\\nProviding the `ListMetrics` API with more context information would make it possible to filter metric lists\\n","Decision":"Reject this change because it degrades the user experience of the plugin.\\n","tokens":58,"id":4595,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-32-wearable-committee-reverts.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe wearables committee needs to be able to revert wearables to a previous version. This is because there is a small time window where the committee might approve one version of a wearable, but the collection owner then deploys a new version.\\nNow, there is a property on the collections subgraph called `contentHash`. This property is a string that can be used by the committee to specify a specific wearable version. The questions is what how to specify such version.\\n","Decision":"Option 1 seems like the easiest, but it has a problem. The validation mechanism uses the entity's timestamp to determine which block to check the values from. Therefore, the timestamp needs to be after the blockchain was modified in step (4), but it also has to be a timestamp of a block that has already been mined. This is because the validation would fail if there is no block for a specific timestamp. So choosing a time in the future that needs to be after step (4) but not too far into the future can be extremely flaky.\\nThat's why we decided to go with option 2.\\n","tokens":101,"id":4598,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-39-dapps-blockchains-support.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nEvery dApp and the explorer support meta transactions following the [EIP-712](https:\/\/eips.ethereum.org\/EIPS\/eip-712) to interact with secondary blockchains as Polygon. Meta transactions are sent by the transaction server which is using a third-party to relay them.\\n[Trezor](https:\/\/github.com\/trezor\/trezor-firmware\/pull\/1568) and Smart contract wallets (we don't support on-chain validation for signatures) can't send meta transactions. Moreover, users may want to not depends on a server to interact.\\nCurrently, the dApps and the explorer only support one chain per domain.\\n","Decision":"To make life easier for the users, we have decided to support multiple blockchains by using the same domain\/subdomain. Users can switch to different networks and the webapp must respond to the network change event by enabling\/disabling features.\\n# Solution\\nThe dApps team will make a breaking change in the [decentraland-dapps](https:\/\/github.com\/decentraland\/decentraland-dapps) lib to support multiple blockchains in the dApps.\\nEach dApp will require a manual update to enable\/disable features. A common strategy for this will be needed across all the dApps.\\nThe explorer & the SDK should provide a way for the scenes to check the network where the user is connected.\\n# Participants\\n- Nacho M.\\n- Brian.\\n- Juanca.\\n- Mendez.\\n","tokens":139,"id":4599,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-2-state-sync-for-builder-in-world.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to find a sustainable path to develop the builder in world without compromising future plans and also enabling experimenting with the static scene definition initiatives and new SDK.\\nThe domain of the problem can be divided in three big chunks:\\n* State management\\n* Synchronization\\n* Behavior\\nToday we are deciding on the State management.\\n","Decision":"#### Alternative 1\\n* Kill worker solution\\n* We are able to play and stop the scene from the builder in world.\\n* Requires code generation\\n* Synchronization is handled by kernel\/renderer\\n* Publish is still an irreversible process\\n#### Alternative 2\\n* Keep worker & stop systems\\n* Delay builder in world until synchronization works (requires work on new SDK)\\n* We can't go on this alternative until smart items are component based (not code-gen)\\n* End goal, but it has so much constraints right now\\n#### Alternative 3 \u2705\\n* Like alt 1 but creating new worker for syncing instead of using kernel\/renderer\\n* Seems like a nice decoupling of new SDK vs existing\/old\\n* Can work as a foundation for new runtime\\n# Participants\\n- Nicolas Chamo\\n- Pablo de Haro\\n- Esteban Ordano\\n- Agustin Mendez\\nDate: 2020-10-06\\n","tokens":72,"id":4602,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/0002-typescript-for-programming-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRuntime errors happens pretty often when using JavaScript. As the application grows, we face errors that would not be thrown if we were using a statically-typed language.\\n","Decision":"Use TypeScript when developing frontends and node.js backends\\n","tokens":36,"id":4603,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/0001-prettier-for-code-formatting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCode formatting is a problem when scaling a team. Each developer has his own code styling preferences, causing unconsented code.\\nWe need a tool for normalize our javascript code formatting.\\n","Decision":"Use prettier as the tool for javascript code formatting\\n","tokens":41,"id":4605,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-16-unity-data-store-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn many use cases, we have a lot of data that needs to be accesed on almost a project-wide (or module-wide) scope.\\nThis need brought us to define a standarized approach for data storage.\\nThis approach should cover the following use cases:\\n* Maintenance ease: new fields should be easy to add, modify and retrieve.\\n* Need to know when specific data points change to react accordingly.\\n* Cyclic assembly references prevention. Data as a leaf node of the dependency graph.\\n* Easy to mock.\\nIn the current state, this kind of necessity is covered by a few static classes wrapped around Unity `ScriptableObjects`.\\nEach of field of the static class is a variant of a custom `BaseVariable` type.\\nThe `BaseVariable` wraps around a value and gives us `OnChange` events.\\n```csharp\\npublic static class CommonScriptableObjects\\n{\\nprivate static Vector3Variable playerUnityPositionValue;\\npublic static Vector3Variable playerUnityPosition => GetOrLoad(ref playerUnityPositionValue, \"ScriptableObjects\/PlayerUnityPosition\");\\nprivate static Vector3Variable playerWorldPositionValue;\\npublic static Vector3Variable playerWorldPosition => GetOrLoad(ref playerWorldPositionValue, \"ScriptableObjects\/PlayerWorldPosition\");\\nprivate static Vector3Variable playerUnityEulerAnglesValue;\\npublic static Vector3Variable playerUnityEulerAngles => GetOrLoad(ref playerUnityEulerAnglesValue, \"ScriptableObjects\/PlayerUnityEulerAngles\");\\nprivate static Vector3Variable playerUnityToWorldOffsetValue;\\n...\\n}\\n```\\nUsing `ScriptableObject` instances like this has the following boons:\\n- As they are serialized assets, they can be referenced from scenes, another assets, etc.\\n- We can look up their value from Unity editor to debug.\\n- When working with artists and designers, we can give them a tool to parametrize certain values.\\nHowever, we are looking at these downsides too:\\n- Serialized assets means that each field is going to take space on our Unity bundle. Small space, but still.\\n- Having hardcoded paths on the static class. Not very maintainable as moving assets around can break the code.\\n- Having to wrap every access in a `GetOrLoad` call that internally uses `Resources.Load`. This has a bit of overhead that we could prevent.\\n","Decision":"We choose to be boundary agnostic because this will give us the versatility of using the best approach for any situation. We discussed that in case of going for POCO and needing an SO later, the need refactoring work would be very low.\\nBy choosing to deliberately use a data store pattern, we can name it accordingly and define the system responsibilities in a more direct way than we have now.\\nAlso, we have the added benefit of a more cohesive design between Kernel and Unity, because Kernel already has the redux data store concept.\\n","tokens":499,"id":4606,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/0001-SYS-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4607,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-45-entities-v4.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to ensure entities once deployed are always valid and every client knows how to read information. For the time being, entities can be deployed with any information as metadata, which can lead to several issues when something is missing or contains invalid data.\\n","Decision":"We defined a list of changes to support the new validations\\n- Entity versions will stop being used, instead the current V3 schema will remain. The schema will evolve in time and the entity signature timestamp will be used to run the validations and schema coercions.\\n- As part of this developments, entities v1 and v2 will be migrated and redeployed as v3. Synchronization support for those entities will stop after the migration and the code will be safe to be deleted.\\n- [Entity schemas][1] will be defined for all entity types and every deployment will be validated against them.\\n- As there will be a transition period in the content server, when getting the deployments through lambdas we will need to transform any entity into a compliant format. Most probably that this change will only be needed for wearables names and descriptions.\\n- Deployment size will be validated per pointer taking into account previous deployments and the final result and not just the current deployment files. This will prevent land owners to exceed the size limits as it happens today.\\n- Wearables size will be validated without taking into account the generated images (thumbnail and image with rarity background) and a new size limit will be set:\\n- Total size: 3MB\\n- All files size without generated images: 2MB\\n- A deadline will be defined for the new validations to start working.\\n- Every active repository in the decentraland github organization where entities types are defined to use [common-schemas][1].\\n","tokens":56,"id":4608,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-30-front-and-back-end-architecture-for-the-marketplace.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe newest version of the Marketplace supports various sources for it's data.\\nWe need to agreggate the blockchain, multiple graphs and data from our partners, which themselves are represented as API requests to different servers.\\nWe don't only need to show the data to the user, but we should also provide different ways to interact with it, mainly buying and selling assets. We should also allow the user to filter the data in different ways.\\nTo do this, assets from different sources have their own version of what it means to filter them, and we might also need to interact with different contracts, depending on the operation.\\n","Decision":"We choose option 2 because it gives us many advantages over centralizing the logic in the front-end, like:\\n- having a server other parts of Decentraland can access\\n- making the front-end lighter and simpler, to be able to deliver the webpage faster to the user\\n- providing caching for problematic endpoints if necessary\\nbut allowing us to keep parts that are Marketplace-centric in the front end, like the interaction with partners and with contracts.\\n","tokens":132,"id":4611,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-50-isolated-scenes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe project is growing with more features\/systems and some of them may be not compatible together\\nlike for example builder and avatars moving through comns. Also, some scenes are starting\\nto be perfomance-intensive. If the user want to give all the resources to a scene to speed it up,\\nit should be able to do it. In order to try to tackle this problem, we need a way to disconnect\\nfeatures\/systems and scenes\\n","Decision":"We went for the option 1: Isolated modes because this way kernel and the renderer will be decoupled and we\\nstill can have control about the systems\/scenes that are enabled\/disabled\\n<!--\\n```sequence\\nparticipant Unity as R\\nparticipant Kernel as K\\nparticipant Scene worker as W\\nnote over R: Start isolated mode\\nR->K: enter isolated mode(mode)\\nK-->K: Enable\/disable required systems\\nK->R: Enable\/disable required systems\\nK-->W: Create new worker (only if necessary)\\nK->R: LoadParcelScenes(sceneId) (only if necessary)\\n-->\\n![resources\/fig-initial-load.svg](resources\/fig-initial-load.svg)\\n","tokens":99,"id":4612,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/0003-sys-use-other-theme-for-adrs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4621,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-22-quests-progress-ui.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow to render the Quests progression?\\nBased on the context of a previous meeting with Pravus, Alex, Pablo, Marcosnc and Mendez on Jan 7, 2021:\\n","Decision":"We choose B because it enables us to render the quests UI without coupling the quests controller and therefore, the server. Also positions Decentraland in a more decentralized path by making centralized servers not required to access the features.\\nAnyone could now implement their own custom Quests servers without asking permission or having the platform as a limitation.\\n","tokens":47,"id":4622,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-35-coms-protocol-optimizations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe are facing some issues that need to be addressed. We can list a couple of them as important:\\n- The hard limit of 100 people we currently have is not that bad for small events or some games and expositions. But we have it for the whole world. We need to make better use of the 100 limit, which is of network size and not of people. We also need to improve this limit for some situations, like big events.\\n- New users have a hard time understanding realms. Since we didn\u2019t make them an integral part of UI\/UX, the only way a user can know in which realm they are is in URL.\\n- We don\u2019t do enough to communicate the status of the realms to the users. A user can be alone in a realm and don\u2019t know why they aren\u2019t seeing anyone, given that there could be a lot of users in another realm.\\n","Decision":"We will start rolling out a series of Protocol optimizations to help Decentraland communications systems to scale in different ways.\\nPlanned optimizations topics are:\\n- Reduce complexity of layers for users, removing them completely, and by doing so, optimize \"finding people in the world\"\\n- Enable vertical & horizontal scallability\\n- Optimizing connections topologies in P2P networks\\n- Reduce latency between users\\n- Increase success rate in WebRTC P2P connections\\n### Rationale behind the removal of \"layers\"\\nLayers are logical \"connection groups\" inside the realms, as of today, realms are mapped 1to1 with a catalyst instance. Inside the catalyst, it would be sub-optimal to connect users all-to-all.\\nBy default every catalyst have 28 layers, each one with a different color name. We can infer then, that the maximum amount of concurrent users connected to a catalyst is 2800 users (28 * 100).\\nLayers are the color after the name of the realm:\\n```\\nartemis-amber\\n^^^^^ layer\\n^^^^^^^       realm\\n```\\nLayers served some purposes during their existence:\\n- To enable horizontal scallability in catalysts, without the need of DAO approval, each catalyst have layers, which are groups of up to 100 users inside a catalyst.\\n- By separating users logically, that gives the renderer some room to render fewer avatars and therefore allocate more resources to the rendering of other assets.\\nLayers served their purpose in early stages of the project, but the time has come to evolve the protocol. Therefore we introduce _connection islands_.\\n#### What are connection islands?\\nConnection islands are groups of connections based on their geographical location, instead of logic layers.\\nThe implementation performs real-time clustering, organizing connections in islands (clusters) of close peers, which will connect to each otheer\\nThat solves many problems:\\n- Reduces the complexity to find people in the world: Only the realm is required now to find a friend.\\n- Optimizes finding people: it was common to join a realm with lots of people, but the connection is assigned to a layer without people in the parcel you are standing. While in other layers you may find people in that layer.\\n- Removes the 2800 user limit: there can be virtually infinite islands in a catalyst, as long as the resources of the server allow it.\\nThe implementation of the algorithm can be found at https:\/\/github.com\/decentraland\/archipelago\\n","tokens":191,"id":4624,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-13-custom-ui-modes-for-builder-in-world.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen switching in and out of the builder in world, we need some way to switch the hide\/show state of arbitrary UI elements of the explorer.\\nRight now, the show\/hide interface responsibility is defined within the `HUDController` class.\\nCurrently, `HUDController` has the following responsibilities:\\n**Kernel <> renderer bridge**\\nKernel sends messages directly to this class for interaction\\n**HUD Factory**\\nSelf-explanatory. Related to instancing the actual HUD elements.\\n**HUD Initialization**\\nSometimes, special initialization should be performed after the HUDs are instanced.\\n**HUD Composite**\\nThe references to the instanced HUD components are stored in this class.\\n<!--\\n```dot\\ndigraph G {\\nsubgraph cluster_0 {\\n#style=filled;\\n#color=lightgrey;\\nnode [style=filled shape=rect];\\na0 -&gt; a1 -&gt; a2;\\nlabel = \"HUDController\";\\n}\\nstart -&gt; a0;\\na0 [label=\"Bridge\"]\\na1 [label=\"Factory\"]\\na2 [label=\"Composite\"]\\na3 [label=\"HUD Elements\" shape=rect]\\nedge [dir=\"back\"]\\na2 -&gt; a3\\nstart [label=\"Kernel\" shape=box3d];\\n}\\n```\\n-->\\n![resources\/ADR-13\/fig-custom-ui-modes-for-builder-in-world-1.svg](resources\/ADR-13\/fig-custom-ui-modes-for-builder-in-world-1.svg)\\n","Decision":"We agreed upon implementing the ScriptableObjects solution as a first stage, and then perform the responsibility segregation redesign on the `HUDController` class later.\\n","tokens":339,"id":4625,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-4-collections-architecture-in-L1-L2.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCollections will be key in the Decentraland ecosystem. To bring more scalability and reduce costs the L2 will be seen as the main chain, and L1, as the gateway for collectors to use\/buy\/sell tokens in other platforms.\\nL1 could be seen as Ethereum mainnet and L2 any other sidechain EVM compatible as Matic or xDai.\\nEach collection follows the ERC721 standard. Every token transferred between layers will keep a reference of itself in the main chain:\\n- Collection address in L2. E.g: `0x2dac71c8c8a4b9547b53c1e1838152ca3277ce76`\\n- Token Id in L2. E.g: `1`\\n- Token URI in L2 (URI used for Decentraland and other platforms to know how the token looks like). E.g: `https:\/\/peer.decentraland.org\/lambdas\/collections\/standard\/erc721\/0x2dac71c8c8a4b9547b53c1e1838152ca3277ce76\/0`\\nThis document presents 2 alternatives on how to manage the collections between L1 & L2.\\n","Decision":"#### Alternative 1\\n#### Pros\\n- N\/A\\n#### Cons\\n- Moving the first token from a collection to L1, involves creating a contract using the factory (gas cost).\\n#### Implications\\n- Keep the same address and token ids between layers.\\n- Users will need to send a transaction for each collection to authorize the usage of tokens.\\n- Third party marketplaces (OpenSea) will not know without any effort if the contract is an official Decentraland collection in L1.\\n#### Alternative 2 \u2705\\n##### Pros\\n- Users will need to send only one transaction per marketplace to authorize the usage of tokens.\\n- We can use the unique collection as a valid and the official one for Decentraland in L1.\\n- Automatic and instant availability in marketplaces with the whitelisted collection.\\n- Fewer moving parts\\n##### Cons\\n- N\/A\\n##### Implications\\n- Collection addresses and token Ids will not be the same in L2 and L1.\\n# Open questions\\n- **Do the token URI changes in L2?**\\nToken URI of L2 is stored in the main collection in L1\\n```yaml\\nOpenSea (mainnet): opensea.com\/mainnet\/tokens\/{collection_l1}\/{hash(collection_l1_1, token)}\\nOpenSea (L2): opensea.com\/matic\/tokens\/{collection_l2_1}\/{token}\\n```\\n```python\\n# L2\\ncollection_l2_1.tokenURI(token_1) = https:\/\/peer.decentraland.org\/collection_l2_1\/token_1\\nmapping(tokenId => string) tokenURI\\n# L1\\ncollection_l1.tokenURI(hash(collection_l2_1, token_1)) = https:\/\/peer.decentraland.org\/collection_l2_1\/token_1\\nmapping(tokenId => string) tokenURI\\n```\\n- **Alt 1. Is it necessary to burn the token when L1->L2?**\\nIt could be not done. It is a common practice.\\n- **Alt 1. We could have the same collection address in L2 and L1**\\nThat was the idea.\\n- **How do we validate allowed\/denied items when moving from L2 to L1?**\\nValidate if the collection was created by factory.\\nTBD.\\n- **How do we prevent abuse on meta-transactions paid by DCL?**\\n.\\n- **What do we do if Matic is no _longer viable_ or _moving_ to another chain?**\\nWe can restore the state from a blockchain snapshot and copy the contracts.\\nUsers may need to manually send the tokens as a result of the migration.\\nThere are coordination risks with third parties (open sea selling items in the middle of the migration).\\nEnforce pause mechanisms? Upgradable contracts?\\n- **Who pays the tx fees when L2->L1?**\\nAs it is today, the user pays the TX.\\n- **How does the approval flow affect any of the options in L2?**\\n> If DAO is in L2 and it is cheap it would be good to vote everything.\\n_Reviewers will have to review several orders of magnitude of collections in L2_\\n# Participants\\n- Esteban Ordano\\n- Ignacio Mazzara\\n- Marcelo Alaniz\\n- Nicolas Santangelo\\n- Juan Cazala\\n- Agustin Mendez\\nDate: 2020-10-13\\n","tokens":260,"id":4627,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-43-catalyst-api-refinment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe [Catalyst API](https:\/\/decentraland.github.io\/catalyst-api-specs\/) has been evolving and adding different functionalities, endpoints and parameters as needed by the evolving platform requirements. Because of this needs, some changes while good decissions at the moment to unblock features, were not thought hollistically or now they may be causing too much unneded Data Transfer cost or have room for perfomance improvement for the Catalysts core processes like Bootstrapping and Synchronization.\\n","Decision":"After reviewing the Catalyst API we defined a set of changes to clean up and close a first version of the Catalyst Protocol. The changes that will be described below will enable us to:\\n- Reduce Data Transfer cost by only narrow down the response bodies to only return the needed information\\n- Reduce public API complexity by hiding endpoints that are only inteended for troubleshooting, this will also reduce hits to endpoints that should not be used externally\\n- Improve the Catalyst bootstrapping and synchronization time by reducing the amount of request and information recieved for the synchronization\\n- Do a more efficient use of the DB\\n- Open the door to create new Catalyst implementation based on this protocol definition\\nList of API changes:\\nDeprecated endpoints won't be part of the Catalyst Protocol definition but we may keep some of them as internal troubleshooting tools, these endpoints will require a key in order to be used.\\nThose endpoints that will be removed from the Protocol but remain available for internal use, have the `SECURE` tag in the change description.\\nEndpoints not mentioned in the following list will not suffer any change.\\n| Endpoint                                                         | Deprecate YES\/NO | Change Description                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\\n|------------------------------------------------------------------|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `\/content\/deployments`                                           | YES              | `SECURE`: This endpoint consumes DB resources inefficiently  as it's equivalent to a SELECT ALL. This endpoint also has a complex UX with the application of many filters and a lot of parameters. We are going to tag this as TROUBLESHOOTING and add 2 new endpoints: one to retrieve the list of all pointers and another to retrieve the history of a given list of pointers. Catalyst synchronization will be done using `\/snapshots` and `\/pointer-changes`. |\\n| `\/content\/pointers\/{entityType}`                                 | New endpoint     | Given an entity type (scene, wearables) this endpoint returns the list of all the pointers paginated                                                                                                                                                                                                                                                                                                                                                                        |\\n| `\/content\/pointers\/{entityType}\/history?pointer=0,0&pointer=1,1` | New endpoint     | Given a list of pointers of the specified entity type, returns the history of deployments                                                                                                                                                                                                                                                                                                                                                                                   |\\n| `\/contents\/{hashId}\/active-entities`                             | NO               | `SECURE`: Change resource to `\/entities` and include a parameter to expand it's functionality in order to be able to retrieve inactive entities                                                                                                                                                                                                                                                                                                                    |\\n| `\/content\/failed-deployments`                                    | YES              | `SECURE`: This endpoint should be removed from the public API, and only be used with a key for the Catalyst Monitor and for troubleshooting or the auto-fix deployments functionality                                                                                                                                                                                                                                                                              |\\n| `\/lambdas\/health`                                                | NO               | We need to add more information like the Unhealthy message. Currently you get an unhealthy state and no information about why the service is unhealthy                                                                                                                                                                                                                                                                                                                      |\\n| `\/lambdas\/contentv2\/scenes`                                      | YES              | Do we need the v2? this endpoint should be generic to retrieve the list of scenes of a given coordinates                                                                                                                                                                                                                                                                                                                                                                    |\\n| `\/lambdas\/contentv2\/parcel_info`                                 | YES              | The information provided by this endpoint is presnet in `\/content\/entities\/{entityType}?id={cid}`. We are going to add some filters to the existing endpoint to help you select which part of the entity you want to retrieve, e.g. content, metadata                                                                                                                                                                                                                       |\\n| `\/lambdas\/contentv2\/contents\/{cid}`                              | YES              | There are old entities v2 that are still requested and may still be valid. We need to see if we can update these entities and deprecate this endpoint                                                                                                                                                                                                                                                                                                                       |\\n| `\/lambdas\/crypto\/validate-signature`                             | YES              | `SECURE`: This should be done with a library in the client side. Add a new lambda to return the Graph URL that we are using or add this information to the `\/lambdas\/contracts\/servers` endpoint                                                                                                                                                                                                                                                                                      |\\n| `\/comms\/islands`                                                 | YES              | `SECURE`: Remove from the public API and add a key to use this information for troubleshooting                                                                                                                                                                                                                                                                                                                                                                     |\\n| `\/comms\/islands\/{islandId}`                                      | YES              | `SECURE`: Remove from the public API and add a key to use this information for troubleshooting                                                                                                                                                                                                                                                                                                                                                                     |\\n| `\/comms\/peers`                                                   | YES              | `SECURE`: Remove from the public API and add a key to use this information for troubleshooting                                                                                                                                                                                                                                                                                                                                                                     |\\n# Status\\nRejected by DAO\\n# Consequences\\n`\/deployments`: We are not aware if this endpoint is being used for anything else besides the Catalyst synchronization and bootstrapping processes.\\nThe `fetchAllDeployments` method from the [Catalyst Client](https:\/\/github.com\/decentraland\/catalyst-client) will only be available to be used by the Catalyst Servers in order to sync all the Content Service history.\\nThe rest of the changes should not cause any impact on the Catalyst clients\\n","tokens":104,"id":4628,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-11-add-version-to-content-as-bundle-url.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe URL for the asset bundles (Unity) are created using the content-server hash of the original GLTF. Therefore, the generated contents may not be immutable, because the asset bundle generator may have bugs or upgrades.\\n","Decision":"Prepend `\/v1` to the original content-as-bundle URL pathname.\\n```yaml\\nOld: https:\/\/content-assets-as-bundle.decentraland.org\/QmfNvE3nKmahA5emnBnXN2LzydpYncHVz4xy4piw84Er1D\\nNew: https:\/\/content-assets-as-bundle.decentraland.org\/v1\/QmfNvE3nKmahA5emnBnXN2LzydpYncHVz4xy4piw84Er1D\\n```\\nThe current (unversioned) URL must continue working.\\n","tokens":50,"id":4629,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-52-content-new-sync.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe Content Servers have to offer the ability to deploy content on them, but also ensure that any content deployed on any other valid and running Catalyst from the DAO List is available there too. Resolving this is called \u201csynchronization\u201d. Due to the increase of users that Decentraland has experienced last year, the synchronization was the bottleneck to support more concurrent users, so the way that it was done had to be changed.\\nThis document does not discuss ideas to change the way that the catalyst communicate with each other or their topology, that topic will be discussed in another document.\\n","Decision":"### New Sync Logic\\nThe main motivation on this new logic is on making the Catalyst network work on a bigger scale. Historical deployments are demoted to be of lower priority and will not be warranteed to succeed. The focus is on keeping all catalysts in the network up to date and serving the same state.\\nAll the catalysts continue to communicate with each other, but in a more efficient way:\\n1. Decreased the amount of requests for bootstrapping from N to 1. Being `N = TOTAL_DEPLOYMENTS \/ PAGE_SIZE (paginated list) + TOTAL_DEPLOYMENTS * 1 (audit data)`\\n2. Deprecate the endpoint \/deployments (which was the most expensive in the db).\\n3. Add missing information to \/snapshot endpoints to include `authChain` to reduce by TOTAL_DEPLOYMENTS the amount of requests.\\n```\\n\/\/ Synchronization code\\nconst allServers = await  getCatalystsFromDAOList() \/\/ fetch list of catalysts from the DAO contract\\nasync function sync(remoteServer) {\\n\/\/ Bootstrapping\\n\/\/ GET \/snapshots : get all entities that are active\\nallDeployments = await remoteServer.getSnapshots()\\ndeployLocally(allDeployments)\\nremoteServer.updateLastSeenTimestamp() \/\/ most recent timestamp seen in snapshots\\n\/\/ Syncing\\nwhile(remoteServer.isOnline()) {\\n\/\/ GET \/pointer-changes : only the new deployments\\nallDeployments = await remoteServer.getNewDeployments()\\nawait deployLocally(allDeployments)\\nremoteServer.updateLastSeenTimestamp() \/\/ most recent timestamp seen in pointer-changes\\n}\\n}\\nfor (server of allServers) {\\n\/\/ concurrently\\nsync(server)\\n}\\n```\\nIn pseudo code both implementations look similar. The key difference is to stop using the `\/deployments` endpoint and implement a new performant `\/snapshots`.\\nThe `\/deployments` endpoint caused a very expensive query to the DB, depending on too many filters and conditions. Indexes were created and changed in the database, but it still caused very expensive queries.\\nThe new snapshots are generated with a certain frequency (currently set to 15 min, but anything up to 6hs sounds reasonable) with a full scan of the database which only filters that the `overwritten_by` field is `null`. This ensures that all active entities are retried. Then that data is stored in a file, so when requesting the `\/snapshots` endpoint you receive the hash of the file with the information and no query to the database is done.\\nThen `\/pointer-changes` retrieves all the deployments done in a period of time, this includes not-active entities too (those who have been overriden by a new deployment). So, the only historical data that may be lost is the data transferred when a Catalyst was down. This endpoint is stored in a separate db and the requests are optimized.\\nA future enhancement to the synchronization contemplates the addition of an endpoint similar to `\/pointer-changes` that only includes the freshest pointers, ignoring the history in between. For a lighter use of the table and indexes holding the active entities.\\n","tokens":122,"id":4630,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-37-explorer-desktop-launcher-technology.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe Desktop Client Project needs an Application that can easily install and update the Desktop Client.\\nIt must be compatible cross-platform with Windows, macOS and Linux.\\nThe launcher will be the main application that the user will download from the website. So it must be a small size app.\\nTo make this application, the team considered the following options.\\n","Decision":"The selected option it's option **C**, `Electron App with React JS`. There are many advantages: for example, that the project can be split up into multiple teams. It has the self-update feature integrated so the team can deliver the product faster.\\nMore details of how the application will work: https:\/\/www.notion.so\/decentraland\/Desktop-Launcher-ed6aadd11d7b4fd48e5a88400d761ed9\\n","tokens":76,"id":4631,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-38-communication-kernel-desktop.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe desktop client will be executed outside of an internet navigator like Chrome or Firefox, and the `explorer-website` will not be used.\\nCurrently, to log in into the world, you must enter through `explorer-website` (or derived), and the authentication happens with exchanges of messages between `kernel` and `website`.\\nOn Desktop, `website` will not be used, and those exchanges will be with `kernel` and `renderer`. So `explorer-desktop` must implement a way to communicate those to be able to log in.\\nThe problem is how to execute `kernel` without a navigator and communicate it with the `renderer`.\\n","Decision":"Launching `kernel` with `Chromium headless` is the most compatible with what currently `explorer` has. But executing `kernel` in `NodeJS` is inevitable because it will execute `kernel` unit tests and other cases. Taking advantage of this approach is the best option not to rewrite code and isolate it as much as possible.\\nRemoving `explorer-website` from the Desktop Client Project is required. So the communication with `kernel` and `renderer` to authenticate will be implemented without other options.\\n# Solution\\nTo implement what it's described above the team decided the following flows:\\n","tokens":143,"id":4632,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-23-entities-meta-data-flow-for-builder-in-world.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nBuilder in-world needs having per-entity meta-data. For now, is believed that this data stores editor specific attributes, like entity names, lock status, and so on.\\nThis data will be replicated across the network, and will take part of the scene state.\\nAt the moment, the following data is needed per-entity:\\n- **Entity name:** Entities need a name to display on the builder in-world editor outline and info panel.\\n- **Lock state:** Entities need a lock state that determines if the entity can be moved around or not.\\n- **Floor trait:** The floor trait is used to handle floor special cases. i.e.: when a floor is replaced, all the entities that have the floor trait have to be replaced.\\n- **Smart item data:** Each smart item will need its config data to work properly.\\nThe options have been discussed on the following needs:\\n- **Legibility:** The data format has to be legible. We want users to read\/write on it.\\n- **Maintenance cost:** The data has to be friendly with our existing systems to minimize maintenance efforts.\\n- **Flexibility:** We want to be able to modify the data specifications without invalidating existing scenes.\\n- **Scalability:** How our entire stack will behave considering we don't have an upper bound on this data size. New data can be added with each builder in-world feature.\\n- **Performance:** Network replication and data encoding\/decoding shouldn't incur new bottlenecks for the editor use cases. We already agreed that performance shouldn't be an issue because the data update frequency will be low for most use cases, so this is not covered in the discussion.\\n","Decision":"- Options #3 and #4 are reliant on having a clear distinction between having an editor mode and runtime mode, and after realizing that the future plans involve having a blurry line between editor and runtime (i.e. users are going to stumble upon globally editable scenes) the options were discarded.\\n- The benefits behind #2 are linked to implementation details that can be ironed out instead of design benefits, so this option was ultimately discarded as well.\\n- This leaves us again with the #1 option. After discussing, we agreed that some of the data can be grouped in a way logical to its domain. This grouping would mitigate the data scaling issues while preserving all the benefits. So, the outcome is to choose this option but put more effort on the components design. If we do this right, down the road we could cluster all the data in a few mid-sized components.\\n### Criteria for components:\\nLooking at the data based on its use cases and future plans for builder in-world, we can group them like this:\\n- **Entity name:** This data is used for organizing and visual purposes. Most likely, there will be the need of having *creator*, *creation date*, *description*, et cetera. A single component can hold all this information.\\n- **Lock state:** There have been talks about having whitelists or user specific lock status. Right now, the **lock** component will have only a single field, but as this feature gets more complex, all the related data can go into a single component.\\n- **Floor trait:** The floor trait is used to handle floor special cases. i.e.: when a floor is replaced, all the entities that have the floor trait have to be replaced. At first glance this looks like can belong into a **traits** component. As this data is too specific, we are going to let it be as a single component and try to improve the design if we need more traits or a behaviour pattern emerges.\\n- **Smart item data:** Each smart item will need its config data to work properly. We already agreed on having the smart items have their own component regardless of the option chosen.\\n","tokens":350,"id":4633,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-12-documentation-as-a-priority.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n* How do I start the explorer in my machine?\\n* Why do the scenes run in a WebWorker?\\n* Where are the builder resources stored?\\n* What are the main repositories?\\n* How can I add a feature to the SDK?\\nThose are common questions that are raised by both experienced contributor or newcomers to the project.\\nWe are searching for alignment for an approach towards a more sustainable future where people can search and find those answers without depending on specific individuals.\\n","Decision":"We choose to _start a cultural shift towards documentation as a priority_ because:\\n* We want to accept more contributions to the project\\n* Our source code is public, but it has proven to be very complex to understand without help\\n* Onboarding new people to the project is very difficult and almost always requires a mentor\\n#### Actionables\\n* Start communicating to the teams this initiative\\n* Try to slowly start documenting new features or architectural changes\\n* Create a gitbook where we can add documentation in a bureaucracy-free fashion. We spoke about playbooks like: `How to clone and start the Decentraland client`\\n* Evaluate if we can start documenting what we currently have retroactively, no need to rush.\\n","tokens":105,"id":4634,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr\/ADR-14-l1-l2-governance-smart-contracts-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nFor L1, Decentraland has a DAO where people, part of the community, can vote and make decisions from basic signaling to executing an on-chain action.\\nDecentraland needs to scale and give a better experience to its users by using an L2 (second layer).\\nUsers will interact with the same set of smart contracts like the Marketplace and Bid in L1, and new\\nones like the upcoming store for collections' primary sales. Every smart contract which uses MANA but, not only, has\\nprotocol parameters that must be managed by some kind of governance.\\nSo far, DAO voting power is based on the user's wrapped MANA and LAND balance in **L1**.\\nThis document presents alternatives on how to manage governance in L2.\\n","Decision":"### Alternative 1\\n#### Pros\\n- N\/A\\n### Cons\\n- A lot of MANA locked on each layer.\\n- Voting Power divided between layers that can not be summed for on-chain voting.\\n### Implications\\n- Users will need to transfer MANA between layers and wrap it before a proposal started.\\n- Users won't be able to use all the voting power together.\\n- LAND and Estate smart contracts along with their bridges should be deployed in L2 to be used as voting power.\\n- Votes will be cheaper in L2 than L1.\\n### Alternative 2 \u2705 \u2705\\n### Pros\\n- Users will have all their voting power in L1.\\n### Cons\\n- Risky by transmitting all the messages by the same bridge. A smart contract upgrade and a marketplace fee change will go to the same smart contract.\\n### Implications\\n- Votes must still happen in L1.\\n### Alternative 3 \u2705 (Could be an iteration from alternative 2)\\n### Pros\\n- Users will have all their voting power in L1.\\n- We can have different implementations on each bridge.\\n### Cons\\n- N\/A\\n### Implications\\n- Votes must still happen in L1.\\n- Clear execution paths.\\n","tokens":163,"id":4635,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0011-search-query-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0005-design-of-es-documents.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to support most of the existing Code Review systems. Monocle queries should be agnostic regarding the data source.\\n","Decision":"Choosen option: \"A crawler by data source and an unique and generic data schema\".\\nBecause, the addition of a data source support must not have any impact on the queries, the CLI or the WEB UI. The DB schema will be complete enough to fill the basic requirements of each source but will not implement specificities. The terminilogy will be generic for instance a Pull Request and a Review (Gerrit) will be called a \"Change\".\\n### Database objects\\n* Change: an object that describe the status of a Pull Request or Gerrit Review. The Change object will be attached attributes such as: the creation data, the author, the amount of commits within the Change, the changed files list, the title, ...\\n* ChangeCreatedEvent: an object that describe the creation event of a Change\\n* ChangeCommentedEvent: an object that describe a comment event on a given Change\\n* ChangeReviewedEvent: an object that describe a review event on a given Change\\n* ChangeCommitPushedEvent: an object that describe a commit push event on a given Change\\n* ChangeCommitForcePushedEvent: an object that describe a commit force push event on a given Change\\n* ChangeMergedEvent: an object that describe a Change merge event\\n* ChangeAbandonedEvent: an object that describe a Change abandoned event\\n","tokens":32,"id":4638,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0003-choice-of-react.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to provide a client software to consume the Monocle web API. This client software must be easily consumable via a WEB browser.\\n","Decision":"Chosen option: \"React\".\\nBecause it is a today standard. It provides a wide variety of libraries to ease building s complex UI. The React community is pretty large and lot of doc resources are available. Also as that library is popular, it might benefit the project in terms of external contributions.\\n","tokens":36,"id":4639,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0006-cache-es-queries.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo handle the load of multiple connected users, the API service is\\ndoing always the same Elasticsearch requests for the same indices. How\\ndo we minmize this number of requests?\\n## Decision Drivers\\nWe need a simple solution to start that is integrating well with our\\nexisting choices and that has the capacity to evolve into a more\\ndistributed solution if needed in the future.\\n","Decision":"We need a simple solution to start that is integrating well with our\\nexisting choices and that has the capacity to evolve into a more\\ndistributed solution if needed in the future.\\nChosen option: \"[option 2]\", because we have an optional\\nauthentication scheme that would have beed difficult to manage with\\n\"[option 1]\".\\nWe picked [Flask Caching](https:\/\/pythonhosted.org\/Flask-Caching\/)\\nusing Simple caching in memory and the memoize method to cache the\\nqueries for some time. It has the capacity to be used in a distrubted\\nsetup later if needed.\\n### Positive Consequences\\n* using [siege](https:\/\/github.com\/JoeDog\/siege) we were able to\\nsimulate 100 users without problems on a single laptop.\\n","tokens":86,"id":4640,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0004-high-level-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to start with a set of components each dedicated to a task to ensure Monocle being able to scale and also to fit well in a cloud like deployment. No monolithic app, but microservices. As we start we also don't want to overengineer, so just keep the basic components and architecture.\\n","Decision":"Choosen Option: \"A database, a crawler, a, CLI, a WEB API, a WEB UI\".\\nBecause, it fits the described context. The database (Elasticsearch) is scalable, and the other components are stateless, each of them use the database or the API as backend so this will ease in term of scalability. This architecture makes it easy to deploy using docker\/podman or even k8s.\\n","tokens":72,"id":4641,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0002-choice-of-elasticsearch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to store changes data (Pull Requests and Reviewes) in a scalable way. The stored data must be easily accessible in an intelligible manner.\\n","Decision":"Chosen option: \"ElasticSearch\".\\nBecause it fits better our need regarding the style of data we expect to store and how we expect to query the data.\\n","tokens":38,"id":4643,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-matching-service-adapter\/0002-hash-pids-uniquely-for-each-loa.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe pid that the user is issued by their identity provider will not necessarily change when the level of assurance they achieve changes. For example a user may uplift from LoA1 to LoA2 in their IdP, but this will present to the relying party with the same identifier. This is an issue because a Cycle 0 match at LoA1 is not valid for use at LoA2, and presents a specific security risk whereby a user may obtain LoA1 as someone else, person A, and then uplift to LoA2 using their own identity (person B), but continue to assume the identity of person A at the higher LoA. So a change of LoA requires matching to be done afresh.\\n","Decision":"We will modify the pid for non-LoA2 assertions to be unique to that LoA so that it is not possible for relying parties to make an insecure match at Cycle 0 when the user moves between levels of assurance. This will be done by introducing the LoA into the pid hash.\\n","tokens":149,"id":4646,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-matching-service-adapter\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4647,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"polarbears\/ADR-2.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen the delivery people deliver meals to the smart fridges or kiosks, they update the system with the delivery information (the location and the meals).\\nTo determine the inventory at a certain location, the Location Inventory component has to reconcile between the two sources of truths: 1) the information fed\\nto the system by the delivery people 2) the inventory reported by the external Fridge\/Toast APIs.\\nUpon receiving the delivery confirmation, the Location Inventory component can 1) query the Fridge\/Toast APIs and update its data upon synchronous\\nreconciliation 2) asynchronously query the Fridge\/Toast APIs and reconcile.\\n","Decision":"We will use asynchronous communication between the Location Inventory and the External Fridge\/Toast APIs for reconciliation.\\nWaiting for the synchronous reconciliation at a location will increase the time delivery people spend at a location and\\neven may not be possible if the External Fridge\/Toast Inventory APIs are down for some reason.\\n","tokens":134,"id":4648,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"polarbears\/ADR-1.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are several databases within the system. One per module.\\n","Decision":"Under no circumstance should a module read\/write from more than one database.\\n","tokens":16,"id":4649,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-004-relayer-domain-decomposition.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe IBC handlers queries and relayer are defined loosely in the [specs].\\nThe goal of this ADR is to provide clarity around the basic domain objects\\nfrom the perspective of the relayer,\\ntheir interfaces as well as dependencies between them in order to\\nguide the implementation. The success criteria for the decomposition is\\nhow well it can be tested. It's expected that any decomposition will\\nlend itself to tight unit tests allowing more collaborators make change\\nin the code base with confidence.\\n","Decision":"The decomposition should be motivated by what we want to test and what\\nwe need to mock out to exercise the core logic.\\nWe want to be able to test the following high-level functions:\\n* Client create and update\\n* With different chain states\\n* Connection handshake\\n* With different chain states\\n* Channel Setup\\n* With different chain states\\n* Datagram Construction\\n* With different chain states\\n* Datagram to Transaction\\n* Batching\\n* Signing\\n* Datagram Submission\\n* With different chain states\\n* Missing Client Updates\\n* With Missing Proofs\\n* Handlers (datagrams, chain state) -> events\\n* Handling the batch of datagrams\\n* With different chain states\\n* Specifically, the key value store\\n* Produce events\\n","tokens":106,"id":4651,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-006-hermes-v0.2-usecases.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOne major problem with planning for the evolution of Hermes is that presently\\nthere is insufficient clarity regarding its requirements.\\nIt is not known who are the typical Hermes users (is it human operators or\\nautomated pipelines?), and what are their primary use-cases.\\nThis ADR proposes a few use-cases that seem interesting from the point\\nof view of a general target base of users, and which will\\nhopefully be a subset of the requirements of (any) future users.\\nThree elements that provide further context for this discussion are:\\n1. Hermes is still at an early stage of implementation, so these use-cases are\\nnot set in stone.\\n2. Some concrete use-cases are starting to emerge ([#628][#628]), which Hermes\\nv0.1.0 either does not cover altogether, or covers poorly (e.g., because of\\ninconsistent UX), thus informing this proposal.\\n3. Hermes is one of _three_ relayer binaries that are being developed roughly in\\nparallel. The other two are being developed in Go and Typescript,\\nrespectively (see the [references](#references) section).\\nIn this context, it is plausible that Hermes will focus on performance,\\nrobustness, and richness of features on a longer term.\\n","Decision":"This is a summary of the use-cases (commands) discussed in the rest of this ADR.\\nNote that the commands below omit the binary name `hermes` , to keep the command\\nlength to a minimum.\\nTo create and update a client:\\n- `create client <host-chain-id> <target-chain-id>`\\n- Optional params: `[--clock-drift <millis>] [--trusting-period <days>] [--trust-threshold <numerator\/denominator>]`\\n- `update client <host-chain-id> <client-id>`\\nTo create a connection:\\n- `create connection <chain-a-id> <chain-b-id>`\\n- Optional: `[--delay <delay>]`\\n- `create connection <chain-a-id> --client-a <client-a-id> --client-b <client-b-id>`\\n- Optional: `[--delay <delay>]`\\nTo create a channel:\\n- `create channel <chain-a-id> <chain-b-id> --port-a <port-id> --port-b <port-id>`\\n- Optional: `[--order <order>] [--version <version>]`\\n- `create channel <chain-a-id> --connection-a <connection-id> --port-a <port-id> --port-b <port-id>`\\n- Optional: `[--order <order>] [--version <version>]`\\nTo start packet relaying:\\n- `start <chain-a-id> <chain-b-id> --port-a <port-id> --port-b <port-id>`\\n- Optional: `[--order <order>] [--version <version>]`\\n- `start <chain-a-id> --connection-a <connection-id> --port-a <port-id> --port-b <port-id>`\\n- Optional: `[--order <order>] [--version <version>]`\\n- `start <chain-a-id> --channel-a <channel-id> --port-a <port-id>`\\nFor finishing pre-initialized, but unfinished object handshakes, for connection and channel:\\n- `establish connection <chain-a-id> --connection-a <connection-id>`\\n- `establish channel <chain-a-id> --channel-a <channel-id> --port-a <port-id>`\\n### Rationale\\nThe primary goal for the uses-cases we decided to cover is to prevent situations\\nwhere users could get stuck. For example, the output of a command may be\\nunclear, or there may be an error and thereby some CLI command\\nfinishes partially, or two relayers concurrently try to perform some\\noperation(s) and interfere with each other, resulting in a chain state that is\\nobscure to the user, and then the user could consequently be stuck.\\nThe first of the patterns below seeks to help \"unblock\" a user.\\nThe second pattern is a variation on the first; this permits more efficiency\\nbecause it allows the reuse of previously-created objects in the\\ncreation of new objects on a chain (e.g., reuse a client in the creation of a\\nconnection, or reuse a connection in the creation of a new channel).\\n#### Patterns\\nWe propose two basic patterns that Hermes should be able to fulfil.\\n1. Simple invocations to perform basic actions.\\n- By _action_ here we mean doing the complete handshake for an object from\\nscratch (specifically _connection_ or _channel_) on two chains, or\\nrelaying packets between two chains.\\n- The focus here is for the command to include retrying mechanisms\\n(perform it _robustly_) and have the simplest interface.\\n2. Allow reusing of pre-existing state for basic commands.\\n- The pre-existing state could be a client with some specific trust options,\\nfor instance, and in this case Hermes would provide support for creating\\na connection that uses this specific client.\\n- This pattern should also include a retrying mechanism.\\n#### Details of Use-Cases\\nApplying the above patterns to a few cases, we get the following concrete\\ncommands that Hermes v0.2.0 should fulfil.\\n##### Create & Update a Client\\n- Minimal invocation: this will create the client from scratch:\\n```\\ncreate client <host-chain-id> <target-chain-id> [--clock-drift <millis>] [--trusting-period <days>] [--trust-threshold <numerator\/denominator>]\\n```\\n**Details:**\\nSubmits a transaction of type [client create][client-create] to chain\\n`<host-chain-id>` (sometimes called the _destination_ chain of this\\ntransaction). The new client will be verifying headers for\\nchain `<target-chain-id>` (often called the _source_ chain).\\nSee also the [limitations](#limitations) section discussing the optional\\nsecurity parameters for this command.\\n- Update a client:\\n```\\nupdate client <host-chain-id> <client-id>\\n```\\n**Details:**\\nSubmits a transaction to chain id `<host-chain-id>` to update the client having\\nidentifier `<client-id>` with new consensus state from up-to-date headers.\\nHermes will automatically infer the target chain of this client from\\nthe [client state][client-state].\\n- Upgrade a client:\\n```\\nupgrade client <host-chain-id> <client-id>\\n```\\n**Details:**\\nSubmits a transaction to chain id `<host-chain-id>` to upgrade the client having\\nidentifier `<client-id>`.\\nHermes will automatically infer the target chain of this client from\\nthe [client state][client-state].\\n- Upgrade all clients that target a specific chain:\\n```\\nupgrade clients <target-chain-id>\\n```\\n**Details:**\\nSubmits a transaction to upgrade clients of all chains in the config that target\\nchain id `<target-chain-id>`.\\n##### Create New Connection\\n- Minimal invocation: this will create the connection from scratch, using\\n_new_ clients:\\n```\\ncreate connection <chain-a-id> <chain-b-id> [--delay <delay>]\\n```\\n**Details:**\\nStarts a transaction to perform the connection open handshake protocol between\\ntwo chains.\\nThe chains are called symbolically `a` and `b`, hence the option names\\n`<chain-a-id>` and `<chain-b-id>`. In all handshakes, Hermes submits the first\\nstep (typically called _init_, e.g., `ConnOpenInit`), to side `a`, then the\\nsecond step (e.g., `ConnOpenTry`) to side `b`, and so on.\\nThe optional parameter `--delay` is the delay period that the new connection\\nshould have. Note also the [limitations](#limitations) around the\\n`delay_period` feature.\\n- Reusing pre-existing state, concretely, with _existing_ clients:\\n```\\ncreate connection <chain-a-id> --client-a <client-id> --client-b <client-id> [--delay <delay>]\\n```\\n**Details:**\\nSimilar to the previous command, this command will perform the connection\\nopen handshake protocol, but will reuse the client with identifier from\\noption `--client-a`. This client is expected to exist on chain `<chain-a-id>`.\\nThe target chain of this client is identified in the\\n[client state][client-state] (concretely, the target chain is represented under\\n`chain_id` field of the client state), which provides the identifier for the\\nside `b` of the new connection. On the side `b` chain, this command will\\nestablish the connection using the client with identifier from the option\\n`--client-b`, which must be verifying headers for chain `<chain-a-id>`.\\n##### Create New Channel\\n- With _new_ connection and clients:\\n```\\ncreate channel <chain-a-id> <chain-b-id> --port-a <port-id> --port-b <port-id> [--order <order>] [--version <version>]\\n```\\n- With _existing_ specific connection:\\n```\\ncreate channel <chain-a-id> --connection-a <connection-id> --port-a <port-id> --port-b <port-id> [--order <order>] [--version <version>]\\n```\\n##### Packet Relaying\\n- relay packets over a _new_ channel, _new_ connection, and _new_ clients:\\n```\\nstart <chain-a-id> <chain-b-id> --port-a <port-id> --port-b <port-id> [--order <order>] [--version <version>]\\n```\\n- relay packets over a _new_ channel that re-uses an _existing_ connection:\\n```\\nstart <chain-a-id> --connection-a <connection-id> --port-a <port-id> --port-b <port-id> [--order <order>] [--version <version>]\\n```\\n- relay packets over an _existing_ channel:\\n```\\nstart <chain-a-id> --channel-a <channel-id> --port-a <port-id>\\n```\\n##### Finishing partially complete handshakes:\\nThese commands serve the purpose of covering certain corner-cases where a\\nhandshake may be partially started.\\n- Finalize handshake for _partially established_ connection:\\n```\\nestablish connection <chain-a-id> --connection-a <connection-id>\\n```\\n- Finalize handshake for _partially established_ channel:\\n```\\nestablish channel <chain-a-id> --channel-a <channel-id> --port-a <port-id>\\n```\\n### Command Output\\nBy default, the command will provide human-readable output, i.e., pretty\\nprinting.\\nIn practice, the final result of a Hermes command is captured in an\\n[Output][output] structure that has support for JSON serialization. To\\nenable JSON, we add a configuration parameter `log_json`. The global section\\nof the config file will look as follows:\\n```toml\\n[global]\\nlog_level = 'error'\\nlog_json = 'false'\\n```\\nBy default, this parameter is `false`. When set to `true`, all the Hermes output\\nwill be in JSON.\\n","tokens":269,"id":4652,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-001-repo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis document provides a basic rundown of the structure of this repository, plus some plans for its evolution.\\nThis repository comprises a Rust implementation of the [IBC](https:\/\/github.com\/cosmos\/ibc) suite of protocols.\\nTo complement this implementation, this repository also comprises specifications, primarily written in TLA+, and\\nsometimes in English.\\nAt the moment we are invested mostly in the development of a relayer and several important modules (client, connection,\\nchannel, and packets).\\nEventually, we hope to cover the full IBC suite.\\n","Decision":"The `ibc-rs` repository comprises three broad parts:\\n1. The codebase for the IBC relayer implementation in Rust is in `relayer\/`, which consists of crate **`relayer-cli`** (the\\nfrontend application of the relayer) as well as crate **`relayer`** (the core relayer functionality).\\n2. The codebase for IBC modules is in `modules\/`, making up the crate called **`relayer-modules`**.\\n3. English and TLA+ specs reside under `docs\/spec`, classified by the component they target, e.g., relayer or connection\\nhandshake.\\nFollowing the work in [#23](https:\/\/github.com\/cosmos\/ibc-proto-rs\/issues\/23), the crate\\n**`ibc-proto`**(originally in a [separate repo](https:\/\/github.com\/informalsystems\/ibc-proto) and [documented here](https:\/\/docs.rs\/ibc-proto\/))\\nshall also become absorbed into the present repo.\\nIn the following, we discuss the current state and proposed evolution of each of the Rust crates.\\n#### Crate `relayer-cli`\\nThe basic concern of this crate is to provide user-facing functionality for the IBC relayer. This means\\nimplementing a CLI application that dispatches a _command_ to a specific part of the relayer, and then outputs the\\nresult of executing that command. This crate builds on\\n[Abscissa](https:\/\/docs.rs\/abscissa_core\/0.5.2\/abscissa_core\/) to simplify command line parsing, application process\\nlifecycle, and error handling.\\nThis crate can accept various sub-commands, e.g. `query` a chain for some specific part of their store, `start` the\\nrelayer, or start the `light` client for a given chain. Note that most commands can be further refined with parameters\\n(for instance, the `query` command can be issued for a `connection` or `channel` or `client`). The bulk of data types\\nand logic resides in `relayer\/cli\/commands`, grouped by each specific command.\\n#### Crate `relayer`\\nThis crate implements the core responsibilities of an IBC relayer. Briefly speaking, there are 3 high-level\\nrequirements on a IBC relayer, in no particular order:\\n- __R1.__ ability to interface with IBC-enabled chains, with the purpose of reading their state and submitting transactions to\\nthese chains;\\n- __R2.__ ability to run a light client for IBC-enabled chains, with the purpose of verifying headers and state of these chains;\\n- __R3.__ implement the IBC relayer algorithms (ICS 018).\\nSome functionality described above overlaps with functionality of IBC Modules. For instance, some logic\\nthat the relayer implements for handling connection handshakes (in ICS18) overlaps with logic in the IBC module specific\\nfor connections (ICS3). Given this overlap, the `relayer-modules` crate serves as the \"ground truth\" implementing the\\nsaid logic, while the `relayer` crate has a natural dependency on `relayer-modules`.\\nIn addition to the dependency on the IBC Modules, the relayer also depends on the `tendermint-rs` crate. This is\\nuseful in particular for interfacing with the light client implementation from this crate, as well as core data types\\nsuch as `SignedHeader`, `Validator`, or `ValidatorSet`.\\n[ADR 002](.\/adr-002-ibc-relayer.md) captures more specific details on the relayer architecture.\\n#### Crate `relayer-modules`\\nThe [canonical IBC specification](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/) is modular in the sense of grouping\\ndifferent components of the specification in modules; for instance, specification _ICS03_ pertains to the abstraction of\\nIBC connections and the IBC connection handshake protocol, while _ICS04_ pertains to IBC channels and packets.\\nWe group the code in this crate to reflect the modular separation in the canonical IBC specification.\\nA few common patterns we employ in this crate are as follows.\\n###### `msgs.rs`\\nMany IBC protocols involve the receiving and processing of messages.\\nThe protocols for establishing a connection (ICS03) or a channel (ICS04), for example, comprise\\nthe processing of four different types of messages each.\\nIn particular, the data structures representing these messages for connection handshake are `MsgConnectionOpenInit`,\\n`MsgConnectionOpenTry`, `MsgConnectionOpenAck`, and `MsgConnectionOpenConfirm`.\\nThe creation and validation of protocol messages for each protocol resides in `msgs.rs` within the respective ICS.\\nEach of these messages should implement the trait `pub trait Msg`, ensuring that all messages implement a basic\\ninterface allowing them to be routed correctly (via the IBC routing module and with the help of the `route()` method)\\nor support basic validation.\\n###### Error handling\\nEach ICS enumerates specific errors that may occur within `icsX_NAME\/error.rs`.\\nThe error-handling pattern here build on [thiserror](https:\/\/lib.rs\/crates\/thiserror) and\\n[anomaly](https:\/\/lib.rs\/crates\/anomaly) for capturing the context of errors plus backtraces (optional).\\nGenerally speaking, an IBC module constructs and propagates errors to the caller by two patterns:\\n```Rust\\nreturn Err(Kind::MissingCounterparty.into())\\n```\\nor if a context can be supplied this is preferable:\\n```rust\\nreturn Err(Kind::InvalidConnectionHopsLength\\n.context(\"validate channel\")\\n.into());\\n```\\nwhere the ICS itself defines `Kind::InvalidConnectionHopsLength` and `Kind::MissingCounterparty`.\\n###### Deserialization\\nSee the details for the crate `ibc-proto` [below](#crate-ibc-proto).\\n#### Crate `ibc_proto`\\nThe `ibc-proto` library gives a developer access to the Cosmos SDK IBC proto-defined structs directly in Rust.\\nThe canonical IBC structs reside presently in [cosmos-sdk](https:\/\/github.com\/cosmos\/ibc-go\/tree\/main\/proto\/ibc),\\ndefined in a proto3 syntax.\\nWe compile these structs via prost directly to .rs files and import them into the other crates typically under the same\\nname prefixed with \"Raw\", for example:\\n```Rust\\nuse ibc_proto::channel::Channel as RawChannel;\\n```\\nFor any Raw data type that is defined in `ibc-proto` we implement the `DomainType` trait, which serves as a translation\\n& validation layer between the proto (\"Raw\") types and the domain types. For example, for a `Channel` we do as follows:\\n```Rust\\nimpl DomainType<RawChannel> for ChannelEnd {}\\nimpl TryFrom<RawChannel> for ChannelEnd {\\ntype Error = anomaly::Error<Kind>;\\nfn try_from(value: RawChannel) -> Result<Self, Self::Error> {\\n\/\/ Translate, validate each field from RawChannel into a Channel.\\n}\\n}\\nimpl From<ChannelEnd> for RawChannel {\\nfn from(value: ChannelEnd) -> Self {\\n\/\/ Translate Channel into a RawChannel\\n}\\n}\\n```\\nThis issue [#130](https:\/\/github.com\/informalsystems\/hermes\/issues\/130) is a good starting place for more context\\non `ibc-proto`.\\n### References\\nThe following resources serve as reference implementations or specifications that we use to guide the development of\\nthe present crates:\\nFor the IBC relayer:\\n- A first implementation of the IBC relayer in Golang is under active development at\\n[iqlusioninc\/relayer](https:\/\/github.com\/iqlusioninc\/relayer).\\n- The English specification of the relayer algorithm is captured in the\\n[ICS018](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/relayer\/ics-018-relayer-algorithms) spec.\\nFor IBC modules:\\n- A Golang implementation of IBC modules is under active development\\nat [cosmos\/ibc-go](https:\/\/github.com\/cosmos\/ibc-go\/tree\/main\/modules).\\n- The English specifications for IBC modules reside in [cosmos\/ibc](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec).\\n","tokens":118,"id":4653,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-002-ibc-relayer.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA relayer is an off-chain process responsible for relaying IBC datagrams between two or more chains by scanning their states and submitting transactions. This is because in the IBC architecture, modules are not directly sending messages to each other over networking infrastructure, but instead they create and store the data to be retrieved and used by a relayer to build the IBC datagrams.\\nThis document provides an initial Rust implementation specification of a relayer that interconnects Cosmos-SDK\/ Tendermint chains.\\nThe diagram below shows a high level view of the relayer and its interactions with the source and destination chains. The next sections go in more details of the different interactions.\\n![IBC Relayer Architecture Diagram](assets\/IBC_relayer.jpeg).\\n","Decision":"> This section explains all of the details of the proposed solution, including implementation details.\\nIt should also describe affects \/ corollary items that may need to be changed as a part of this.\\nIf the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n(e.g. the optimal split of things to do between separate PR's)\\n","tokens":154,"id":4654,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-007-error.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis document describes the reason behind the switch from using\\n`anomaly` for error handling to\\nthe [`flex-error`](https:\/\/docs.rs\/flex-error\/) crate that is developed in-house.\\n","Decision":"### Problem Statement\\nTo keep things brief, we will look at the issue of error handling from a specific example\\nin `relayer\/src\/error.rs`:\\n```rust\\npub type Error = anomaly::Error<Kind>;\\n#[derive(thiserror::Error)]\\npub enum Kind {\\n#[error(\"GRPC error\")]\\nGrpc,\\n...\\n}\\nimpl Kind {\\npub fn context(self, source: impl Into<Box<dyn std::error::Error>>) -> anomaly::Context<Self> {\\nContext::new(self, Some(source.into()))\\n}\\n}\\n```\\nThe design above is meant to separate between two concerns:\\n- The metadata about an error, as captured in `Kind`.\\n- The trace of how the error occurred, as captured in `anomaly::Context`.\\n- The type `Error` is defined to be `anomaly::Error<Kind>`, which is a newtype wrapper to `Box<anomaly::Context<Kind>>`.\\nThere are a few issues with the original design using `anomaly`:\\n- The error source type is erased and turned into a `Box<dyn std::error::Error>`, making it difficult to recover metadata\\ninformation about the original error.\\n- The `Kind::context` method allows any error type to be used as an error source, making it difficult to statically\\nanalyze which sub-error has what kind of error source.\\nWe can demonstrate the design issue with a specific use case:\\n```rust\\npub fn unbonding_period(&self) -> Result<Duration, Error> {\\nlet mut client = self\\n.block_on(QueryClient::connect(self.grpc_addr.clone()))\\n.map_err(|e| Kind::Grpc.context(e))?;\\nlet request = Request::new(QueryParamsRequest {});\\nlet response = self\\n.block_on(client.params(request))\\n.map_err(|e| Kind::Grpc.context(e))?;\\n...\\n}\\n```\\nWithout the help of an IDE, it would be challenging to figure out that\\nthe first use of `Kind::Grpc.context` has `tonic::Status` as the error source\\ntype, while the second use has the error source type\\n`tonic::TransportError`.\\nThe mixing up of `tonic::Status` and `tonic::TransportError` as error sources\\nare not too critical in this specific case. However this would not be the\\ncase if we want to use the error source information to determine whether\\nan error is _recoverable_ or not. For instance, let's say if we want to\\nimplement custom retry logic only when the error source is\\n`std::io::Error`, there is not easy way to distinguished if an error\\nvariant `Kind::Grpc` is caused by `std::io::Error`.\\n### Proposed Design\\nA better design is to define error construction functions with _explicit_\\nerror sources. The proposed design is as follows:\\n```rust\\npub struct Error(pub ErrorDetail, pub eyre::Report);\\npub enum ErrorDetail {\\nGrpcStatus {\\nstatus: tonic::Status\\n},\\nGrpcTransport,\\n...\\n}\\nimpl Error {\\npub fn grpc_status(status: tonic::Status) -> Error {\\nlet detail = ErrorDetail::GrpcStatus { status };\\nError(detail, Eyre::msg(detail))\\n}\\npub fn grpc_transport(source: tonic::TransportError) -> Error {\\nlet detail = ErrorDetail::GrpcTransport;\\nlet trace = Eyre::new(source).wrap_err(detail);\\nError(detail, trace)\\n}\\n}\\n```\\nThere are a few things addressed by the design above:\\n- We use the `eyre::Report` type as an _error tracer_ to trace\\nthe error sources, together with additional information such as backtrace.\\n- Depending on the error source type, we want to have different strategies\\nto trace the error.\\n- For example, we may not care about the metadata\\ninside `tonic::TransportError`, so we just discard the data\\nafter tracing it using `eyre`.\\n- We define _error constructor functions_ that handle the error source using\\ndifferent strategies. The function constructs the `ErrorDetail` and\\n`eyre::Report` values, and then wrap them as the `Error` tuple.\\nIn general, when the error sources are defined by external libraries,\\nwe have little control of how the types are defined, and need to have\\ndifferent ways to handle them.\\nBut when we have multiple error types that are defined in the same crate,\\nwe want to have special way to handle the propagation of error.\\nFor example, consider the `LinkError` type, which has the error\\nwe defined earlier as the error source:\\n```rust\\nuse crate::error::{Error as RelayerError, ErrorDetail as RelayerErrorDetail};\\npub struct LinkError(LinkErrorDetail, eyre::Report);\\npub enum LinkErrorDetail {\\nRelayer {\\nsource: RelayerErrorDetail\\n},\\n...\\n}\\nimpl LinkError {\\npub fn relayer_error((source_detail, trace): RelayerError) -> LinkError {\\nlet detail = LinkErrorDetail::Relayer(source_detail);\\nLinkError(detail, trace.wrap_err(detail))\\n}\\n}\\n```\\nWe propagate the error detail to LinkErrorDetail so that we can recover\\nadditional detail later on. Furthermore, we extract the `eyre::Report`\\nfrom the error source and use it to add additional information\\nwhen we construct `LinkError`.\\n### `flex-error`\\nThe proposed design has a lot of boilerplate required to properly define\\nthe error types. To reduce boilerplate, we have developed\\n[`flex-error`](https:\/\/docs.rs\/flex-error\/) with the `define_error!`\\nmacro which makes it straightforward to implement the error types\\nusing a DSL syntax. With that, the error types can instead be defined as:\\n```rust\\nuse flex_error::{define_error, TraceError};\\ndefine_error! {\\nError {\\nGrpcStatus\\n{ status: GrpcStatus }\\n| e | { format!(\"GRPC call return error status {0}\", e.status) },\\nGrpcTransport\\n[ TraceError<tonic::TransportError> ]\\n| _ | { \"error in underlying transport when making GRPC call\" },\\n...\\n}\\n}\\n```\\nAside from the syntactic sugar provided by the `define_error!` macro, `flex-error`\\nalso allows error tracer implementation to be switched based on the Cargo feature\\nflags set on the `flex-error` crate. For example, we can switch from the\\n[`eyre`](https:\/\/docs.rs\/eyre\/) tracer to the [`anyhow`](https:\/\/docs.rs\/anyhow\/)\\ntracer by disabling `\"flex-error\/eyre_tracer\"` and enabling `\"flex-error\/anyhow_tracer\"` features.\\nIf all error tracer features and the `\"flex-error\/std\"` feature are disabled,\\na simple `flex_error::StringTracer` is used for tracing errors. The `StringTracer`\\ndo not provide additional information such as back trace, but it is useful\\nfor supporting `no_std`, where standard constructs such as `std::error::Error` and\\nerror backtrace are not available.\\nThe full documentation for `flex-error` is available at [Docs.rs](https:\/\/docs.rs\/flex-error\/).\\n","tokens":44,"id":4655,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"container\/0002-use-spring-dependency-incjection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo facilitate testing, a clean, object oriented architecture as well as the plugin systems for various components a configurable Inversion of Control (IOC) container is required.\\nThis container is responsible for plugin discovery, as well as injecting services required by the API to serve it's \"external\" customers.\\n## Decision Drivers\\n* Support for a plugin system that can discover additional components not originally compiled into the deployed WAR\\n* Support for minimal configuration, allowing easy modification and discovery by convention\\n","Decision":"* Support for a plugin system that can discover additional components not originally compiled into the deployed WAR\\n* Support for minimal configuration, allowing easy modification and discovery by convention\\nThe chosen IoC container is Spring, because it supports plugin discovery at minimal configuration and has easy support for servlet-based injection with `spring-mvc` and `spring-web`.\\n### Negative consequences\\nThe Plugins loaded cannot be adjusted at runtime.\\nAt time of writing, no such capability is required or planned.\\n","tokens":103,"id":4657,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"container\/0001-use-maven.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo enable portability and reproducibility of building the container application, a buildtool is of utmost importance.\\nIt simplifies the buildprocess and should preferrably allow a minimal configuration effort for IDEs.\\n## Decision Drivers\\n* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\n","Decision":"* Core Concern is management of building the container, preferrably in a single command invocation\\n* Support for dependency management is optional, but highly appreciated\\n* The tool should also support deployment of the finished web application to an application server\\n* Cross Platform tooling, Automation and support for Running a Test suite are also necessary\\nMaven was chosen both for it's maturity and integration with established IDEs.\\nIt also was previously used to automate the container build when the container was still deployed as an OSGI application.\\nAdditionally maven dependencies are the de-facto standard distribution mechanism in the java ecosystem.\\n","tokens":120,"id":4658,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Endjin.RecommendedPractices.NuGet\/0001-practices-via-nuget.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur initial way of implementing standard build practices was to copy a bunch of files into place in every project. There were various problems:\\n- there was no single master location from which to copy the files\\n- each project tended to make its own tweaks and it was never clear which were project-specific, and which were useful enhancements\\n- there was no procedure defined for pushing updates to the standards out to all projects\\n- there was no established mechanism around how to handle .NET Core 3.1 or various .NET Standard versions\\n- there was an inconsistent mixture of mechanisms\u2014some use of imports, some use of Directory.props\\nWe could have tweaked the existing mechanism. For example we could have defined a repository somewhere whose job was to hold the latest master copies of the files. We could have established conventions that could have clarified which parts of the config were the unaltered standard mechanism, and which parts are project-specific modifications.\\nHowever, our `Endjin.RecommendedPractices.AzureDevopsPipelines.GitHub` works a lot more smoothly, partly as a result of not going down this path. Instead, it uses a mechanism designed specifically to support reuse (Azure DevOps pipeline templates), and which ensures there is exactly one master copy that everything shares, which has built-in extensibility points to enable project-specific adjustments.\\nBecause projects don't copy and then modify the relevant files\u2014instead they just refer to them and then plug into the extensibility points supplied\u2014it is absolutely clear when something is a project-specific tweak. And this also provides a straightforward mechanism for updates: when something breaks the build (e.g., as happened when .NET Core SDK 3.1.200 came out) we can push a single update to fix everything.\\nWhat if `Endjin.RecommendedPractices` could enjoy all these same benefits as `Endjin.RecommendedPractices.AzureDevopsPipelines.GitHub`? The key characteristics are:\\n- one definitive copy of the relevant files, distributed through a mechanism designed to enable sharing of such files\\n- clearly-defined extensibility points to provide individual projects with the flexibility they require, without needing them to have their own copies of everything\\n","Decision":"Move our standard build files into a NuGet package: [Endjin.RecommendedPractices.NuGet](https:\/\/www.nuget.org\/packages\/Endjin.RecommendedPractices.NuGet). Define extensibility mechanisms as required.\\n","tokens":448,"id":4659,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Endjin.RecommendedPractices.NuGet\/0003-defer-links-to-solution-level-files.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe optionally generate certain solution-level files (e.g., `PackageIcon.png`, `stylecop.json`) and where appropriate or\\nnecessary, we also arrange for these files to show up in project. (This is necessary for `PackageIcon.png`\u2014the file\\nneeds to be in the project for the corresponding `<PackageIcon>` property to work.) However, this causes problems\\nwhen first adding `Endjin.RecommendedPractices` to a project.\\nThe problem is that we don't get the opportunity to generate these files until a build occurs. However, Visual Studio\\nwill attempt to display any files we add to the project immediately after the reference to `Endjin.RecommendedPractices`\\nhas been added, and before any build has occurred. The upshot is that for these kinds of files, Visual Studio shows\\nthem with a big red X, because the physical files they refer to don't exist. Worse, even after the build creates the\\nfiles, the crosses remain, because Visual Studio appears not to update such things when the filesystem changes.\\nThe effect of this was that after adding `Endjin.RecommendedPractices` to a project you would need to build, then\\nunload the solution, and then reload it, before everything looked OK.\\n","Decision":"We now make all additions of files to a project conditional on the file existing. E.g.:\\nWe are going to require all projects to contain this line at the top of the `csproj`:\\n```xml\\n<ItemGroup Condition=\"($(EndjinDisableCodeAnalysis) != 'true') and (Exists('$(SolutionDir)stylecop.json'))\">\\n<AdditionalFiles Include=\"$(SolutionDir)stylecop.json\" Link=\"stylecop.json\" \/>\\n<\/ItemGroup>\\n```\\nThis is how we make a file link to `stylecop.json` appear in a project. It used to be conditional only on the\\n`EndjinDisableCodeAnalysis` build variable. But now we have an additional `Exists` test.\\n","tokens":259,"id":4661,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cache-flush-slack-app\/0003-request-initial-view-open-from-slash-command.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen the app is running locally and being worked on during the development\\nprocess the speed of response from the functions is good enough so that the\\nrequest to open the initial view can be done once the request has been\\nvalidated. However, when deployed there appears to be an increase in the\\nresponse times. This isn't a problem for the majority of the interactions,\\nhowever, the opening of the initial view needs to be completed within\\n[3 seconds](https:\/\/api.slack.com\/surfaces\/modals\/using#opening_modals).\\nDuring testing there were many times the view wasn't being opened. The way to\\nmitigate this is to make the request to open the initial view asap within the\\nslash command life cycle.\\n","Decision":"The request to open the initial view has been changed so it is the first thing\\nwithin the slash command life cycle.\\n","tokens":156,"id":4662,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cache-flush-slack-app\/0002-use-csv-rather-than-json-for-allowed-user-list.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing a JSON file for the `allowed-users` listing makes the code within the\\napplication straight forward and ofc, the default data format within a JS app\\nshould be JSON (controversial). However, managing the file is (at least atm)\\nbeing done by a human. Editing a JSON file and ensuring it is valid JSON is\\nmore challenging than a CSV file. If the file is malformed the application is\\nunavailable, therefore, reducing this risk is paramount.\\n","Decision":"The decision is to use a CSV format for the listing of allowed users.\\n","tokens":102,"id":4663,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cache-flush-slack-app\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4664,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"OpenTermsArchive\/0001-service-name-and-id.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo scale up from 50 to 5,000 services, we need a clear way for choosing the service name and the service ID.\\n### We need\\nA name that reflects the common name used by the provider itself, to be exposed in a GUI. This name is currently exposed as the name property in the service declaration.\\nAn ID of sorts that can be represented in the filesystem. This ID is currently exposed as the filename of the service declaration, without the .json extension.\\n### Use cases\\nThe service name is presented to end users. It should reflect as closely as possible the official service name, so that it can be identified easily.\\nThe ID is used internally and exposed for analysis. It should be easy to handle with scripts and other tools.\\n### Constraints for the ID\\nAs long as this ID is stored in the filesystem:\\n- No `\/` for UNIX.\\n- No `\\` for Windows.\\n- No `:` for APFS and HFS.\\n- No case-sensitive duplicates to support case-insensitive filesystems.\\n- No more than 255 characters to support transfer over [FAT32](https:\/\/en.wikipedia.org\/wiki\/File_Allocation_Table#FAT32).\\nUTF, spaces and capitals are all supported, even on case-insensitive filesystems.\\n### However\\n- UTF in filenames can be [a (fixable) problem with Git and HFS+](https:\/\/stackoverflow.com\/questions\/5581857\/git-and-the-umlaut-problem-on-mac-os-x).\\n- UTF in filenames is by default quoted in Git, leading for example `\u00e9t\u00e9.txt` to be displayed as `\"\\303\\251t\\303\\251.txt\"`.\\n- Most online services align their brand name with their domain name. Even though UTF is now officially supported in domain names, support is limited and most services, even non-Western, have an official ASCII transliteration used at least in their domain name (e.g. \u201cqq\u201d by Tencent, \u201crzd.ru\u201d for \u201c\u0420\u0416\u0414\u201d, \u201cyahoo\u201d for \u201cYahoo!\u201d).\\n- We currently use GitHub as a GUI, so the service ID is presented to the user instead of the service name. The name is used in email notifications.\\n","Decision":"1. The service name should be the one used by the service itself, no matter the alphabet.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443`_.\\n2. We don't support non-ASCII characters in service IDs, at least as long as the database is Git and the filesystem, in order to minimise risk. Service IDs are derived from the service name through normalization into ASCII.\\n- _Example: `\u0442\u0443\u0442\u0443.\u0440\u0443` \u2192 `tutu.ru`_.\\n- _Example: `historiel\u00e6rer.dk` \u2192 `historielaerer.dk`_.\\n- _Example: `RT\u00c9` \u2192 `RTE`_.\\n3. We support punctuation, except characters that have meaning at filesystem level (`:`, `\/`, `\\`). These are replaced with a dash (`-`).\\n- _Example: `Yahoo!` \u2192 `Yahoo!`_.\\n- _Example: `Last.fm` \u2192 `Last.fm`_.\\n- _Example: `re:start` \u2192 `re-start`_.\\n- _Example: `we:\/\/` \u2192 `we---`_.\\n4. We support capitals. Casing is expected to reflect the official service name casing.\\n- _Example: `hi5` \u2192 `hi5`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n- _Example: `LINE` \u2192 `LINE`_.\\n5. We support spaces. Spaces are expected to reflect the official service name spacing.\\n- _Example: `App Store` \u2192 `App Store`_.\\n- _Example: `DeviantArt` \u2192 `DeviantArt`_.\\n6. We prefix the service name by the provider name when self-references are ambiguous, separated by a space. For example, Facebook refers to their Self-serve Ads service simply as \u201cAds\u201d, which we cannot use in a shared database. We thus call the service \u201cFacebook Ads\u201d.\\n- _Example: `Ads` (by Facebook) \u2192 `Facebook Ads`_.\\n- _Example: `Analytics` (by Google) \u2192 `Google Analytics`_.\\n- _Example: `Firebase` (by Google) \u2192 `Firebase`_.\\n- _Example: `App Store` (by Apple) \u2192 `App Store`_.\\n","tokens":468,"id":4666,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"OpenTermsArchive\/0002-service-history.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to be able to regenerate versions from snapshots. As documents is aim to change over time (location or filters) we can't rely on the last version of the declaration to regenerate the version from an old snapshot. So we need a system to keep track of declaration changes, that's what we called declarations and filters versioning.\\n","Decision":"[After consulting the community](https:\/\/github.com\/OpenTermsArchive\/engine\/issues\/156), the options 2A is retained as it hide complexity (compared to Option 1) of the history while increasing its discoverability (compared to Option 3) for contributors who might become more \u201cadventurous\u201d.\\n","tokens":74,"id":4667,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0015-building-oci-images.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to build OCI images based on Dockerfiles inside our Kubernetes cluster.\\n## Decision Drivers\\n* MUST run completely in userspace (no privileged rights required)\\n* MUST be runable on Kubernetes\\n* MUST be controlled in an automatic manner (CI\/CD)\\n* SHOULD be compatible with Knative Build\\n","Decision":"* MUST run completely in userspace (no privileged rights required)\\n* MUST be runable on Kubernetes\\n* MUST be controlled in an automatic manner (CI\/CD)\\n* SHOULD be compatible with Knative Build\\nChosen option: Kaniko, because it is designed for the use case we need. It works on Kubernetes and build OCI images without any daemon and without privileged rights.\\n","tokens":71,"id":4668,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0026-implementation-of-complex-eai-patterns-with-faas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSome [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO).\\nWe have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration.\\nWhile this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS.\\nThis is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.\\n## Decision Drivers\\n* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n","Decision":"* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n**Where to implement logic**: To be decided\\n**State of configuration channels**: To be decided\\n**Stateful functions**: To be decided\\n**Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present.\\nThe routing slip has to support multiple destinations for one routing step.\\nThis will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.\\n**Intermediate requests**: To be decided\\n","tokens":231,"id":4669,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0001-java-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to use a well established Java framework in order to bootstrap and boost our development process.\\n","Decision":"We chose option 1, Spring Boot, since existing knowledge and experience with this framework is available.\\nTo reduce training costs (time) we chose this option.\\n","tokens":26,"id":4670,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0025-generic-component.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to realize the following user story: As a user, I want to connect two components of a distributed app. Both components write\/listen to different topics and\\nuse a different message format.\\nTo realize this, a message router and a message transformer ([EAI Pattern](https:\/\/www.enterpriseintegrationpatterns.com\/)) is needed as depicted in the following sketch.\\n![First scenario](images\/FirstScenario.png)\\nThe transformation and the routing logic is provided via functions which are hosted on a FaaS solution. This functions are user specific and could be provided via a function store. The transformer doesn't need to know how to transform\/split\/enrich\/filter the messages. It simply reads the message from a specific topic, hands it over to the function and sends the (transformed) result of the function to an output topic. Likewise, the router does not need to know how to route the messages (content or context based, e.g. if `itemPrice>50000` than route to high-value department). It reads the message from a topic, hands it over to a function and sends the message to a function determined topic. Both components have in common that they only read messages from a topic, hand the message to a function and act upon the result. We could draw the image above like this:\\n![generic component sketch](images\/genericComponent.png)\\n","Decision":"Chosen option: Kafka-faas-connector, because we don't want to mix messaging and business logic in the functions and need to have dynamic topics which are not supported by the OpenFaaS Kafka connector at the moment.\\n","tokens":285,"id":4671,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0002-vrs-type.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Voice Assistance Platform's job is to identify if the call is implicit or an explicit utterance. If explicit, then it will engage and call VRS.\\nOur goal is to identify who is responsible for determining the VRS in the utterance.\\n","Decision":"Based on the meeting's (03.04.2021) discussion, the quorum for the best part forward is option 1. The biggest reason is keeping the role of the VRS as straightforward as possible. The team acknowledged that the lower-level details solutions need to be done in partnership with Voice Assistant Platform.\\n<br>\\n","tokens":55,"id":4672,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0006-ui-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to use a well established UI framework in order to bootstrap and boost our development process for state-of-the-art web applications.\\n","Decision":"We chosen option 1, Angular, since existing knowledge and experience with this framework is available.\\n","tokens":33,"id":4673,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0019-architecture-for-background-jobs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSeveral operations (crawl repo, deploy services, ...) are required to run in the background. Therefore, a background job architecture is required being able to launch arbitrary jobs in the background.\\n## Decision Drivers\\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\n","Decision":"* MUST be implemented in Java\\n* MUST offer a callback functionality\\nChosen option: Java CompletableFuture, because of the possibility to chain multiple actions and the option to attach callbacks on completion of a background task.\\n### Positive Consequences\\n* Possibility to chain together multiple actions after a succesful execution of a background task.\\n* Use of FixedThreadPool, i.e., a pool of threads with a predefined number of threads which are reused. If all threads are active at a time and additional tasks are submitted, they will wait in a queue.\\n### Negative Consequences\\n* With the use of a FixedThreadPool, a background task may be delayed if all threads are active at the time of submission.This may produces time of waiting in the frontend of MICO.\\n","tokens":61,"id":4674,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0010-rest-api-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nRESTful APIs can be designed differently, e.g., there are discussions about how to name resources correctly or what kind of HTTP methods should be used to trigger actions.\\n","Decision":"We follow the guidelines proposed by the [Learn REST: A RESTful Tutorial](https:\/\/www.restapitutorial.com).\\n","tokens":40,"id":4675,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0017-istio-vs-envoy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to have telemetry (performance, monitoring) and log collections for each MICO service that should be added automatically during deployment.\\n## Decision Drivers\\n* MUST be compatible with Kubernetes\\n* MUST at least provide all required features (telemetry, logs)\\n* MUST be independent of the technology used in the respective service\\n* SHOULD consume less resources\\n","Decision":"* MUST be compatible with Kubernetes\\n* MUST at least provide all required features (telemetry, logs)\\n* MUST be independent of the technology used in the respective service\\n* SHOULD consume less resources\\nChosen option: Istio, because it is easier to configure than only using Envoy. By using Istio the Envoy proxies can be used as the data plane and Istio as the control plane to manage the whole service mesh.\\nMoreover Istio is the better choice for the future, because it has all features that could be potentially helpful for the management of a service mesh.\\n### Positive Consequences\\n* Istio enables more than just having telemetry and log collections. In addition it provides many more features that could be useful for MICO in future (follow-up decisions required).\\n[Examples](https:\/\/istio.io\/docs\/concepts\/what-is-istio\/#why-use-istio):\\n+ Automatic Load Balancing\\n+ Routing rules, retries, failovers, and fault injection\\n+ Pluggable policy layer with access controls, rate limits and quotas\\n+ Secure service-to-service communication in the cluster\\n### Negative consequences\\n* Istio consumes many resources\\n* Istio brings more complexity into the MICO system\\n","tokens":78,"id":4676,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0016-documentation-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to have a well defined structure for what to document where.\\n","Decision":"Chosen option: Leave it as is.\\n### Clarification for what to document where:\\n#### Docs Repository\\nRationale: If I want wo use mico this documentation should contain all I need to know.\\nExamples:\\n* Design Decisions\\n* Architecture\\n* Component descriptions (mico-core, mico-admin, mico-grapheditor)\\n* API Interfaces (REST Api, etc.)\\n* How To's (How To install mico, How To use mico, etc.)\\n#### Mico Repository\\nRationale: If I want to change something in the code this documentation should contain additional information about code specifics.\\n* Javadoc, Typedoc, etc.\\n* Descriptions for important classes and how to use them\\n* How to write a new REST Endpoint, Unit Test, Plugin, etc.\\n* Links to dependencies (with documentation), relevant documentation in docs repo\\nIf possible use links to the relevant documentation instead of describing the whole thing.\\n","tokens":21,"id":4677,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0024-language-for-a-generic-composition-pattern-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to decide which language to use for implementing a MICO composition service.\\nThe framework will be [Apache Kafka](https:\/\/kafka.apache.org) [See ADR0021](0021-kafka-as-messaging-middleware.md)\\n## Decision Drivers\\n* Should be known by everyone (on the developement team)\\n* Must support Apache Kafka\\n","Decision":"* Should be known by everyone (on the developement team)\\n* Must support Apache Kafka\\nWe want to use Java. Since existing knowledge and experience is given for everyone.\\n","tokens":80,"id":4678,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0021-kafka-as-messaging-middleware.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo compose service with the mico application we need a messaging middleware that is highly scalable and supports the [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com).\\n## Decision Drivers\\n* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\n","Decision":"* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\nWe want to use Kafka as our messaging middleware.\\n### Important things to know about Kafka\\nKafka topics have a different semantic than traditional message queues. Kafka also does NOT log wether a message was consumed. It only stores the last comitted offset for a consumer. Because of that and the different topic semantic some patterns need to be adapted to work with Kafka.\\nFor more detailed information about the inner workings of Kafka consult the following links:\\n*  [Client subscriptions](https:\/\/kafka.apache.org\/intro#intro_consumers)\\n*  [Client positions](https:\/\/kafka.apache.org\/documentation\/#design_consumerposition)\\n*  [Guarantees for message delivery](https:\/\/kafka.apache.org\/intro#intro_guarantees)\\n*  [Message delivery modes (how to achive exactly once)](https:\/\/kafka.apache.org\/documentation\/#semantics)\\n*  [Use topics storage for key based data](https:\/\/kafka.apache.org\/documentation\/#compaction)\\n","tokens":63,"id":4679,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0027-simple-composition-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nA Simple Composition Component is a [Generic Component](https:\/\/github.com\/UST-MICO\/mico\/issues\/724), that follows the concept of a certain [Enterprise Integration Pattern](https:\/\/www.enterpriseintegrationpatterns.com) by invoking a user-defined (or predefined) openFaaS function and acting upon its results. In contrast to the [Complex Integration Components](https:\/\/github.com\/UST-MICO\/docs\/blob\/master\/adr\/0025-implementation-of-complex-eai-patterns-with-faas.md) there are no further efforts (such as maintaining a state) needed for implementing the corresponding Simple Composition Component.\\n## Decision Drivers\\nWe want to have a list of all Enterprise Integration Patterns, that can be implemented as Simple Composition Components with a reasonable amount of effort.\\n","Decision":"We want to have a list of all Enterprise Integration Patterns, that can be implemented as Simple Composition Components with a reasonable amount of effort.\\nWhich of those do we want to implement??\\n### Routing Patterns\\nThe following tables describes for all Routing, Transformation and Management patterns, if they can be implemented as Simple Composition Components (As defined above).\\nThe following assumptions are made:\\n- We have two Generic Components:\\n- A Generic Transformation Component: Provide each message to a user-defined function and forward the result to a predefined target topic\\n- A Generic Routing Component: Provides each message to a user-defined function and forward the result to the topic, that is returned by the user-defined function.\\n- Both components read from a single topic only\\n- There are no external resources available (e.g. for storing a state)\\n| Name                       | Implementation Strategy                                                                                                              | Return Value                                                                |         Possible as <br\/>Simple Composition Component         |\\n|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|:--------------------------------------------------------:|\\n| Content-Based router       | All rules are implemented in the user-defined function                                                                               | The topic, to which it is routed                                            | Yes                                                      |\\n| Message Filter             | All rules are implemented in the user-defined function                                                                               | Either nothing, or the topic to which it is routed                          | Yes                                                      |\\n| Dynamic Router             | All rules are implemented in the user-defined function                                                                               | The topic, to which it is routed                                            | No                                                       |\\n| Recipient List             | The recipients are hard coded in the user-defined function | All topics, to which the message shall be forwarded                         | Yes                                                       |\\n| Splitter                   | The splitting rules are implemented in the user-defined function                                                                     | The sub messages, that contain the splitted <br\/>content of the original message | Yes                                                      |\\n| Aggregator                 | Would require that the user provides a message storage, <br\/>in which the messages can be stored until they are aggregated                                   | The aggregated message                                                      | No                                                       |\\n| Resequencer                | Would require that the user provides a message storage, <br\/>in which the messages can be stored until they are forwarded in the right order                 | The (correct) sequence of messages                                          | No                                                       |\\n| Composed Message <br\/>Processor | Could be implemented by combining a Splitter, Router and Aggregator                                                                    | The composed message                                                        | No (since the aggregator <br\/>is not considered to be simple) |\\n| Scatter-Gather             | Could be implemented by combining an aggregator, multiple <br\/>transformers and a topic, to which all transformers subscribe                | the transformed and aggregated message                                      | No (since the aggregator <br\/>is not considered to be simple) |\\n| Routing Slip               | Probably too complex <br\/>for Simple Composition Component                                                                                      | -                                                                           | No                                                       |\\n| Process Manager            | Probably too complex <br\/>for Simple Composition Component                                                                                      | -                                                                           | No                                                       |\\n| Message Broker             | Probably too complex <br\/>for Simple Composition Component                                                                                      | -                                                                           | No                                                       |\\n### Transformation Patterns\\n| Name                 | Implementation Strategy                                                                                                                                                                        | Return Value                     | Possible as Simple <br\/>Composition Component |\\n|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|------------------------------------------|\\n| Content Enricher     | The user-defined function contains all the information, that may eventually be added to a message  | The transformed message          | Yes                                       |\\n| Content Filter       | The user-defined function implements the whole process of filtering <br\/>out content of a message                                                                                                   | The filtered message             | Yes                                      |\\n| Envelope Wrapper     | A Content Enricher for wrapping a message in an envelope. A <br\/>Content Filter for unwrapping                                                                                                      | -                                | Yes                                      |\\n| Claim Check          | Combination of a special Content Filter for replacing message data <br\/>with a 'claim', and a Content Enricher <br\/>for recovering the data. Also a data storage for storing the data until it is recovered  | -                                | No                                       |\\n| Normalizer           | A combination of a Router, and a set of Translators                                                                                                                                            | The transformed message          | Yes                                      |\\n| Canonical Data Model | The user-defined function is implemented in such a way, that <br\/>receives\/returns messages with a canoncial data format                                                                            | Message in canonical data format | Yes                                      |\\n### System Management\\n| Name            | Implementation Strategy                                                                      | Return Value                                | Possible as Simple Composition Component |\\n|-----------------|----------------------------------------------------------------------------------------------|---------------------------------------------|:----------------------------------------:|\\n| Control Bus     | It would be necessary to receive from more than one topic                                    | -                                           | No                                       |\\n| Detour          | Would require a control bus                                                                  | -                                           | No                                       |\\n| Wire Tap        | Special case of Recipient List                                                               | The topic to which the message is forwarded | Yes                                      |\\n| Message History | Every user-defined function adds a reference of itself to the message history of a function  | The modified message                        | Yes                                      |\\n| Message Store   | Would require, that a data storage is available                                              | -                                           | NO                                      |\\n| Smart Proxy     | Would require a data storage for storing the return address                                  | -                                           | No                                       |\\n| Test Message    | Would require, that the Test Data Verifier receives from two topics                          | -                                           | No                                       |\\n| Channel Purger  | This would be a feature, that is implemented in the Generic Component. It is not planned yet | -                                           | No                                       |\\n","tokens":163,"id":4680,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0003-quality-attribute-security.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nShould we consider and address security requirements?\\n","Decision":"For the initial start of the project we do not consider security as a must have.\\n","tokens":15,"id":4681,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0008-browser-compatibility.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhich browser should we actively support with the MICO web interface?\\n","Decision":"We will only actively support Chrome (above version 70). We may test the web interface occasionally with Firefox (above version 63) to fix issues rendering the web interface unusable with firefox.\\n","tokens":20,"id":4682,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0013-source-to-image-workflow.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to have a Source-to-Image workflow to import services based on a GitHub repository. It should run inside our Kubernetes cluster, however currently Kubernetes doesn't have a resource build-in that is able to build container images. Therefore another technology is required.\\n## Decision Drivers\\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\n","Decision":"* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\nChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.\\n### Positive Consequences\\n* By using *Knative Build* we have the choice to use different kinds of `Builders`, follow-up decision is required: [Building OCI images](.\/0015-building-oci-images.md)\\n### Negative consequences\\n* *Nothing known*\\n","tokens":114,"id":4683,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0005-mono-repo.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nShould we use different repositories for Frontend and Backend or use a single repository?\\n","Decision":"We chose option 1, Mono Repo, because as a starting point it reduces unnecessary complexity when working with frontend and backend logic.\\nFurthermore, we can have a single CI build in order to verify the whole project source code.\\n","tokens":23,"id":4685,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0014-Spring-Data-REST.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to use CRUD operations to manipulate our domain model.\\nFurthermore, we don't want to write REST controllers if it is not necessary.\\n## Decision Drivers\\n* Must allow CRUD operations\\n* It must be possible to extend the API and to add resources or sub resources which are not managed by Neo4j\\n* It must be compatible with Springfox\\n* The API must be RESTful\\n","Decision":"* Must allow CRUD operations\\n* It must be possible to extend the API and to add resources or sub resources which are not managed by Neo4j\\n* It must be compatible with Springfox\\n* The API must be RESTful\\nChosen option: Spring Data REST, because it automatically exposes or domain model and so we don't have to write the required rest controllers our self. In addition to that paging, sorting and the HAL media type are supported out-of-the-box. Extending or overwriting Spring Data REST Response Handler is also possible and allows us to add custom logic if needed [[1]].\\n","tokens":88,"id":4686,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0009-features-first.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow should we prioritise certain non-\/extra- functional requirements?\\n","Decision":"For the initial project we won't implement the requirements from above to increase our efficiency in implementing features.\\n```eval_rst\\n.. seealso::\\n* :doc:`0002-quality-attribute-performance`\\n* :doc:`0003-quality-attribute-security`\\n```\\n","tokens":20,"id":4687,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0011-json+hal.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe backend needs to serialize the REST resources before sending them via http to the client.\\n## Decision Drivers\\n- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\n","Decision":"- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\nWe will use json+hal (or the spring implementation for json with hyperlinks) without the [`_embedded`](https:\/\/tools.ietf.org\/html\/draft-kelly-json-hal-08#section-4.1.2) attribute (because of its complexity to implement correctly).\\n```eval_rst\\n.. seealso::\\n* :doc:`0010-rest-api-design`\\n* `Richardson Maturity Model <https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html>`_\\n* `HATEOAS is for Humans <http:\/\/intercoolerjs.org\/2016\/05\/08\/hatoeas-is-for-humans.html>`_\\n```\\n","tokens":79,"id":4688,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0020-configurable-service-dependencies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nServices currently have a static network of dependencies. This makes it impossible to have a service that depends on a SQL based database use a different database that is compatible with the service.\\n## Decision Drivers\\n* Should fit in the current domain model\\n* Easy to add afterwards\\n* Should support existing services with their static dependencies\\n","Decision":"* Should fit in the current domain model\\n* Easy to add afterwards\\n* Should support existing services with their static dependencies\\nTo be decided.\\nThis adr purely documents possible decisions for this problem.\\n","tokens":71,"id":4690,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0023-faas.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo execute a variety of different functions that are based on different messaging patterns we want to use the Function as a Service (FaaS) approach.\\n## Decision Drivers\\n* MUST support Kubernetes\\n* MUST support Kafka to trigger events\\n* SHOULD be a well known and proven solution\\n* SHOULD support the most common programming languages (at least Java, Python, JavaScript)\\n* SHOULD support the Serverless framework\\n","Decision":"* MUST support Kubernetes\\n* MUST support Kafka to trigger events\\n* SHOULD be a well known and proven solution\\n* SHOULD support the most common programming languages (at least Java, Python, JavaScript)\\n* SHOULD support the Serverless framework\\nWe want to use OpenFaaS.\\n","tokens":89,"id":4691,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0004-monolith-first.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhich architectural style should we use to build MICO?\\n","Decision":"For the initial project we apply the \"Monolith first\" approach in order to avoid the additional complexity of a microservice architecture.\\n","tokens":18,"id":4692,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4693,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0003-vrs-integration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIdentify the components the VRS will integrate with that includes the parameters that needed to be passed.\\n","Decision":"Components that will call the VRS are the following:\\n### 1. Voice Assistant Platform (VAS)\\n- Search for registered names (GET)\\n### 2. VRS User Interface for registration, searching, and updating status\\n- Register new names (post)\\n- Update information for current registered names (PUT)\\n- Cancel registration for current registered names (PUT)\\n- Search for registered names (GET)\\n### 3. Local VRS resolver (Internet Of Things)\\n- Search for registered names (GET)\\n> ![](docs\/..\/..\/..\/components\/component_assets\/vrs_003_01.png?raw=true \"Fig. 1 - VRS Integrations\")\\nLocal implementation of VRS is a requirement to highlight the importance of privacy and enterprise-level implementation.\\nIn summary, VRS will always have a system-to-system integration and not directly to the user. These integrations are in alignment with our architecture guiding principle about VRS.\\n","tokens":24,"id":4696,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docs\/0002-quality-attribute-performance.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nShould we consider and address performance requirements?\\n","Decision":"For the initial start of the project we do not consider performance as a must have.\\n","tokens":15,"id":4697,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0002-global-operator.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThis proposal explains the raison d'\u00eatre for the *global operator*. The *global operator* is an operator that contains controllers for resources that span more than one deployment.\\nAdditionally, this proposal outlines deploying an operator per namespace that is responsible for managing namespace-local resources, such as individual Elasticsearch clusters and Kibana nodes.\\n## Decision Drivers\\n### Prior state\\nAs of when this proposal was written, a single stack operator was running in the \"stack-operators-system\" namespace:\\nAll controllers watch all the namespaces for CRDs we define and reconcile them within the namespace they are defined in.\\n### Motivation\\nWe want to be able to address the following concerns:\\n- Security: RBAC does not allow us to limit the resources we may be watching based on their labels or annotations so our operator has to have a very wide set of permissions (practically close to full admin on all K8s cluster resources)\\n- All controllers watch a lot of K8s API resources in all namespaces. This has multiple downsides:\\n- Unnecessary load put on K8s apiserver as we'll be watching resources that are not related to our use case (e.g listening to all Services, ConfigMaps, Secrets etc). We can expect some improvements from controller-runtime in this area, but the time-frame is not clear.\\n- Single point of failure: if the operator crashes, not only the triggering resources will be affected, but an entire region.\\n- Slowly responding clusters \/ clusters with a large cluster state may negatively affect performance \/ resource consumption.\\n- Resource usage as number of deployed resources grow is difficult to ascertain a-priori.\\n- To upgrade the version of the operator, all operations need to go down temporarily (this may be mitigated at least somewhat by running multiple operators and introduce leader election between them).\\n- May or may not be fine: No ability to slowly roll changes out, needs to be all or nothing.\\n","Decision":"### Prior state\\nAs of when this proposal was written, a single stack operator was running in the \"stack-operators-system\" namespace:\\nAll controllers watch all the namespaces for CRDs we define and reconcile them within the namespace they are defined in.\\n### Motivation\\nWe want to be able to address the following concerns:\\n- Security: RBAC does not allow us to limit the resources we may be watching based on their labels or annotations so our operator has to have a very wide set of permissions (practically close to full admin on all K8s cluster resources)\\n- All controllers watch a lot of K8s API resources in all namespaces. This has multiple downsides:\\n- Unnecessary load put on K8s apiserver as we'll be watching resources that are not related to our use case (e.g listening to all Services, ConfigMaps, Secrets etc). We can expect some improvements from controller-runtime in this area, but the time-frame is not clear.\\n- Single point of failure: if the operator crashes, not only the triggering resources will be affected, but an entire region.\\n- Slowly responding clusters \/ clusters with a large cluster state may negatively affect performance \/ resource consumption.\\n- Resource usage as number of deployed resources grow is difficult to ascertain a-priori.\\n- To upgrade the version of the operator, all operations need to go down temporarily (this may be mitigated at least somewhat by running multiple operators and introduce leader election between them).\\n- May or may not be fine: No ability to slowly roll changes out, needs to be all or nothing.\\nSuperseded by [005](https:\/\/github.com\/elastic\/cloud-on-k8s\/blob\/main\/docs\/design\/0005-configurable-operator.md).\\n### Positive Consequences <!-- optional -->\\n+ Can limit what each operator can do based on namespace (trivial through RBAC).\\n+ Can attribute resource usage of the operator to namespaces \/ customers \/ projects.\\n+ Enables rolling out updates in a controlled manner.\\n### Negative Consequences <!-- optional -->\\n- Introduces more than one operator, complicating deployment and debugging.\\n- Controllers in the global operator, such as the CCR controller still need to be scaled on a number-of-clusters \/ associations basis (but it does not need to connect to individual ES clusters).\\n","tokens":405,"id":4700,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0009-pod-reuse-es-restart.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThis design proposal is focused on how best to handle Elasticsearch process restart inside a pod, to catch up with configuration changes.\\nThe primary use case for this is switching for one license type to another with network configuration change. For example, switching from a cluster with a Basic license (no TLS) to a cluster with a Gold license (TLS) requires a full cluster restart. It's not possible to \"just\" replace the pods with new ones here: we would need data to be migrated between pods that cannot communicate with each others. Reusing PersistentVolumes could be a solution here, but may not play well with local persistent volumes bound to a particular node: what if the node gets scheduled to another pod in-between the migration?\\nReusing pods may also be useful in other situations, where simply restarting Elasticsearch process can be way faster and more efficient than replacing the pod:\\n- minor configuration changes\\n- plugin changes\\n## Decision Drivers\\n* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don't restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\n","Decision":"* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don't restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\nChosen option: option 1, because that's the only one we have here? :)\\n### Positive Consequences\\n* Handles pod and cluster restart, rolling or coordinated\\n* Allows humans to trigger a restart through annotations\\n* Safe from cache inconsistencies, operator restart, reconciliation retries, volume propagation\\n### Negative Consequences\\n* Needs to be implemented!\\n* Additional complexity.\\n* Need to be extra careful about chaining steps in the right order, and make them idempotent.\\n* Once a restart is scheduled, it will go through completion. If the user modifies settings again, we'll wait for the current restart to be done.\\n","tokens":303,"id":4703,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0006-certificate-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAny production-grade Elasticsearch setup must be set up with TLS certificates. It is even more important in a Kubernetes cluster, where any running service could misbehave and impersonate an Elasticsearch node.\\nHow do we manage nodes certificates with our operator?\\n## Decision Drivers\\n* Security\\n* Each node should have its own private key, signed by a CA.\\n* Private keys for both nodes and CA should be stored in such a way they very hard to obtain and\/or decipher.\\n* Certificate rotation should be easily doable by design.\\n* ES pods should not access the Kubernetes API directly.\\n* Ease of use\\n* Human operator intervention in running the operator regarding certificate management should be restricted as a minimum (ideally, the human operator should not care at all).\\n* Cluster impact\\n* Certificate rotation should not provoke any downtime on the cluster.\\n* Certificate rotation should preferably be doable without well-written clients experiencing issues with certificate validation.\\n","Decision":"* Security\\n* Each node should have its own private key, signed by a CA.\\n* Private keys for both nodes and CA should be stored in such a way they very hard to obtain and\/or decipher.\\n* Certificate rotation should be easily doable by design.\\n* ES pods should not access the Kubernetes API directly.\\n* Ease of use\\n* Human operator intervention in running the operator regarding certificate management should be restricted as a minimum (ideally, the human operator should not care at all).\\n* Cluster impact\\n* Certificate rotation should not provoke any downtime on the cluster.\\n* Certificate rotation should preferably be doable without well-written clients experiencing issues with certificate validation.\\nWe decided to go with Option 1 (manage certificates in the operator). Major drawback is the additional complexity in the operator (and in init\/sidecar containers).\\nAlso, reaching the Elasticsearch cluster from any other service in the k8s cluster requires the CA cert to be available from that service. Which can be complex to handle, especially considering CA certificate rotation. This could be mitigated by adding an extra ingress proxy layer in front of Elasticsearch which would have its own certificate (potentially signed from a public well-known CA). Then forwarding requests to the ES cluster, using a second TLS handshake.\\nEven though choosing option 1 by default, option 3 could also be handled through configuration options to use user-provided certificates.\\n","tokens":200,"id":4704,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0007-local-volume-total-capacity.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nOur current [dynamic provisioner for local volumes](https:\/\/github.com\/elastic\/local-volume) does not handle maximum storage available on nodes. It means a pod can get assigned to a node for which we'd need to create a PersistentVolume, even though the physical disk might be full already.\\nThe way it currently works is the following:\\n1. a single cluster-wide provisioner dynamically creates PersistentVolume resources matching non-bound PersistentVolumeClaim resources. These PersistentVolumes are not bound to any particular node yet, they will be once the pod is scheduled to a node.\\n2. a driver on each node is responsible for creating the actual volume on disk when a pod is scheduled to the node. It also updates the PersistentVolume NodeAffinity to match the node (for PV reuse).\\nIn step 1, we do not assign the PV to any node yet. The pod can be scheduled to any node by the kubernetes scheduler. In step 2, the pod might have been assigned to a node with no free space.\\nWe need a way to make sure pod scheduling takes available disk space into consideration.\\n## Decision Drivers\\n* A Pod requesting an `elastic-local` PVC which would not fit on a node's remaining disk space should not be scheduled on this node.\\n* The solution should not require any particular human operator intervention.\\n* Performance impact in kubernetes pod scheduling should be acceptable.\\n* Node affinity and failure domain mapping should still be possible.\\n","Decision":"* A Pod requesting an `elastic-local` PVC which would not fit on a node's remaining disk space should not be scheduled on this node.\\n* The solution should not require any particular human operator intervention.\\n* Performance impact in kubernetes pod scheduling should be acceptable.\\n* Node affinity and failure domain mapping should still be possible.\\nTODO. Option 2 seems way better IMHO.\\n","tokens":305,"id":4709,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0010-license-checks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to control behaviour in the operators based on the current\\nEnterprise license applied to the operator. An example for conditional\\nbehaviour are any commercial features available to licensed operators only.\\n## Decision Drivers\\n* we currently support multiple deployment options for the operator: single namespace and multi-namespace.\\n* for the activation of commercial features we need a minimal amount of protection against tampering.\\n* we consider admission controllers as optional means of verification and don't want to rely on them for functionality that does not exist in other places as well.\\n* we regard licenses as somewhat sensitive data that should not be shared freely across all namespaces and controllers.\\n","Decision":"* we currently support multiple deployment options for the operator: single namespace and multi-namespace.\\n* for the activation of commercial features we need a minimal amount of protection against tampering.\\n* we consider admission controllers as optional means of verification and don't want to rely on them for functionality that does not exist in other places as well.\\n* we regard licenses as somewhat sensitive data that should not be shared freely across all namespaces and controllers.\\nOption 3.\\n### Additional design details\\nCreate a new CRD called `ControllerLicense` that will be created\\nby the (global) license controller. It CAN use the same signer we use\\nfor the Enterprise licenses but a different installation specific\\nprivate\/public key pair.\\nThe public key CAN be shared with the namespace operator at\\ninstallation time. The individual controllers that are interested in\\nlicense checks will then verify any `ControllerLicenses` appearing in\\ntheir namespace using that key and MUST toggle commercial features only if a valid license\\nis present.\\nThe license controller MUST create controller licenses only when either a valid\\nEnterprise license or a valid Enterprise trial license is present in the system. It CAN\\nissue controller licenses with shorter lifetimes than the Enterprise license and\\nauto-extend them as needed to limit the impact of accidental license leaks. But license leaks\\nare currently understood to be much less a concern than cluster licenses leaks as controller licenses have no validity\\noutside of the operator installation that has created them.\\n```\\n+--------------------------------+   +----------------------------------+\\n|                                |   |                                  |\\n|  elastic-system                |   |       foo-system                 |\\n|                                |   |                                  |\\n|                                |   |                                  |\\n|                                |   |                                  |\\n|             +------------+     |   |        +-------------+           |\\n|             |            |     |   |        |             |           |\\n|             |  license   |     |   |        |    foo      |           |\\n|             |  ctrl      |     |   |        |    ctrl     |           |\\n|             |            |     |   |        |             |           |\\n|             +-+-----+--+-+     |   |        ++-----------++           |\\n|               ^     |  |       |   |         ^           ^            |\\n|               |     |  |       |   |         |           |            |\\n|               |     |  |       |   |         |           |            |\\n|   +-----------+-+   |  |       |   |   +-----+------+  +-+---------+  |\\n|   |             |   |  |       |   |   |            |  |           |  |\\n|   |  Enterprise |   |  |       |   |   | Controller |  |  PubKey   |  |\\n|   |  License    |   |  +-------------->+ License    |  |           |  |\\n|   |             |   |  creates |   |   |            |  |           |  |\\n|   +-------------+   |          |   |   +------------+  +------+----+  |\\n|                     |          |   |                          ^       |\\n|                     |          |   |                          |       |\\n|                     |          |   |   creates                |       |\\n|                     +-----------------------------------------+       |\\n|                                |   |                                  |\\n+--------------------------------+   +----------------------------------+\\n```\\nDue to restrictions in controller runtime the license +\\nsecret would need to be deployed into the managed namespace not into\\nthe control plane namespace. Unless of course we run everything in one\\nnamespace anyway or we implement a custom client\\nthat has access to the control plane namespace of the namespace\\noperator (the latter is the underlying assumption for the license controller graph).\\n### Positive Consequences\\n* Does not give every namespace operator access to the full Enterprise license.\\n* Allows to issue short term controller licenses to limit impact of exposure.\\n* Does allow relative protection against tampering.\\n### Negative Consequences\\n* Effort and complexity of the implementation.\\n* The key\/pair to verify the controller license can be manipulated. The global license\\ncontroller can to some extend counteract that by deleting\/recreating the correct license\\nand public key resources.\\n","tokens":138,"id":4710,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cloud-on-k8s\/0004-licensing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nPurpose of this decision record is to outline an implementation for license management for Elasticsearch clusters managed by the Elastic k8s operator.\\n## Decision Drivers <!-- optional -->\\n* gold\/platinum level licenses can only be applied to clusters using internal TLS\\n* user applying the license needs to have `manage` privileges if security features are enabled (which is always the case)\\n* enterprise licenses should be shared between multiple clusters\\n* in some use cases we might want to isolate enterprise licenses from clusters by using a different namespace\\n","Decision":"* gold\/platinum level licenses can only be applied to clusters using internal TLS\\n* user applying the license needs to have `manage` privileges if security features are enabled (which is always the case)\\n* enterprise licenses should be shared between multiple clusters\\n* in some use cases we might want to isolate enterprise licenses from clusters by using a different namespace\\nOption 1 and 2 are both valid as two separate implementation phases. Without a running license controller option 1 still ensures licenses can be applied to Elasticsearch.\\n### Positive Consequences <!-- optional -->\\n* both options are orthogonal to each other\\n* when used in combination user just needs to create Enterprise licenses in the namespace of the license controller\\n* one license controller can manage licenses for all Elasticsearch clusters\\n### Negative Consequences <!-- optional -->\\n* we are limited to Enterprise licenses for option 2 at the moment (could be revisited)\\n","tokens":111,"id":4712,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"latis3\/0004-Custom-adapters-defined-outside-core-FDML-schema.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThere are an unlimited number of adapters that can be defined by the FDML schema syntax.  A way to reduce complexity suggests moving adapter schemas out of the core FDML schema.\\n## Decision Drivers\\n* keep the main schema simple\\n* increased complexity of multiple schema files\\n*\\n","Decision":"* keep the main schema simple\\n* increased complexity of multiple schema files\\n*\\nChosen option: Keep a core FDML.xsd separate from specialized adapter schemas\\n### Positive Consequences <!-- optional -->\\n* the core schema will be stable for periods of months\\n* many adapter schemas can be created without changing the core schema\\n### Negative Consequences\\n* some datasets will require two linked schemas\\n","tokens":64,"id":4714,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"latis3\/0003-Sample-objects-implemented-as-domain-range-tuple.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSample objects are a fundamental concept in the functional data model.  Several different approaches to represent Sample objects are available.\\n## Decision Drivers\\n* performance\\n* conceptual integrity\\n* mathematical correctness\\n","Decision":"* performance\\n* conceptual integrity\\n* mathematical correctness\\nDomain variables and range variables will be separated in two vectors\\n### Positive Consequences\\n* removing the integer describing the size of the domain reduces complexity\\n* concept of functions as a domain mapping to a domain is emphasized\\n### Negative Consequences\\n* samples are now broken down into 2 parts\\n","tokens":45,"id":4715,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"latis3\/0002-Decouple-data-and-datatype-objects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nPreviously LaTiS V2 combined Data and Metadata in the Variable trait.  What appeared to be a good object-oriented design decision turns out to create more problems than are solved by combining these two concepts.\\n## Decision Drivers\\n* performance, computing  with metadata is simply inefficient\\n* separation of concerns\\n* inconsistency between data and metadata\\n","Decision":"* performance, computing  with metadata is simply inefficient\\n* separation of concerns\\n* inconsistency between data and metadata\\nChosen option: Data and metadata will be separated by removing metadata from the Sample class.\\n### Positive Consequences\\n* Loading data into Spark will be greatly simplified by removing the metadata\\n* Applying transformations to the data and the model separately will reduce code complexity\\n### Negative Consequences\\n* data and metadata may become out of sync\\n","tokens":75,"id":4717,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"GeositeFramework\/adr-001-layer-selector-rewrite.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe layer selector plugin has grown in complexity over the years and has\\naccumulated a fair amount of technical debt. Fixing bugs and adding new\\nfeatures has become increasingly difficult and time consuming as a result.\\nSome issues with the current implementation:\\n* The widget we use ([Ext JS Tree Panel](https:\/\/docs.sencha.com\/extjs\/6.0\/components\/trees.html))\\ndoes not provide enough control over how nodes are rendered.\\n* It is difficult to add buttons and other interactions to each leaf node\\n(zoom to extent, download button, etc.) using Ext JS Tree Panel.\\n* Separation between *saved state* and *actual state*. The in-memory representation\\nof the tree often differs from the serialized tree state, which has led to\\nsynchronization issues.\\n* Lack of high-level data abstractions make it difficult to understand\\nhow the plugin transforms data for rendering.\\n* Mutable state occasionally leads to data corruption caused by performing\\ncertain sequences.\\n","Decision":"We have decided to rewrite the layer selector from scratch before adding\\nnew features.\\nTo resolve the problems previously described, the new layer selector should\\nhave the following qualities:\\n* The UI should be defined using simple `underscore` templates so we retain\\nfull control over the look & feel.\\n* All user interactions should aspire to update a singular state object,\\nwhich will then cause the UI to be redrawn. In this way, the UI will always\\nbe a reflection of what actually exists in the saved state.\\n* There should be a separation between the layer config and the layer data\\nthat is loaded at runtime. This is to prevent bugs that can result from\\nmutating a single state repeatedly.\\n* Services and layer data should be lazily loaded when possible.\\n","tokens":208,"id":4718,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"polaris\/vision_node_python_refactor.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe focus of this ADR is the ROS vision node. Specifically, the programming language the node is based in. AUVic's software team is putting effort towards developing vision solutions for the sub, notably object detection and associated actions. There is a need to ensure a smooth transition and ensure that solutions are developed as soon as possible for testing, as it is a core functionality of the sub. Furthermore, there has been interest to assign new team members to efforts related to `OpenCV`.\\n","Decision":"A decision is proposed to fully refactor the vision node, including all its implementation in C++, to Python 3. The main motivation is the need to have a fully functioning vision node, with code that is easily unit testable and who\u2019s syntax is easier to understand than C++.\\nSpecific motivations are listed:\\n-\tAs it is shown that Python and C++ functionality is interchangeable [1], and Python syntax is easier to understand, a preference exists to focus on higher level applications.\\n- As opposed to employing the use of `CMakeLists.txt`, using Python only requires the Python interpreter itself [2]. As shown, nodes in Python with similar\\nfunctionality may be created with minimal dependencie on those files [3]\\n- A transition to Python as opposed to another scripting\/programming language such as **Go, Rust, Java, or C** [4] is preferred as Python is a syntactically more preferable language for team members, and does not entail compile-time considerations.\\n- A transition to Python eliminates the risk of dealing with memory management and desctructors. The need for those features have not\\nbeen determined at the time of this writing.\\n- A transition to Python will simplify the approach to writing automated unit tests, as syntax and lack of compile-time overhead\\nis minimized.\\n- The transition will provide us access to `numpy`, a computationally more efficient way of computing results. Furthermore, packages\\nlike `keras`, `theano` and `tensorflow` will lessen the barrier to entry for team members to develop deep learning solutions.\\n","tokens":103,"id":4719,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"polaris\/developmentLanguage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nReactNative supports both JavaScript and TypeScript.\\n","Decision":"Javascript was chosen at this time as it is a more widely used development language and, whilst Babel enables you to use TypeScript, it only enables transpilation of TypeScript and not type checking.\\n","tokens":12,"id":4720,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ADR-structure\/0002-RegularSessionLastSeen.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need this to make the 'Referrals Notification' work.\\nNeeded a way to store the 'last_seen' value during the whole session. Because it is updated every hour by a middleware, we can't use it directly from the user model \/ database, as that would cause us to never show a notification ('last seen' is updated before we even get a chance to show it on the page.)\\n### Options\\n1. Laravel Sessions - Didn't work\\n2. Regular PHP Sessions\\n","Decision":"Option 2 worked, so it's implemented.\\nIn the LastSeen middleware we check for a 'prev_last_seen' session variable.\\n- If missing, we update it with the 'last_seen' value.\\n- If it exists, we don't change it.\\nLater, the User->hasNewReferrals() checks it to determine the right cutoff time, either 10 days ago or the more distant 'last_seen' value.\\n","tokens":105,"id":4721,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jskatas.org\/002-use-lit-html.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn [this PR][1] I started to give the site some dynamic feature, and using (p)react for it\\ndoes always mangle state handling with the DOM lib (which (p)react are).\\nThis is an anti-pattern imho, I want the business logic separated and independent of the\\n(DOM) rendering part.\\n","Decision":"Therefore I switched to lit-html, since it seems to provide this.\\n[SSR][2] might be a bit more work here, but it feels right.\\n","tokens":75,"id":4723,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jskatas.org\/001-server-side-rendering.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe web was born as HTML, lately we fallback too often to JS as our default.\\nThis makes pages slow, adds unnecessary load to the web and excludes users.\\nSSR is the least we can do to make sites easier to use, have less load on the\\nclients and deliver speed by default.\\n","Decision":"Use SSR.\\n","tokens":67,"id":4724,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"biosamples-v4\/0006-submitting-the-same-sample-twice.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhat should we do if a user submitted same sample twice ?\\n1. Update the first one by checking name and domain fields\\n2. Create another sample with an accession without considering the first one\\n3. Send an error message explaining that a sample exist with same name and domain\\n","Decision":"For now we decided to create a new sample with a new accession as this updating a sample could cause accidental data loss.\\n","tokens":61,"id":4725,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"biosamples-v4\/0003-new-release-process.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUpdating the release process and branches to give us flexibility in term of fixing issues when a release\\nis not working fine. What was happening with the current release process is that we had to skip one or more versions\\nwith issues inside, creating confusion and putting in Maven central invalid versions\\n","Decision":"After long discussion we have decided to update our branches and how we do the release with a new process.\\nFirst of all, in order to standardize with other projects (USI in primis) we have decided to use a *dev* branch for\\ncontinuous development and the *master* branch to release new version of the software. Here the process described\\nstep-by-step\\n*From `dev` to `master`*\\n- From the `dev` branch we create feature branch - usually associated to a ticket - to work on a specific task\\n- When the developer has finished the task, he creates a pull request (PR) from the feature branch back into the `dev` branch on github\\n- If everybody is ok with the PR, it is merged into the `dev` branch\\n- When the team wants to do a new release of the software, any user can create the release notes and prepare a PR from the `dev` branch to the `master` branch\\n- When everybody is ok with the release notes, the developer can merge the PR\\n- Update the version on dev to the next SNAPSHOT (e.g. 4.1.6-SNAPSHOT --> 4.1.7-SNAPSHOT)\\n*From `master` to release of the software*\\n- In the `master` branch, the developer needs to update the version of the software to a release candidate (e.g. 4.1.6-SNAPSHOT --> 4.1.6-RC1)\\n- The software with the corresponding new version is then pushed to the remote master branch, which will trigger a build and release plan on snowy\\n- If everything is going to be fine, the developer can promote the artifact to scooby and then wwwdev\\n- After the release on wwwdev, the developer should send an email to the biosamples-users mailing list to announce the new software release and ask people to test it\\n- If any error is found, the developer need to make an hot-fix on master, update the version of the software (e.g. 4.1.6-RC1 --> 4.1.6-RC2) and release on the various environments till wwwdev\\n- After the software has been on wwwdev for a week and everybody is happy with it, is time to make the final release\\n- Update the version to the final version (e.g. 4.1.6-RC2 --> 4.1.6)\\n- Tag the current commit on github with `git tag v4.1.6` and push both the code and the tag to the remote repository\\n- Release on all the environments one after the other: snowy --> scooby --> wwwdev --> www\\n- After the release on production is done, send an email to the biosamples-announcements mailin list announcing the new release\\n- Merge the `master` branch back into `dev` to make sure `dev` also contains all the hot-fixes done on master\\n","tokens":60,"id":4727,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"biosamples-v4\/0005-release-and-update-date.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSample should have a release and update date when we store it in database.\\nWhen user submit a sample we could give flexibility to the user by filling release and update date to today if they are missing.\\nBut filling release date can accidentally make a sample public.\\n","Decision":"We decided that it is best to send an error message if sample release date is not provided.\\nOn the other hand, we will fill update date to today's date if it is missing.\\n","tokens":57,"id":4728,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"biosamples-v4\/0002-have-taxid-attribute-at-top-level.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nENA presentation requires a top-level, numerical only taxID.\\n","Decision":"We have added a taxID field at the top-level of all biosamples.\\n- This is a single, top-level, numeric field named 'taxId'.\\n- If no taxon available the value of taxId will be 0.\\n- There is no support for multiple taxId as there is no data that meets this requirement.\\n","tokens":16,"id":4729,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"biosamples-v4\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to record architectural decisions made in this project.\\nWhich format and structure should these records follow?\\n","Decision":"* Google docs - hard to find and keep structured\\n* Confluence - hard to edit and search\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), because\\n* Implicit assumptions should be made explicit.\\nDesign documentation is important to enable people understanding the decisions later on.\\nSee also [A rational design process: How and why to fake it](https:\/\/doi.org\/10.1109\/TSE.1986.6312940).\\n* The MADR format is lean and fits our development style.\\n* The MADR structure is comprehensible and facilitates usage & maintenance.\\n","tokens":28,"id":4730,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0022-remove-stop-words-during-query-transformation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen quering for a title of a paper, the title might contain stop words such as \"a\", \"for\", \"and\". Some data providers return 0 results when querying for a stop word. When transforming a query to the lucene syntax, the default Boolean operator `and` is used. When using IEEE, this often leads to zero search results.\\n## Decision Drivers\\n* Consistent to the Google search engine\\n* Allow reproducible searches\\n* Avoid WTFs on the user's side\\n","Decision":"* Consistent to the Google search engine\\n* Allow reproducible searches\\n* Avoid WTFs on the user's side\\nChosen option: \"Remove stop words from the query\", because comes out best.\\n","tokens":109,"id":4731,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0001-use-crowdin-for-translations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe JabRef UI is offered in multiple languages. It should be easy for translators to translate the strings.\\n","Decision":"Chosen option: \"Use Crowdin\", because Crowdin is easy to use, integrates in our GitHub workflow, and is free for OSS projects.\\n","tokens":28,"id":4732,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0012-handle-different-bibEntry-formats-of-fetchers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAll fetchers (except IDFetchers) in JabRef return BibEntries when fetching entries from their API.\\nSome fetchers directly receive BibTeX entries from their API, the other fetchers receive their entries in some kind of exchange format such as JSON or XML and then parse this into BibEntries.\\nCurrently, all fetchers either return BibEntries in BibTeX or BibLaTeX format.\\nThis can lead to importing BibEntries of one format in a database of the other format.\\nHow can this inconsistency between fetchers, and their used formats be addressed?\\n","Decision":"Chosen option: \"Introduce a new layer between fetchers and caller, such as a FetcherHandler, that manages the conversion\",\\nbecause it can compose all steps required during importing, not only format conversion of fetched entries.\\n[As described here (comment)](https:\/\/github.com\/JabRef\/jabref\/pull\/6687)\\n","tokens":122,"id":4733,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0018-use-regular-expression-to-split-multiple-sentence-titles.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSome entry titles are composed of multiple sentences, for example: \"Whose Music? A Sociology of Musical Language\", therefore, it is necessary to first split the title into sentences and process them individually to ensure proper formatting using '[Sentence Case](https:\/\/en.wiktionary.org\/wiki\/sentence_case)' or '[Title Case](https:\/\/en.wiktionary.org\/wiki\/title_case#English)'\\n","Decision":"Chosen option: \"Regular expression\", because we can use Java internal classes (Pattern, Matcher) instead of adding additional dependencies\\n### Positive Consequences\\n* Less dependencies on third party libraries\\n* Smaller project size (ICU4J is very large)\\n* No need for model data (OpenNLP is a machine learning based toolkit and needs a trained model to work properly)\\n### Negative Consequences\\n* Regular expressions can never cover every case, therefore, splitting may not be accurate for every title\\n","tokens":86,"id":4734,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0017-allow-model-access-logic.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n- How to create a maintainable architecture?\\n- How to split model, logic, and UI\\n## Decision Drivers\\n- Newcomers should find the architecture \"split\" natural\\n- The architecture should be a help (and not a burden)\\n","Decision":"- Newcomers should find the architecture \"split\" natural\\n- The architecture should be a help (and not a burden)\\nChosen option: \"`org.jabref.model` may use `org.jabref.logic` in defined cases\", because comes out best \\(see below\\).\\n","tokens":58,"id":4735,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0008-use-public-final-instead-of-getters.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen making immutable data accessible in a java class, should it be using getters or by non-modifiable fields?\\n","Decision":"Chosen option: \"Offer public static field\", because getters used to be a convention which was even more manifested due to libraries depending on the existence on getters\/setters. In the case of immutable variables, adding public getters is just useless since one is not hiding anything.\\n### Positive Consequences\\n* Shorter code\\n### Negative Consequences\\n* newcomers could get confused, because getters\/setters are still taught\\n","tokens":29,"id":4736,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0003-use-gradle-as-build-tool.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhich build tool should be used?\\n","Decision":"Chosen option: \"Gradle\", because it is lean and fits our development style.\\n","tokens":14,"id":4737,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0019-implement-special-fields-as-separate-fields.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow to implement special fields in bibtex databases?\\n","Decision":"Chosen option: \"Special fields as separate fields\", because comes out best (see below)\\n","tokens":18,"id":4739,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0007-human-readable-changelog.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nChanges of a release have to be communicated. How and which stile to use?\\n","Decision":"Chosen option: \"Keep-a-changelog format with freedom in the bullet points\", because\\n* [Keep-a-changelog](https:\/\/keepachangelog.com\/) structures the changelog\\n* Each entry can be structured to be understandable\\n* Forcing to prefix each line with `We fixed`, `We changed`, ... seems to be read strange.\\nWe nevertheless try to follow that style.\\nFurther discussion can be found at [\\#2277](https:\/\/github.com\/JabRef\/jabref\/issues\/2277).\\n","tokens":24,"id":4740,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0011-test-external-links-in-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe JabRef repository contains Markdown (`.md`) files documenting the JabRef code.\\nThe documentation contains links to external resources.\\nFor high-quality documentation, external links should be working.\\n## Decision Drivers\\n* Checking external links should not cause issues in the normal workflow\\n","Decision":"* Checking external links should not cause issues in the normal workflow\\nChosen option: \"\\[option 1\\]\", because \\[justification. e.g., only option, which meets k.o. criterion decision driver \\| which resolves force force \\| \u2026 \\| comes out best \\(see below\\)\\].\\n### Positive Consequences\\n* Automatic notification of broken external links\\n### Negative Consequences\\n* Some external sites need to [be disabled](https:\/\/github.com\/JabRef\/jabref\/pull\/6542\/files). For instance, GitHub.com always returns \"forbidden\". A [filter for status is future work of the used tool](https:\/\/github.com\/tcort\/markdown-link-check\/issues\/94#issuecomment-634947466).\\n","tokens":61,"id":4743,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0023-localized-preferences.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nCurrently, JabRef uses some localized preferences. We want to remove the Localization-dependency from the JabRefPreferences and move the Localization to where the String is used.\\nThe problem is how to store the default values.\\n","Decision":"Chosen option: \"_Store the unlocalized String._ Consumers then check the String they got as a preference against the defaults. If it matches, localize it. Otherwise, use it.\", because Achieves goals without requiring too much refactoring and without (known) downsides.\\n","tokens":50,"id":4744,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0004-use-mariadb-connector.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nJabRef needs to connect to a MySQL database. See [Shared SQL Database](https:\/\/docs.jabref.org\/collaborative-work\/sqldatabase) for more information.\\n","Decision":"Chosen option: \"Use MariaDB Connector\", because comes out best \\(see below\\).\\n","tokens":45,"id":4745,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0013-add-native-support-biblatex-software.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nRight now, JabRef does not have support for Biblatex-Software out of the box, users have to add custom entry types.\\nWith citing software becoming fairly common, native support is helpful.\\n## Decision Drivers\\n* None of the existing flows should be impacted\\n","Decision":"* None of the existing flows should be impacted\\nChosen option: \"Add a new divider\", because comes out best (see below).\\n### Positive Consequences\\n* Inbuilt coverage for a entry type that is getting more and more importance\\n### Negative Consequences\\n* Adds a little bit more clutter to the Add Entry pane\\n","tokens":61,"id":4746,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0005-fully-support-utf8-only-for-latex-files.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe feature [search for citations](https:\/\/github.com\/JabRef\/user-documentation\/issues\/210) displays the content of LaTeX files. The LaTeX files are text files and might be encoded arbitrarily.\\n","Decision":"Chosen option: \"Support UTF-8 encoding only\", because comes out best \\(see below\\).\\n### Positive Consequences\\n* All content of LaTeX files are displayed in JabRef\\n### Negative Consequences\\n* When a LaTeX files is encoded in another encoding, the user might see strange characters in JabRef\\n","tokens":47,"id":4749,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0020-use-Jackson-to-parse-study-yml.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe study definition file is formulated as a YAML document.\\nTo accessed the definition within JabRef this document has to be parsed.\\nWhat parser should be used to parse YAML files?\\n","Decision":"Chosen option: Jackson, because as it is a dedicated library for parsing YAML. yamlbeans also seem to be viable. They all offer similar functionality\\n","tokens":44,"id":4750,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0009-use-plain-junit5-for-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow to write readable test assertions?\\nHow to write readable test assertions for advanced tests?\\n","Decision":"Chosen option: \"Plain JUnit5\", because comes out best \\(see below\\).\\n### Positive Consequences\\n* Tests are more readable\\n* More easy to write tests\\n* More readable assertions\\n### Negative Consequences\\n* More complicated testing leads to more complicated assertions\\n","tokens":25,"id":4751,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0006-only-translated-strings-in-language-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nJabRef has translation files `JabRef_it.properties`, ... There are translated and untranslated strings. Which ones should be in the translation file?\\n## Decision Drivers\\n* Translators should find new strings to translate easily\\n* New strings to translate should be written into `JabRef_en.properties` to enable translation by the translators\\n* Crowdin should be kept as translation platform, because 1\\) it is much easier for the translators than the GitHub workflow and 2\\) it is free for OSS projects.\\n","Decision":"* Translators should find new strings to translate easily\\n* New strings to translate should be written into `JabRef_en.properties` to enable translation by the translators\\n* Crowdin should be kept as translation platform, because 1\\) it is much easier for the translators than the GitHub workflow and 2\\) it is free for OSS projects.\\nChosen option: \"Only translated strings in language file\", because comes out best \\(see below.\\n","tokens":113,"id":4752,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"jabref\/0016-mutable-preferences-objects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo create an immutable preferences object every time seems to be a waste of time and computer memory.\\n","Decision":"Chosen option: \"Alter the exiting object\", because the preferences objects are just wrappers around the basic preferences framework of JDK. They\\nshould be mutable on-the-fly similar to objects with a Builder inside and to be stored immediately again in the\\npreferences.\\n","tokens":26,"id":4753,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"integration-framework-bundle\/001-amqp.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere was a need to consume the messages **in a non-blocking way** on our ESB. The former driver -PHP's native driver- didn't support such functionality.\\nThe former driver also ignored signals, due to its inability to acknowledge them **while waiting for a message**.\\nThe possible solutions were:\\n- To find a way to implement a non-blocking `consume()` function with the native driver.\\n- To use an AMQP library that offers support for such functionalities, like php-amqp-lib.\\n### Native Driver\\n*Pros*:\\n- Faster than any other library, due to it's a native extension written in C.\\n- Already implemented in the application.\\n*Cons*:\\n- Does not offer support to consume messages in a non-blocking way.\\n- Cannot listen to posix signals while consuming.\\n### PHP-AMQP-LIB\\n*Pros*:\\n- It's RabbitMQ's recommended library.\\n- Offers the possibility to consume messages in a non-blocking way.\\n- Signals can be dispatched to the worker even if it's consuming messages.\\n- Offers other options like: heartbeats, channels, and multiple hosts.\\n*Cons*:\\n- Difficult to implement correctly, without bypassing our own interfaces.\\n- Different approach demands more time to learn and implement it.\\n","Decision":"We decided to use the PHP-AMQP-LIB.\\nAn initial investigation was done to force the previous driver to listen to signals while consuming, but proved to be impossible. The `consume` function of PHP's native driver blocks the execution and nothing else can be done **until a message arrives to the consumer**.\\nWe decided that it was worth to adapt the library into the framework, which greatly impacted the structure of the consumers and queue drivers. Because these classes were designed with a sync protocol in mind (STOMP, in this case) they didn't fit the asynchronous nature of AMQP, prompting for a significant redesign **of all the interfaces and abstractions involved** - which ultimately drove us to break backwards compatibility and think in the 2.0 version of the framework.\\n","tokens":276,"id":4754,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"integration-framework-bundle\/002-deprecate-response-fallback.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently the RestConfigurableProducer will pass the payload received from the target system to the serializer. If this one fails, it will catch the exception, fallback to the original response, **and continue the execution**.\\nThis was done to allow certain APIs to return a non-deserializable response while still being able to continue the execution, either **by ignoring the payload** (i.e.: when the response code is enough) or to evaluate it to something else.\\nThis makes the evaluator vulnerable, as it has no guarantee that what it will expect is something **that can be evaluated**.\\nDue to the fallback is present by default, one endpoint **could randomly fail** if the target system decides to send back a response in a format that is not expected by the application.\\n","Decision":"We decided to deprecate the response fallback in this version and remove it **in the next major version**.\\nA new parameter was introduced in the `RestConfigurableProducer` (`response_format`) that will define the expected response format, and this format is passed to the deserialize function to instantiate the appropriate visitor. For example, a PlainTextVisitor could be implemented to process non-JSON responses while still producing **a evaluable object**. By default, if `response_format` is not setup, `encoding` will be passed.\\n","tokens":162,"id":4755,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"subscribe-with-google\/001-google-play-developer-account.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur SKUs exist within a Google Play Developer account which is not linked to the \"The Guardian\" project (instead it is linked to the \"Google Play Android Developer\" project).\\nThis means we are unable to programmatically obtain Play related data (i.e. SKUs) via Google APIs.\\n","Decision":"Create a new Google Play Developer account and link it to \"The Guardian\" project. This allows us to query Google APIS for Play related data.\\n","tokens":64,"id":4756,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"test_track\/adr-003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to allow apps to ship independently, we need to ensure that\\nsplit names defined in different apps don't collide. The strategy until\\nnow has been for developers to run all the apps that might have\\nconflicting splits in local dev, but this doesn't scale well as they may\\nnot have the latest version of each app at all times and a\\nfully-upgraded local testtrack server to detect conflicts that might\\narise in production.\\nWe need to make sure that migrations don't get jammed up in production\\neven if a developer only has their own app downloaded locally.\\n","Decision":"New migration runners will be expected to prefix their split names with\\ntheir app names and a dot. Legacy runners will be grandfathered out of\\nthis constraint, but we will begin soft enforcing that any new split\\nname conform to using at most a single dot, and that the text before\\nthat dot be the app name of the split's owner_app.\\n","tokens":126,"id":4757,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"test_track\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSplits are accessible to any app in the same ecosystem, but not always\\ndefined by within that app.\\nIncreasingly, migrations may not be validated centrally in local\\ndevelopment ecosystems as we build out a standalone migration runner,\\nand there's no guarantee that all apps in an ecosystem will be up to\\ndate, so forcing central validation could introduce significant\\ndeveloper friction.\\nTherefor it is unlikely that we could strongly valiate the variants\\nof split that was defined by another app at the time that an\\napp_remote_kill is created.\\nAlso, such validations, if they're only possible in production, could\\nlead to broken migrations in production that are already applied in\\nother ecosystems.\\n","Decision":"We will not validate the override_to value of an AppRemoteKill against\\nthe split. The registry knock-out will simply no-op at runtime if the\\nchosen variant is not available, allowing the team to quickly ship another\\nmigration to disable the feature.\\nAt this time, we will still strongly couple app_remote_kills to split\\nexistence, however. This may change in the future if it proves to cause\\noperational issues\\n","tokens":150,"id":4758,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"test_track\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen rapidly iterating on API for prerelease features, we will commonly\\nwant to make breaking API changes without revving the API endpoints and\\nmaintaining the previous versions of the endpoints indefinitely for\\nversions of the app that will never be seen by customers.\\nIn that context, remote kills stand to be an incredibly useful tool.\\nWhen you decide to make a breaking API change, you simply create\/update\\na remote kill for the app from version `0` through the first client\\nversion conforming to the new contract. Then, instead of getting crash\\nbugs on dev builds, the feature simply turns itself off, and testers\\nwill have to upgrade in order to see the feature again.\\nThis stands to eliminate false bug reports of crash bugs from\\ninternal\/alpha testers who simply need to upgrade.\\nBut if we want to rely on this as a safety mechanism, we need to make\\nsure that chrome extension assignment overrides don't have the\\nopportunity to win-out over a remote kill, because they are too blunt of\\na tool. As an alpha tester, you might see that the feature went away and\\nperform another override in the Chrome extension. If that happens,\\nyou'd be back in crash bug land.\\n","Decision":"On TestTrack server, remote kills will be absolute and won't be\\noverridden by force assignments regardless of the relative recency of\\nthe override and the remote kill.\\n","tokens":254,"id":4759,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"insight\/0002-tweepy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"insight\/0001-python.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0003-css.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":4762,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0004-branching-and-release-strategy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?\\n## Decision Drivers\\n* We need to have confidence in our releases.\\n* We want more structured releases while we're still getting our footing in a shared monorepo.\\n* We need simplicity and clear [takt time] so different teams can plan for what is going out the door from them.\\n* It should work well with our agile work environment.\\n","Decision":"* We need to have confidence in our releases.\\n* We want more structured releases while we're still getting our footing in a shared monorepo.\\n* We need simplicity and clear [takt time] so different teams can plan for what is going out the door from them.\\n* It should work well with our agile work environment.\\nChosen option: \"OneFlow\" because it provides a single eternal branch with well structured releases.\\nWe'll implement OneFlow with these details:\\n* Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.\\n* Release and quality managers from each team are responsible for reviewing and approving releases.\\n* Releases apply to all apps in the monorepo.\\n* Releases are versioned like this: `{cycle}.{sprint}.{hotfix}`. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.\\n* Feature branches are merged using \"Squash and merge\", so they can be easily reverted.\\n* There are two ways to build larger features.\\n* If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.\\n* If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.\\n* If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.\\n### Future strategy\\nWith time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.\\n### Hosting environments\\nWe'll set up continuous delivery to different hosting environments:\\nEnvironment | Git source            | Databases\/services | Features\\n------------|-----------------------|--------------------|----------\\nsandbox     | feature branch        | Test               | All\\ndev         | master                | Test               | All\\nstaging     | master                | Prod               | All\\npre-prod    | release\/hotfix branch | Prod               | Finished\\nprod        | latest release tag    | Prod               | Finished\\nWe'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage.\\n","tokens":108,"id":4763,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0011-open-source-license.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\nThere are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\n## Decision Drivers\\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\n","Decision":"* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\nThe MIT license was chosen, for the following reasons:\\n* It is the least restrictive of the licenses.\\n* It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\n* Digital Iceland does not require protection of patents or existing intelletual property.\\n* Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\n* The MIT license is the best known and most widely used free and open-source license in the world.\\n","tokens":285,"id":4764,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0007-viskuausan-static-site-generator.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe're going to create a web app for Viskuausan (API Catalog).\\nViskuausan's static content will be written using markdown.\\nThe problem statement is therefore:\\n> What static site generator to use that has markdown support\\n> and aligns as well as possible to the technical direction?\\n## Decision Drivers\\n* Should conform to Stafr\u00e6nt \u00cdslands technical direction\\n* Should be able to support markdown content\\n* Should be open source\\n* Should be customizable to island.is UI design\\n","Decision":"* Should conform to Stafr\u00e6nt \u00cdslands technical direction\\n* Should be able to support markdown content\\n* Should be open source\\n* Should be customizable to island.is UI design\\nChosen option: Docusaurus\\nIt is a SSG that focuses on doing documentation sites really well.\\nIt has support for markdown and mdx out-of-the-box and has quick setup.\\nBeing built on React provides the options to do custom pages and add more\\ncomplex logic later on if needed.\\n","tokens":114,"id":4765,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0008-use-oauth-and-openid-connect.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhat protocol(s) shall we use as the new standard for authentication and authorization.\\nIt would be supported by our new centralized authority server and should be implemented in all new clients and\\nresource systems needing authentication or authorization. A requirement might be made that the authority service need\\nto support other protocols for legacy systems but all new systems should be encourage to use the same protocol.\\n## Decision Drivers\\n- Secure\\n- Well defined and well reviewed standard\\n- Easy to implement by client and resource systems\\n- Support for non web client systems i.e. mobile devices\\n","Decision":"- Secure\\n- Well defined and well reviewed standard\\n- Easy to implement by client and resource systems\\n- Support for non web client systems i.e. mobile devices\\nChosen option: \"OAuth 2.0 + OpenID Connect\", because it is secure and well\\nexamined and and has support libraries for our tech stack.\\n","tokens":121,"id":4768,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0001-use-nx.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\n","Decision":"* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\nChosen option: \"Nx\", because:\\n* It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n* It's relatively easy to learn with focused documentation.\\n* It has schematics to generate apps, libraries and components that includes all of our tools.\\n* It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":4769,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0010-cms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\\nWhich CMS system would best suit the needs of island.is?\\n## Decision Drivers\\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\n","Decision":"- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\nDevs narrowed the choice down to two options Contentful and Contentstack.\\nBoth systems meet the required featureset.\\nA decision from management was made to use Contentful.\\nContentful is deemed to have a larger presence in the Icelandic dev community.\\nContentful is also believed to have a stronger funding base.\\nContentful is already implemented in some of our projects.\\n","tokens":191,"id":4770,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"island.is-glosur\/0005-error-tracking-and-monitoring.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nKnow it before they do!\\nWe need a tool to discover, triage, and prioritize errors in real-time.\\n","Decision":"Chosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\\n","tokens":31,"id":4772,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"batonopc\/0001_one_device_or_many.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nThe purpose of this library is to collect together data from one or more sensor devices, save that data to a database and periodically create and send batches of data for processing.\\nShould the library be setup to handle multiple devices, checking and logging data from each device in turn or should that be left to the application using the library?\\nPros for Multi device\\n-------\\n1. easier for application developer - just provide the access details of each device and away you go.\\nPros for Single device\\n-------\\n1. Each device may well have different timings and manage data in different ways. At two extremes:\\n- data is stored on the device and collected daily\\n- device requires sampling at 10 second intervals\\nThis level of complexity is better handled by the application.\\n2. If there are delays in collecting data from one device, this could have a knock on effect in the collection interval of another.\\n3. If one goes down they all go down.\\nDecision\\n----------\\nKeep it Simple - Library written for one device.  In future may add settings to accomodate threading if this proves useful.\\nConsequences\\n--------------\\n","Decision":"----------\\nKeep it Simple - Library written for one device.  In future may add settings to accomodate threading if this proves useful.\\nConsequences\\n--------------\\n","tokens":239,"id":4773,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/004-Identifying-packages-in-npm.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nsfpowerscripts users typically utilize a [scaled trunk based branching model] (https:\/\/trunkbaseddevelopment.com\/#scaled-trunk-based-development). In this model, packages would be built from both develop\/master\/main branches and then during the hardening phase from release branches. When using package registry such as Github Package Manager or Gitlab Package Manager which follow the concept of a package repository tied into the source code repository. These packages get intermingled and there is no easy mechanism to identify where the packages originated from. This becomes a prolem if the user wants to rollback to an older version of the package, the  current solution is to identify the package  manually by figuring the last released packages by checking in at the release deployment logs.\\n","Decision":"All the above solutions make it complicated and not necessary, as  changelog has more accurate details. It is better to use changelog to identify the packages\\n","tokens":156,"id":4775,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/002-parallel-development-process.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen creating a parallel development streams (e.g. release), there are a list of manual steps that must be performed:\\n- package versions may need to be updated so that packages between streams do not share the same version-space\\n- a new artifact feed or npm tags need to be created\\n- a set of artifacts needs to be created for the new development stream\\n","Decision":"All this issues arised from the fact that with the assumption of using multiple feeds. As we are moving to ask users to utilize a single feed\/artifact repository and how it is in most platforms like GitHub or GitLab, there is no specific need for a helper tool. Users should be versioning their artifacts using semantic version when dealing with multiple development streams\\n","tokens":80,"id":4776,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/001-artifact-version-duplication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nOn some CICD platforms, where build numbers are not unique across the organisation, artifacts versions might be duplicated when creating release candidates for the first time and the build number starts from zero on a release pipeline. The outcome would be a mutation error on the artifact registry that prevents the artifact from being published.\\nThis is a problem for source & data packages only. Unlocked package versions are handled by Salesforce.\\n","Decision":"Due to various complexities, it is better to utilize #2, user's has to follow semantic version rather than sfpowerscripts doing any form of automation\\n","tokens":89,"id":4777,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/003-release-autorollback.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen one releases a set of packages into an environment, there could be situations where one of the package fails and resulting in an incorrect org in\\nterms of functionality.\\nFor eg: A release which consists of packages A,B and C failed during the installation of C, has new versions of A and B, introducing new functionality\\nto the org, where as its not accurate without package 'C'.\\nThough number of these instances are pretty low in higher environments, as a particular release would be tested a number of times in multiple environments in lower environments. There are still instances where packages fail to install mostly due to a missing manual step. This results in a potential downtime till the team addresses the failure by a roll-forward.\\nIt would be ideal in this scenario to have a rollback option, which basically realigns the org back to the versions of the packages that were available in the org before the release was intiated.\\n","Decision":"### Release command to support a rollback function\\nRelease command will support an optional rollback function enabled through `rollback:true` parameter in the release defintion. Once this functionality is activated, release command will keep track of existing packages in the org (in memory) before deploying packages as part of the current release. In case of any failures, release command will fetch the old artifacts from the artifact repository and proceed to installing these packages into the org.\\nUnlocked packages have its own lifecycle and Salesforce would maintain deprecation and removal of unused components arising from a rollback. However for source packages, it needs to have a destructive manifest to remove the items already deployed. This functionality only attemps to install an earlier set of packages, and doesnt attempt to destroy any deployed components, especially in the case of source packages. However we could let the users know what are the components left dangling by providing a table of metadata components that will not be removed.\\n","tokens":197,"id":4778,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/005-cutting-release-branch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen cutting release branches, a set of rc artifacts are not automatically created. The user must disable `--diffcheck` to generate a set of rc artifacts, or manually publish the last dev artifacts to the rc feed, or update the rc tags in the case of single registry.\\nThis process might be made simpler for the end-user, so that intervention is not required when cutting a release branch.\\n","Decision":"We move away from recommending multiple feeds, rather to use a single feed originating from the trunk, and release definitions will utilize the LATEST_TAG or LATEST_GIT_TAG to do a release from the from the release branch. Users are expected to follow semantic versioning to prevent conflicts. In the case of parallel development, it is still possible to use semantic versioning in a single artifact repository.\\n","tokens":87,"id":4779,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/001-release.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nsfpowerscripts currently does not have a notion of 'release', which some CICD platforms like Azure pipelines support, allowing users to create release definitions, each with their own list of artifacts and task configurations. To achieve this across CICD platforms, the fetch, deploy and changelog generator commands all need to be combined to form a notion of release. Doing so will ensure that the notion of a release is available on all CICD platforms.\\n","Decision":"Chosen option: 1. `orchestrator:release` command\\nAn orchestrator command is more intuitive to use than a shell script, and it's independent from OS. Though this is bit inflexible and tied to the options being provided by the sfpowerscripts, it fastens adoption of the tooling. Users who have requirements that are not satisfied by the release commands can switch to a shell script and orchestrate it.\\n","tokens":99,"id":4780,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/001-sfpowerscripts-artifact-customsettings.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nsfpowerscripts artifacts is an unlocked package that keeps track of all the packages installed in the org. The current implementation utilizes a custom object to store the information. However, this has a downside, that when an org gets refreshed, the entire data gets wiped out and the traceability is lost.\\nTo solve this, these records should be stored in custom settings, which are preserved during refreshes. The cli should support utilising custom settings to store these records with a backward compatibility layer, so that none of the data gets lost while the migration is underway\\n","Decision":"This migration will be done in the following manner in the intermediate release\\n- CLI commands to check for the existence of sfpowerscripts_artifact_c object and any associate records on every run\\n- If associated records are found, proceed to migrate all the existing records into the newly created custom setting object\\n- Delete all the records in sfpowerscripts_artifact_c object.\\n- Utilize custom settings moving forward\\nAn upgrade to sfpowerscripts_package will be pushed to all the users who are utilizing the package and rest of them will be asked to build from source and deploy a package to the org\\nOn a subsequent release, the custom object will be deprecated and CLI commands will remove the associated check and migration code\\n","tokens":119,"id":4781,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/001-aliasified-data-sourcepackages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nsfpowerscripts support the concept of aliasisifed data and source packages, with the idea being a single package can have variants identified by aliases of an org and deployment will utilize the particular variant to be deployed to an org.  For eg: consider the below aliasified package\\n```\\nsrc-env-specicific-alias-post\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 dev\\n\u2502   \u2514\u2500\u2500 <contents>\\n\u251c\u2500\u2500 sit\\n\u2502   \u2514\u2500\u2500 <contents>\\n\u2514\u2500\u2500 st\\n\u2514\u2500\u2500 <contents>\\n```\\nsfpowerscripts matches the alias of the org ( depending on the alias that was set during the time of authentication), with a folder with the same name and deploy contents. As the number of orgs grow in the path to production, the content need to duplicate across each environment in the pathway or else sfpowerscripts will not allow the package to be deployed.  This is also concerning around deployment to scratch org, as this package has to be ignored.\\n","Decision":"To solve the above problem statement and issues, it is decided for sfpowerscripts to support a 'default' folder in aliasified packages.  The revised structure would be as follows\\n```\\nsrc-env-specific-alias-post\\n\u251c\u2500\u2500 README.md\\n\u2514\u2500\u2500 main\\n\u251c\u2500\u2500 default\\n|   \u2514\u2500\u2500 <contents>\\n\u251c\u2500\u2500 dev\\n\u2502   \u2514\u2500\u2500 <contents>\\n\u251c\u2500\u2500 sit\\n\u2502   \u2514\u2500\u2500 <contents>\\n\u2514\u2500\u2500 st\\n\u2514\u2500\u2500 <contents>\\n```\\nThe behaviour would be altered in the following manner\\n- sfpowerscripts will try to match the `alias` provided by the user by recursively searching across the project path.\\n- if the alias is matched, the contents inside the <alias> directory will be deployed\\n- if the alias is not been able to found, sfpowerscripts will fallback to the default folder.\\n- if the default folder is not found, throw an error saying default folder or alias is missing.\\nAlso to ensure care is being taken not to allow contents in default is being deployed to production, sfpowerscripts will validate the type of target org, whether its a sandbox or a prod environment. sfpowerscripts will fail to deploy aliasfied packages if it doesnt match the alias of the production environment.\\nThe above also considers backward compatibility for existing users.\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":218,"id":4782,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sfpowerscripts\/002-release-installing-packages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs part of a release, we need to install package dependencies. There are two different ways we could represent the package dependencies to be installed, each with their own merits.\\n","Decision":"Chosen option: 2. Duplicate dependencies in the release definition\\nAlthough it requires more input from the user, re-defining dependencies within the release definition is more reliable, as you have precise control over what dependencies are being installed. It also allows package dependencies to be tracked within the release definition file.\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":41,"id":4783,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"web-mapviewer\/2021_04_14_drawing_library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe application will support drawing, as its predecessor. The question is, can we achieve an approach that doesn't rely on (or isn't intertwined with) the mapping framework.\\nThe goal is also to pave the way to enable users to draw with some snapping help (on roads, or other geometries)\\n### Potential paths prospected\\n- No framework, implement a minimalist drawing library with only Javascript\\n- Use a library, such as PaperJS, to handle the drawing. Translate the output into geographic geometries.\\n- Use OpenLayers as a drawing tool (as in the viewer `mf-geoadmin3`)\\n","Decision":"After looking into all paths above, decision has been made to go with the OpenLayers approach. Here's reasons why :\\nPure Javascript approach could fit neatly in the current technology stack, but will require a lot of investment to achieve any kind of snapping\\nPaperJS has a nice toolbox for drawing, but will also require some work in order to support snapping. So the balance benefice (good drawing tools) vs. work needed isn't positive.\\nThis leaves us with the OpenLayers approach, that has already proven itself on the viewer `mf-geoadmin3`. We know that snapping is a possibility, as there's some snapping (only on the current drawing though).\\n","tokens":131,"id":4784,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"web-mapviewer\/2021_03_16_url_param_structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe mapviewer application is configured with several URL parameters. The current format for the layer configuration looks as follows (example for topic \"Snow\"):\\n```text\\n...\\nlayers=ch.swisstopo.pixelkarte-farbe-winter,ch.swisstopo.hangneigung-ueber_30,ch.swisstopo-karto.hangneigung,ch.bafu.wrz-jagdbanngebiete_select,ch.bafu.wrz-wildruhezonen_portal,ch.bazl.gebirgslandeplaetze,ch.swisstopo.schneeschuhwandern,ch.swisstopo-karto.schneeschuhrouten,ch.swisstopo-karto.skitouren,ch.swisstopo.skitourenkarte-50.metadata,ch.bav.haltestellen-oev&\\nlayers_opacity=0.85,0.5,0.2,0.6,0.6,1,0.7,0.8,0.8,1,1&\\nlayers_visibility=true,false,true,true,true,false,true,true,true,false,false\\n...\\n```\\nand including layers with different timestamps probably also\\n```text\\nlayers_timestamp=18641231,,,\\n```\\nThe current format has some limitations:\\n- confusing for configurations in case of multiple layers (order determines attribution)\\n- a lot of unnecessary characters used to represent default values\\n","Decision":"### Generic Pattern for Layer Configuration\\nThe new format looks as follows (in the generic form):\\n```text\\nlayers={layerID1},{visibility=t|f},{opacity=%f};{layerID2},{visibility=t|f},{opacity=%f}\\n```\\n### Examples\\n- one layer without changing defaults for opacity defined in catalog and for visibility in topics (e.g. opacity=0.8, visibility=true)\\n- `layers={layerID1}`\\n- two layers without changing defaults\\n- `layers={layerID1};{layerID2}`\\n- one layer with changed visibility\\n- `layers={layerID1},f`\\n- one layer with changed opacity and visibility\\n- `layers={layerID1},f,0.7`\\n- one layer with default visibility and changed opacity\\n- `layers={layerID1},,0.7`\\n- two layers with changed visibility and opacity\\n- `layers={layerID1},t,0.7;{layerID2},f,0.5`\\nIn case a wrong format is given, the errors are printed on the console.\\nThe above example boils down to the following (given that all opacity values are the defaults defined in the topic):\\n```text\\n...\\nlayers=ch.swisstopo.pixelkarte-farbe-winter;\\nch.swisstopo.hangneigung-ueber_30,f;\\nch.swisstopo-karto.hangneigung;\\nch.bafu.wrz-jagdbanngebiete_select;\\nch.bafu.wrz-wildruhezonen_portal;\\nch.bazl.gebirgslandeplaetze,f;\\nch.swisstopo.schneeschuhwandern;\\nch.swisstopo-karto.schneeschuhrouten;\\nch.swisstopo-karto.skitouren;\\nch.swisstopo.skitourenkarte-50.metadata,f;\\nch.bav.haltestellen-oev,f\\n...\\n```\\n### layerID\\nThe `layerID` can be one of the following\\n- an id from the catalog (e.g. `ch.swisstopo.schneeschuhwandern`)\\n- an id from the catalog, parametrized with e.g. time (and possibly other parameters in the future): e.g. `ch.swisstopo.zeitreihen@time=18641231@height=100m` (note: this allows for further parametrization in the future like e.g. height)\\nThe timestamp format must be ISO8601 compliant, i.e.\\n- `YYYY-MM-DD`\\n- `YYYY-MM-DDThh:mm`\\n- `YYYY-MM-DDThh:mm:ss`\\n- `YYYY-MM-DDThh:mm:ss.sss`\\n- `YYYY-MM-DDThh:mm:ss+hh:mm`\\n### External Layers\\nThe layer ID of the external Layers are in the following format (note that only one `|` is used and the WMS order is changed to have consistently `TYPE|URL|OTHER OPTIONS`)\\n- an external WMS: `WMS|GET_CAP_BASE_URL|LAYER_ID`\\n- The WMS version is taken from the Get Capabilities\\n- an external WMTS: `WMTS|GET_CAP_BASE_URL|LAYER_ID`\\n- The WMTS version is taken from the Get Capabilities\\n- an external KML: `KML|URL|TITLE`\\n- TITLE is optional and used as display in the active layers, if omitted then it will be displayed as `KML`\\n- a geoadmin KML: `KML|URL|TITLE`\\n- TITLE is set to `Drawing` upon drawing creation and in the current language at that time\\n- a geoadmin KML with adminId: `KML|URL|TITLE@adminId=ADMIN_ID`\\n- an external GPX: `GPX|GPX|TITLE`\\n- TITLE is optional and used as display in the active layers, if omitted then it will be displayed as `GPX`\\n- an external KMZ: `KMZ|KMZ|TITLE` (needs to pass by proxy to be unzipped)\\n- TITLE is optional and used as display in the active layers, if omitted then it will be displayed as `KMZ`\\n","tokens":301,"id":4786,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"data\/0001-data-lifecycle-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nContext here...\\n","Decision":"Decision here...\\n```\\nsd:               { \"guid\": \"ff58025f-6854-4195-9f75-3a3058dd8dcf\",\\n\"typeName\":\\n\"hive_storagedesc\"\\n}\\npartitionKeys:    null\\naliases:          null\\ncolumns:          [ { \"guid\": \"65e2204f-6a23-4130-934a-9679af6a211f\",\\n\"typeName\": \"hive_column\" },\\n{ \"guid\": \"d726de70-faca-46fb-9c99-cf04f6b579a6\",\\n\"typeName\": \"hive_column\" },\\n...\\n]\\n```\\n","tokens":7,"id":4787,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"data\/0002-thinking-in-rule-engine.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRule Engine is part of Data Process, Thinking in add mini version of Rule Engine.\\nA simple example can see in: https:\/\/github.com\/nikunjy\/rules\/blob\/master\/JsonQuery.g4\\nBusiness Rules Management System\uff1a[https:\/\/www.drools.org\/](https:\/\/www.drools.org\/)\\n```\\neq|==: equals to\\nne|!=: not equals to\\nlt|<: less than\\ngt|>: greater than\\nle|<=: less than equal to\\nge|>=: greater than equal to\\nco: contains\\nsw: starts with\\new: ends with\\nin: in a list\\npr: present\\nnot: not of a logical expression\\n```\\n","Decision":"Decision here...\\n","tokens":155,"id":4788,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teg\/adr003-a-new-rust-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTegh is moving to Rust and some problems have developed in the prototype that need to be addressed to fully embrace Rust:\\n1. It's becoming a mess of files.\\n2. Sled's unstable binary & not-widely supported file format does not lend itself to easy user debugging like sqlite.\\n3. Sled's pub\/sub model has led to complicated debugging and using it as the primary means of synchronizing data means that even things that shouldn't be persisted are being saved to disk just to get pub\/sub functionality.\\n4. Without Sled's pub\/sub data is not easily synchronized. Actors may replace this more robustly.\\nWe need to make some bigg-ish changes and that makes this an opportune moment to write up an updated architecture to organize this.\\n## Decision Drivers\\nInitial experiments in rewriting Tegh modules in rust have shown the language to generally reduce the amount of bugs created and their debugging difficulty. Bug hunts were sometimes quite difficult in Javascript due to it's lack of type safety and null.\\nBut let's be real, in the beginning I just wanted to try out Rust.\\n","Decision":"Initial experiments in rewriting Tegh modules in rust have shown the language to generally reduce the amount of bugs created and their debugging difficulty. Bug hunts were sometimes quite difficult in Javascript due to it's lack of type safety and null.\\nBut let's be real, in the beginning I just wanted to try out Rust.\\n### Machine\\n`packages\/machine\/`\\nFor `packages\/machine\/src\/[machine|config]`\\n- `mutations`\\n- `resolvers`\\n- `messages`\\n- `actors` - persistent state machines or ephermal context able to receive messages one at a time.\\n- `data` or `structs`(?) - SQL or config-backed non-actors with helper functions.\\n```mermaid\\nclassDiagram\\nclass Machine {\\n<<Ephemeral Data>>\\nID\\nSQLX db\\n+MachineConfig config\\n}\\nclass MachineConfig {\\n<<Config File Backed>>\\nID\\n}\\nMachine --> MachineConfig\\n```\\nCurrently machine information is spread across 3 unintegrated modules:\\n- machine_config\\n- ephemeral_machine_data\\n- machine\\n- receive_loop\\n- send_loop\\nThis is cumbursome so instead:\\n- `ephemeral_machine_data`, `receive_loop` and `machine` should be merged into an actor and not persisted.\\n- `send_loop` should be replaced by handlers called from GraphQL mutations with the following messages:\\n- stop_machine\\n- reset_machine\\n- pause_task\\n- delete_task_history\\n- send_task_to_machine\\n- `machine_config` should be included in tegh-machine package but config file updates managed in a seperate actor.\\n- tegh-marlin should replace it's duplicated config implementation with a dependency on the machine package.\\n### Core\\n`packages\/core`\\n`packages\/core\/src\/[backup|bin|invites|materials|users|video]`\\n- rename tegh-auth to `core`\\n- includes `users`, `invites`, and `video` moved up to the top level\\n### Print Queue\\n`packages\/print-queue`\\n`packages\/print-queue\/src\/[print-queue|print|task|package|part|macros]`\\n```mermaid\\nclassDiagram\\nclass PrintQueue {\\n<<SQL Data>>\\nID\\n}\\nclass Print {\\n<<SQL Data>>\\nID\\n}\\nclass Task {\\n<<SQL Data>>\\nID\\nmachine_id\\n+Print print\\n}\\nclass Package {\\n<<SQL Data>>\\nID\\n}\\nclass Part {\\n<<SQL Data>>\\nID\\n}\\nPrintQueue *-- Print\\nPrint *-- Task\\nPackage *-- Part\\nPart *-- Print\\n```\\n- print should be refactored into it's own file.\\n","tokens":236,"id":4789,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"teg\/adr001-configuration-api-for-heterogeneous-cnc-networks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nA hypothetical future maker space has many networked CNC devices; Mills, Lathes, 3D Printers, Laser Cutters, and so on. The hypothetical maker space uses various future versions of Teg with different plugins to control each of these machines. The maker space has an additional combinator Teg host that combines the other Teg instances and allows makers to securely control and configure all of the machines through a single WebRTC connection to a single GraphQL schema.\\nWhat would the configuration API of Teg look like to accomodate all the different machines\/teg version\/teg plugin variations in that hypothetical future makerspace with one GraphQL Schema accessed by users from a statically hosted printspool.io GUI?\\n## Decision Drivers\\n* Current `extendedConfig` GUI puts Plugin-specific settings into a JSON text field under 'Advanced'. The UX of which is that Plugin settings do not feel as tightly integrated as other settings. It is felt that plugin-specific settings should not feel like second class citizens and that having them feel that way limits their usefulness.\\n* Teg is being designed with intent to support combinators in a future version. Any current configuration architecture decision that does not take combinators into account today may cause breaking changes and cause unneccessary rewrites when combinators are introduced in the future.\\n","Decision":"* Current `extendedConfig` GUI puts Plugin-specific settings into a JSON text field under 'Advanced'. The UX of which is that Plugin settings do not feel as tightly integrated as other settings. It is felt that plugin-specific settings should not feel like second class citizens and that having them feel that way limits their usefulness.\\n* Teg is being designed with intent to support combinators in a future version. Any current configuration architecture decision that does not take combinators into account today may cause breaking changes and cause unneccessary rewrites when combinators are introduced in the future.\\nChosen options: [Queries: JSON Flat Objects] and [Mutations: JSON Flat Objects] because they are the only Query and Mutation option that meets the criteria of maintaining security via internal type safety, preventing schema collisions in a combinator and presents both plugin-specific and core settings as first class citizens.\\nChosen GUI option: [GUI: Dynamic Forms] because it is the only GUI option that allows plugin-specific settings to be placed in the ideal arbitrary position within the GUI for an optimal UX.\\n","tokens":273,"id":4790,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mnt-teet\/ADR-4-AWS-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTEET project originates on a similar Finnish project named Velho. The solution designed is based on and utilizes the Amazon Cloud Environment (AWS) turnkey solutions and components. AWS (Amazon Web Services) is a leading cloud service provider, who has excellent experience in service lifecycle and support systems. Cloud services make the solution flexible, scalable, and cost-effective for the environment.\\n","Decision":"Both Frontend and Backend is implemented using the Datomic Cloud in conjunction with AWS services.\\nDatomic's data model - based on immutable facts stored over time - enables a physical design that is fundamentally different from traditional RDBMSs. Instead of processing all requests in a single server component, Datomic distributes transactions, queries, indexing, and caching to provide high availability, horizontal scaling, and elasticity. Datomic also allows for dynamic assignment of compute resources to tasks without any kind of pre-assignment or sharding.\\nThe durable elements managed by Datomic are called Storage Resources, including:\\n- the DynamoDB Transaction Log\\n- S3 storage of Indexes\\n- an EFS cache layer\\n- operational logs\\n- a VPC and subnets in which computational resources will run\\nThese resources are retained even when no computational resources are active, so you can shut down all the active elements of Datomic while maintaining your data.\\nEvery running system has a single primary compute stack which provides computational resources and a means to access those resources. A Primary Compute Stack consists of:\\n- a primary compute group dedicated to transactions, indexing, and caching.\\n- Route53 and\/or Network Load Balancer (NLB) endpoints\\n- a Bastion Server\\nRequests are forwarded to Datomic using the AWS API Gateway solution.\\nDatabase solution is based on AWS Aurora which is a cluster of PostgreSQL instances which consists of:\\n- a single primary DB instance - supports read and write operations, and performs all of the data modifications to the cluster volume.\\n- 2 read replicas - connects to the same storage volume as the primary DB instance and supports only read operations.\\nRequests to the Database are sent using PostgrestAPI solution which ran in dockerized confinement.\\nAWS ECS with Fargate task is used to deliver the PostgrestAPI solution.\\nApplication and services CI\/CD will be fully automatized using AWS Codebuild.\\nCloudFormation templates will support TEET environment's deployment to new AWS accounts.\\n","tokens":78,"id":4791,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mnt-teet\/ADR-3-database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are various requirements for the database containing master data\\nthat TEET is authoritative for.  These include ACID properties\\n(Atomicity, Consistency, isolation, Durability), preservation of\\nhistory, extensibility, GIS data support, file attachments.\\n","Decision":"The master database containing business data, workflows, etc is\\nDatomic, as the data model providing history preservation and data\\nmodel extensibility is a good fit for TEET business requirements\\nidentified so far. In addition the AWS-native deployment and operating\\nmodel and and excellent fit with Clojure data structure paradigms are\\nsignificant pluses.\\nPostgreSQL + PostGIS and AWS S3 are used as auxiliary databases for\\nimported GIS data (that we are not the master of) and file\\nattachments.  PostgreSQL (as provided by AWS RDS) is used via\\nPostgREST to provide an API for map layer and geometry data.\\nPostgreSQL will also be used as our full text search engine by caching\\nfull-text data set in it.\\n","tokens":57,"id":4792,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mnt-teet\/ADR-1-Code-and-documentation-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTEET is a multi-site project that will have development work done both in Oulu and Tallinn\\noffices. The developers and customer do not have the same native language.\\n","Decision":"Code, databases, API definitions and documentation will be in English.\\nExisting code that is reused from other projects that have different source\\nlanguage may be used as is if necessary.\\nAll domain concepts must be well defined and updated to a glossary of terms.\\n","tokens":40,"id":4793,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mnt-teet\/ADR-2-Frontend-app-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTEET is a large web application and will contain many namespaces and different types of namespaces:\\nviews, controllers, common UI components, etc.\\nThe way to structure namespaces affects how easy it is to find and refer to a given piece of code.\\nCommon structure makes it more predictable where developers can expect to find things.\\n","Decision":"We use feature based grouping with layer suffix.\\n`teet.<feature>.<feature>-<layer>`\\nExample: `teet.search.search-view` and `teet.search.search-controller`\\nWhen referring to namespaces, use the last part of the name fully, e.g. `search-view`\\n```clojure\\n(ns teet.search.search-view\\n(:require [teet.search.search-controller :as search-controller]))\\n```\\nFeatures may use implementation specific sub-namespaces as seen fit.\\nNon-feature functionality, like common UI utilities, are placed under the layer, e.g. `teet.ui.panels`.\\n","tokens":69,"id":4794,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"entusiasme-kotlin\/000001_kotlin_language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### **Discussion\/Context**\\nIn all discussions of technical development communities groups appears the difficult for women in all professional level a place to improvement their knowledge.\\n___________\\n### **Decision**\\nWe create a real software development project to learn about:\\n* Agile project practices\\n* Project structuring\\n* Architecture concerns and make decisions\\n* Backend development environment\\n* Local development environment setup\\n* Object-Oriented Programming Paradigm\\n* Clean code\\n* TDD (Test-Driven Development)\\n* DDD (Domain-Driven Development)\\n* Design Pattern\\n* SOLID\\n* Testing\\n* Automated testing\\n* Continuous Integration\\n* Continuous Delivery\\n* Continuous Deployment\\n* Share knowledge learned\\n* Coaching\\n___________\\n### **Status**\\nAccepted\\n___________\\n### **Way\/State\/Version\/Model**\\nversion 1.32.0 with Java 11\\n___________\\n### **Consequences**\\n* Dependency with JVM (Java)\\n*\\n___________\\n### **Updates**\\n| Information | From | To | Date |\\n|---|---|---|---|\\n| | | | |\\n","Decision":"We create a real software development project to learn about:\\n* Agile project practices\\n* Project structuring\\n* Architecture concerns and make decisions\\n* Backend development environment\\n* Local development environment setup\\n* Object-Oriented Programming Paradigm\\n* Clean code\\n* TDD (Test-Driven Development)\\n* DDD (Domain-Driven Development)\\n* Design Pattern\\n* SOLID\\n* Testing\\n* Automated testing\\n* Continuous Integration\\n* Continuous Delivery\\n* Continuous Deployment\\n* Share knowledge learned\\n* Coaching\\n___________\\n### **Status**\\nAccepted\\n___________\\n### **Way\/State\/Version\/Model**\\nversion 1.32.0 with Java 11\\n___________\\n### **Consequences**\\n* Dependency with JVM (Java)\\n*\\n___________\\n### **Updates**\\n| Information | From | To | Date |\\n|---|---|---|---|\\n| | | | |\\n","tokens":248,"id":4795,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"entusiasme-kotlin\/000000_entusiasme_application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### **Discussion\/Context**\\nIn all discussions of technical development communities groups appears the difficult for women to choose a technology to start learn software development.\\n___________\\n### **Decision**\\n* Google's tecnology\\n* Great company supporting and improving\\n* Good documentation\\n* Small learning curve for Java developer\\n* Easy switch to other OOP language\\n* SO Interoperability\\n* Easy to learn\\n* Sugar Syntax\\n___________\\n### **Status**\\nAccepted\\n___________\\n### **Way\/State\/Version\/Model**\\nN\/A\\n___________\\n### **Consequences**\\nN\/A\\n___________\\n### **Updates**\\n| Information | From | To | Date |\\n|---|---|---|---|\\n| | | | |\\n","Decision":"* Google's tecnology\\n* Great company supporting and improving\\n* Good documentation\\n* Small learning curve for Java developer\\n* Easy switch to other OOP language\\n* SO Interoperability\\n* Easy to learn\\n* Sugar Syntax\\n___________\\n### **Status**\\nAccepted\\n___________\\n### **Way\/State\/Version\/Model**\\nN\/A\\n___________\\n### **Consequences**\\nN\/A\\n___________\\n### **Updates**\\n| Information | From | To | Date |\\n|---|---|---|---|\\n| | | | |\\n","tokens":172,"id":4796,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"coloseo\/0003-use-mern-stack.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n","Decision":"The project is build with the MERN Stack.\\n","tokens":3,"id":4797,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"coloseo\/0002-use-docker-compose.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe solution has to be portable and lightweight and work without special infrastructure.\\n","Decision":"Docker-compose is used as multi-container solution.\\n","tokens":18,"id":4798,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"coloseo\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4799,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nr-arch\/2020-03-14-ARCH-record-architecture-decisions-detailed-template.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and\/or doing?\\nThe change that we're proposing or have agreed to implement.\\n","tokens":218,"id":4800,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nr-arch\/2020-04-16-ARCH-liferay-community-edition.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and\/or doing?\\nThe change that we're proposing or have agreed to implement.\\n","tokens":218,"id":4801,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"java-template\/0002-hexagonal-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n- How can we maintain a clean design?\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\n## Decision Drivers\\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","Decision":"* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","tokens":63,"id":4805,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"definitions\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n#### Original year range behavior\\nStarting in [v3.2.0](https:\/\/github.com\/holidays\/holidays\/releases\/tag\/v3.2.0) of the ruby gem [\\[1\\]](#footnote-1) we have had the ability to specify year ranges for specific holidays. This allows for a holiday\\nto only be considered 'valid' or 'active' based on specific criteria surrounding years. The available criteria were:\\n* `before` - holiday only valid if it takes place on the target year or before\\n* `after` - holiday only valid if it takes place on the target year or after\\n* `limited` - holiday only valid if it takes place on at least one of an array of target years (e.g. [2002, 2004])\\n* `between` - holiday only valid if it takes place between an inclusive year range (e.g. 2002..2004)\\nThis change added useful functionality and its use has since spread to multiple regions.\\n#### Confusion about criteria behavior\\nOn January 24th, 2019 [an issue was opened](https:\/\/github.com\/holidays\/definitions\/issues\/117) expressing that the `before` and `after` criteria were named in a confusing manner since it was not clear whether they operated inclusively or exclusively on the target year.\\nAs an example, the value `after: 2018` could be construed by some to mean the holiday is valid starting in 2019 and onward. In reality the current implementation is that the holiday is valid starting in 2018 itself and onward.\\nWhile this is ultimately up to individual perception it is true that the current names do not provide strong guidance on how the definition will behave.\\n","Decision":"The [above issue](https:\/\/github.com\/holidays\/definitions\/issues\/117) also contained a proposal to make the following changes:\\n* Rename `before` to `until`\\n* Rename `after` to `from`\\nThese names give a clearer understanding of the desired behavior as `until` and `from` are more generally understood to indicate inclusivity rather than exclusivity.\\nIf we take the example from above and make the change then the value `from: 2018` would intend for the holiday to be valid\/active starting in 2018 and onward.\\n#### Additional changes\\nWhile looking into the above issue I noticed two important things that will also be addressed alongside the above changes:\\n* The definition validator does not currently prevent users from specifying multiple selectors at a time for a `year_ranges` entry and we perform no validation that these selectors do not conflict with one another.\\n* The `between` key currently accepts a string representation of a Ruby range (e.g. '2008..2012'). While this was not causing any issues today we would like to remove all Ruby-specific values from our definitions so that other languages could more easily parse them.\\nTo that end the following changes will also be made:\\n* Update the definition validation to only allow a single selector per `year_ranges` entry and update all definitions to match. This will result no behavior changes but will make clear the expected behavior.\\n* `between` will no longer accept a ruby-like range string but instead require explicit `start` and `end` keys with integer values representing a year.\\n","tokens":364,"id":4807,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"project-blueprint\/0002-use-google-closure-compiler.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":4808,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"project-blueprint\/0003-test-distributed-files-only.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":4809,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"project-blueprint\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4810,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0007-representation-of-semantic-metadata-in-markdown-documents.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSemantic information has to be inserted into the markdown document. How is this represented?\\n","Decision":"The chosen option is **2**, as this\\n* preserves the markdown for future updates of the document\\n* is typically not visible to readers of the markdown.\\n","tokens":23,"id":4811,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0004-supports-a-limited-number-of-java-data-types.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs the library will a) generate markdown text from Java objects and b) read in the markdown and generate Java objects, the data types need to be  converted to text and back again.\\nAs not all data types can be implemented, what data types should be supported by the library?\\n","Decision":"Data types that a native to Java are supported, i.e.:\\n* Primitive Java data types\\n* Java wrappers for the primitive data types\\n* Java Strings\\n* Java classes representing dates.\\n* Java classes representing URLs (for use in document links).\\nUsing a language agnostic set of data type would provide extra complexity in translating from the Java data types and was rejected\\n","tokens":65,"id":4812,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0009-fluid-interface.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nHow does a client use the interface?\\n","Decision":"Chosen option is to implement as a **fluent API**, as early use of the library showed that this was easier to understand and provided a means\\nto expand the evolve the functionality.\\n","tokens":15,"id":4813,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0005-type-representation-in-markdown-documents.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe values of fields need to be represented in the markdown documents. How are different types represented?\\n","Decision":"Default Java representations are used.\\nRepresentations chosen by the user in the template have not been rejected, but are maybe a feature for a future release.\\n","tokens":26,"id":4814,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0006-agnostic-as-to-markdown-type.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nMore then one type of markdown exists. Which one should be supported?\\n","Decision":"The library can handle all forms of markdown.\\n### Positive Consequences <!-- optional -->\\n* The form of markdown used is determined by the user supplied template.\\n### Negative Consequences <!-- optional -->\\n* The library can only use a subset of the features that are common to all markdown languages (e.g. single new lines are ignored).\\n","tokens":21,"id":4815,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0003-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhat are the main components of the library?\\n","Decision":"Chosen option is to implement as a **single class**, as the foreseen complexity of the library does not justify the extra complexity of developing a facade pattern implementation.\\n","tokens":16,"id":4816,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0002-easily-integrated-with-java-projects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n`semplate` is intended to provide functionality to java developers so that they can build tools that create, use and\/or manage documents that are written in markdown. As such, it should be easily to integrate it with programs written in Java.\\n","Decision":"Chosen option is to create as a standard Java JAR library\\nThe option to create a microservice was rejected as these costs of hosting it could not be met. However, this option has not been excluded as additional to a standard Java JAR.\\n","tokens":55,"id":4817,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"semplate\/0008-not-to-use-semantic-triples.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nSemantic information can be represented using sematic triples  as defined in the [W3C Resource Description Framework - RDF](https:\/\/www.w3.org\/RDF\/).\\nIs this an option for `semplate`?\\n","Decision":"As the library is intended to be directly call from Java programs, the **notation is based on the fieldname of Java object being processed**.\\n","tokens":52,"id":4818,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"protraffic\/itd-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nProcessing center should be hosted. Where to host it?\\n","Decision":"Even though using an on-premise data center might result in cost savings, a data center is not a product differentiator. Meanwhile, it requires additional operational and implementation efforts. It\u2019s easier to focus on product value and rely on a cloud service provider.\\n","tokens":18,"id":4819,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0003-preservation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have agreed that we will preserve digital objects by saving resources in\\nGoogle Cloud Storage in a directory structure which preserves both the binaries\\nthe resource is made up of as well as the JSON serialization of the resource\\nitself.\\n","Decision":"1. Preserving\\n1. We will preserve materials in Google Cloud Coldline Storage with\\n`versioning` enabled. Versions will be kept indefinitely and without\\nlimit. All files will go in a single bucket.\\n- Staging bucket is configured with the following command:\\n```\\ngsutil mb -c regional -l us-west1 -p pulibrary-figgy-storage-1 gs:\/\/figgy-staging-preservation\\necho '{\"rule\": [{\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 2}}]}' > lifecycle.json\\ngsutil lifecycle set lifecycle.json gs:\/\/figgy-staging-preservation\\nrm lifecycle.json\\ngsutil bucketpolicyonly set on gs:\/\/figgy-staging-preservation\\ngsutil iam ch serviceAccount:figgy-staging@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs:\/\/figgy-staging-preservation\\n```\\n- Production bucket is configured with the following command:\\n```\\ngsutil mb -c coldline -l us-west1 -p pulibrary-figgy-storage-1 gs:\/\/figgy-preservation\\ngsutil bucketpolicyonly set on gs:\/\/figgy-preservation\\ngsutil iam ch serviceAccount:figgy-preservation-production@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs:\/\/figgy-preservation\\ngsutil versioning set on gs:\/\/figgy-preservation\\n```\\n1. When a resource is `complete` and marked with the `cloud` preservation\\npolicy it will save itself and all resources contained in `member_ids` in\\na directory structure in Google Cloud Storage that looks like the\\nfollowing:\\n```\\n- <resource-id>\\n- data\\n- <child-id>\\n- <child-id>.json\\n- <binary.tif>\\n- <resource-id>.json\\n```\\n1. Children are preserved on save if their parents are preserved.\\n1. Related objects such as collections, Ephemera Terms, etc. will not be\\npackaged inside the preserved object. If it's important they be preserved,\\nthose objects should be marked with the `cloud` preservation policy.\\n1. When a FileSet is added to a resource which is already complete and marked\\nwith the `cloud` preservation policy, it will upload the new binary\\ncontent to both the repository and to Google Cloud Storage.\\n1. If a child is marked to be preserved, but its parent is not, it will still\\nsave in a nested directory structure, but will not automatically create\\nbackups of its parents.\\n1. This behavior will be attached to the ChangeSetPersister.\\n1. Packaging Details\\n1. When preserved a `PreservationObject` will be created in Figgy with a\\n`preserved_object_id` property which points to the object it's preserving.\\n1. Each `PreservationObject` will contain `FileMetadata` for the binary\\nobject as well as a serialized JSON file of the resource it's preserving.\\nOn upload to preservation, those items' checksums will be calculated and\\nstored on the `PreservationObject`.\\n1. JSON metadata will have the use `pcdm:PreservedMetadata` and binary\\ncontent will have the use `pcdm:PreservationCopy`\\n1. We will only keep the most recent version of any file, overwriting any\\nfiles which match the same file name, but relying on versioning to go back\\nif necessary.\\n1. When a preserved resource is deleted, we will delete its directory from\\npreservation storage. If we need to get it again, we will look at Google\\nCloud Storage's stored versions.\\n1. If a child's hierarchy changes (it moves parents), we will move the\\ncontent in the preservation storage to match.\\n1. When a file's binary content is replaced on disk, we will upload a new\\ncopy of the file to preservation and calculate a new checksum.\\n2. Fixity Checks\\n1. Technical details of fixity checking will occur in a later ADR.\\n1. A random subset of the preserved copies will have their files pulled down\\nfrom preservation storage, their checksums calculated as they're streamed,\\nand then compared to the checksum of the object stored locally.\\n1. In the case of a failure it will be reported to Figgy and displayed in a\\ndashboard for further follow-up and repair.\\n","tokens":50,"id":4820,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0002-queues.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0005-data-migrations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are several ways of migrating data, including:\\n* Using an ActiveRecord Migration\\n* Writing a Rake task and running it after deploying a code update\\n","Decision":"1. When migrations do not change the database structure or otherwise break the application, we should\\nwrite a Rake task to migrate data.\\n","tokens":35,"id":4822,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0001-document-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4823,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0009-unlinked-files.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen an ingest fails in the middle of a transaction which is adding files, the\\nFileSets will not get persisted. However, the files will have already been\\ncopied to the repository via `FileAppender`. This results in files in the\\nrepository which have no corresponding database record.\\nFixing this will require development of a transactional disk StorageAdapter\\nwhich moves files at the end of a metadata transaction.\\n","Decision":"1. We don't have time to implement a transactional disk StorageAdapter at this\\ntime.\\n2. Accept this situation, document it here, and know we can free up space in the\\nfuture by looking for unlinked files and deleting them.\\n","tokens":90,"id":4824,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0004-preservation-fixity.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have agreed to check the file integrity of our preserved objects in Google\\nCloud Storage by occasionally downloading and comparing the MD5 checksum of\\nthose objects to our stored checksums. This process will happen in a Google\\nCloud Function.\\n","Decision":"1. Fixity Checking Timeline\\n1. Every day a random 0.027% of the repository's preserved materials\\nwill be queued up to have their integrity checked. This process can be executed via\\n```\\nRAILS_ENV=production bundle exec rake fixity:request_daily_cloud_fixity\\n```\\nIt will be configured as a cron job on a single worker machine.\\n2. Process\\n1. To verify a PreservationObject, each attached FileMetadata will generate a\\nmessage to a Google Cloud Pub\/Sub Request Queue in the following format:\\n```json\\n{\\n\"md5\": \"[md5_of_file]\",\\n\"cloudPath\": \"[path_in_bucket_converted_from_file_identifier]\",\\n\"preservation_object_id\": \"[preservation_object_id]\",\\n\"file_metadata_node_id\": \"[file_metadata_id]\",\\n\"child_property\": \"[property_file_metadata_node_is_stored_in]\"\\n}\\n```\\n1. A Google Cloud Function will listen for events on the request pub\/sub\\ntopic, download the given cloudPath, and verify the MD5. The cloud\\nfunction will be deployed via developers using the\\n`cap [staging\/production] deploy:google_cloud_function` command in Figgy.\\n- An increased quota (10x default) for inbound traffic must be requested\\nfrom Google to handle the download, and the function restricted to 100 concurrent\\nworkers. If we need more concurrency later we can request a higher\\ninbound traffic quota.\\n1. The Google Cloud Function will send a message to a Status Topic, which\\nwill contain a message giving the status of the operation. It looks like\\nthe following:\\n```json\\n{\\n\"status\": \"[SUCCESS\/FAILURE]\",\\n\"resource_id\": \"[preservation_object_id]\",\\n\"child_id\": \"[file_metadata_id]\",\\n\"child_property\": \"[property_file_metadata_node_is_stored_in]\"\\n}\\n```\\n1. A daemon will run on each Figgy worker machine which pulls these events\\nand sends them to Sidekiq. This worker can be executed via\\n`RAILS_ENV=production bundle exec rake fixity:run_worker`, but does not\\nmanually need to be started - it is handled by a systemd service deployed\\nvia Ansible.\\n1. Sidekiq will create `Event` objects for each status sent, which will be\\ndisplayed in the Fixity Dashboard.\\n","tokens":51,"id":4825,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0007-pyramidal-tiffs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe have several TB of images to deliver through a IIIF service, more than any of\\nour physical boxes can currently hold all in one place. We have to pay\\nKakadu a licensing fee in order to get acceptable tile delivery, and the\\nCantaloupe server we use has been proven to be easily Denial-of-Service'd to the\\npoint of freezing and no images being able to be served.\\nWe will remedy this problem of scale by storing Pyramidal Tiffs in the cloud\\nwhich an AWS Lambda service (scale-able to at least a thousand simultaneous\\nrequest) will use to respond to IIIF image server requests, using Northwestern's\\n[serverless-iiif](https:\/\/github.com\/nulib\/serverless-iiif) project.\\n","Decision":"1. Generate Pyramidal tiffs using VIPS and upload them to an AWS bucket using\\n`valkyrie-shrine`.\\n* Any image greater than 15k pixels on the long-side will be downsized by\\nhalf to reduce the derivative size and allow the image server to respond\\nin a reasonable amount of time.\\n2. Configure and deploy\\n[serverless-iiif](https:\/\/github.com\/nulib\/serverless-iiif) to serve IIIF\\nImage API requests using those pyramidal tiffs.\\n3. Configure an Amazon CloudFront cache in front of the lambda to automatically\\ncache tiles and info.jsons for one year.\\n","tokens":165,"id":4826,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0006-preservation-tombstones.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAfter an item is deleted from the database all traces of it are gone, but if\\nit's been preserved then it's possible to restore it. However, finding the\\nparticular ID of the item to restore is difficult if all that's known are pieces\\nof its metadata (title, source metadata ID, etc.)\\nWe will remedy this by storing a \"tombstone\" of deleted items as a small record\\nof what was deleted.\\n","Decision":"1. When a preserved item is deleted it will create a \"tombstone\" containing the following\\nmetadata:\\n1. ID of deleted resource\\n1. Title of deleted resource\\n1. Original Filename of deleted resource (if a FileSet)\\n1. Embedded PreservationObject that existed at time of deletion\\n1. ID of Parent at time of deletion.\\n2. These tombstones will be displayed in the Figgy UI and used as a way to\\ndiscover material that can be recovered.\\n","tokens":96,"id":4827,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"figgy\/0008-jpeg2000-ingest.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSupport for ingestion of JPEG2000 images was added to accommodate\\nscanned map records for which we cannot find original TIFFs. The VIPS library,\\nused to generate pyramidal tiffs and thumbnails, can't read JP2s\\ndirectly, so a temporary intermediate TIFF was generated using the OpenJPEG `opj_decompress`\\ncommand. This command uses a large amount of memory and will eventually\\ncrash the host server if several JP2s are decompressed simultaneously.\\n","Decision":"1. Derivative generation functionality for JPEG2000 images was removed.\\n","tokens":106,"id":4828,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"paths\/0002-use-a-single-machine.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMultiple machines add a bunch of complexity - we don't want to deal with that while we're experimenting with how this\\nactually works. The advantages of microservices are pretty minimal for a 1 person team.\\n","Decision":"1. Put server logic on 1 machine\\n","tokens":46,"id":4829,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"paths\/0003-separate-reads-and-writes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nModelling reads and writes with the same model does not match reality - AddResult is very different from getResults. We should make sure to keep those models seperate.\\n","Decision":"Use seperate models for reads and writes (or actions).\\n","tokens":37,"id":4830,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"micro\/0002-have-a-single-repository-for-all-microservices.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nA repository for each microservice or only one for all of them?\\n## Decision Drivers\\n* Development agility\\n* Microservices decoupling\\n* Big-picture scenario visibility\\n","Decision":"* Development agility\\n* Microservices decoupling\\n* Big-picture scenario visibility\\nChosen option: A single repository for all microservices and service definitions this time, because this way its easy to get a big picture of the approach, for the purposes of this example.\\nIn production, I'll consider separating microservices and service definitions on repos, in order to have separate triggers on CI for them.\\nI think [this approach](https:\/\/medium.com\/namely-labs\/how-we-build-grpc-services-at-namely-52a3ae9e7c35) will be a good fit for address this concern in production.\\n","tokens":42,"id":4831,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ansible-docker\/choose_database.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ansible-docker\/choose_webapp.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context: A web applicaiton is necessary to fufill the requirements for this assigment\\n* Having used PHP in previous courses and personal projects, I found it to be a good choice to pair with mySQL\\n* There is alot of documentation and examples online demonstrating how to interact with mySQL using PHP.\\n### Decision: The change proposed to the current implementation is to add the functionality of a PHP web application to display SQL data in a broswer with the help of a webserver (see choose_webserver.md for information concerning the server)\\n### Consequences: PHP is not the most sophisticated of web applications, it is possible I could be limited by CSS and HTML styling effects.\\n","Decision":"### Consequences: PHP is not the most sophisticated of web applications, it is possible I could be limited by CSS and HTML styling effects.\\n* Example:\\n> say that our PHP web application reflects a libarary mangement system, the librarian must be able to add books to a library members checkout list, delete books from library members checkout list once returned, update a library members contact information, or view books that are checked out or member contact information.\\n### Consequences: extending the web application to support CRUD operations increases the complexity of the application and requires more time and effort to manage and maintain the system.\\n### Consequences: employing the MVC can add another layer of abstraction to the application.\\n","tokens":144,"id":4834,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0003-session-storage-using-jwt.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nVery small amount of data stored in session\\n","Decision":"JSON Web Tokens (JWT) for session storage (rather than DynamoDB or ElastiCache)\\n","tokens":12,"id":4835,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0013-application-layer-naming.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need clearing naming for each layer of the Use an LPA service.\\nThere are 3 layers:\\n- A front service layer through which Actors will access the service.\\n- A front service layer through which Viewers will access the service.\\n- A backend service shared by the two front services which will provide data access and some domain logic.\\n","Decision":"The 3 services will be called:\\n- Actor Front\\n- Viewer Front\\n- Api\\n","tokens":76,"id":4836,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0018-session-lifetime-and-session-based-cookies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to supply the user with prompts that they have been logged out due to inactivity. This is not currently possible\\ndue to our session lifetime being dictated by the expiry of our session cookie (currently 20 minutes). Once a cookie has\\nexpired we do not recognise a user in order to tell them that they have been logged out.\\n","Decision":"* We will lengthen the cookie expiry to be as long as the business requires to display a _\"logged out due to\\ninactivity\"_ message. Somewhere in the region of 1 day - 1 week will likely be acceptable.\\n* We will store the last access time of the user within the cookie and compare that upon new requests with our\\ndesired session time (20 minutes). This middleware could then force a user to re-login with an appropriate message if\\nthe time has expired.\\n","tokens":74,"id":4837,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0014-library-for-application-views.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to use a common library for managing HTML views within the service.\\nTwo libraries were investigated; Plates and Twig.\\n","Decision":"To use Twig.\\n","tokens":29,"id":4838,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0012-zend-dependency-injection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nZend Expressive allows us to pick from a number of Dependency Injection libraries.\\nThe developers would specifically like to use a library that supports autowiring.\\n","Decision":"To use [PHP-DI](http:\/\/php-di.org\/). This was the only library that supported autowiring with no additional setup.\\n","tokens":34,"id":4839,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0005-automated-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMoJ guidance is that tests should be automated where possible\\n","Decision":"Include and implement automated code coverage tests, accessibility tests, and security and penetration testing\\n","tokens":15,"id":4840,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0004-no-cdn-required.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* No expected high demand\\n* There will be a natural ramp up in usage, as the service is only available to applications registered after a certain date\\n* Not complex to add a CDN after\\n","Decision":"No Content Delivery Network (CDN) for delivery of static assets\\n","tokens":43,"id":4842,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0010-to-host-the-service-in-eu-west-1.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe've received the following guidance from the MOJ:\\n```\\nThe MOJ does not by default or routine require \u2018UK only hosting\u2019 or \u2018UK only services\u2019 for data privacy,\\ndata protection or information security reasons.\\n```\\nand\\n```\\nThe MOJ has no plans to inshore data (i.e. limiting and \/ or returning data to the UK) for privacy or security\\nreasons, nor is the MOJ asking its partners (for example, commercial suppliers) to do so.\\n```\\nFull details here: [MOJ data sovereignty questions](https:\/\/ministryofjustice.github.io\/security-guidance\/mythbusting\/data-sovereignty\/#data-sovereignty-questions)\\n","Decision":"Use an LPA's infrastructure will be based in eu-west-1; thus not in the alternative, eu-west-2.\\n","tokens":151,"id":4844,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0019-will-use-symfony-translation-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to provide translation capabilities in a way that slots into our twig based rendering pipeline\\n","Decision":"We're going to use symfony\/translation to provide the translation capabilities\\n","tokens":21,"id":4845,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0017-brute-force-protection-of-application-endpoints.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n> This ADR details the mitigation of brute force attacks on certain endpoints - other types of protection are out of\\n> scope.\\nWe have a need to mitigate brute force attacks on certain endpoints in the application. Currently these are:\\n* Actor Login\\n* One Time Passcode (adding an LPA)\\n* Viewer Code (viewing an LPA)\\nBrute force can be defined as multiple failed attempts to carry out these actions.\\n","Decision":"Protecting against these kinds of attacks is a problem in two parts; reliable identification of users and the recording\\nof attempts by that user against protected endpoints.\\n#### Identification\\nThe bare minimum of information that we can use to identify a user is their originating IP address. This is not without\\nissue as it can be spoofed, or more likely the user is behind a NAT or proxy layer (as will likely be the case with our\\ncorporate users). We can couple the browser sent headers that offer extra user identifying information (such as 'Accept'\\n'DNT', 'User-Agent' etc) in a hashing function to generate an ID that should more uniquely identify a user.\\nWe will identify users using a hash calculated using the incoming IP and associated headers.\\nWe will track this identity in the session to guard against changing headers within a session.\\n#### Attempt Tracking\\nWe will use a cache service (AWS Elasticache) to track failure attempts at each of the three endpoints defined above.\\nWe will use a per-endpoint moving window rate-limit to reduce brute force impact - these will be individually\\nconfigurable in terms of window size and request limit.\\n","tokens":95,"id":4846,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0006-continuous-delivery.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFrequent small releases are preferred strategy to prevent accumulating risk and deliver benefits to users more quickly\\n","Decision":"Aim for continuous integration and continuous delivery. [Proposed Flow](..\/diagrams\/CI%20CD%20Pipelines.png)\\n","tokens":22,"id":4849,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0011-the-same-zend-application-will-be-used-for-both-viewer-and-actor-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUse an LPA will be made up of two components - those for use by LPA _actors_, and those used by third\\nparty groups who are the _viewers_ of the LPA.\\nAt present it is expected that these two components will be hosted on two different domains.\\n","Decision":"That both `Viewer` and `Actor` will both be separate modules of the same Zend application.\\nNote: it is still expected that they will be deployed separately into two containers.\\n","tokens":63,"id":4850,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0015-library-for-application-forms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to use a common library for managing HTML forms used within the service.\\nTwo libraries were investigated; Zend Form and Symfony Forms.\\n","Decision":"We will use Zend Form.\\n","tokens":32,"id":4851,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"opg-use-an-lpa\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4853,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bfi-discovery\/0001-choice-of-rdbms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nBFI's IIIF Univeral Viewer auditing platform requires that each event\\nwhich is subject to long term persistence is captured in a manner that\\nis secure, robust and performant.\\n## Decision Drivers\\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support \/ maintenance.\\n","Decision":"* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support \/ maintenance.\\nMySQL is selected as the solution as it aligns provides BFI with an RDMS\\nwith which they already have experience and are able to support and\\nmaintaining in the long term.\\n### Positive Consequences\\n* No initial or ongoing licensing costs associated with the use of\\nMySQL.\\n* Ease of ongoing maintenance for BFI owning to their existing\\nfamiliarity with MySQL.\\n* Availability of multiple storage engines (InnoDB, MyISAM, etc) and\\ndistributions (MySQL Community Edition, MariaDB, PerconaDB, etc).\\n* Simple deployment and configuration (e.g., MySQL publish official\\nDocker images).\\n### Negative Consequences\\n* Certain SQL features are not available (although it is not expected\\nthat these will be required).\\n","tokens":104,"id":4854,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bfi-discovery\/0004-universal-viewer-customisations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe Universal Viewer currently has a model for customisation using\\nforked repositories. We need to use BFI's own fork that exists on GitHub\\nand modify it to track events for this project. We also need a way to\\ndeploy the various built assets for the viewer.\\n## Decision Drivers\\n* Ease of future customisation for BFI\\n* Ease of integration for Digirati\\n","Decision":"* Ease of future customisation for BFI\\n* Ease of integration for Digirati\\nChosen option: \"Submodule the example repository\", because it will keep\\nthe application largely agnostic of the viewers that are integrating\\nwith it and will reduce build complexity.\\n### Positive Consequences\\n* No frontend build considerations\\n* Updates can be automated with Dependabot\\n### Negative Consequences\\n* Universal viewer (UV) submodule contains the built viewer in source\\ncontrol, but this is a pattern of the UV currently.\\n","tokens":88,"id":4855,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bfi-discovery\/0003-choice-of-technology-and-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nTo deliver BFI's IIIF Universal Viewer auditing platform, custom\\ndeliverables must be produced to support the serving of the Universal\\nViewer, and an underlying API which records audit events and persists\\nthem into a database.\\nThis ADR provides options on technologies and frameworks which can be\\nused to produce these deliverables.\\n## Decision Drivers\\n* Ability for Digirati to rapidly deliver the solution.\\n* Long-term support and stability of the technology and framework\\nchoices.\\n* Ease of deployment across different environments \/ operating systems.\\n* Robustness and performance of the solution.\\n* Any technology and framework should be open source.\\n","Decision":"* Ability for Digirati to rapidly deliver the solution.\\n* Long-term support and stability of the technology and framework\\nchoices.\\n* Ease of deployment across different environments \/ operating systems.\\n* Robustness and performance of the solution.\\n* Any technology and framework should be open source.\\nKotlin is selected as the solution owing to Digirati's vast experience\\nwith the JVM, and when combined with Spring will make this the\\ndeliverables straightforward.\\nKotlin does not have the boilerplate cost associated with Java, and as a\\nresult internal there should be fewer barriers in identifying and\\nunderstanding the domain specific code required for the deliverables.\\nIn addition, since Spring handles significant amounts of the heavy\\nlifting, we do not expect to consume time writing code that is not\\nconcerned entirely with handling these audit events.\\n### Positive Consequences\\n* Faster development cycle for the parts of the auditing platform which\\nrequire custom code.\\n* Spring can provide both the Universal Viewer assets and the API,\\nunifying everything under one service and keeping authentication\\nsimple\\n* The deployment is straightforward, whether using a containerisation or\\ndeploying directly onto a bare metal server.\\n### Negative Consequences\\n* Lack of JVM \/ Kotlin experience within internal BFI teams.\\n","tokens":141,"id":4856,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"draft-store\/adr-001-only-128-bit-encryption.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nData persisted by the draft-store service needs to be encrypted. No firm steer has been given on the exact level of encryption so latest industry best practice is to be applied.\\n256 bit keys were attempted, however, to enable 256 bit encryption, the [JCE Unlimited Strength Jurisdiction Policy](http:\/\/www.oracle.com\/technetwork\/java\/javase\/downloads\/jce8-download-2133166.html) files must be installed into the JRE. Without them the maximum length supported in Java is 128 bit.\\nBruce Schneier says:\\n> \u201cfor new applications I suggest that people don\u2019t use AES-256. AES-128 provides more than enough security margin for the foreseeable future. But if you\u2019re already using AES-256, there\u2019s no reason to change.\u201d\\nThis was based on research conducted in 2009, however, thinking around this is hasn't changed drastically since.\\n","Decision":"We will implement 128 bit length keys using AES encryption and monitor the latest thinking on AES encryption to potentially update our approach in the future.\\n","tokens":188,"id":4858,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cape-cod-coastal-planner\/0002-use-graphql-for-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBecause this project is going to be delivered as a Beta project by the end of the year, it will be likely that its needs will grow as feedback from real users begins to filter in, as new ideas are conceived, new platforms are required, and as sources of funding are renewed. In order to accommodate these concerns, CHIP needs a simple, flexible, yet powerful API that is well situated for both changes in feature requirements and changes in technology.\\n","Decision":"GraphQL is a specification for an alternative to traditional REST APIs that provides a declarative development experience where you tell your program _what_ you want to happen and the GraphQL library will handle the _how_. It was initially developed by Facebook in 2012, announced by them in 2015, and has since grown exponentially to where 2018 will likely see more new APIs being written in GraphQL than in REST.\\nThe Absinthe library is a fully-featured, spec-compliant GraphQL implementation for Elixir and Phoenix applications and will be used for this project.\\n","tokens":93,"id":4860,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cape-cod-coastal-planner\/0001-use-elixir-as-backend-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBecause this project is going to be delivered as a Beta project by the end of the year, it will be likely that its needs will grow as feedback from real users begins to filter in, as new ideas are conceived, new platforms are required, and as sources of funding are renewed. In order to accommodate these concerns, CHIP needs a simple, flexible, yet powerful backend that is well situated for both changes in feature requirements and changes in technology.\\n","Decision":"Elixir is a modern language built on the rock-solid foundation of the Erlang\/OTP runtime which has over 30+ years of running highly critical systems with as little downtime as possible. While Erlang itself has a reputation of being arcane, Elixir introduces an elegant new syntax and many new features that have created a vibrant and enthusiastic following both commercially and within dev circles. Phoenix, the de-facto web framework for Elixir, capitalizes on the features of Elixir and Erlang\/OTP to provide a very rich developer experience and the capabilities to develop any kind of app you could imagine: traditional MVC, REST, GraphQL, real-time streaming, etc.\\n","tokens":93,"id":4861,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"cape-cod-coastal-planner\/0003-use-elm-as-frontend-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBecause this project is going to be delivered as a Beta project by the end of the year, it will be likely that its needs will grow as feedback from real users begins to filter in, as new ideas are conceived, new platforms are required, and as sources of funding are renewed. In order to accommodate these concerns, CHIP needs a reliable and fast UI\/UX that can be easily maintained.\\n","Decision":"Elm is a pure functional language for building reliable web apps with great performance and no runtime exceptions.\\n","tokens":83,"id":4862,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mysql-monitoring-release\/0002-common-go-linter.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe `mysql-monitoring-release` comprises several self-contained `src` modules.\\nThe expectation is that each contains a `bin` directory, with a `test` bash\\nscript representing the set of unit tests for a given module.\\nThis approach makes sense, but is leading to some copied linting code.\\n","Decision":"We considered:\\n1. Making a separate module that each of the other modules' tests would source\\n1. Pulling the linting code out of the modules entirely, and putting it in CI\\nBoth options eliminate the redundant, difficult-to-maintain copypasta.\\nWe opted for option 1, as we felt that it enabled the quickest feedback loop on\\ncode formatting, because it still existed inside of the `bin\/test` scripts.\\n","tokens":67,"id":4863,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mysql-monitoring-release\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4864,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tfl-graphql\/0001-namespacing-tfl-types.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTfL's Unified API has an extensive type system to describe the data returned from various endpoints.\\nIn the OpenAPI Spec these follow a dotted naming convention to group related type definitions together.\\nSee [the definition section of the OpenAPI file for reference](https:\/\/api.tfl.gov.uk\/swagger\/docs\/v1).\\nInitially, the GraphQL types were all grouped together under the root `Type` module. This risked making the GraphQL implementation hard to follow as it didn't follow the organisation principles of the TfL API.\\n","Decision":"`tfl-graphql` groups together GraphQL types into modules that closely map to the type definitions in the [Open API spec](https:\/\/api.tfl.gov.uk\/swagger\/docs\/v1) for TfL's Unified API.\\nWe ignore extra and unnecessary namespacing for example `Api.Presentation` when translating to Ruby modules.\\nThis results in the following Open API Definition -> Ruby Module mappings.\\n| Open API Definition                            | Ruby Module                     |\\n| ---------------------------------------------- | ------------------------------- |\\n| `Tfl.Api.Presentation.Entities`                | `Tfl::Entities`                 |\\n| `Tfl.Api.Presentation.Entities.JourneyPlanner` | `Tfl::Entities::JourneyPlanner` |\\n| `Tfl.Api.Common`                               | `Tfl::Common`                   |\\n| `System`                                       | `Tfl::System`                   |\\n| `System.Data`                                  | `-`                             |\\n| `System.Data.Spatial`                          | `Tfl::System::Spatial`          |\\nAutomated tests are also be organised into a directory structure that follows this namespacing approach.\\nWe introduce a new namespace to encapsulate Enums used in the Api. `Tfl::Enums`.\\n","tokens":111,"id":4865,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"tfl-graphql\/0002-using-json-schema-in-specs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGraphQL recommends [testing the behaviour of a GraphQL API from the client's perspective](https:\/\/graphql-ruby.org\/testing\/integration_tests.html).\\nWe need to test that the data returned from GraphQL queries conforms to the type definition system.\\n### Option explored\\n1. Individual expectations\\nFor example validating each key in a response.\\n```ruby\\nexpect(response).to include('id')\\nexpect(response).to include('commonName')\\nexpect(response).to include('stops)\\n```\\n2. FactoryBot\\nThis potentially could be achieved using [FactoryBot](https:\/\/github.com\/thoughtbot\/factory_bot). However validating against dynamic GraphQL queries may not be possible. For example if we wanted to test that a GraphQL query can return just one key from a type AND test the same query can return multiple keys.\\nIn testing GraphQL responses, we are primarily interesting in the shape and structure of data and not really the content.\\n3. JSONSchema\\nJSONSchema provides a declarative domain specific language to define and validate the schema for JSON objects. Furthermore, it allows us to define a JSON schema and test conformity.\\n","Decision":"`tfl-graphql` uses [JSON Schema](https:\/\/json-schema.org\/understanding-json-schema\/) validation to test responses from GraphQL queries.\\nIt implements a [custom RSpec Matcher](\/spec\/support\/schema_matcher.rb) based on [this article from Thoughtbot](https:\/\/thoughtbot.com\/blog\/validating-json-schemas-with-an-rspec-matcher).\\nThis matches a given object against schema defined in `spec\/schemas`.\\n","tokens":234,"id":4866,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"GDD-app\/0004-decouple-domain-models-from-representation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDomain models were used as-is for persistence and over-the-wire communication, making it difficult\\nto define a proper and strong domain model.\\n","Decision":"Separate DTOs should be defined for persistence and over-the-wire communication, which can have a\\nmore adequate representation for their use-case, without affecting our domain model.\\n","tokens":33,"id":4869,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"GDD-app\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4870,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0006-tagging-governments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nContent published by Whitehall can be marked as 'political' and associated with a government (based on it's publication date). This is used to help indicate the content may no longer accurate as governments change over time.\\nMarking content as a political is a semi-automated process, which has the effect of adding two extra attributes to the payload we send to the Publishing API. Both attributes are required for them to be used by the frontends.\\n```\\n{\\n...,\\n\"details\": {\\n\"government\": {\\n\"title\": \"My Current Government\",\\n\"slug\": \"\/my-current-government\",\\n\"current\": true\\n},\\n\"political\": true\\n}\\n}\\n```\\nThere were 3 options considered:\\n### 1. Do not implement\\nThis would require changing the [content schema for news articles][news-article-schema] so that the `government` and `political` attributes are not required. We've checked to see the frontends do not require these attributes to be set.\\n### 2. Implement like Whitehall\\nThis would require us to replicate the [semi-automatic logic][whitehall-political-identifier] in Whitehall to identify political content, as well as extend our UI to allow users to mark content as political and change the publication date.\\n### 3. Implement from scratch\\nThis would involve designing our own UI to associate content with a government and\/or mark it as political. After some discussion with the people who worked on adding this feature to GOV.UK, we believe there is scope for improvement.\\n","Decision":"We decided to go with option (1) and defer implementing support for political content. The implementation in Whitehall has several disadvantages, which we think merit further research with users:\\n* The `political` and `government` attributes are part of the content of a document, which means they contain redundant data.\\n* Storing the attributes in the content also means all documents must be republished when the current government changes.\\n* The effect of marking content as political is unclear, and the link between publication date and government is not visible.\\n* It's unclear if associating content with a government and marking it as political are separate concerns or the same thing.\\nSince manual intervention is required whether or not we implement the current approach, we think it's reasonable to defer supporting it until we have more knowledge to address the issues it presents.\\n","tokens":329,"id":4871,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0007-minimal-model.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe currently have several classes that we think of as models.\\n* ActiveRecord classes that reflect persisted data in the database. These are conventionally found in `app\/models`.\\n* Example: `Document`\\n* Plain Ruby classes that reflect readonly data stored in configuration files. These currently have no place to live :-(.\\n* Example: `DocumentType`\\nWe also have other classes that resemble models, but aren't e.g.\\n* `DocumentUrl` is not a class we think of as a model e.g. because it has no public attributes\\n* `UserFacingState` is not a class we think of as a model e.g. because its instances have no unique ID\\nWe would like to make it clear which classes belong in `app\/models`.\\n","Decision":"We should treat a class as a model if it has the following properties.\\n* **The name of the class is a noun that represents a domain concept**\\n* Example: a `Document` represents all revisions of a piece of content, etc.\\n* Guideline: it should be difficult to rename the class to a verb\\n* Guideline: the class should be documented in the README nomenclature\\n* **Instances of the class have multiple attributes that encapsulate it**\\n* Example: a `DocumentType` encapsulates publishing metadata, tags, etc.\\n* Guideline: The attributes should be public and required in our code\\n* **Instances of the class should have a unique identifier attribute**\\n* Example: each `Document` has a unique `id` assigned by the database\\n* Example: each `DocumentType` has a unique `id` in its config file\\n* **Instances of the class support mass-assignment of their attributes**\\n* Example: a `Document` supports assignment as an ActiveRecord model\\n* Example: a `DocumentType` supports assignment on initialize (readonly)\\n","tokens":163,"id":4873,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0014-political-and-government-tagging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn [ADR 6](0006-tagging-governments.md) we discussed our decision to defer\\nthe tagging of governments and political status to content. In December 2019\\nthe political climate determined that we would need to implement this as it\\nwas expected that content published through Content Publisher would have to be\\nable to enter [history mode](..\/history-mode.md).\\nIn order for content to be in history mode it needs to be associated with a\\ngovernment that is no longer the current one and the content itself be flagged\\nas political. This tagging broadly means that the content represents the\\npolicies of the government it is associated with.\\nThe business rules behind this feature dictate that the political status of\\ncontent is determined dynamically based on attributes and associations of the\\ncontent. The government is determined based on the time the content is first\\npublished or backdated to.\\n","Decision":"To implement this feature we decided to add a [service][edit-edition-service]\\nthat runs each time content is edited which can determine whether the content\\nis political or not. If the content has been published or is backdated it can\\nalso determine the government. This data is stored in the editions database\\ntable. As this value is dynamically calculated only the current value is\\nstored as it is not considered an aspect of the publications history.\\nThe political value for the content can be overwritten by a publisher. This\\nvalue is known as `editor_political` and supersedes the value stored on an\\nedition, which is known as `system_political`. As this data is set by a\\npublisher it is considered part of a revision of a document and is stored\\nfor prosperity.\\nWhen content has not been published or backdated it is not associated with a\\ngovernment. At the point of content being published the current time is used\\nto determine the government.\\nWhen sent to the Publishing API, these fields are represented in the form of\\na boolean attribute for political and an edition link for the government.\\nIn the time since ADR 6 governments have been added to the Publishing API\\nwhich means that content associated with a government no longer needs to be\\nrepublished when the government is no longer current.\\n","tokens":184,"id":4874,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0005-avoiding-initialisation-flickers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA common problem with website initialisation is that there can be elements of the\\npage that you want to be hidden until a user performs an action via JavaScript.\\nHowever if a user does not have JavaScript you want the item to always be\\nvisible.\\nWhen the page loads this can cause a distracting flash as it quickly hides or\\nreplaces content and can appear like the page has a loading issue to a user.\\nOn content publisher there have been a number of issues flagged in testing\\nrelating to these flashes, with different ad-hoc fixes applied. This ADR is\\nintended to define a route forward for them to be handled consistently.\\nThere were 3 options considered:\\n### 1. Do nothing\\nThis option involved considering the flash to be a natural consequence of\\nprogressive enhancement.\\n### 2. Hide content before JavaScript initialisation with a `js-enabled` class\\nWe have a class that is inserted in the body element of a HTML document while\\nthe page is loaded that sets whether the user has JavaScript enabled.\\nThis class can therefore be used to hide elements on the page in advance of\\nJavaScript initialisation to avoid the flicker.\\nThe downside that this approach has is that there is not a fallback for if\\nJavaScript is enabled on a device but fails for some reason.\\n### 3. Implement an initial hiding approach resilient to JavaScript failing\\nThe final approach considered was the introduction of a second class in addition\\nto `js-enabled` on the body element that would be `js-failed`. This would\\ninvolve enhancing the initial adding of a class from:\\n```\\ndocument.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');\\n```\\nto something like:\\n```\\ndocument.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');\\nsetTimeout(function() {\\nif (!window.jsInitialised) {\\ndocument.body.className += ' js-failed';\\n}\\n}, 2000);\\n```\\nWhich would allow a 2 second grace period before changing the body class so\\nfallback content is displayed.\\nCSS rules would need to be updated to show content when either `js-enabled` or\\n`js-failed` was present\\n","Decision":"A decision was made that in interim we should take option 2 and just use the\\n`js-enabled` class. This was deemed the most pragmatic approach as option 1 was\\ncausing problems for our testers and option 3 was considered something unusual\\nthat wanted to be investigated further.\\n","tokens":482,"id":4877,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0010-sending-email-notifications.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to send email notifications about user actions, such as publishing a document, we need to:\\n- Work out who should receive the email\\n- Generate the content for the email (subject and body)\\n- Send the email to the recipient(s)\\nWe don't anticipate sending email to be a critical step of any user action, but it is still a step that is prone to error and delay, due to interaction with external systems. We should ensure sending emails is robust against transient failure and that emails are only sent when we are confident they are accurate i.e. after DB updates and critical API calls.\\nWe should have a means of sending notifications in non-production environments, which allows us to see what notifications would be sent in production, but prevents the emails from being sent to the intended recipient(s). It should also be possible to see the notifications we have sent in production in order to debug issues if users report they are not receiving notifications, or that the content of the notifications is incorrect.\\n","Decision":"We will use [GOV.UK Notify][notify] in combination with [Action Mailer][action-mailer], [Active Job][active-job] and [Sidekiq][sidekiq].\\n### GOV.UK Notify\\nWe will use [GOV.UK Notify][notify] to handle the low-level concerns of sending emails:\\n- This avoids having to setup our own mechanism for sending email, including logic to handle low-level errors and retries, which are [handled by Notify by default][notify-features]\\n- Notify provide a [Ruby gem][notify-gem] that makes it easy to integrate the service into our app, without specific knowledge of the underlying APIs\\n- We can use the [Notify dashboard][notify-features] to see emails we have sent, logs for individual emails, as well as aggregate stats over all emails\\n- Notify provides the facility to restrict the recipients of emails to a [specific list][notify-allowed-list], which we can use to restrict notifications outside of production\\nIn order to integrate Notify with our app, we will setup 3 'GOV.UK Publishing' accounts as follows:\\n- One account will be for production use by any GOV.UK publishing app\\n- The other two accounts will be for testing notifications on integration and staging\\nWe agreed to use the generic 'GOV.UK Publishing' name as part of a wider GOV.UK strategy to have a single Notify account for all GOV.UK publishing apps, which helps to limit future infrastructure growth.\\n### Action Mailer\\nWe will use [Action Mailer][action-mailer] to generate the individual emails to send to notify:\\n- Using Action Mailer to process email is the recommended approach for Rails apps\\n- Action Mailer comes with in-built support for testing and sending email asynchronously\\nAlthough Notify provides a [templating feature][notify-features], we will use a single, generic template for all 'GOV.UK Publishing' notifications, with recipient, subject and body parameters; the body supports a limited form of Markdown. Using Action Mailer to handle the generation of emails means we limit our reliance on Notify templates; we can make most of our changes in code, with the usual benefits of testing, faster debugging and version control.\\nNotify only supports a [single recipient for each email][notify-gem-send-email], so we will need to generate and send separate emails for each recipient. Although this requires more API calls, it's easier to reason about failure for individual emails.\\nNotify will reject an email when a recipient is not in a specific list, if one has been setup. In order to retain end-to-end email functionality but avoid emailing users outside of production, we will use an [Action Mailer Interceptor][action-mailer-interceptor] to redirect emails to a Google Group in integration and staging environments. The Google Group(s) will act as a dumping ground for viewing the emails that have been generated, which enables us to check they appear correctly in text and HTML forms.\\n### Active Job\\nWe will use [Active Job][active-job] in combination with [Action Mailer][action-mailer] to send emails asynchronously.\\n- This means any delay in calling the Notify APIs does not impact on the user request\\n- We can use the [retry feature][active-job-retry] of Active Job to cope with transient failures from Notify\\nAction Mailer integrates with Active Job instead of specific queueing systems, and recent [improvements to the Active Job framework][active-job-improvements] also make it a viable for other background jobs, so we can avoid having to use a mixture of frameworks.\\nActive Job does not handle asynchronous processing directly and instead delegates to a queueing provider. Initially, we will use [Sidekiq][sidekiq], which is already common on GOV.UK and one of the [in-built queueing adapters][active-job-sidekiq] provided with Active Job.\\n### Sidekiq\\nWe will use [Sidekiq][sidekiq] in order to handle the actual processing of background jobs. Sidekiq is already used by other apps on GOV.UK for background job processing, to the extent that including it in apps is managed by a [wrapper gem][sidekiq-govuk].\\nIn order to avoid [duplicate retry behaviour][active-job-sidekiq-retry], we will disable Sidekiq retries and rely solely on the retry behaviour of Active Job. When a failure occurs, the job is marked as 'processed' by Sidekiq and any retry manifests as a newly enqueued job; when the retries are exhausted, the exception will propagate to Sidekiq and the job will be marked as 'dead'. Normally, the job would be marked as 'failed' by Sidekiq and requeued; this means we lose visibility on the number of retries.\\nWe already use [Raven] to report errors to Sentry, which automatically [integrates with Sidekiq][sidekiq-sentry]. Note that Raven also [integrates with Active Job][raven-active-job], but this is disabled when Sidekiq is present. Active Job catches exceptions as part of its retry behaviour, so an error will only get reported to Sentry when the exception is not handled by Active Job, or the retries for an exception we do handle are exhausted. We already [export Sidekiq metrics to Graphite][sidekiq-grafana].\\n","tokens":206,"id":4880,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0013-permitted-deletion.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[ADR 8: Restricted deletion][ADR-8] explained the decision that\\n[foreign key][foreign-keys] associations in the Content Publisher database\\nwill all use the `restrict` constraint. At that point in time it was\\nidentified that we do not have any use cases for deleting data from the Content\\nPublisher database.\\nSince then we have developed the access limit feature. This feature prevents\\nusers who are not part of a particular organisation from viewing content. A\\nvulnerability in the access limit feature is that production data is\\nsynchronised to an integration environment where the access controls\\naren't as stringent, thus potentially allowing unauthorised users access to\\nsensitive content. To resolve this issue GOV.UK applications have taken the\\napproach of either removing or redacting data during the data sync process.\\nFor Content Publisher to be consistent with other GOV.UK applications we had\\nto choose between redacting or removing data associated with an access limited\\nedition.\\n","Decision":"We decided that data would be removed from Content Publisher during the\\nproduction to integration sync. This is significantly simpler than redacting\\nthe data and less likely to require adaptation for the needs of particular\\nformats.\\nThe approach taken to achieve this is to delete all data related to an\\naccess limited edition that isn't shared with another edition. To allow\\nthis we decided to [replace `restrict` constraints\\nin the editions table with `cascade` behaviour][editions-commit] that\\nautomatically deletes associated records. A [similar approach was\\napplied to revisions][revisions-commit] to allow them, and associated data,\\nto be deleted when they are no longer associated with an edition.\\n","tokens":208,"id":4881,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0015-bulk-data-loading.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nContent Publisher regularly has to present users with long lists of data that\\nis loaded from the Publishing API, examples of these are organisations,\\ncontacts and the topic taxonomy. Content Publisher typically needs to\\ndownload all the records of a particular datatype, which can involve\\nmultiple HTTP requests. When this is done during a web request the application\\nis at an increased risk of returning a slow response or even timing out.\\nThe risks of this have been partially mitigated by caching.\\nThese situations where Content Publisher needs all the data records for a\\nparticular dataset are colloquially referred to as bulk data. This is\\ndistinguished from other API look ups where we may need to access a\\nsingle resource (such as looking up the topic tags) that have less risks of\\nperformance penalties.\\nThis particular problem has been faced and solved for GOV.UK before. Whitehall\\ntakes an [approach](https:\/\/github.com\/alphagov\/whitehall\/pull\/3298) for the\\ntopic taxonomy where the data is loaded at runtime from\\n[Redis](https:\/\/redis.io\/). A periodic background job then refreshes the\\ndata in Redis. This avoids the need to look up the data during a web request.\\n","Decision":"In Content Publisher we have chosen to implement a similar pattern to Whitehall.\\nData is read from Redis at runtime and a background process runs a periodic\\njob to re-populate the Redis data.\\nThis approach makes use of the [Rails Redis cache store][redis-cache-store]\\nwhich is wrapped within an object, [BulkData::Cache][bulk-data-cache], for\\nwriting and accessing data. We have introduced a concept of\\n[repository][government-repository] classes that can be used to work with the\\nloading and modelling of data that is stored in the cache.\\nOn application boot and on a recurring 15 minute interval a\\n[background job][populate-bulk-data-job] will run to try to re-populate the data.\\nIn the event of an error occurring this job will log this issue and retry. If\\nretries are exhausted a decision will be made whether to\\n[send the error to Sentry or not][error-handling] based on whether the error\\nis likely to be a problem we will investigate.\\nThe data within the bulk data cache is stored with an expiry of 24 hours, which\\nis a lot longer than the interval we use to re-populate the cache. The reason\\nfor such a long time is to provide as a safety net for problems to occur and\\nnot to present any sign of issues to users until this period has expired. If a\\nfrontend request is made for a resource that requires bulk data, and the\\ncache entry for that is empty, a [503 response][unavailable-response] is\\nreturned to the user and the job to re-populate the cache is enqueued.\\nIf we hit a scenario where there are errors with the jobs there is a chance\\nthat we will slowly build up a large queue of this same job multiple times. To\\nprevent this situation causing any significant problems (such as flooding the\\nPublishing API with requests) the job will only populate data where [the\\ndata is older than 5 minutes][older-than-check].\\n","tokens":259,"id":4884,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0011-moving-business-logic-out-of-controllers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOver time the controller classes for Content Publisher have become\\n[increasingly complex][complex-controllers] as the application grew in size\\nand functionality. This has led to action methods that are long, difficult to\\nunderstand and expensive to test. Looking towards the future it can be\\nanticipated that there will be more functionality in controllers, such as\\naccess limiting, concurrent editing protection and invalid state handling.\\nTherefore, there is an expectation that controllers would become an increasing\\npain point for the application due to their complexity.\\nThis is a common scenario for a growing Rails application. A customary\\napproach to address this is to distinguish between business and rendering\\nlogic in an action (where business logic is the action the user requested,\\nsuch as changing application state, and rendering logic is the process of\\nbuilding the response to be returned to the user) and to move business logic\\noutside of the controller action to another class.\\nWithin the Content Publisher team we have been evaluating\\na number of patterns and tools for creating business logic classes. We came to\\nthe following conclusions:\\n- [Service objects][service-objects]: This is a pattern already used in\\nContent Publisher, however it is used to perform distinct application tasks\\nrather than being coupled to particular controller actions. We felt the\\nservices directory had become a mess in Content Publisher, with little\\nconsistency between service objects.\\n- [Dry-transaction gem][dry-transaction]: This provides a nice interface for\\nmanaging responses through [ruby blocks][calling-a-dry-transaction]. However\\ndry-transaction requires a strict adherence to a provided DSL which can make\\na transaction class very different from a plain old Ruby object.\\n- [Interactor gem][interactor-gem]: This is a simple, relatively free-form\\npattern (no DSL) that uses a `context` object for basic control flow. It has\\na disadvantage that the input and output are not particularly clear.\\n- [Trailblazer operation][trailblazer-operation]: This gem provides a DSL for\\ncreating classes to perform a business operation. It works best within a\\nRails application that has embraced the collection of Trailblazer utilities.\\n- [Publishing API command pattern][pub-api-command]: This is an effective\\npattern at moving logic out of controllers. However, we felt it had become\\nconvoluted and inflexible due to domain logic coupling.\\n","Decision":"Content Publisher will make use of the [Interactor gem][interactor-gem] as the\\ntool to implement classes for performing controller business logic. The logic\\nwithin controller actions will be focused on producing a HTTP response.\\nInteractors for controller actions are intended to be coupled to a\\nparticular action and not be reused outside that context. This is to maximise\\nclarity (in purpose and naming) and to minimise the logic, options and outcomes\\nof an interactor.\\nInteractors should be created for all controller actions that mutate\\napplication state (typically POST\/PUT\/PATCH\/DELETE requests). Only in cases\\nwhere an action is very simple should an interactor not be used.\\nFor controller actions that don't mutate application state (such as GET\\nrequests) interactor classes may be created, if beneficial in abstracting\\ncomplexity. In most cases these types of requests have low amounts of code\\nand so use of this pattern would not be advantageous.\\nInteractors are stored in the `app\/interactors` directory, within here\\nthere are directories created for each controller to store the interactors for\\neach action, in a pattern similar to views. Each interactor class name is\\nsuffixed with \"Interactor\", to make their purpose clear. As an example,\\ngiven a `create` action on a `DocumentsController` there is a corresponding\\ninteractor to create a document, `Documents::CreateInteractor`, which is stored\\nin `app\/interactors\/documents`.\\nThe Interactor gem was chosen over writing our own framework for handling\\nbusiness logic, as per the Publishing API command approach, for the following\\nreasons:\\n- a bespoke problem is not being solved, so it shouldn't require a bespoke\\nsolution;\\n- authoring a new pattern is something that requires documentation, maintenance\\nand iteration;\\n- It\u2019s harder to [bikeshed][] on decisions made externally.\\nWe felt that compared to other community gems, such as dry-transaction and\\nTrailblazer, Interactor offered a lower learning curve, greater flexibility\\nand a lower dependency overhead. We also felt that as the gem is quite simple,\\nusing it would not preclude us from building, or changing to, a different\\napproach were our needs to change or we found a problem with the gem.\\n","tokens":497,"id":4885,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0006-contributing-back-to-open-source.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nQuite often, in commercial projects, reporting back to the community is entirely in the developer's hands. They are allowed to do this, but commonly there is no dedicated time in the plans.\\nOne of the ideas behind Project Klokwrk is to streamline development efforts as much as possible. As we are using many different open-source tools and libraries, it is in our best interest to try to\\nhelp with the issues we encountered.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**At least, we will report the issues in 3rd party open source software that are encountered during `klokwrk-project` development.** We'll strive to do this as consistently as possible. Besides being\\nconstructive community members, resolving encountered issues should benefit us with an easier and streamlined workflow.\\nWe will actively maintain [the list of reported issues and monitored issues](..\/..\/misc\/klokwrkRelatedIssuesInTheWild.md) that are relevant for `klokwrk-project.`\\nPutting open source contribution in ADR makes it an official guideline and moral obligation for all `klokwrk-project` members.\\n","tokens":108,"id":4887,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0009-testing-inter-module-dependencies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAlthough architectural testing ([ADR-0008 - Testing Architecture](0008-testing-architecture.md)) verifies some application dependencies, there are still dependencies that might be overlooked.\\nIn general, we are not talking about 3rd party dependencies but rather about in-project modules at the lower level of abstraction compared to applications.\\nThose project modules represent various pieces of functionality developed to streamline and simplify application development. The majority of lower-level modules will typically deal with\\ninfrastructure concerns, but some of them might deal with the domain itself. If we have many of those modules, we might accidentally end up in situations where applications use something they should\\nnot use. This might be just plain error, or it might be a suggestion that the lower-level module needs refactoring or splitting into more modules.\\nTo avoid accidental inter-module dependencies, we should have in place a mechanism for verifying them. Also, as project modules grow, we should have some capable visual tooling that quickly can show\\nand explore inter-module dependencies for easier comprehension and communication.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use [Structure101 Studio for Java](https:\/\/structure101.com\/products\/studio\/) for displaying, exploring, and ad-hoc monitoring inter-module dependencies.**\\n**We will use the [ArchUnit](https:\/\/www.archunit.org\/) library for checking inter-module dependencies.**\\nStructure101 Studio is an excellent tool for visualizing and exploring bytecode-level dependencies between software artifacts. To quickly get an overview of dependencies between project modules, it\\nis enough to click on a module in an appropriate view, as is shown in a picture below.\\n![Image 1 - Revealed dependencies in Structure101 Studio](images\/ADR0009-01-revealed-dependencies.jpg0.jpg \"Image 1 - Revealed dependencies in Structure101 Studio\") <br\/>\\n*Image 1 - Revealed dependencies in Structure101 Studio*\\nAlthough Structure 101 is very useful for visualization, exploration, and time-to-time monitoring of dependencies, we are still missing automated dependency checks. We can implement them by\\nleveraging the ArchUnit library.\\nAs an example, take a look at the `BookingCommandSideAppDependenciesSpecification` test. It is checking dependencies for the `cargotracking-booking-app-commandside` module. After a closer look,\\nit can be seen that we are testing dependencies displayed in the picture above. This is very convenient since we can use the Structure101 diagram as direct input for our test.\\n`BookingCommandSideAppDependenciesSpecification` test is a basic test since it enumerates all dependencies of a module without any additional criteria. Of course, the test can be expanded to verify\\nmore precise details if needed.\\n","tokens":228,"id":4888,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0004-hexagonal-architecture-for-applications.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want improved application design architecture that supports and promotes the following characteristics:\\n* An explicit promotion of application features as first-class citizens, instead of promoting technical aspects or database design.\\n* Explicit support for DDD tactical patterns.\\n* Is domain-centric with clear separation of the domain from infrastructure.\\n* Improves testability, maintainability, flexibility, extensibility, and adaptability.\\n* Discourages the use of any non-supported shortcuts.\\n* Is explicit enough to allow testing of architecture constraints and rules.\\n* Has sound structure and aesthetics where it is easy to find concrete artifacts.\\n* Allows parallel work on features.\\n* Is well-known and battle-tested already in the wild.\\nIt is proven that traditional layer-centric designs fail on almost every characteristic above. Layer-centric application design architecture can work for simpler cases but often fails when the\\nfeature set grows, and the domain becomes more complicated. We want architecture that can scale cleanly in terms of design and development.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use hexagonal architecture as a primary choice of architecture for designing applications.**\\n**We will provide concrete guidance for creating structure and concrete artifacts** (see [\"Packaging for applications\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#packaging-for-applications) for more\\ndetails).\\n**We will adapt hexagonal architecture for efficient use with CQRS\/ES component architecture** (see\\n[\"Applying hexagonal architecture\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#applying-hexagonal-architecture) for more details). This includes adding support for enforcing and testing the structure and\\nbehavior of the architecture (see [\"Behavior and architectural testing\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#behavior-and-architectural-testing) for more details).\\n","tokens":221,"id":4891,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0011-component-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen developing application monoliths, it is common to have a relatively small number of end-to-end tests (a.k.a. [broad-stack tests](https:\/\/martinfowler.com\/bliki\/BroadStackTest.html) or system\\ntests). They are usually run on a full system, including necessary infrastructure, either by exercising UI or public system API.\\nWhen developing a microservices based system, instead of a single, we have many runnable components. They are usually grouped into corresponding bounded contexts, where each bounded context\\ncontains a group of closely related microservices. Further, depending on technology choices, each logical microservice might be split into multiple runnable components.\\nIn an environment like that, spawning a whole system for testing might be challenging to set up and time-consuming to execute. Also, broad-stack tests are not focused on a single logical\\nmicroservice, which can bring in a whole range of issues, from responsibility for development and maintenance to the expected but not yet developed system features.\\nFor testing microservices, we need tests that have a very different focus than broad-stack tests.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use [component tests](https:\/\/martinfowler.com\/bliki\/ComponentTest.html) for exercising running instances of a single or multiple logical microservices inside a single bounded context.**\\nThe technical implementation of component tests is very similar to broad-stack tests. Component tests are usually run through the public API of the component. If we have micro-frontends based UI,\\nit can also be used. Therefore, the applied scope of tests is the major difference from broad-stack testing. Component tests are scoped and focused at bounded context only.\\nImplementation of component tests will rely on the Testcontainers library. In contrast with [integration tests](0010-integration-testing-with-containerized-infrastructure.md), component tests do not\\nonly containerize infrastructure but also logical microservices under the test. With such a setup, component tests usually leverage an external client for exercising the public API of microservices.\\nAs an example of component tests, take a look at the `klokwrk-project` module `cargotracking-booking-test-component`.\\n","tokens":244,"id":4892,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0012-response-format.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEvery server-side application sends some kind of responses to its clients. Unfortunately, the exact structure of these responses is rarely defined.\\nThe most probable cause is that each **successful** response has a different content and structure as it communicates various domain data depending on the requested endpoint. Consequently, it is\\nimpossible to prescribe a predefined structure for such responses carrying only \"concrete\" data in its payload.\\nHowever, there is often a need to communicate some kind of metadata with every response in more elaborate systems. While metadata can be useful for successful responses, they are almost unavoidable\\nfor responses communicating failures. No matter if the failure cause originates from the client or the server.\\nThe standard way for sharing metadata is through means of a concrete protocol. For example, HTTP uses statuses and headers. Messaging systems also employ headers but of different format, etc.\\nHowever, if there is a need for more detailed metadata, headers are often combined with payloads.\\nSuch a situation brings fragmentation and inconsistency in metadata transfers, especially when multiple protocols and channels have to be supported. While standard protocol features must be obeyed,\\nfrom the application perspective, it might be helpful having all necessary data and metadata in one place.\\nTo enable generalized creation on the server-side and generalized processing on the client-side, metadata should follow some kind of prescribed shape and format.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will render responses following the prescribed format.**\\nFormat structure is independent of concrete protocol or channel and should be used for all generated responses.\\nConcrete details of the prescribed format are given in the \"[The format of rendered responses](..\/..\/article\/response-format\/responseFormat.md)\" article. Although the article presents prescribed\\nstructures in JSON, concrete format implementation is not essential. It can be anything else that is more suitable for some chosen protocol, as long as defined structures are followed.\\nImplementations of response renderers are specific for each supported channel. At the moment, we have an implementation for the Spring MVC channel that can be found in `cargotracking-lib-web` module\\nin `org.klokwrk.cargotracking.lib.web.spring.mvc` package.\\n","tokens":290,"id":4893,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0005-using-groovy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe would like to have a language or language extension that is very close, very similar, and 100% compatible with Java, but that offers significant productivity boosters at the same time.\\nJava syntax is often verbose, and it is common to use various IDE tricks (folding\/unfolding) or 3rd party libraries (e.g., Lombok) to deal with some of that clutter. It would be much better to have\\na language with a clear, concise syntax that is almost naturally understandable for Java developers. At the same time, the language should provide user-friendly extensions of Java SDK and best\\npractice implementations of various programming patterns.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use Groovy as a primary language.**\\n","tokens":152,"id":4894,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0013-validation-taxonomy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWith today's technologies and frameworks, validation of input data looks like a solved problem. You just add few annotations on DTOs (Data Transfer Objects), hook up the validation service with the\\napp, and move on.\\nHowever, there are much more under the cover. When digging into the details, various questions pop up. Where exactly is the best place for input data validation? Are there multiple levels or phases\\nof validation? What if we need a system state for validation? What happens when numerous inbound channels are in play?\\nAfter trying to answer these, it turns out that validation is more involved than what is commonly shown in basic examples. Solutions are not too complex but require establishing some principles and\\nrules.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will execute syntactic validation at the application facade layer. If necessary for UI requirements, it may also be executed on the adapter layer.**\\nCore validation at the application facade layer ensures that input data from any inbound channel will be validated in the same way. An example can be seen in `BookingOfferCommandApplicationService`\\nclass from `cargotracking-booking-app-commandside` module.\\nValidation at the inbound channel adapter layer is used when there is a need for UI or client-specific validations. A typical example is an email verification scenario where, with repetition, we want\\nto assure that the user submitted the correct email.\\n**We will organize validation into syntactic and semantic phases. Each of them will be divided further into subphases if necessary.**\\n**We will execute semantic validation inside the domain layer. Depending on nature, this will happen either in the aggregate or in the application facade.**\\n### Syntactic validations\\nWe consider validation being syntactic if it does not require any system state. It validates input data fields in isolation and possibly some interdependencies between them (cross-field validation).\\nSyntactic \"stateless\" validations are executed first. If they fail, semantic validations won't be triggered at all.\\n### Semantic validations\\nOn the other hand, semantic validation requires access to the system state. It may deal with checking if, for the current aggregate state, the request is valid or not. It may also check some other\\nnon-aggregate conditions, such as the existence of necessary data in registries. We are reporting semantic validation failures as domain exceptions.\\nRegarding the order, it is advisable to group and execute all non-aggregate-related validation first (checking against registry data, for example). Such validations do not require transactional\\naggregate locking. They may use the transaction but must not have the aggregate in that transaction. Only if they pass, we should proceed with validations that require opening a transaction for the\\naggregate (for example, if input data are valid for the current aggregate state).\\n### Syntactic validation subphases\\nWe can divide syntactic validation into several internal phases. Those phases are ordered from the simplest to the more complex - existence, size, lexical content, and syntax format.\\nExistence validation ensures that provided data exist and are not empty. Further processing makes no sense when data is empty. In this subphase, we check for null objects, empty strings (zero-length\\nor whitespace only), empty collections, etc.\\nSize validation verifies if data are reasonably big. Before further phases, we are checking the length\/size of input data no matter od the data type. Size checks will prevent additional processing of\\ntoo big data, which might cause performance issues. Also, reporting about size failures might be helpful from the user perspective as it is a widespread mistake.\\nLexical content validation checks if data contain the correct characters and encoding. This phase might be helpful if we are receiving the data in complex formats like JSON, XML, or HTML. For simpler\\ndata inputs like size-limited strings, this phase is commonly executed as a part of the following stage.\\nSyntax format validation verifies if the format is correct. For strings, this is often achieved with regular expressions. When regex is too complicated, we might get better results with specialized\\nvalidator implementations.\\nWe are reporting syntactic validation failures through means of an employed validation framework. As we are using JSR 380 (Jakarta Bean Validation 2.0) implementation, syntactic validation failures\\nare reported through `jakarta.validation.ConstraintViolationException`. With JSR 380, we are implementing syntactic validation ordering with `GroupSequence` annotation, as demonstrated by\\n`BookCargoCommandRequest` class from `cargotracking-booking-app-commandside` module.\\n","tokens":165,"id":4895,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0015-handling-exceptions-in-distributed-cqrs-system.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDevelopers consider dealing with local (runtime) exceptions in JVM quite convenient and easy. As a consequence, exceptions are often used as a means for an alternative execution flow. One typical\\nexample is reporting structural and business validation violations.\\nIn the local JVM environment, expected execution flow exceptions are usually created with a full stack trace included. However, as these exceptions represent \"normal\" conditions, creating a full\\nstack trace is wasteful in terms of performance and resources.\\nThe full stack trace problem is additionally emphasized in a distributed JVM environment. Not only the stack trace creation is wasteful, but it will take much more bandwidth to transfer it on the\\nother side.\\nAnd finally, in a distributed JVM environment, we cannot safely assume that our custom exception classes are present in the classpath of both JVMs. Therefore, we can not assume the exception created\\non the server-side can be deserialized on the client.\\nTo remedy the situation, we can use a small number of stack-less exception classes shared between JVMs included in the communication. They belong to the boundary layer and are part of the API used\\nfor distributed communication. To read more about boundary layer, take a look near the end of \"Domain libraries\" section in\\n\"[Organizing modules and packages](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#domain-libraries)\" article.\\nFor communicating various custom exceptions, we can use violation (or error) codes. The violation code defines its severity and contains several additional properties for describing violation details.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**For distributed error reporting, we will use stack-less exceptions for communicating custom error codes between JVMs in a distributed Axon environment.**\\nIn a distributed Axon environment, the dispatching side (let's call it a client) sends commands and queries over Axon Server to the handling side (let's call it a server). When the handling side\\ndetects broken business invariants, it can raise an exception to report the details to the dispatching side.\\nAxon framework already contains base infrastructure for reporting such distributed errors via so-called exception details. Exception details can be any DTO-like class shared between communicating\\nparties, including all necessary data describing error conditions.\\n### Decision details\\n#### Business exception handling\\nIn our case, as exception details DTO, we will use the `DomainException` class and its descendants. It is a stack-less exception that carries `ViolationInfo` property. `ViolationInfo` contains\\n`Severity` and `ViolationCode` properties. `ViolationCode` contains `code` and `codeMessage` in English. There are also a `resolvableMessageKey` and `resolvableMessageParameters` that support error\\ncode resolving through a resource bundle for internationalization purposes.\\nIt is worth noting that `DomainException` can be used on the dispatching side (client) too. Exception (un)wrapping on the handling side and centralized exception handling on the dispatching side are\\nhidden in infrastructural code. From the developer's perspective, he works with `CommandException` or `QueryException` only (both are extended from `DomainException`). This is a nice addition as\\nbusiness invariants are handled the same way on handling and dispatching sides, without depending on classes from the Axon framework.\\nAs a usage example on the handling side, take a look at `BookingOfferAggregate` or `BookingOfferSummaryQueryHandlerService` classes. An example for the dispatching side can be found in\\n`BookingOfferFactoryService` class.\\n#### Unexpected exception handling\\nWhen handling business exceptions, we are not interested in stack traces. Since business exceptions are just a form of the alternate execution flow, there isn't much benefit in logging their\\nstacktrace.\\nContrary, we want to log the stack trace when an unexpected exception (i.e., NullPointerException) occurs at the remote handler. However, communication constraints still hold, and we still have to\\nuse error codes for communicating exceptions. Further, to correlate exceptions on handling and dispatching sides, we must use an exception identifier and put it in the log messages on both sides.\\nThe logic of handling unexpected exceptions on the handling side can be seen in the `CommandHandlerExceptionInterceptor` and `QueryHandlerExceptionInterceptor` classes.\\n","tokens":327,"id":4897,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0008-testing-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nApplication architecture is commonly expressed as a set of guidelines (written or not) that the development team needs to follow. Although essential, architectural guidelines are rarely reviewed or\\nenforced appropriately. The typical result is that architecture degrades through time.\\nIn some cases, application use component ([ADR-0003 - CQRS and Event Sourcing (ES) for Applications](0003-cqrs-and-event-sourcing-for-applications.md)) and\\ndesign ([ADR-0004 - Hexagonal Architecture for Applications](0004-hexagonal-architecture-for-applications.md)) architectural patterns that promote architectural guidelines, but usually there is\\nnothing to verify them.\\nThe hexagonal architecture provides a well-defined placeholder for every significant application artifact. But there are also some rules regarding dependencies between those artifacts. It is not\\nallowed that each class or interface access anything that it wants. When you add additional CQRS\/ES aspects, there are even more rules to follow.\\nWe want to ensure that rules will not be broken and that developers new to the hexagonal architecture and CQRS\/ES can comfortably work with them without breaking anything. It will help if we\\nhave in place tests that verify all architectural invariants.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use architectural testing for verifying architectural constraints.**\\nBuilding on top of the [ArchUnit](https:\/\/www.archunit.org\/) library, Klokwrk provides DSL for specifying hexagonal architecture layers for CQRS\/ES applications. There is support for several subtypes\\nof CQRS\/ES flavored hexagonal architecture corresponding to the commandside, projections, and queryside aspects.\\nFor more insight and details, take a look at [\"Behavior and architectural testing\"](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#behavior-and-architectural-testing). There is also a video\\n[\"Project Klokwrk: how it helps defining software architecture and solves integration\"](https:\/\/www.youtube.com\/watch?v=35oUxjXWNYU) that, besides other things, talks about architectural testing in\\n`klokwrk-project`.\\n","tokens":265,"id":4898,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"klokwrk-project\/0010-integration-testing-with-containerized-infrastructure.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIntegration testing is commonly used in contemporary development. Frameworks like Spring and Spring Boot makes integration testing seamless, and it often looks like plain unit testing in terms of\\nsimplicity and speed of execution.\\nHowever, the way that integration tests are used today commonly exercises only integration between internal application components (e.g., integration between controllers and services, integration\\nbetween services and domain objects, configuration correctness, etc.). If the tests require some external infrastructure, it is usually mocked out or replaced by alternative in-memory variants.\\nInternal application integration tests are important, but they do not cover integration with realistic external infrastructure.\\nAs containerized solutions (e.g., Docker) are a norm today, there is an obvious opportunity to use containerized infrastructure in our integration tests. There is a lot of value in such tests since\\nthe infrastructure is (almost) the same as it will be in the production.\\nThe most obvious examples are databases. A typical pattern in integration tests is to use an in-memory database, for example, H2. Although very convenient, it is not a real database as it will be in\\nproduction. It will not behave like an actual database either in a functional (SQL dialect) or non-functional (security, performance) way. If our production uses, say, the PostgreSQL database, it\\nwill be much better to use that same database in integration tests.\\nIf we look at other non-database examples of infrastructure components, say some modern messaging system, we will probably encounter no in-memory equivalent exists. In these cases, infrastructure in\\nintegration tests is usually mocked at something like the repository layer, and we end up with tests that do not exercise infrastructure at all. Not even inadequate in-memory replacement.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use the [Testcontainers](https:\/\/www.testcontainers.org\/) library and Docker for implementing realistic integration tests whenever it makes sense.**\\nTestcontainers relies on Docker for spawning required infrastructure during integration testing. Therefore, Docker must be available on a system where tests execute. Depending on the concrete\\nexternal component, starting up the container can require non-negligible time, which can prolong test execution time.\\nFor those reasons, we will use Testcontainers library only in scenarios when a realistic external structure is really required or when having it significantly simplifies tests (e.g., when mocking\\nexternal infrastructure is complex).\\nWe must reuse containerized infrastructure whenever we can and run as much as possible test suites on the same container instances. It would be inefficient to start new containers, say, for each\\nrelated test class. We can achieve it with the Testcontainers library by creating abstract parent test classes that start up containers via static initializers.\\nOf course, when we already have Testcontainers based tests that spawn relevant infrastructure components, we can reuse it for tests that otherwise will be simple internal application integration\\ntests. Test execution time should not extend significantly.\\nRunning unit tests and internal application component integration tests must be separate from running containerized integration tests. Each category of tests needs to have a different execution\\ncommand. That way, we can trigger containerized tests only when appropriate.\\nAs an example, take a look at `AbstractCommandSideIntegrationSpecification` abstract class, and related test classes `BookingOfferCommandWebControllerIntegrationSpecification` and\\n`AbstractBookingOfferCommandApplicationServiceIntegrationSpecification`.\\n","tokens":367,"id":4900,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"toolkit\/0381-glob-module.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis ADR proposes adding a `glob` function to the toolkit.\\nFirst party actions should have a consistent glob experience.\\nRelated to artifact upload\/download v2.\\n","Decision":"### New module\\nCreate a new module `@actions\/glob` that can be versioned at it's own pace - not tied to `@actions\/io`.\\n### Signature\\n```js\\n\/**\\n* Constructs a globber from patterns\\n*\\n* @param patterns  Patterns separated by newlines\\n* @param options   Glob options\\n*\/\\nexport function create(\\npatterns: string,\\noptions?: GlobOptions\\n): Promise<Globber> {}\\n\/**\\n* Used to match files and directories\\n*\/\\nexport interface Globber {\\n\/**\\n* Returns the search path preceding the first glob segment, from each pattern.\\n* Duplicates and descendants of other paths are filtered out.\\n*\\n* Example 1: The patterns `\/foo\/*` and `\/bar\/*` returns `\/foo` and `\/bar`.\\n*\\n* Example 2: The patterns `\/foo\/*` and `\/foo\/bar\/*` returns `\/foo`.\\n*\/\\ngetSearchPaths(): string[]\\n\/**\\n* Returns files and directories matching the glob patterns.\\n*\\n* Order of the results is not guaranteed.\\n*\/\\nglob(): Promise<string[]>\\n\/**\\n* Returns files and directories matching the glob patterns.\\n*\\n* Order of the results is not guaranteed.\\n*\/\\nglobGenerator(): AsyncGenerator<string, void>\\n}\\n\/**\\n* Options to control globbing behavior\\n*\/\\nexport interface GlobOptions {\\n\/**\\n* Indicates whether to follow symbolic links. Generally should set to false\\n* when deleting files.\\n*\\n* @default true\\n*\/\\nfollowSymbolicLinks?: boolean\\n\/**\\n* Indicates whether directories that match a glob pattern, should implicitly\\n* cause all descendant paths to be matched.\\n*\\n* For example, given the directory `my-dir`, the following glob patterns\\n* would produce the same results: `my-dir\/**`, `my-dir\/`, `my-dir`\\n*\\n* @default true\\n*\/\\nimplicitDescendants?: boolean\\n\/**\\n* Indicates whether broken symbolic should be ignored and omitted from the\\n* result set. Otherwise an error will be thrown.\\n*\\n* @default true\\n*\/\\nomitBrokenSymbolicLinks?: boolean\\n}\\n```\\n### Toolkit usage\\nExample, do not follow symbolic links:\\n```js\\nconst patterns = core.getInput('path')\\nconst globber = glob.create(patterns, {followSymbolicLinks: false})\\nconst files = globber.glob()\\n```\\nExample, iterator:\\n```js\\nconst patterns = core.getInput('path')\\nconst globber = glob.create(patterns)\\nfor await (const file of this.globGenerator()) {\\nconsole.log(file)\\n}\\n```\\n### Action usage\\nActions should follow symbolic links by default.\\nUsers can opt-out.\\nExample:\\n```yaml\\njobs:\\nbuild:\\nsteps:\\n- uses: actions\/upload-artifact@v1\\nwith:\\npath: |\\n**\/*.tar.gz\\n**\/*.pkg\\nfollow-symbolic-links: false    # opt out, should default to true\\n```\\n### HashFiles function\\nHash files should not follow symbolic links by default.\\nUser can opt-in by specifying flag `--follow-symbolic-links`.\\nExample:\\n```yaml\\njobs:\\nbuild:\\nsteps:\\n- uses: actions\/cache@v1\\nwith:\\nhash: ${{ hashFiles('--follow-symbolic-links', '**\/package-lock.json') }}\\n```\\n### Glob behavior\\nPatterns `*`, `?`, `[...]`, `**` (globstar) are supported.\\nWith the following behaviors:\\n- File names that begin with `.` may be included in the results\\n- Case insensitive on Windows\\n- Directory separator `\/` and `\\` both supported on Windows\\nNote:\\n- Refer [here](https:\/\/www.gnu.org\/software\/bash\/manual\/html_node\/Pattern-Matching.html#Pattern-Matching) for more information about Bash glob patterns.\\n- Refer [here](https:\/\/www.gnu.org\/software\/bash\/manual\/html_node\/The-Shopt-Builtin.html) for more information about Bash glob options.\\n### Tilde expansion\\nSupport basic tilde expansion, for current user HOME replacement only.\\nFor example, on macOS:\\n- `~` may expand to `\/Users\/johndoe`\\n- `~\/foo` may expand to `\/Users\/johndoe\/foo`\\nNote:\\n- Refer [here](https:\/\/www.gnu.org\/software\/bash\/manual\/html_node\/Tilde-Expansion.html) for more information about Bash tilde expansion.\\n- All other forms of tilde expansion are not supported.\\n- Use `os.homedir()` to resolve the HOME path\\n### Root and normalize paths\\nAn unrooted pattern will be rooted using the current working directory, prior to searching. Additionally the search path will be normalized prior to searching (relative pathing removed, slashes normalized on Windows, extra slashes removed).\\nThe two side effects are:\\n1. Rooted and normalized paths are always returned\\n2. The pattern `**` will include the working directory in the results\\nThese side effects diverge from Bash behavior. Whereas Bash is designed to be a shell, we are designing an API. This decision is intended to improve predictability of the API results.\\nNote:\\n- In Bash, the results are not rooted when the pattern is relative.\\n- In Bash, the results are not normalized. For example, the results from `.\/*` may look like: `.\/foo .\/bar`\\n- In Bash, the results from the pattern `**` does not include the working directory. However the results from `\/foo\/**` would include the directory `\/foo`. Also the results from `foo\/**` would include the directory `foo`.\\n","tokens":38,"id":4901,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mercury-rust\/0002-choosing-capnproto.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need something better than ProtoBuf\\n","Decision":"We checked the Cap'n'Proto documentation and examples on RPC.\\nSeems to be a perfect match for our needs, so we started integrating it to our SDK for communication between client\/server, replacing the previous ProtoBuf.\\n","tokens":11,"id":4902,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mercury-rust\/0001-choosing-rust.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIoP had a Mercury\/Connect SDK before this, but it was implemented in Java\/C# and it was not maintainable anymore. We had to decide if we want to maintain somehow or\\nrewrite it from scratch.\\n","Decision":"The Rust language was chosen to be used based on it\u2019s almost C level speed and rusts memory safety.\\nThe language also possesses really good bindings. Basically you can bind any code written in C into Rust.\\nWhile Rust is still in its early years, it\u2019s growing steadily, and it also has a good, stable, and growing community.\\n","tokens":48,"id":4903,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mercury-rust\/0003-logging.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIt'd be convenient to have a well-defined logging strategy for our applications. It makes controlling logging easier, and provides post-mortem debugging with useful data.\\n","Decision":"We use the log4rs (https:\/\/github.com\/sfackler\/log4rs) crate for logging purposes.\\n### Loglevels ###\\nWe choose loglevels for messages based on the following criteria:\\n- error!(): When an unexpected condition occurs which likely results in application termination. Usually coupled with an Error struct.\\n- warn!(): When some error happens, which can be likely recovered. Usually coupled with an Error struct.\\n- info!(): Status message about application level progress, important state changes.\\n- debug!(): Additional program state info that can be used for debugging.\\n- trace!(): High frequency or very detailed messages, that helps precisely tracking program execution\/state.\\n### Controlling loglevels from the command line or config file ###\\nThe default loglevel is \"info\", this means that info!(), warn!() and error!() level messages are shown. Loglevel can changed by -v\/--verbose or -s\/--silent command line options.\\n- -s -s -s => loglevel == none\\n- -s -s => loglevel == err\\n- -s => loglevel == warn\\n- -v => loglevel == debug\\n- -v -v => loglevel == trace\\n[Optional proposal]\\nLoglevel can be specified with -l\/--loglevel <LOGLEVEL> command line option (where loglevel can be trace, debug, info, warn, error, none)\\n### Controlling log output ###\\nBy default log output shall be sent to the console and to a file defaulting to a well defined place (e.g.: \/var\/log\/<appname>.log). Two options can be used to specify alternative output:\\n- --syslog: send log output to syslog\\n- --logfile <LOGFILE>: send log output to a file\\nLog rotation should be handled by an external package (logrotate or similiar) if possible.\\n### Printing Error structs ###\\nIt's very important for post-mortem debugging that error messages are logged properly. Usually an error consists of two parts:\\n- message (what went wrong?)\\n- error code (how the system responded?), this is usually having a numeric and a textual description\\nExample:\\nIn case of a network issue we might face connection issues. We should report it as:\\n\"failed to connect to homeserver at xxx.xxx.xxx:xx (111: Connection refused)\"\\nError structs can be stacked. We have to take care to log internal errors as well to see the full error stack.\\n### Log messages ###\\nLog messages should be compact readable and informative. It's hard to give exact rules, but the following advices must be followed.\\n- avoid irrelevant messages (e.g. instead of the message \"program started\", one can print also important arguments, program version, ... too)\\n- for error messages always provide enough context for later analysis (important state variables)\\n- avoid dumping long binary data, public keys, memory garbage. If binary content should be tracked, consider logging some hash of the content.\\n- messages which are issued from tight loops with big frequency, should always happen on the trace!() loglevel to avoid flooding the log output\\n- avoid writing multiple sentences (instead use single sentence without capitalization), that helps log processing with scripts\\n- types that are usually written into logs shall implement the Display trait, to provide compact and informative output\\n","tokens":35,"id":4904,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mercury-rust\/0004-error-handling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSince some of our applications are expected to operate in an error resistant way, it's important to lay out how we detect and manage unexpected situations. The prinicples described in this article are just partially Rust-specific, some of them applies to the design or testing activity space.\\n","Decision":"Principles described below should be followed by every contributor. Existing code should be refactored ASAP.\\n","tokens":60,"id":4905,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0004-layered-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nWe are going to use Layered Architecture to help us solve those problems. In a nutshell, Layered Architecture breaks our system into four layers:\\n1. **Presentation:** This layer is responsible for showing the user all the elements for interaction.\\n2. **Application:** This layer is responsible for handling user interactions and processing them accordingly.\\n3. **Domain:** This layer is accountable for your core business. It contains all business logic to control your entities and required resources.\\n4. **Infrastructure:** As the name suggests, this layer is responsible for handling your infrastructure: interacting with databases, handling memory, and so forth.\\nThis pattern is like a Russian doll. If you drill-down inside a microservice from the presentation layer, it can have their Presentation, Application, Domain, and Infrastructure likewise.\\nA layer can also only talk with the layer below it (except for the Infrastructure layer). For example, the Presentation layer can only interact with the Application layer and the Infrastructure layer. And so forth.\\n![Diagram explaining the Layered Architecture hierarchy](..\/assets\/0004-layered-architecture\/hierarchy-diagram.png)\\n","tokens":34,"id":4906,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0010-licensing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nAfter further research, we\u2019ve decided to use [Apache License 2.0](https:\/\/www.apache.org\/licenses\/LICENSE-2.0) based on the following assumptions:\\n* The source code doesn\u2019t need to be public when a distribution of the software is made.\\n* Modifications to the software can be released under any license.\\n* Changes made to the source code must be documented.\\n* It offers the same patent usage protection as GPLv3.\\n* It explicitly prohibits the use of trademarked names found in the project.\\n","tokens":45,"id":4907,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0014-reducing-initial-technical-complexity.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nTaking a look at our decisions and considering what we've developed so far (mainly our design system), the major offenders are:\\n* **Decoupled design system:** Since we have a single presentation application, there is no reason to code a stable and robust design system.\\n* **Using ThemeUI:** A more opinionated framework would provide more tools to reduce our time coding simple components.\\n* **Decoupled Application and Domain layers:** Since we don't have any users, scalability and reliability are not our primary concerns.\\n* **CQRS+ES:** Although relevant, this structure adds enormous complexity to our back-end, increasing the required time to develop our systems.\\n* **Test and behavior-driven development:** Unit and acceptance tests are essential tools to reduce side-effects while refactoring or coding new features. But, they increase the development time and can slow our pace.\\n* **Stack:** Some tools that we've decided to use are new for our team. We must reduce our learning curve by using familiar and straightforward languages and frameworks.\\nBased on those offenders, here is a list of actions that we're going to take to reduce our technical complexity:\\n1. We're going to reject [ADR#0005](005-cqrs+es.md). Our first version will **not** be event-sourced, but we can add it in future releases.\\n2. We're going to submit an update in [ADR#0013](\/0013-microservices-overview.md) to merge our application and domain applications layers into a **business** application that crosses both layers. That is not optimal and will create legacy code in the future, but will increase our development speed considerably.\\n3. We're going to remove our design-system application in [ADR#0013](0013-microservices-overview.md) and archive the [design-system repository](https:\/\/github.com\/budproj\/design-system) until further notice. Our first release will have only a single presentation application, so there is no sense in having a robust and complex design-system.\\n4. We're going to deprecate [the entire design-system ADL](https:\/\/github.com\/delucca\/design-system\/tree\/feature\/design-system-structure\/docs\/adl). Since we're not using a design-system anymore, there is no sense in maintaining them.\\n5. We're going to move from ThemeUI to MaterialUI since our team already has good knowledge of it, which can boost our development speed.\\n6. We're going to reject both [ADR#0006](006-test-driven-development.md) and [ADR#0007](007-behavior-driven-development.md). TDD and BDD are great, and we're going to use them shortly, but they would add little benefit to our first version.\\n7. We're going to submit a change request to [ADR#0011](011-stack.md), moving from an optimal stack to a more MVP-like stack.\\n","tokens":34,"id":4908,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0008-development-workflow.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nWe are going to use three main patterns for our development process:\\n* [Forking](https:\/\/docs.github.com\/en\/free-pro-team@latest\/github\/getting-started-with-github\/fork-a-repo)\\n* [Gitflow](https:\/\/www.atlassian.com\/git\/tutorials\/comparing-workflows\/gitflow-workflow)\\n* [Commit](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/)\\nThe first one relates to **how we host our development code**. Based on this ADR, you should not create branches in the `origin` remote. Instead, create your fork and make your branches there. This decision avoids many unwanted side effects and accidents.\\nYou can create branches at `origin` if those branches are long-lived and relate to a newer version or a massive work from multiple developers. There are some default branches all our repositories might have:\\n* `main` (default branch)\\n* `develop` (next stable release branch)\\n* `test` (working environment for testing purposes)\\nThe second one relates to naming and Git workflow. Our branches naming should follow Gitflow standards, like: `feature\/<sub-path>\/<name>`, and others.\\nAlso, the `develop` branch must remain stable, receiving only new finished features in it.\\nThe third one relates to linting our commits. We should learn how to write useful commit messages, using that convention, to improve our developer experience.\\n","tokens":45,"id":4911,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0012-linting.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nWe've decided to use a linter to avoid this problem, no matter the project size. Each language has its way of linting code, but every stack has at least a single tool to solve this problem.\\nAlso, alongside our linters, we're going to use [editorconfig](https:\/\/editorconfig.org\/), a great tool to standardize how we configure our text editors.\\nSome linters can extend properties. If your language linter can do so, we suggest creating a company-wide configuration and extend it in your application.\\nBelow, we've listed the linters we're using for each language:\\n* **Javascript (React, Node, Typescript, ...):** [ESLint](https:\/\/eslint.org\/) + [Prettier](https:\/\/prettier.io)\\n","tokens":34,"id":4912,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0009-open-source.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nBased on the stated context, we've decided to go open-source for our entire codebase. Every new application our developers think must consider it's community, collaborations, and possible improvements. By doing so, we increase our ecosystem coverage, security, and allow modifications and customizations by our customers.\\n","tokens":34,"id":4913,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0011-stack.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Updates](#updates)\\nBelow, I've listed the current stack of our company. We've grouped those into layers, each one being responsible for a different part of our company:\\n### Presentation Layer\\nThis layer is responsible for presenting interfaces for our customers' interaction. We currently don't have any machine interface, since most of our applications are web or mobile apps.\\nBased on that, here is the stack definition for our presentation layer:\\n* **Programming language:** Javascript\\n* **Runtime:** NodeJS\\n* **Superset:** Typescript\\n* **Framework:** NextJS\\n* **Interface Library:** React\\n* **State management:** Recoil\\n* **Design System:** Custom, based in Material-UI\\n### Business Layer (Application + Domain)\\nThis layer merges our Application and Domain layers. It is responsible for our API and domain execution. It controls and orchestrates our presentation layer and external services. It must be a easy to use, scalable, lightweight structure since our presentation layer would rely on it for processing.\\nBased on that, here is what we've chosen as our stack:\\n* **Language:** Javascript\\n* **Runtime:** NodeJS\\n* **Superset:** Typescript\\n* **Framework:** Koa\\n### Infrastructure Layer\\nAlthough most definitions for this layer resides inside the platform's repositories, some are meaningful enough to mention in this ADR:\\n* **Relational Database:** PostegreSQL\\nAs I've said before, this stack is flexible, but you should have a good reason to avoid it.\\n","tokens":42,"id":4914,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0003-domain-driven-design.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nTo prevent these issues, we decided to implement the Design-driven Development framework to architect our domain collaboratively with the product team and business owners.\\nWith DDD, we can ensure proper refactors in the future, having a clear overview of our entities' relationships and domains. To learn more about DDD, [check this summary](https:\/\/medium.com\/@ruxijitianu\/summary-of-the-domain-driven-design-concepts-9dd1a6f90091).\\nWe can't have a clear, detailed overview of our entire domain structure (that would be overwhelming), but we've organized our DDD in a way that we can have a high-level concept of the whole company and drill-down to see a more detailed view of each bounded context.\\n**The high-level concept of our Domain-Driven Design** is located in this repository, at [ADR#0015](0015-model-overview.md). You can take a look at that record to learn more about our architecture overview.\\n**Bounded context detailed domains** are located in the same folder (`records\/domains`), but there is a single file for each bounded context. Those architectures focus only on the given bounded context and any entity that relates to it.\\nEach microservice we have are related to a single bounded context. So, to create new microservices, you must add a new bounded context to our domain architecture.\\n","tokens":45,"id":4915,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0015-model-overview.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.- [Context](#context)\\n- [Decision](#decision)\\n- [Status](#status)\\n- [More reading](#more-reading)\\n","Decision":"- [Status](#status)\\n- [More reading](#more-reading)\\nTo reduce that gap, we've decided to use the Domain-Driven Design technique (as you can see in [ADR#0003](0003-domain-driven-design.md)).\\nWe've divided our domain architecture into two scopes:\\n1. **Model Overview:** As you can see here, an overview of our architecture, considering only the most relevant entities.\\n2. **Local Domain Overview:** Every other file in this `domain` folder. They're only considering their domain, with a more detailed view and exploring their inner objects and considering only external entities that affect them.\\n![Snapshot of last know state of our domain model 2021-05-04](..\/assets\/0015-model-overview\/2021-05-04-diagram.jpg)\\n","tokens":35,"id":4916,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0006-test-driven-development.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to start using the Test-Driven Development technique. This process creates a simple way to ensure quality while it improves the reliability of our applications.\\nInstead of coding your feature immediately, you must follow this Test-Driven Development cycle:\\n1. You code a test that fails (testing the non-existent feature).\\n2. You develop that feature.\\n3. You make the test pass.\\n4. You refactor that feature to improve it without breaking the test.\\nIt is a simple, yet powerful cycle.\\nWe can develop multiple types of test, but below we've listed the most useful for us:\\n* **End-to-end tests:** Tests the entire system, integrating every required dependency to make a trial close to the production environment.\\n* **Acceptance tests:** Tests a feature of a system, mocking every external application dependency.\\n* **Contrac tests:** Tests the contract between two (or more) integrated microservices.\\n* **Unit tests:** Tests the behavior of a given function, mocking all file dependencies.\\nIn this decision, we've decided to use at least some form of testing in all applications.\\n","tokens":53,"id":4917,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"architecture-decision-log\/0005-cqrs+es.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nCommand Query Responsibility Segregation and Event-Driven Architecture (CQRS+ES) solves those problems with ease. When we took a look, especially at [Martin's Event-Sourcing technique](https:\/\/martinfowler.com\/articles\/201701-event-driven.html), we understood that we could create an event-driven application that can react only to events.\\nThe idea is simple: since we're dealing with compliance and audit departments, we can see **every** action in the platform as **commands** and dispatch **events** based on the result of those commands. That way, we can restore **any** entity from scratch. Even if we suffer from a mass failure in all our servers in this architecture, we could still reconstruct every entity from their event store.\\nThis pattern adds an extra layer of complexity: reading data. If we try to build our entities from scratch at every query, we would have a poor performance. That's where CQRS (Comand Query Responsibility Segregation) comes in handy.\\nCQRS segregates (as the name suggests) the **command** and **query** stacks. Before digging into the concept, you must understand the definition of those two concepts:\\n* **Command:** is an intention, an action. Always imperative, like: `RaiseSalary`. Our domain layer receives commands to execute side effects based on those.\\n* **Event:** is the result of a command. Always in the past, like: `SalaryRaised`. Our domain dispatches events based on resulting side-effects.\\nIn a nutshell, the CQRS architecture focuses our domain layer in handling commands (received from a message broker queue). In contrast, our application layer can return our entities' last know state by accessing the database directly. Like the following diagram:\\n![Diagram explaining the CQRS Layered Architecture hierarchy](..\/assets\/0005-cqrs+es\/cqrs-layered-architecture.png)\\nOur domain layer will not have any updated copy of our entities. Executing all actions again could cost a lot in compute time, so we're going to create some snapshots to consolidate our entities' state in time. This strategy is a pretty common CQRS standard, and most databases support it.\\nThe last concept we must cover is the sync between our command and query stacks. In our architecture, we will do it using a consumer that will consume dispatched events and update our query database accordingly.\\n**IMPORTANT:** This architecture is not mandatory across all bounded contexts. Some of those may not have any benefits from event-sourcing at all. Those could use a more standard approach instead of the suggested in this ADR.\\n","tokens":53,"id":4920,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"time-tracker\/002-use-cljs-for-web-ui.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe web frontend of time-tracker is crucial, since it is the only way to access certain important features such as user management and invoicing. It will be built as a single-page application. Web frontend development is complex nowadays, and choosing the right stack can help mitigate this complexity and speed up the development process. We are evaluating Typescript and ClojureScript.\\nTypescript is widely used and has a lot of mindshare. As such, setting up a Typescript project and making use of the Typescript\/Javascript ecosystem is straightforward.\\nClojureScript offers a number of advantages over Typescript, such as programming in a functional-first style with immutable data structures, LISP syntax, seamless live code reloading, a fast REPL-driven workflow and generally less code and boilerplate. In addition to this, the [`re-frame`](http:\/\/day8.github.io\/re-frame\/) framework is a well thought-out approach to state management. It manages state and side effects very well and with much less boilerplate compared to popular JS\/TS equivalents such as Redux. All programmers at nilenso are familiar with Clojure (if not ClojureScript already), and learning ClojureScript and getting productive shouldn't be an issue for them.\\nOne potential concern with ClojureScript is easy access to the JavaScript\/NPM ecosystem of libraries and tooling features. This concern is addressed by `shadow-cljs`, a popular build tool for ClojureScript which (among other things) makes access to NPM packages as frictionless as possible.\\nSome features readily available in JS\/TS such as CSS modules are tricky to set up or unusable with ClojureScript.\\n","Decision":"1. We will use [ClojureScript](https:\/\/clojurescript.org\/) as the programming language for the web frontend.\\n2. We will use [re-frame](http:\/\/day8.github.io\/re-frame\/) as the main application framework.\\n3. We will use [shadow-cljs](http:\/\/shadow-cljs.org\/) as the build tool for ClojureScript. For other assets such as CSS, we will use other build tooling as necessary.\\n","tokens":336,"id":4921,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"feathers\/002-personal-assistant.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nSooner or later, experienced developers find themselves carrying a toolkit from job to job, even job to home. This toolkit might contain: favorite shell configs, aliases, color schemes, keymappings, and so on. They may have also developed some sort of \"launcher\" that quickly opens up logs, dashboards, and frequently accessed pages and documents, in 2-3 keystrokes. Some go as far as integrating mouse \/ keyboard automations, even pixel-scanning to help those automations.\\nI've personally known several such developers (VM, AP, TB, EO) besides myself, and having listened to their process and demonstrations, I'm certain about one thing: Toolkits are not for sharing.\\nThe moment someone even thinks about having a toolkit, they've desired something truly for their self.\\n_TODO_\\n### Approaches\\n#### 1. TODO\\nTODO\\n#### 2. TODO\\nTODO\\n#### 3. TODO\\nTODO\\n### Decision\\nTODO\\n### Accepted Tradeoffs\\nTODO\\n### Retrospective\\nTODO\\n","Decision":"TODO\\n### Accepted Tradeoffs\\nTODO\\n### Retrospective\\nTODO\\n","tokens":217,"id":4922,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"feathers\/003-tracking-external-files-revisited.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nThe strategy accepted in [ADR-001](https:\/\/github.com\/slackwing\/feathers\/blob\/master\/adr\/001-tracking-external-files.md) is difficult to recall, therefore difficult to be confide in. An alternative is proposed.\\n### Approaches\\n#### 1. Document logical changes\\nThe main kind of external files we want to track are configuration files, such as .zshrc and .vimrc. For these, it would be better to document logical changes, _e.g._ as steps in [tooling](https:\/\/github.com\/slackwing\/feathers\/tree\/master\/tooling), for at least 3 advantages: (1) logical changes are easier to comprehend than a raw configuration; (2) steps are pickable; and (3) layering configurations in steps is more robust, especially across different systems, than dropping them all in at once.\\n#### 2. Track snapshots\\nHowever, documenting logical changes comes with the overhead of having to prepare and finalize configurations for commiting. This in turn carries the risk of piling up uncommitted configurations. We can balance things by committing snapshots of raw configurations, _e.g._ in [tooling\/snapshots](https:\/\/github.com\/slackwing\/feathers\/tree\/master\/tooling\/snapshots).\\n#### 3. Other external files\\nCurrently there are no other kinds of external files we want to track. Disregarding.\\n### Decision\\nAdopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n","Decision":"Adopting #1 and #2.\\n### Accepted Tradeoffs\\nNone known.\\n### Retrospective\\n_TBD_\\n","tokens":325,"id":4923,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"road-registry\/004-temporary-and-permanent-identifiers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRoad nodes, road segments and grade separated junctions are brought into existence by an operator.\\nIn doing so, an operator assigns (often thru tooling) an identifier in his \/ her local environment for a newly created entity.\\n","Decision":"We call these identifiers created by the operator temporary identifiers. The tooling usually creates them as rather big numbers as to not collide with the already allocated identifiers of existing entities.\\nIt's important to note that the notion of a temporary identifier only exists when adding a type of entity.\\nWhile changing the road network we assign a permanent identifier for each temporary identifier. This is usually the next free number. This happens for each of the type of entities separately (nodes, segments, grade separated junctions).\\nWe remember the mapping between the temporary and the permanent identifier by recording it in the events that add the specific entity type.\\nWe give UI feedback to the operator about a particular upload using the temporary identifiers since those are the only ones that make sense to them.\\nOnce they re-download the road registry they will start using the permanent identifiers.\\n","tokens":50,"id":4927,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"road-registry\/005-etl.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe road registry existed before this version of the software that operates on it. We can't expect operators to start from scratch and redraw every node and segment.\\n","Decision":"We extract the necessary road registry data, in the form of events, from its _legacy_ database, using a tool aptly named `RoadRegistry.Legacy.Extract`. This tools takes a connection string to the legacy database and transforms some of the tables and rows into a very long list of events, each time noting which stream the event will need to go into as well as the event itself (effectively a `(stream, event)` tuple), putting those into a file called `streams.json` (looks like a very big json array) and compresses that file in turn into a `import-streams.zip` file. This file can then be uploaded to an appropriate AWS S3 Bucket (see the AWS Infrastructure repository to determine which one it is in production), either via the tool itself (if you configure it accordingly) or via the AWS CLI.\\nThe reason for disconnecting the extraction and loading from each other via a file is because the legacy database is usually running on premises or in a different cloud (Azure, in case of the legacy road registry) or a developer's machine, but not in the AWS environment. To avoid putting in extra infrastructure for what is essentially going to be a once in a lifetime job, putting AWS S3 between the extraction and loading seemed like a reasonable trade off.\\nThe loading of the extracted events is done via a tool named `RoadRegistry.Legacy.Import`, which takes the uploaded file from an AWS S3 Bucket and a connection string to the road registry event store (probably an RDS instance running in AWS) and appends the events contained in the uploaded file to their respective streams. The order in which the `(stream, event)` tuples appear in the file is the order in which they will be appended to the mentioned stream.\\n","tokens":35,"id":4928,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"road-registry\/003-reusable-dynamic-attribute-identifiers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nLanes, Widths, Surfaces are dynamic attributes which might change both in number and attributes over the life time of a road segment.\\nEach lane, width, surface is identified using an attribute identifier.\\nHanding out a new attribute identifier upon each change could cause us to exhaust the attribute identifier space rapidly.\\n","Decision":"We keep track of all the attribute identifiers we have handed out for a road segment.\\nWe do this for Lanes, Widths and Surfaces separately.\\nUpon each change in the number of dynamic attributes we try to exhaust the previously used identifiers first, that is we reuse them.\\nIf additional attribute identifiers are needed we allocate them, continuing with the next free attribute identifier across road segments.\\nAs soon as an identifier is allocated, it becomes dedicated to the road segment.\\nIt remains so even after a road segment was removed.\\n","tokens":68,"id":4929,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"road-registry\/001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n### Changes to Nygard's approach\\nWe will keep ADRs in the project repository under `docs\/adr\/NNN-explanation-of-adr.md`.\\n","tokens":16,"id":4932,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/005. Serverless-offline.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI can't be doing a deployment to CloudFormation every time I want to test a new line of code. Serverless-offline is a plugin that allows local emulation of AWS Lambda and API Gateway. By running it via node --debug, you can [use it as a debugger from VS Code](https:\/\/medium.com\/@OneMuppet_\/debugging-lambada-functions-locally-in-vscode-with-actual-break-points-deee6235f590).\\n","Decision":"I will use the serverless-offline plugin to run a local copy of my Serverless stack.\\n","tokens":97,"id":4933,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/002. AWS.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI built the site as a front-end learning experience, using only browser local storage for persisting data. What the app is most missing is permanent data storage. For that it will need a hosted web service beyond GitHub pages.\\nSo, now it's time to figure out cloud service architecture. I've inherited an AWS project elsewhere that isn't in the greatest shape, and my team has ideas about how to do better, and I'd like to try some of those concepts out here on my personal project.\\n","Decision":"I will build a backend system on AWS.\\n","tokens":106,"id":4934,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/007. Terraform.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nServerless has the problem of narrowing available functionality from CloudFormation, which itself narrows functionality from AWS. For instance, CloudFormation can create stacks no larger than 200 resources, and Serverless wants to put everything in one CloudFormation stack. In short, Serverless is good for a basic API, but it's not very appropriate for other cloud resources.\\nIn my research I kept running across Terraform. It is a less magical, more granular tool. It's open source and has very broad support for provisioning AWS resources and other services. It was called out in the book Infrastructure as Code several years ago, and it has had all that time to mature.\\n","Decision":"I will use Terraform for infrastructure definition and deployment, not Serverless.\\n","tokens":139,"id":4935,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/006. Cognito.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe backend has Lambda functions run by an API gated with an API key and a usage plan. It is currently reading sample data from a Dynamo table. I have made progress toward storing data in AWS from the client.\\nNext, each user needs to its own pool of data. That means I need a notion of users. I need some piece of the backend to authenticate someone and vend a token that they can send to the API to get access to a bucket of data.\\nMy ideal is to sign in with GitHub, Google, or some other identity provider and get a token. AWS has a service called Cognito that does a few things, and I don't necessarily understand all of those things, but I think it does the thing I want.\\n","Decision":"I will try to use Cognito to authenticate users.\\n","tokens":155,"id":4936,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/008. Artifact-based deployment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt work we've been talking about ideal continuous delivery setups, and a goal we have is to separate our build and deployment processes into discrete steps. Now that I'm using Terraform and I'm looking carefully at how Lambda functions get their code, it's obvious that pointing Lambda at build artifacts in S3 is a good way to do that. So, I should have a build-package-archive process for the API (and in the future for other components), and a plan-verify-deploy process for Terraform.\\n","Decision":"I will archive artifacts as a build step, and refer to them in deployment configuration.\\n","tokens":107,"id":4938,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/004. Docker.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSo I'm arranging the project, and I'm trying to think about how to make a development environment include the right versions of aws-cli, Serverless, and other tools. Docker is the way I've done that recently, so let's try it.\\nIf I make Lambda functions, for instance, I can use a Docker container to create a local execution environment running AWS's same Python or Node version, without interfering with my own system's configuration.\\nIf I make a CI\/CD pipeline, the same Docker container can be used to perform tests and deployments.\\n","Decision":"I will use Docker to create an environment for working on the cloud provisioning project.\\n","tokens":118,"id":4939,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0010-use-nginx-for-simple-https-proxies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nWe currently use Apache on all our web hosts. While writing a profile\\nfor forwarding HTTPS to a local HTTP port, I found that the puppet\\nApache module didn't provide a simple approach to this. I would have had\\nto use `mod_proxy` and then add custom fragments.\\nOn the other hand, if I went with nginx instead of Apache, such a proxy\\ncould be configured with a single line:\\nproxy => \"http:\/\/localhost:${port}\",\\nThis carries with it a cost however. If we use nginx in this one case,\\nthen we can no longer claim to use a single web server, as we'll be\\nusing Apache on some servers and nginx on others.\\nDecision\\n--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n","Decision":"--------\\nWe will use nginx for simple cases where all we need is to forward to a\\nlocal port.\\nConsequences\\n------------\\nForwarding https traffic to a local port is very easy. We now use both\\nnginx and Apache.\\n","tokens":205,"id":4941,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0004-roll-our-own-kubernetes-profile.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nPuppetlabs maintains [a kubernetes module][1] in the forge, and Rancher\\nmaintains software for managing multiple clusters running anywhere.\\nThe puppetlabs module claims to support Debian, but, in practice, it\\nappears to only support Ubuntu. We tried forking it and making changes\\nto get it working, but it was a rabbithole of error after error. Also,\\ntheir solution to managing SSL keys was to generate them locally and\\nstore them all in hiera, which was cumbersome. They also didn't support\\nmanaging the CIDRs for the internal network.\\nRancher seemed very nice at first, but it was unstable in practice.\\nDespite running a highly available set of control nodes, bringing one\\ndown did in fact break the cluster. Also, if \/var\/lib\/docker filled up\\non any machine, the only solution was to destroy and recreate the entire\\ncluster.\\nMy assessment of the puppetlabs module is that it tries to do too much\\non its own. My assessment of rancher is that it's not yet stable enough\\nto rely on.\\n[1]: https:\/\/forge.puppet.com\/puppetlabs\/kubernetes\\nDecision\\n--------\\nWe will roll our own kubernetes profiles instead of relying on someone\\nelse's solution.\\nConsequences\\n------------\\nWhile our profiles are sufficient for creating an environment where\\nkubernetes can flourish, bootstrapping must be done outside of puppet,\\nby hand. It's not ideal, but it also shouldn't come up very often.\\nWe haven't yet tried upgrading docker or kubernetes on an existing\\ncluster, but it's easy to imagine that being tricky if we have to do it\\nby hand.\\nIt's worth checking back on rancher in a year or so to see if they've\\nimproved.\\n","Decision":"--------\\nWe will roll our own kubernetes profiles instead of relying on someone\\nelse's solution.\\nConsequences\\n------------\\nWhile our profiles are sufficient for creating an environment where\\nkubernetes can flourish, bootstrapping must be done outside of puppet,\\nby hand. It's not ideal, but it also shouldn't come up very often.\\nWe haven't yet tried upgrading docker or kubernetes on an existing\\ncluster, but it's easy to imagine that being tricky if we have to do it\\nby hand.\\nIt's worth checking back on rancher in a year or so to see if they've\\nimproved.\\n","tokens":385,"id":4942,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0009-manage-prometheus-node-exporter-package-ourselves.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nThe Prometheus model is that nodes, applications, and services expose\\nmetrics, and a Prometheus server scrapes them every 10 seconds. It's up\\nto the Prometheus server to organize metrics, track them over time, and\\nstore them long-term.\\nHowever, it's up to each node to decide exactly what metrics it exposes.\\nFor example, the node exporter (which exports general system metrics)\\nhas changed the names of its metrics as it's changed versions. It's in\\nthe apt repos for Debian and Ubuntu, but with different versions for\\ndifferent releases.\\nSo if we stick with Debian\/Ubuntu-maintained packages, we might end up\\nwith jessie machines running v0.11.2, stretch running v0.13.1, buster\\nrunning v0.16.0, and bionic running v0.18.1. In this case, we'd be using\\none aggregation server to scrape four different types of metrics all\\nbaring the same name.\\nNot only would this be cumbersome to aggregate, but also if someone at\\nDebian or Ubuntu did upgrade the exporter in a repo, then all time\\nserieses for computers using that repo would essentially be reset.\\nDecision\\n--------\\nWe will manage our own version of the Prometheus node exporter in our\\napt repository. Rather than mimic the existing Debian package, it will\\nonly install the binary. Puppet will manage the users, groups,\\nservices, and files.\\nConsequences\\n------------\\nWe won't be able to depend on `apt-get dist-upgrade` for fixing security\\nvulnerabilities if any are found in the node exporter. We'll need to\\nregenerate our debs with patched versions when they come out.\\n","Decision":"--------\\nWe will manage our own version of the Prometheus node exporter in our\\napt repository. Rather than mimic the existing Debian package, it will\\nonly install the binary. Puppet will manage the users, groups,\\nservices, and files.\\nConsequences\\n------------\\nWe won't be able to depend on `apt-get dist-upgrade` for fixing security\\nvulnerabilities if any are found in the node exporter. We'll need to\\nregenerate our debs with patched versions when they come out.\\n","tokens":367,"id":4943,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0011-prefer-apt-to-cpan.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nWe have a number of applications and servers that need numerous Perl libraries\\ninstalled. We have historically managed the libraries at a system level and\\ncoordinated versions with application teams. While there are now compelling\\noptions like [Carton](https:\/\/metacpan.org\/pod\/Carton), which allow an\\nappliction to manage and isolate its dependencies, most of our apps would\\nrequire changes to be ready to use something like this.\\nTo aid in keeping the versions stable and ensuring that underlying system\\nlibraries like OpenSSL are in sync, Debian packages many CPAN libraries at\\nspecific versions for a release. These are still at a system level, but we\\nhave an already-running process that would catch updates in the case of a\\nmajor bug fix or security release.\\nIn service of completing OS and hardware upgrades, we are seeking primarily to\\nidentify the dependencies and to be able to reproduce our application\\nenvironments. In the long term, applications are likely to be containerized or\\notherwise placed under PSGI\/Plack, where many of these concerns could be\\nisolated more readily. This decision is a record of current realities, rather\\nthan a long-term position.\\nDecision\\n--------\\nWhere possible, when installing Perl libraries at the system level, we will\\nuse the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\n","Decision":"--------\\nWhere possible, when installing Perl libraries at the system level, we will\\nuse the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\n","tokens":419,"id":4944,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0003-run-kubernetes-locally.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nThere are many cloud kubernetes offerings (Amazon, Microsoft, and Google\\nall offer it), and part of the appeal in general is to not have to think\\nabout where the servers are and what kind of maintenance they need.\\nHowever, we already operate servers in three datacenters, and we fully\\nmanage one of those. We may prefer this not to be the case, but as of\\nwriting this, we deal with bare-metal servers regardless of what we'd\\nprefer. Adding ec2 instances costs extra money, where running additional\\nsoftware on servers (with an electric bill we're already paying) costs\\nthe labor of setting it up.\\nThe attitude of the field so far appears to be \"definitely just run it\\nin AKS,\" but the field is largely made up of people whose primary\\ninterests are reliability and profits. As a digital academic library, we\\nalso have those interests, but we have an additional responsibility to\\nalways have our own copy of everything.\\nDecision\\n--------\\nWe will provide kubernetes clusters at our datacenters with the\\npossibility of expanding outward into cloud providers as needed.\\nConsequences\\n------------\\nBy providing a service that mirrors a cloud service, we can use the same\\ncode and configuration to deploy locally and remotely. We'll need to\\nfigure out how we want to manage applications and how we'll want them to\\nscale depending on where they live. Since developers will essentially be\\ndeploying to three different clusters, we'll need to decide how they\\nshould do that.\\n","Decision":"--------\\nWe will provide kubernetes clusters at our datacenters with the\\npossibility of expanding outward into cloud providers as needed.\\nConsequences\\n------------\\nBy providing a service that mirrors a cloud service, we can use the same\\ncode and configuration to deploy locally and remotely. We'll need to\\nfigure out how we want to manage applications and how we'll want them to\\nscale depending on where they live. Since developers will essentially be\\ndeploying to three different clusters, we'll need to decide how they\\nshould do that.\\n","tokens":325,"id":4945,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0005-stack-k8s-controllers-with-etcd.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nThere are three basic ways to lay out controller nodes in a kubernetes\\ncluster:\\n1.  A single master is the easiest way to do it, but if that node is\\never unavailable, then the entire cluster is unavailable.\\n2.  A highly-available group of at least three controller nodes that\\nalso run etcd.\\n3.  Two highly-available groups: one of at least three controller nodes,\\nand the other of at least three etcd nodes.\\nThe kubernetes website has [a detailed rundown][1], but it boils down to\\nthis: high availability complicates the set-up significantly, and\\nseparating etcd from the control plane complicates it yet more.\\nHowever, if we want to allocate a lot of resources to high availability\\nin our kubernetes clusters, then etcd is a nice seam to further divide\\nresponsibility as we scale up the count of control nodes.\\n[1]: https:\/\/kubernetes.io\/docs\/setup\/independent\/ha-topology\/\\nDecision\\n--------\\nWe will start with the middle path of minimal (but extant) high\\navailability. Low availability would fail to grant us a lot of what\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\nready to commit the extra resources to get the highest availability\\npossible.\\nConsequences\\n------------\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\nwe should consider revisiting this decision.\\n","Decision":"--------\\nWe will start with the middle path of minimal (but extant) high\\navailability. Low availability would fail to grant us a lot of what\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\nready to commit the extra resources to get the highest availability\\npossible.\\nConsequences\\n------------\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\nwe should consider revisiting this decision.\\n","tokens":311,"id":4947,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0002-use-systemd-as-cgroup-driver-for-docker.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nWhen I first installed kubernetes with nothing but default settings, I\\ngot this warning:\\n```\\n[WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https:\/\/kubernetes.io\/docs\/setup\/cri\/\\n```\\nTurns out, they don't recommend using docker's default cgroup driver in\\ntandem with systemd long-term.\\nDecision\\n--------\\nSo I'm doing as the kubernetes website recommends, because they seem\\ntrustworthy.\\nConsequences\\n------------\\nThat warning message no longer appears.\\n","Decision":"--------\\nSo I'm doing as the kubernetes website recommends, because they seem\\ntrustworthy.\\nConsequences\\n------------\\nThat warning message no longer appears.\\n","tokens":141,"id":4949,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0007-only-manage-certain-netfilter-chains-in-kubernetes-machines.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nWhen using the puppetlabs firewall module, one of the recommended\\nsettings is to purge all rules not managed by puppet:\\n```puppet\\nresources { 'firewall':\\npurge => true,\\n}\\n```\\nSince kubernetes and calico perform networking and load balancing at the\\nkernel level, this means puppet finds itself with hundreds of rules to\\npurge every 30 minutes.\\nThe firewall module also has an undocumented feature that lets you\\nmanage individual netfilter chains:\\n```puppet\\n# This will remove all `-A INPUT` rules unless they're either (a)\\n# managed by puppet or (b) contain `--comment \"you can trust me ;)\"`.\\nfirewallchain { 'INPUT:filter:IPv4':\\nensure => 'present',\\npurge  => true,\\nignore => [\\n'--comment \"you can trust me ;)\"',\\n],\\n}\\n```\\nThis doesn't work in conjunction with `purge => true` for all firewall\\nresources. If we use `firewallchain` resources instead, then any chains\\nwe don't explicitly define will be ignored by puppet. So, for example,\\nif we define only `INPUT:filter:IPv4`, then any existing `OUTPUT` rules\\nwill be ignored rather than purged by puppet.\\nDecision\\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we'll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\n","Decision":"--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we'll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\n","tokens":452,"id":4950,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"nebula\/0006-use-calico-for-kubernetes-networking.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context\\n-------\\nKubernetes requires an internal private network, just for its pods and\\nservices (i.e. not for physical\/virtual nodes). It has specific\\nexpectations, but it doesn't actually provide networking itself. There\\nare a few options for internal networking: flannel, calico, canal, and\\nweave.\\nOf these, Flannel is the most popular. It's known for being easy to set\\nup and then never having problems ever. The network it creates is a\\nlayer 3 IPv4 overlay network that spans across every node in the\\ncluster. Each node gets its own subnet for allocating internal IP\\naddresses for docker containers.\\nProject Calico is also popular, but is less simple and (allegedly) more\\nperformant. The network it creates isn't an overlay but rather a layer 3\\nnetwork using the BGP protocol to route packets between hosts.\\nCanal is a combination of flannel and calico. It's not a project anymore\\nbecause it collaborated itself out of existence by making pull requests\\nto flannel and calico until they worked well enough together that there\\nwas nothing extra for canal to do. People still use the word \"canal\" to\\nrefer to using a combination of flannel and calico.\\nWeave Net creates a mesh overlay network between each of the nodes in\\nthe cluster. Each host becomes a router, and they're always exchanging\\ntopology information with each other. One of weave's unique features is\\nthat it can (with some network overhead) encrypt all routed traffic.\\nThe rancher project wrote [a pretty informative comparison][1] if you're\\ninterested in more details.\\n[1]: https:\/\/rancher.com\/blog\/2019\/2019-03-21-comparing-kubernetes-cni-providers-flannel-calico-canal-and-weave\/\\nDecision\\n--------\\nWe will use calico because it's rancher's default. It's easy enough to\\nset up.\\nConsequences\\n------------\\nAs of yet unknown.\\n","Decision":"--------\\nWe will use calico because it's rancher's default. It's easy enough to\\nset up.\\nConsequences\\n------------\\nAs of yet unknown.\\n","tokens":430,"id":4951,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0001-mvp-webidl.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWhen [deciding to build this tool](.\/0000-whats-the-big-idea.md), the main risk identified was that we'd spend too\\nmuch time on an ultimately unworkable or unmaintainable idea. What early design decisions can we make to mitigate\\nthis risk? What things are an existential risk to the success of this project that must be included in the first\\nversion, and what things can we safely defer to future work?\\nIn other words: how do we build an MVP of this tool that is both *minimal* and *viable*?\\n## Decision Drivers\\n* Strictly timebox our efforts to \"prove out\" the approach.\\n* Establish whether we can effectively maintain this kind of tool as a team.\\n* Support initial development of a new rust component with externally-imposed, near-term deadlines.\\n","Decision":"* Strictly timebox our efforts to \"prove out\" the approach.\\n* Establish whether we can effectively maintain this kind of tool as a team.\\n* Support initial development of a new rust component with externally-imposed, near-term deadlines.\\nChosen options:\\n* **Option A: Use an external interface definition file based on WebIDL.**\\n* **Option E: Provide a tool that developers need to run by hand.**\\n* **Option F: Go broad, implementing many data types and API capabilities, even if they're slow or incomplete.**\\nThe set of options chosen here makes an explicit tradeoff, preferring to get something up and running quickly\\nand accepting a certain amount of jank in the developer experience. We don't have to build the perfect tool\\nright away, we only have to build something that's better than doing this work by hand. If we like the result\\nwe can polish it from there.\\nThe MVP tool will read API definitions from an external WebIDL file. This will be a bit weird and inconvenient\\nfor consumers because WebIDL is not a precise fit for our needs, but it avoids us bikeshedding the perfect\\nAPI-definition experience during this first phase.\\nThe MVP developer experience will involve `cargo install`ing the tool onto your system and manually running it\\nor integrating it into your build process. This risks being mildly inconvenient for consumers, but gives us\\nlots of flexibility while we learn about what a better workflow might look like.\\nThe MVP tool may support more features than turn out to be strictly necessary, in the interests of ensuring\\nmultiple team members can be involved in its development at this early stage. As a tradeoff, the MVP generated\\ncode will be allowed to contain inefficiencies and limitations that hand-written code might not,\\non the premise that our first consumers are not very performance-sensitive, and that there is a lot of scope for\\nimproving these implementation details over time.\\nWe are likely to ***revisit every single one of these choices*** if the MVP of the tool proves successful,\\nand will attempt to build it in such a what that they're easy to revisit.\\n","tokens":180,"id":4953,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0005-arc-pointers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUniFFI currently manages object instances using the `HandleMap` struct in the ffi-support crate.\\nThis means that external consumers of UniFFI-wrapped interfaces never see\\nany pointers to structs - instead, they get what is (roughly) an index into\\nan array, with the struct being stored in (and owned by) that array.\\nThis has a number of safety characteristics which are particularly important for\\nhand-written FFI interfaces, but it does cause some issues in evolving UniFFI in\\ndirections we consider important. In addition to the slight performance overhead,\\nthe use of `HandleMap`s makes it difficult to support:\\n* Passing object instances as arguments ([#40](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/40)).\\nGetting objects out of a `HandleMap` involves a closure, so accepting multiple\\nobject-typed arguments would involve code-generating nested closures.\\n* Returning object instances from functions ([#197](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/197)).\\nDoes the returned object already exist in the handlemap? If so, what is its handle?\\nHow will we manage the lifetime of multiple references to the object?\\nThese restrictions mean that UniFFI's `Object` type is currently only suitable\\nas the `self` argument for method calls, and is forbidden in argument position,\\nas record fields, etc.\\nThis ADR considers ways to evolve the handling of object instances and their\\nlifetimes, so that references to structs can be used more widely than currently allowed.\\n## Decision Drivers\\n* We desire the ability to have more flexible lifetimes for object interfaces, so\\nthey can be stored in dictionaries or other interfaces, and be returned by\\nfunctions or methods other than constructors.\\n* We would like to keep the UniFFI implementation as simple as possible while\\nproviding a suitable degree of safety - in particular, a promise that it\\nshould be impossible to misuse the generated bindings in a way that triggers\\nRust's \"undefined behavior\" or otherwise defeats Rust's safety\\ncharacteristics and ownership model (and in particular, avoiding things like\\nuse-after-free issues).\\n* We would like to keep the overhead of UniFFI as small as possible so that it\\nis a viable solution to more use-cases.\\n","Decision":"* We desire the ability to have more flexible lifetimes for object interfaces, so\\nthey can be stored in dictionaries or other interfaces, and be returned by\\nfunctions or methods other than constructors.\\n* We would like to keep the UniFFI implementation as simple as possible while\\nproviding a suitable degree of safety - in particular, a promise that it\\nshould be impossible to misuse the generated bindings in a way that triggers\\nRust's \"undefined behavior\" or otherwise defeats Rust's safety\\ncharacteristics and ownership model (and in particular, avoiding things like\\nuse-after-free issues).\\n* We would like to keep the overhead of UniFFI as small as possible so that it\\nis a viable solution to more use-cases.\\nChosen option:\\n* **[Option 2] We replace `HandleMap<T>` with raw pointers to Rust's builtin `Arc<T>`**\\nThis decision is taken because:\\n* We believe the additional safety offered by `HandleMap`s is far less\\nimportant for this use-case, because the code using these pointers is\\ngenerated instead of hand-written.\\n* Correctly implementing better lifetime management in a thread-safe way is not\\ntrivial and subtle errors there would defeat all the safety mechanisms the\\n`HandleMap`s offer. Ultimately we'd just end up reimplementing `Arc<>` anyway,\\nand the one in the stdlib is far more likely to be correct.\\n* There are usability and familiarity benefits to using the stdlib `Arc<>` rather\\nthan a special-purpose container like `triomphe::Arc`, and the way we currently\\ndo codegen means we're unlikely to notice any potential performance improvements\\nfrom using a more specialized type.\\n### Positive Consequences\\n* There will be less overhead in our generated code - both performance overhead\\nand cognitive overload - it will be much easier to rationalize about how\\nthe generated code actually works and performs.\\n### Negative Consequences\\n* Errors in our generated code might cause pointer misuse and lead to \"use\\nafter free\" type issues.\\n* Misuse of generated APIs may be able to create reference cycles between Rust\\nobjects that cannot be deallocated, and consumers coming from a garbage-collected\\nlanguage may assume that such cycles will be collected.\\n","tokens":491,"id":4954,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0003-threadsafe-interfaces.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUniffi currently uses a very coarse locking strategy for managing concurrent access to object instances,\\nwhich has caused us to accidentally ship code in a product that [blocked the main thread on network I\/O](https:\/\/jira.mozilla.com\/browse\/SDK-157).\\nWe need to enable finer-grained concurrency control in order to provide the desired API for a key consumer.\\nCurrently, every interface has a corresponding [ffi_support::ConcurrentHandleMap](https:\/\/docs.rs\/ffi-support\/0.4.0\/ffi_support\/handle_map\/struct.ConcurrentHandleMap.html) that is responsible for owning all instances of\\nthat interface and for handing out references to them in a mutability-safe and threadsafe manner. This\\nensures that the generated code is safe in the face of concurrent operations, but has a substantial\\nruntime cost: only one method call can be executed on an instance at any time. Any attempt to call an\\nobject method while a concurrent method is already executing, will block until the previous call has completed.\\nThe desired API for Project Nimbus includes methods that will be called synchronously from the main thread,\\nand hence must not block on network or disk I\/O. Such an API cannot be built with uniffi as currently\\nimplemented.\\n## Decision Drivers <!-- optional -->\\n* Enabling consumers to control the potential blocking behaviour of their generated APIs.\\n* Ensure safety of the generated code.\\n* Ship a solution in a timely manner to unblock Project Nimbus.\\n","Decision":"* Enabling consumers to control the potential blocking behaviour of their generated APIs.\\n* Ensure safety of the generated code.\\n* Ship a solution in a timely manner to unblock Project Nimbus.\\nChosen option:\\n* **Option B: Let consumers mark interface definitions as threadsafe to opt in to a less-locking handlemap.**\\nThe choice here comes down to safety and simplicity. By making a more-concurrency-friendly handlemap\\nwe can maintain the current strict enforcement of Rust's mutability-safety and thread-safety guarantees,\\neven in the face of errors in the generated bindings. It seems to be a relatively small change, and\\nby making it opt-in we avoid creating busywork for other consumers who are not urgently facing this\\nproblem.\\nOne downside is that consumers need to opt-in to the fix, meaning that the default behavior may still\\nbe surprising to new consumers. We'll mitigate this with docs and will consider revisiting the default\\nbehaviour if the majority of consumers adopt the new approach.\\nThis choice does also punt some potential performance improvements to future work, but that seems in keeping\\nwith where we are in the project's lifecycle.\\n","tokens":312,"id":4955,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0000-whats-the-big-idea.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nOn the Application Services team, we have successfully built several re-useable components for sync- and storage-related browser\\nfunctionality by following what we've dubbed the \"rust-components\" approach: write the bulk of the code in rust\\nso we can cross-compile it for different target platforms, have it expose a C-compatible FFI layer, then write a small amount\\nof FFI bindings code to expose the functionality to each of several different target languages (e.g. Swift and Kotlin).\\nThe FFI layer and foreign-language bindings code is currently written by hand, a tedious and potentially error-prone\\nprocess.\\nGiven that we expect to build additional components in this style in the future, and expect more teams at Mozilla to\\ndo the same, can we increase the efficiency and reliability of this work by auto-generating some of this code?\\n## Decision Drivers\\n* Reduce time taken to launch a new rust component.\\n* Improve maintainability of existing rust components.\\n* Reduce possibility of errors in hand-written foreign language bindings code.\\n* Continue shipping components on a regular cadence.\\n","Decision":"* Reduce time taken to launch a new rust component.\\n* Improve maintainability of existing rust components.\\n* Reduce possibility of errors in hand-written foreign language bindings code.\\n* Continue shipping components on a regular cadence.\\nChosen option:\\n* **Option D: Write and maintain a custom tool that automates our current best practices**\\nOn balance, this option provides us with the best tradeoff of potential upside and the ability to limit downside.\\nIf the approach succeeds then we expect to realize significant improvement in maintenance costs of rust-components\\ncode by reducing boilerplate and human error. Building our own will involve the least up-front investment before\\nwe can start to show results, because we did not identify any existing tools that were a close-enough fit for our needs.\\nThe first versions of the tool don't have to be perfect, or even particularly *good* - they just have to have a\\nbetter value-proposition than writing the generated code by hand.\\nWe accept the risk that writing our own tool for this may turn out to be much more complex than expected, and\\nwill mitigate it by aggressively time-boxing initial prototypes, by developing it in parallel with a real shipping\\nconsumer with real deadlines, and by regularly asking the hard questions about whether the approach is working out.\\n","tokens":229,"id":4956,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0006-wrapping-types.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nUniFFI was not able to support types from external crates because Rust's orphan\\nrule prevents implementing the `ViaFfi` trait.  In order to add support we\\nneeded to choose between updating the `uniffi` traits or updating the\\n`lift_py` and `lower_py` scaffolding functions.\\nThe same general question comes up often.  When adding new features we often\\nneed to choose between two paths:\\n* Updating the code in the target language\\n* Updating the template wrapping code\\nThis ADR discusses this particular decision and also the general pros and cons of each\\npath.\\n## Decision Drivers\\n* We wanted to support external crates that define new types by wrapping an\\nUniFFI primitive type.  For example supporting `serde_json::Value` that wraps\\n`string` or a `Handle` that wraps `int32`.  We wanted this kind of wrapping\\ncode to exist outside of `uniffi` to allow for more experimentation with\\nwrapped types and to support types that were specific to particular libraries\\n(for example the application-services `Guid` type).\\n","Decision":"* We wanted to support external crates that define new types by wrapping an\\nUniFFI primitive type.  For example supporting `serde_json::Value` that wraps\\n`string` or a `Handle` that wraps `int32`.  We wanted this kind of wrapping\\ncode to exist outside of `uniffi` to allow for more experimentation with\\nwrapped types and to support types that were specific to particular libraries\\n(for example the application-services `Guid` type).\\nChosen option:\\n* **[Option 2] Update the `uniffi` code and generalize the `ViaFfi` trait**\\nThis decision is taken because:\\n* It was relatively easy to implement wrapper types by allowing the external\\ncrates to add custom scaffolding code.  This code could wrap primitive types\\nbecause all lifting\/lowering\/reading\/writing was handled by Rust code.  If we\\nhad gone with option 1, then the wrapping code would need to hook into the\\ntemplate functions (`lift_rs`, `lower_rs`, etc.).  We couldn't see a simple\\nway to implement this.\\n* Updating the code in the target language results in more readable generated\\ncode.  The newtype pattern makes the generated code more difficult to read,\\nespecially when types are wrapped in `Option<>`, `Vec<>`, etc.\\n* The same pattern could be used to implement wrapping on the bindings side.\\n### Positive Consequences\\n* Paved the way for wrapper types.\\n* Simplified the template code.\\n### Negative Consequences\\n* Implementing wrapping with template functions can lead to more direct code.\\nFor example, lifting a integer value is a no-op, but we still generate a\\nfunction call to do it.  This is not an issue with Rust, since the compiler\\nwill optimize the call away, but it could be an issue for the bindings code.\\nIf we decide that it is an issue, we could go with a hybrid solution:\\ngenerate the lifting\/lowering code in the target language, but also have\\nlift\/lower filter functions that exist solely to optimize lifting\/lowering\\nsimple types.\\n","tokens":233,"id":4957,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"uniffi-rs\/0002-serialize-complex-datatypes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nPassing complex data-types from Rust to foreign-language code and back again can be, well, *complex*.\\nGiven a Rust struct with named fields, and a corresponding autogenerated data class in the foreign language\\nbindings, how does one turn into the other?\\n## Decision Drivers\\n* Ensuring safety of the generated code.\\n* Fitting with our [MVP goal](.\/0001-mvp-webidl.md) of favouring initial-time-to-ship over performance.\\n","Decision":"* Ensuring safety of the generated code.\\n* Fitting with our [MVP goal](.\/0001-mvp-webidl.md) of favouring initial-time-to-ship over performance.\\nChosen option:\\n* **Option C: Implement a simple direct serialization scheme, pass them as serialized bytes.**\\nThe choice here comes down to simplicity and safety for the MVP.\\nSerializing complex datatypes into a bytebuffer makes it easier for us to pass the data safely across the FFI,\\nbecause it strictly controls shared access to memory on each side of the boundary. Using our own simple\\nserialization scheme reducing the number of moving parts relative to third-party serialization libraries.\\nThis choice comes with non-trivial performance costs, but that's acceptable for MVP.\\nWe are likely to ***revisit this choice*** if the MVP of the tool proves successful\\nand expect that we can do so incrementally without changing the consumer-facing experience.\\n","tokens":105,"id":4958,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Playground\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe storage service needs a function for uploading files both large and small. It also needs to be able to handle interruptions that can occurr while uploading data.\\nWhile searching for a solution it became clear that AspNetCore does have basic uploading capabilites in which files can be uploaded using an `IFormFile` that is bound to a model. This is problematic because the file is completely buffered in memory or on disk which makes this solution not ideal for large files and high frequency uploads.\\nTaken from [Microsoft's File Uploading Documentation](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/mvc\/models\/file-uploads#file-upload-scenarios):\\n> Any single buffered file exceeding 64KB will be moved from RAM to a temp file on disk on the server. The resources (disk, RAM) used by file uploads depend on the number and size of concurrent file uploads. Streaming isn't so much about perf, it's about scale. If you try to buffer too many uploads, your site will crash when it runs out of memory or disk space.\\nWhen uploading large files AspNetCore doesn't provide any form of resilience (as of version 3.1). If a network connection is dropped the client if forced to restart the upload from the beginning. As files take longer to upload there is an increasing chance for the connection to be interrupted. File validation occurrs after the upload process is complete. If the file is corrupted then the file will need to be re-uploaded.\\n","Decision":"We will use the `tusdotnet` for our TUS protocol implementation.\\nWe will integrate the Identity Service with `tusdotnet`.\\nWe will implement a proof of concept custom storage container for `tusdotnet` that connects to Azure Blob Storage.\\n","tokens":307,"id":4959,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Playground\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe Identity, Metadata, and Storage currently have no way to be scaled. As it stands, they are only able to be ran as a single instance in a cloud specific way.\\nAn engineering goal for this product is to be cloud agnostic. This means that we need to be able to run on abstracted hardware provided by some Cloud or Host.\\n","Decision":"In order to achieve One of the engineering goals for this project is to be cloud agnostic.\\n","tokens":75,"id":4960,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0004-generate-help-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4962,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0006-use-dependency-injection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4964,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0002-use-swagger-to-document-apis.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4965,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0005-hardcode-dependencies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4966,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4967,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0008-use-autofac-for-dependency-injection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4968,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-poc\/0007-use-unity-for-dependency-injection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4969,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0008-layering-modular-ios-application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur problem was monolith single view app. Our project will grow out of control.\\n","Decision":"We decided splitting it into individual feature modules. This is similar to the micro-service architecture.\\n","tokens":21,"id":4970,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0014-use-CoreTracker-for-trackingEvents.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur team created new Core Tracker Interface for tracking events. For consistency we should replace and use CoreTrackable instead of Legacy Trackable Interface.\\n","Decision":"Every new events must use CoreTrackable. Also every new event should use new Tracker approach.\\n","tokens":32,"id":4971,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0017-theme-manager.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are creating a design guideline. All team members should follow defined rules.\\n","Decision":"* Do not assign\\n- corner radius\\n- shadow\\n- border width\/color\\n- text color\\n- background color\\n- text size\\n- tint color\\non storyboard.\\n* Do not use any color that isn't in our color palette. If that is the case contact with designer team.\\n* Add icons as SVGs. Choose single scale while adding.\\n","tokens":19,"id":4972,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0005-inject-ab-config-global-values-toPresenter.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\\n","Decision":"We decided to inject this variables to related presenters from their constructors.\\n","tokens":45,"id":4973,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0002-use-view-model-on-necessary-place.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur problem was that if we need to manipulate the response we was doing it on DTO object. And that object getting larger. Also if some field changes on response object we need to update the ui's fields. But nothing change on user interface. That's why we need to seperate DTO and UI's Model's from each other.\\n","Decision":"~~If it is necessary to manipulate the DTO models that the UI will use, the ViewModel class needs to be created.~~\\n","tokens":70,"id":4975,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0019-naming-branch-name.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe run too many separate sprints and our naming gets mixed up. Also it is difficult to manage when too many branches are gathered under one folder.\\n","Decision":"Branch names should be named in such way:  `Channel Name\/Sprint Name\/Task Code + Name`\\nFor Example: `MPDP\/Sprint1\/MPDP-1050-ProductDetailPage`  or  `DISCO\/Sprint50\/IOSDISCO-1250-FixWidgetHeight`\\n","tokens":34,"id":4976,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0004-use-mock-generator.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSince we started to use `XCTest` framework to write unit tests, we faced a problem that is we need to write a lot of mocks manually. However using a mock is almost always a must for every unit test, we were spending too much time to create them. We need either a framework\/library or something like an Xcode plugin to create them for us automatically.\\n","Decision":"We found the [Swift Mock Generator](https:\/\/github.com\/seanhenry\/SwiftMockGeneratorForXcode) to solve this specific problem. It is an Xcode plugin and it lets you to create mocks for protocols automatically.\\nOne of the advantages of using a plugin is, it works independently, which means you do not have to import a huge dependency on your project and don't bother about it's maintenance.\\nIt has 4 different options to create the mock, spy, stub, dummy and partial spy. All of these options and also the usage of them have described in the Github page.\\n","tokens":79,"id":4977,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0010-use-localizable-kit-for-localization.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nLocalizable files have been defined in different ways to date. Therefore, we may need to duplicate text for other modules, or if we add new language support, it will be difficult to manage localizable files for common kits & modules from many different places within the channels.\\n","Decision":"* Common localization files across the channels modules' should be defined in the associated channel LocalizableKit.\\n* Localizable files of common kits, modules and components used throughout the application should be defined within itself. It should not be defined in any LocalizableKit.\\n* Localizable files of channels that only concern their own modules should be defined within their own modules.\\n","tokens":58,"id":4978,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0003-dont-use-chain-delegate.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur problem was using chain delegate without doing any changes on every scope. It generates too many repeated codes.\\n","Decision":"Don't use chain delegate if you are not make any changes on every scope.\\n","tokens":25,"id":4979,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0015-use-UITestablePageProtocol-for-AccessibilityIdentifiers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur team created new UITestablePage Interface for setting accessibility Identifiers. For consistency we should replace and use UITestablePage.\\n","Decision":"Every View Controller should conform UITestablePage.\\n","tokens":31,"id":4981,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0000-use-xctest-for-unit-test.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur problem was that using third party library for unittest instead of XCTest. And our unittests were most likely behavioural, we also wanted to change this approach and write unittest directly\\n","Decision":"Every new unittest will be written with XCTest instead of Quick\/Nimble\\n","tokens":38,"id":4982,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0013-unit-tests-required.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUnit testing finds problems early in the development cycle.\\n","Decision":"Adding unit tests is required when a change is made to related Presenter.\\n","tokens":14,"id":4984,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0016-selector-function-unit-test.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe aim is to cover presenter's private selector's functions and increase unit tests coverage results.\\n","Decision":"A new variable should be created with AnyObject type\\nThe presenter should be set to the new variable\\nSelector function should be trigger with perform method which inside the new variable.\\n","tokens":22,"id":4985,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0009-layering-modular-ios-application-approach.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur problem was defining the limit of modules.\\n","Decision":"We decided splitting frameworks with their user flows.\\n","tokens":13,"id":4986,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0011-name-test-methods.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nName the test methods in a convenient form decided by the team.\\n","Decision":"Methods should be named in such a way:  `test_methodToTest_Condition_Result`\\n- If we use default value in create method and define condition in function name it has to be suitable with create function call parameters\\n**Preferred**:\\n```swift\\nfunc test_methodToTest_ConditionOne_ConditionTwo_Result() {\\nreCreate(conditionOne: true, conditionTwo: \"title\")\\n....\\npresenter.methodToTest()\\n}\\n```\\n**Not Preferred**:\\n```swift\\nfunc test_methodToTest_ConditionOne_ConditionTwo_Result() {\\n....\\npresenter.methodToTest()\\n}\\n```\\n","tokens":17,"id":4987,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0018-use-preferredNavigationBarType.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe aim is to cover doubled or none navigation bar issues with deeplink routings.\\n","Decision":"Should use `BaseViewController` or `TYRootViewController` and manage it there, and override if need your preferred `NavigationBarType`\\n","tokens":22,"id":4988,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0022-use-dependency-engine-to-register-and-access-modules.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe had been struggling with dependencies that break our incremental builds for a while. We were using a standalone dependency manager that manages all of these dependencies, but it was not flexible as we want.\\n","Decision":"We decided to create a brand new `DependencyEngine` to deal with these dependencies all over the app and *register* and *access* these modules by using `DependencyEngine`, and not exposing the explicit types through the protocols.\\n* `DependencyManager` is deprecated, and must not be used in brand new modules anymore. Also, existing usages should be transitioned.\\n* `DependencyEngine` must be used to access the modules through the other modules.\\n* The `arguments` that we use to initiate a module should be placed in `Interface` target of the module.\\n","tokens":42,"id":4989,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0020-inject-theme-manager-for-common-places.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEach channel should be able to give its own theme to the common views. We need to follow common rules about theme in common places.\\n","Decision":"For common views, theme manager should injected into the associated presenter.\\n","tokens":31,"id":4990,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ios-architecture-decision-logs\/0021-testing-only-one-function.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFor some of the unit tests, we had to invoke multiple functions on presenters to prepare the state we need to test. E.g. we test `viewWillAppear` here:\\n```swift\\nfunc test_viewWillAppear_InvokesNecessaryMethods() {\\nXCTAssertFalse(view.invokedShowLoading)\\nXCTAssertFalse(interactor.invokedFetchSellerFollowingStatus)\\npresenter.handleFetchOrderDetailResult(.success(response))\\npresenter.viewWillDisappear()\\npresenter.viewWillAppear()\\nXCTAssertTrue(view.invokedShowLoading)\\nXCTAssertTrue(interactor.invokedFetchSellerFollowingStatus)\\n}\\n```\\nThis way we could not be sure whether view or interactor mocks invoked during `viewWillAppear`, `viewWillDisappear` or `handleFetchOrderDetailResult`. Also, if anyone move `view.showLoading()` call from `viewWillAppear` to `viewWillDisappear`, this test does not fail even though we have a bug.\\n","Decision":"So it is better to move \"preparation calls\" above all of the assertions like this:\\n```swift\\nfunc test_viewWillAppear_InvokesNecessaryMethods() {\\npresenter.handleFetchOrderDetailResult(.success(response))\\npresenter.viewWillDisappear()\\nXCTAssertFalse(view.invokedShowLoading)\\nXCTAssertFalse(interactor.invokedFetchSellerFollowingStatus)\\npresenter.viewWillAppear()\\nXCTAssertTrue(view.invokedShowLoading)\\nXCTAssertTrue(interactor.invokedFetchSellerFollowingStatus)\\n}\\n```\\n","tokens":198,"id":4991,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sware-server\/0001-general-arch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThere are three  major decisions we need to make that have an impact on what technologies are used and how the code is implemented:\\n- Should this be a single process or multiple processes?\\n- Should distinct pieces of functionality live in separate GitHub repos?\\n- How many threads should be used?\\n## Decision Drivers\\n* I want to use RocksDB in any solution, but RocksDB does not support multiple processes unless all of them are readonly. Since I need a writer, I am forced into having all reads\/writes happening on a single process.\\n* Multiple repos ended up being a distributed monolith, and I needed a `wx-shared` repo to hold shared functionality and domain structs. I do not want to deal with this again.\\n* ZeroMQ was fun to learn, but the serde and IPC added a layer of complication whenever I tried refactoring or troubleshooting.\\n","Decision":"* I want to use RocksDB in any solution, but RocksDB does not support multiple processes unless all of them are readonly. Since I need a writer, I am forced into having all reads\/writes happening on a single process.\\n* Multiple repos ended up being a distributed monolith, and I needed a `wx-shared` repo to hold shared functionality and domain structs. I do not want to deal with this again.\\n* ZeroMQ was fun to learn, but the serde and IPC added a layer of complication whenever I tried refactoring or troubleshooting.\\nChosen option: `Single-repo, single process` because it keeps things simple and allows me to focus on writing new features and making the existing feature more bulletproof instead of trying to wire up multiple repositories\/processes.\\n### Positive Consequences\\n* Reduced complexity is easier to read and understand\\n* Reducing the repos involved allows the shared library to just be a module that all other code uses\\n* Removing multiple processes from the solution means that IPC isn't needed, which means all of the code around ZMQ transport and serde can be removed. This greatly simplifies interactions with the store.\\n### Negative Consequences\\n* Unable to scale horizontally if needed, however RocksDB makes that difficult to do anyways\\n* I don't get to learn with functionality like crate workspaces, ZMQ, or MPSC queues\\n* Have to rebuild RocksDB when making changes to unrelated things like API routes\\n","tokens":185,"id":4992,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"halfpipe\/0002-task-cache-volumes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nConcourse has [task caches](https:\/\/concourse-ci.org\/tasks.html#task-caches) to save state between runs of the same task on the same worker. This can greatly speed up tasks - making users happy, and reduce load - making operators happy. win win.\\n","Decision":"Change halfpipe to provide one directory `\/var\/halfpipe\/cache` instead of a list of directories specific to common build tools.\\nThis will allow users to configure any build tool to use the cache instead of the onus being on halfpipe to add support.\\nAlso there is a small overhead to mounting each cache volume, so one volume is better than n.\\n","tokens":60,"id":4993,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"court-case-matcher\/0002-retain-and-process-sns-metadata.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe service receives case messages from the SQS queue `court-case-matcher-queue`. These messages, being produced from an SNS subscription, are embedded in JSON containing metadata about the message. It is possible to remove this metadata through configuration of AWS, thereby allowing for easier processing of the case itself in court-case-matcher or to retain it.\\nThe metadata includes the following fields\\n* `messageId`\\n* `topicArn`\\n* `timestamp`\\n","Decision":"We have decided to retain the message metadata. There is no immediate use for the fields at but the cost of processing is low and there's a possibility that `messageId` will be useful for tracing.\\n### Links\\nDescription of the terraform field for enabling delivery of the message without metadata.\\nhttps:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/sns_topic_subscription#raw_message_delivery\\n","tokens":97,"id":4994,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"court-case-matcher\/0003-use-hexagonal-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHistorically the `court-case-matcher` has only had to deal with data from one source (Libra) and output to one other (`court-case-service`). With the introduction of Common Platform as a source and the corresponding changes at the `court-case-service` to support this, our single source, single sink data model requires significant refactoring. Doing this in small steps without making breaking changes is impossible with the current multi-purpose data structures.\\n","Decision":"To make accepting and posting multiple different data formats easier the decision was taken to precede this work with a major refactor to explicitly separate out the incoming, domain, and outgoing data models using the [hexagonal\/ports and adapters](https:\/\/en.wikipedia.org\/wiki\/Hexagonal_architecture_(software)) architectural pattern. This will allow each interface to vary independently of the others allowing us to make small, isolated, non-breaking changes.\\nThis pattern has the disadvantage of requiring extensive tedious mapping back and forth between superficially similar objects. There are libraries that will do this automatically but we've decided not to go down this route, at least initially, as previous experience with these libraries leads us to believe they ultimately produce more of a maintenance headache than they save. Manual mapping is tedious, but it's also simple and easy to understand and when it fails it does so in an obvious and easily debuggable way.\\n### General principles:\\n- The domain model is pervasive and may be passed freely between services\\n- Interface models should only exist on the periphery of the application and should not be passed around. This is to localise tight coupling to external interfaces.\\n- Interfaces can know about the domain model but the domain model should never know about an interface. This is to prevent tightly coupling the domain model to interfaces.\\n### In practice:\\n- The domain model lives under [`java\/uk\/gov\/justice\/probation\/courtcasematcher\/model`](java\/uk\/gov\/justice\/probation\/courtcasematcher\/model). This model should be used for all internal logic and operations.\\n- Each external interface should provide its own POJOs, for example [`java\/uk\/gov\/justice\/probation\/courtcasematcher\/messaging\/model\/libra`](java\/uk\/gov\/justice\/probation\/courtcasematcher\/messaging\/model\/libra) and [`java\/uk\/gov\/justice\/probation\/courtcasematcher\/restclient\/model\/courtcaseservice`](java\/uk\/gov\/justice\/probation\/courtcasematcher\/restclient\/model\/courtcaseservice)\\n- All port and adapter model instances should provide an `asDomain()` method to convert themselves to the domain model\\n- All port and adapter models should provide a static `of(<DomainModel> obj)` method to convert from the domain model to the interface model\\n- Prefixing interface models makes it easier to spot them and differentiate them (e.g. [`CCSCourtCase`](java\/uk\/gov\/justice\/probation\/courtcasematcher\/restclient\/model\/courtcaseservice\/CCSCourtCase.java))\\n","tokens":93,"id":4995,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"court-case-matcher\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4996,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mediawiki-extensions-WikibaseManifest\/0002_use_open_api_spec.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance.\\nWe need to decide on the format of the information the manifest will provide.\\nWe took into consideration the following projects:\\n- the recent [REST API Prototype](https:\/\/github.com\/wmde\/wikibase-rest-fantasies) (by WMDE) and its [OpenAPI spec](https:\/\/raw.githubusercontent.com\/wmde\/wikibase-rest-fantasies\/gh-pages\/openapi.json)\\n- [OpenRefine's initiative](https:\/\/github.com\/OpenRefine\/wikibase-manifests) to collect manifests from different wikibases and their [json-schema spec](https:\/\/github.com\/OpenRefine\/wikibase-manifests\/blob\/master\/wikibase-manifest-schema-v1.json)\\nThe MediaWiki REST API which we are using (please refer to ADR 1 for more info) implements neither json schema nor openAPI.\\n### OpenAPI (swagger)\\nThe OpenAPI Specification (formerly Swagger Specification) is an API description format for REST APIs. An OpenAPI file allows you to describe your entire API. [Swagger](swagger.io) is a set of open-source tools built around the OpenAPI Specification, like the [api editor](https:\/\/editor.swagger.io\/).\\n- It\u2019s popularly used for mocking services and generating SDKs. It's not commonly used for run-time functionality.\\n- Useful when you want to describe your entire API.\\n- **OpenAPI is both a subset of JSON Schema Draft 5 and a superset**\\n### Json Schema\\nJSON Schema is a vocabulary that allows you to validate, annotate, and manipulate JSON documents.\\nThe specification is split into three parts, [Core](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-core.html), [Validation](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-validation.html), and [Hyper-Schema](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-hypermedia.html).\\n- JSON Schema is a good option when there are data models whose schema needs to be defined\\n","Decision":"Use OpenAPI spec.\\nWe acknowledge that both are good options. We chose OpenAPI because the Wikidata team has created several products (e.g. [termbox](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikibase\/termbox\/+\/refs\/heads\/master\/openapi.json)) using the OpenAPI spec and plan on continue to do so when we have the opportunity.\\nThere're tools (e.g. [OpenAPI Schema to JSON Schema](https:\/\/github.com\/openapi-contrib\/openapi-schema-to-json-schema)) for converting from OpenAPI Schema Object or Parameter Object to JSON Schema in case the need arises to use json schema.\\n","tokens":439,"id":4998,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mediawiki-extensions-WikibaseManifest\/0000_deliver_as_extension.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe considered two options for delivering Wikibase Manifest:\\n- Built as MediaWiki Extension\\n- Include in Wikibase Core\\n|   Options\t|  Consistency across 3rd party Wikibases \t|   Installation\/Setup Burden\t|  User Adoption \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Third-party Wikibase admins will have to provide the Manifest via extension.\t|   Additional effort of setting up an extension. It could be part of the docker bundle\t|   Assuming lower as it involves installing a separate extension\t|\\n|  Wikibase core\t|   Tool builders could rely on the Manifest always being there in any third-party Wikibase. This would be an advantage over the extension. But the user still needs to configure it. |   None\t|  Assuming higher\t|\\n|   Options\t|  Maintenance Burden \t|   Backwards Compatibility\t|  Testing Infrastructure\t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to set up our own CI\t|   compatible|   Less straightforward to test in Beta if we make this an extension, will have to set up our own cloudvps test instance.\t|\\n|  Wikibase core\t|   Same as Wikibase\t|   compatible\t|   Could use Beta or a cloudvps test instance for testing infrastructure. Probably an advantage    |\\n|   Options\t|  Documentation\t|   Speed of release to Wikibase users\t|  Feedback loop with tool builders \t|\\n|---\t|---\t|---\t|---\t|\\n|  MW Extension \t|   Need to document installation of the extension, as well as setup. Probably minimal additional effort \t|   Potentially faster\t|   Probably shorter, for the same reasons stated on speed of release to wb users\t|\\n|  Wikibase core\t|   Document how to work with the file\t|   Waiting until a new release is made (e.g. 1.36) or we have to do some backporting\t|   Not sure\t|\\n","Decision":"Build as a MediaWiki Extension.\\nPlan to move the functionality to Wikibase core in the future.\\n","tokens":460,"id":4999,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"mediawiki-extensions-WikibaseManifest\/0001_use_mediawiki_rest_api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance in an automated way.\\nWe can achieve this by using either the [Wikibase Action API](https:\/\/www.mediawiki.org\/wiki\/Wikibase\/API) or the [MediaWiki REST API](https:\/\/www.mediawiki.org\/wiki\/API:REST_API).\\n### Wikibase Action API\\n`+` The developers on the team have experience working with Wikibase's API\\n### MediaWiki REST API\\n`+` Built more recently\\n`+` Has good test coverage thanks to [mediawiki-tools-api-testing](https:\/\/github.com\/wikimedia\/mediawiki-tools-api-testing)\\n`+` Developers want to try out this new REST API\\n`+` Broadly accepted as a standard and probably easier to use for tool builders than the Action API\\nWe don't see any significant downsides of using one or the other.\\n","Decision":"Use MediaWiki REST API.\\n","tokens":196,"id":5000,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"webwritertechandhumanity.com\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5002,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"grout-2018-fellowship\/rename-package.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":5005,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hmpps-book-secure-move\/0002-api-versioning.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThird-party programme suppliers have begun integrating with [the API](https:\/\/github.com\/ministryofjustice\/hmpps-book-secure-move-api) provided by this service. Our in-house\\n[client application](https:\/\/github.com\/ministryofjustice\/hmpps-book-secure-move-frontend) has been consuming the API for a while and breaking changes were previously made sparingly\\nand with constant communcation so strict versioning wasn't required.\\nMoving forward we need to be more strict with versioning when there are any breaking changes to the API as\\nwe now have more consumers, including those outside of our team.\\n","Decision":"We will attempt an **API Evolution** approach to support backwards compatability.\\nWhen that is not possible, we will support **Global URI versioning**. For example moving from version 1 to version 2 with a breaking\\nchange to the `moves` endpoint:\\n```javascript\\n\/v1\/moves\\n\/v1\/people\\n```\\n```javascript\\n\/v2\/moves \/\/ breaking change\\n\/v2\/people \/\/ no change\\n```\\nExample backwards compatible changes:\\n- adding query parameters (they should always be optional)\\n- adding header or form parameters, as long as they are optional\\n- adding new fields in JSON or XML data structures, as long as they are optional\\n- adding endpoints, e.g. a new REST resource\\n- adding operations to an existing endpoint, e.g. when using SOAP\\n- adding optional fields to the request interfaces\\n- changing mandatory fields to optional fields in an existing API\\nExample incompatible changes (would require major version change):\\n- removing or changing data structures, i.e. by changing, removing, or redefining fields in the data structure\\n- removing fields from the request or response (as opposed to making it optional)\\n- changing a previously optional request field in the body or parameter into a mandatory field\\n- changing a previously required response field in the body or parameter into an optional field\\n- changing the URI of the API, such as hostname, port or path\\n- changing the structure or relationship between request or response fields, e.g. making an existing field a child of some other field\\n- adding a new mandatory field to the data structure\\n","tokens":131,"id":5007,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"hmpps-book-secure-move\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5008,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a system to manage the lifecycle of a grant application.\\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0006-split-service-into-two.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe TAP service comprises of 2 distinct responsibilities:\\n- Provide a public facing grant application form styled using the GOVUK GDS style guide.\\n- Provide a private portal to manage grant applications in a formal recorded manner.\\nThe private grant management area will need to be secured behind the DIT VPN while the public facing grant application\\nprocess will not.\\nBoth areas will require an authentication system but they will not necessarily be required to use the same\\nauthentication system.\\nThese two areas should not share the same user accounts or user types.\\nA person could feasibly have the need to access both areas for different purposes but this should be done using two\\nunconnected user accounts.\\nGrant application data will need to be shared between these two areas.\\n","Decision":"We have decided to separate these two systems into services.\\nThese services will be split out in the codebase as separate projects. However, we will leave these within the same git\\nrepository for a few reasons:\\n- Having these services share a single git repo will remove any need for versioning between the two services. Thus\\nsaving valuable development effort.\\n- The development environment will be much easier to spin up locally because all services can be managed within a\\nsingle docker-compose.yml file\\n- We do not foresee that the codebase for these services will become very large.\\nWe will name these services `frontend` and `backoffice`.\\nDue to developer constraints within the team (we are a small team with 1 backend developer) we will use the Django\\nframework to create both services.\\nThese services will communicate via json REST requests only. They will each have their own database and will not have\\nread or write access to the other's database.\\n","tokens":158,"id":5010,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0003-events-from-fixture-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTrade show events are stored in many places across the TAP team.\\n- At the beginning of the year the Glasgow operations team validates the list of events in a spreadsheet stored on sharepoint.\\n- Events are uploaded to a hosted service called Aventri so that they are visible on great.gov.\\n- Any updates to event details are communicated via email.\\nAs of 2020-12-17 there are still discussions ongoing about how to manage trade show event data centrally within DIT\\nand move away from the spreadsheet being the golden source of truth. One idea is to use digital workspace for this.\\nHowever as of right now no decision on this has been made.\\nIn the meantime the our services still need a way of displaying trade show events to our users to select and view.\\n","Decision":"Without a firm decision on where events will ultimately be centrally stored we have decided to use a fixture file\\nto load events into our backoffice as a temporary solution.\\nThe fixture file exists at `backoffice\/web\/trade_events\/fixtures\/trade_events.json` and is automatically loaded on\\nstartup.\\n","tokens":166,"id":5011,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0007-passwordless-magic-link-authentication-for-frontend.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a authentication system for the frontend service.\\nThe authentication method must be publicly accessed.\\n","Decision":"A few authentication options were suggested and discussed:\\n- username\/password\\n- social authentication (facebook, twitter, etc.)\\n- passwordless email link authentication (also known as magic link)\\nIt was decided that we would use passwordless email link authentication to authenticate our public users (grant\\napplicants).\\nThe user will enter their email address into a login page served by the frontend service. The service will then\\nsend a authentication link containing a one time use token in the url. The user can then visit this url to start or\\ncontinue their application.\\nConstraints\\n- The token will be limited to one time use.\\n- The token will have a datetime expiry assigned to it.\\n- On issuing a new token to a user all previous tokens will be invalidated.\\n","tokens":22,"id":5012,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0002-deployments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a way of deploying our services to our various hosted environments (dev, staging, prod, etc.). We would\\nprefer these deployments to be automated with minimal to zero human interaction.\\n","Decision":"The DIT infrastructure already has a lot of tooling and infrastructure around deployments so we will utilise this. We\\nwill use Jenkins to automatically deploy from dedicated git branches - these will be:\\n- development\\n- staging\\n- uat\\nWe will have 5 Jenkins jobs in total - these will be:\\n- trade-access-program-backoffice\\n- trade-access-program-frontend\\n- trade-access-program-polling-dev\\n- trade-access-program-polling-staging\\n- trade-access-program-polling-uat\\nThe role of the \"polling\" jobs are to watch a related git branch for any commit changes. Once a change is detected then\\nit will trigger the `trade-access-program-backoffice` and `trade-access-program-frontend` jobs with a set of\\nenvironment parameters triggering the deployment to one of our three environments.\\nThis allows us to simply merge or push to one of the three dedicated git branches above and a full automated deployment\\nwill occur for that environment.\\n","tokens":42,"id":5013,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0004-templates-over-js-framework-md.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide how we want to develop and maintain our service UI. In fact we require 3 different user interfaces\\nfor the TAP project:\\n- A grant application UI for external companies and individuals to apply for a grant.\\n- A grant management UI for internal TAP team members to help them organise the approval\/rejection process.\\n- An admin interface to manage the inner workings of the TAP service.\\nAs of right now the development team consists of 1 developer with experience in backend development. Therefore we will\\ntake the simpler solutions when making decisions about UI development.\\n","Decision":"### Grant application\\nThe service will be available to the public via a gov.uk domain, therefore we are required to use\\n[GDS](https:\/\/design-system.service.gov.uk\/) styling.\\nBecause of our development team constraints we will use standard html template views served from Django.\\n### Grant management\\nThe grant management portal will only be available to internal TAP team members and approved grant administrators\\n(TCPs, ITAs). In the [grant management portal ADR](0005-viewflow-for-grant-management-portal.md) we decided to use\\n[Viewflow](http:\/\/viewflow.io\/) to create a grant management portal. Viewflow has a\\n[frontend library](http:\/\/docs.viewflow.io\/viewflow_frontend.html) we can utilise to provide a frontend grant management\\nportal with minimal development work required from us.\\nBecause of our development team constraints we will use this viewflow frontend library.\\n### Admin\\nWe will use the built in Django admin panel for administration of our service.\\n","tokens":125,"id":5014,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"trade-access-program\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5015,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"RaPPMap\/0000-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5017,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0011-use-rails-for-backend.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt the onset of the Link Platform project we needed to decide what framework to use for our backend. After some\\ndiscussion two choices presented themselves:\\n1. Serverless architecture using AWS (lamda, API Gateway, etc.)\\n1. Ruby on Rails\\n","Decision":"The decision was made to use Ruby on Rails for our backend.\\n","tokens":59,"id":5019,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0012-use-jest-and-enzyme-for-unit-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJest is a general JavaScript testing framework. Enzyme is a testing utility that makes it easier to assert, manipulate, and traverse React components. [This](https:\/\/medium.com\/welldone-software\/an-overview-of-javascript-testing-in-2018-f68950900bc3) article goes into detail about various testing alternatives and [this](https:\/\/www.codementor.io\/vijayst\/unit-testing-react-components-jest-or-enzyme-du1087lh8) article details using Jest with Enzyme.\\n","Decision":"Based on the familiarity of our engineers with Jest and Enzyme and the large community of support, use Jest and Enzyme to unit test our React application.\\n","tokens":112,"id":5020,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0014-use-redux.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRedux is a state container for JavaScript applications. It helps to manage state across an application.\\n[This article](https:\/\/hackernoon.com\/the-react-state-museum-a278c726315) provides a nice non-comprehensive listing of various alternative state management libraries.\\n","Decision":"We will use Redux in `link_platform` for state management.\\n","tokens":58,"id":5021,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0013-use-fela-css-in-js-for-styling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs part of the process of rewriting the admin components for link platform, we need to decide on what strategy we want\\nto use for including styling in our application.\\nThere were two main choices:\\n* CSS stylesheets\\n* CSS-in-JS\\n### Why CSS-in-JS?\\nThere are many benefits to CSS-in-JS which are nicely outlined in\\n[this article]('https:\/\/medium.com\/seek-blog\/a-unified-styling-language-d0c208de2660'):\\n* Enforces fundamental scoping without relying on pure convention (automatically generated classes).\\n* Renders critical CSS baked in, as components bring their styles with them automatically.\\n* Removes the necessity to hard code class names.\\n* Increases reusability of styles.\\n* Improves maintainability and dead code detection\\n### Why Fela?\\n[Fela](https:\/\/github.com\/rofrischmann\/fela) is a high performance, framework agnostic\\ntool to handle state-driven styling in Javascript. It also uses Atomic Class design to\\ngenerate class names, and creates stylesheets with those class names, which allows for\\nmodular style reuse. Fela performs about\\n[four times faster](https:\/\/github.com\/A-gambit\/CSS-IN-JS-Benchmarks\/blob\/master\/RESULT.md)\\nthan Glamorous. Fela has also been used by Zendesk engineers in the past and so comes\\nwith the added bonus of having some internal knowledge already in place to hit the\\nground running.\\n","Decision":"We will use CSS-in-JS, in the form of Fela and React-Fela.\\n","tokens":317,"id":5025,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0004-use-garden-to-rewrite-admin-ui.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn general, we have decided to rewrite the admin UI as we port link-sf into the new Link Platform project. This is because we intend to make a number of improvements on its reliability and UX consistency.\\nWe considered using the [Admin on rest](https:\/\/github.com\/marmelab\/admin-on-rest) library which would give us a lot of data handling for free, including a REST client and validation. However, when we use [Zendesk Garden](https:\/\/garden.zendesk.com\/) we will also get validation styles, and then will be able to a) control how we handle data ourselves, b) support another one of Zendesk's open source projects, and c) get the nicer styles provided by Garden which are also reflective of the Zendesk brand.\\n","Decision":"Admin pages will be rewritten using React, Zendesk Garden, and a REST client to be determined. We will use Create React App to scaffold the directory. Its contents will be separated into folders for each stateful component.\\n","tokens":162,"id":5027,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0003-use-postgresql.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want the ability to run in many environments, including Heroku, and PostgreSQL is the most well supported in our desired environments.\\n","Decision":"We will use PostgreSQL as our data persistence layer.\\n","tokens":30,"id":5028,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0015-use-storybook.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are starting to develop multiple components for `link_admin`, and the idea of\\nusing some kind of component development environment to provide structure and\\nincreased productivity to this endeavor was broached by Mr. Jacob Pandl.\\n### Why Component Explorers?\\nAdding a component explorer to our project and including it in our workflow would\\nresult in a couple of key benefits. Component explorers allow engineers to build\\nmodular UIs in isolation of the app's business logic, increasing interchangability\\nand component reuse potential. They also allow for easier parallel production, as\\ndevelopers can work on different pieces of UI without state pollution. Using component\\nexplorers would also help with component durability, as they allow a developer to\\nmitigate inconsistency by being able to test many states of the application, in particular those that can be difficult to replicate through mocking. Finally,\\ncomponet explorers allow developers to create easily sharable artifacts which can be\\nshared with PR reviewers and other stakeholders.\\n### Why Storybook Instead of Styleguidist?\\nIt seems that while Storybook and Styleguidist both have similar toolsets, Storybook\\nwill better suit our needs. Storybook is a workshop application, meaning that it is\\ndesigned to allow a developer to create UI components in isolation, mock state, data\\nand adjust props. It was also one of the first tools for UI components, which means\\nit comes with a good deal of maturity and momentum. Styleguidist, meanwhile, seems\\nto be more of a documentation tool for UI, creating pages in markdown and importing\\nUI components.\\n","Decision":"For the purposes of developing UI components in isolation and speeding up\\ndevelopment, Storybook seems like the right choice. While Styleguidist would be a\\nuseful tool, Storybook is the right choice when it comes to development.\\n","tokens":337,"id":5029,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0009-use-openreferal-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[Open Referral](https:\/\/openreferral.org\/) is non-profit who develops a public API spec for health, human and social services data. Part of their mission is to standardize these civic services data.\\nZendesk has historically partnered with Open Referral in an effort to support the spread of this standardization; we modeled our previous LinkSF data to resemble that spec.\\nIn this new project, we want to further support these efforts by exposing an open API which adheres as closely as possible to the Open Referral standard. Documentation on this standard can be found [here](https:\/\/openreferral.readthedocs.io\/en\/latest\/).\\n","Decision":"Adopt and extend the OpenReferal API and data structure.\\nWe will extend the model to accommodate data that is specific to the link platform system.\\n","tokens":136,"id":5030,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0007-use-rspec-and-factorybot-for-unit-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want a unit test framework that is more feature rich than MiniTest.\\nFactoryBot adds more features to test data than provided with fixtures.\\n","Decision":"To add RSpec and FactoryBot to the project.\\n","tokens":33,"id":5031,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0018-put-openreferral-resources-in-an-api-namespace.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nLink-API is primarily an API application, but it has a few additional responsibilities beyond\\nthe API for the [OpenReferral](https:\/\/openreferral.readthedocs.io\/en\/latest\/) spec. To create a\\nclear distinction of the resources specific to the platform and the OpenReferral compliant\\nresources, I propose that we keep all OpenReferral resource API routes in a unique namespace and\\nthe Link Platform logic in an independent namespace.\\nFor example, if we add the concept of a User, the route should look something like:\\n```\\n\/users\/123\\n```\\nThe API for a Service should look something like:\\n```\\n\/api\/services\/123\\n```\\nIn the future we may want to iterate on the API, and this requires a way to version the API to\\nallow consumers to transition. We could version the API by URL namespace, for example `\/api\/1`,\\nor we could version the API with request metadata such as a header. I propose we take the latter\\napproach and use request metadata to control API versions.\\n","Decision":"Routes for the OpenReferral resouces will be placed in the `\/api` namespace. If we need to version the API,\\nthese routes will be versioned using request metadata such as media type or a header to specify the\\nversion of the API.\\nRoutes for non-OpenReferral resources will be in the root namespace, for example `\/link_instances\/new`.\\n","tokens":223,"id":5032,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0005-create-react-app-npm-no-yarn.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe needed to decide how best to create and enforce a uniform file structure for our separate directories containing\\nreact applications. This also led to a discussion about whether we should use Yarn or NPM.\\n","Decision":"After discussion, it was determined that we would use [Create React App](https:\/\/github.com\/facebook\/create-react-app)\\nto create a uniform project structure. With regard to Yarn vs. NPM, we concluded that using Yarn would introduce yet\\nanother dependency and level of abstraction, and that it would not bring enough value to the project to justify those\\ndrawbacks.\\n","tokens":44,"id":5034,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0017-use-strings-for-all-model-id-fields.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo enhance the ability to import data from any source that is compliant with the OpenReferral\\nmodle use strings for fields that are keys.\\n","Decision":"Convert existing models to use strings instead of integers as the id fields and references.\\n","tokens":33,"id":5036,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"link_platform\/0008-use-docker-images-to-deploy-the-application.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUse docker images to deploy the application.\\n","Decision":"Run the application as a docker image even without container orchestrator.\\nFor exmaple, we may run the new build as a container with a different port for testing on the same host as the\\ncurrent production version.\\nThis doesn't require all execution contexts to be docker images.\\nFor example,  we can run the platform in a development container format but run it in Heroku as a non docker instance.\\n","tokens":12,"id":5037,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-local-matching-service-example\/0002-http-status-code-for-validation-errors.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere can be validation errors when the request does not match the contract of the endpoint. These errors are mapped to\\nhttp status code 422(Unprocessable Entity) with one or many error message(s).\\n","Decision":"Matching Service Adapter(MSA) ignores if the response has any other status code other than http status code 200 (OK).\\nSo for now we are letting the Dropwizard handle exception and return status code to MSA.\\nWhen there is validation error, it returns http status code 422 with the error message.\\n","tokens":44,"id":5038,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-local-matching-service-example\/0003-we-will-follow-dwps-proposed-strategy.org","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.** Context\\nDWP performed an investigation into matching using their dataset. They propose a\\nmatching strategy which works well for their particular use case.\\nThe following is a much-simplified version of the proposed matching strategy.\\n#+BEGIN_SRC plantuml :exports results :file images\/flow.png\\nskinparam shadowing false\\nskinparam activity {\\nBorderColor black\\n}\\nstart\\n:Select all records matching any historical surname and date of birth;\\nif (One or more match?) then (yes)\\nif (Postcode present in Verify matching dataset?) then (yes)\\n:Filter only records matching any historical postcode;\\nif (One or more match?) then (yes)\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\nendif\\n:Filter only records matching given name or middlename;\\nif (Exactly one match?) then (yes)\\n#00823B:Match;\\nstop\\nelseif (More than one match?) then (yes)\\nif (Cycle 3 attribute present?) then (yes)\\n:Filter only records matching Cycle 3 attribute;\\nif (Exactly one match?) then (yes)\\n#00823B:Match;\\nstop\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\nelse (no)\\n#DF3034:No match;\\nstop\\nendif\\n#+END_SRC\\n#+RESULTS:\\n[[file:images\/flow.png]]\\n** Decision\\nWe will implement the above matching strategy.\\nFor simplicity, we will ignore any attributes where \"verified\" is set to \"false\".\\n** Consequences\\nThis strategy should be a good starting point for services in similar situations\\nto DWP (large, pre-existing dataset, high chance of new users already being\\npresent in the dataset).\\nThe strategy may work less well for services in other situations, for example\\nthose without a pre-existing database of users who plan on creating a new\\ndataset from Verify users.\\n","Decision":"We will implement the above matching strategy.\\nFor simplicity, we will ignore any attributes where \"verified\" is set to \"false\".\\n** Consequences\\nThis strategy should be a good starting point for services in similar situations\\nto DWP (large, pre-existing dataset, high chance of new users already being\\npresent in the dataset).\\nThe strategy may work less well for services in other situations, for example\\nthose without a pre-existing database of users who plan on creating a new\\ndataset from Verify users.\\n","tokens":456,"id":5039,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"verify-local-matching-service-example\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5040,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"push-sdk-android\/0003-fcm-over-gcm.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe had to decide on whether to implement Firebase Cloud Messaging (FCM) or\\nGoogle Cloud Messaging (GCM), as they both provide push notification support\\nfor Android.\\n","Decision":"We decided to implement FCM (despite there being a fair bit of GCM-related\\ninformation available) since, in Google's own words:\\n> Firebase Cloud Messaging (FCM) is the new version of GCM. It inherits the\\n> reliable and scalable GCM infrastructure, plus new features! See the FAQ to\\n> learn more. If you are integrating messaging in a new app, start with FCM.\\n> GCM users are strongly recommended to upgrade to FCM, in order to benefit\\n> from new FCM features today and in the future.\\nWe believe that GCM will have a long-tail, but that FCM is the recommended way\\nto move forward.\\n","tokens":39,"id":5041,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"push-sdk-android\/0004-divorce-the-sdk-from-firebase.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere was much discussion over whether the SDK should implement the\\nFirebase-related services for token refresh and notification handling, or leave\\nthat up to the consuming app. If the SDK handles it, it's less setup for the\\nconsumer; however, this comes at the cost of much flexiblity.\\n","Decision":"In the end, we decided that the lack of flexibility (e.g. for supporting both\\n\"register on first launch\" and \"register\/unregister on log in and log out\"\\nmodels) warranted removing the services from the SDK. They will be implemented\\nin the demo application instead.\\n","tokens":65,"id":5042,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"push-sdk-android\/0002-avoid-runtime-dependencies.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe don't want to create an SDK that requires customers to add a slew of\\nexternal dependencies.\\n","Decision":"We will avoid external runtime dependencies. For instance, we will use the core\\nJava networking libraries (e.g. HTTPUrlConnection) instead of introducing a\\ndependency on something like [Volley][1].\\nExternal development dependencies (those necessary for contributing, but which\\nare not bundled with the SDK) will be added as necessary.\\n","tokens":24,"id":5043,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"push-sdk-android\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5044,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sesopenko_diamond_square\/ADR-1.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.Context goes here. Describe the forces at play, including technological political, social, and project local. These forces are likely in tension and should be called out as such. The language in this section is value-neutral. It is simply describing facts. Rationale should be self-evident from the context\\n","Decision":"This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\u201c\\n","tokens":61,"id":5045,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"testtrack-cli\/adr-003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs we rely more heavily on generated schemas for validation, undoing\\ndestructive actions like deciding or creating a previously-retired split\\ncould result in deviation between the schema and production, meaning\\nlocal\/prod disparities in the form of missing variants in the schema\\nfile.\\nWhile this is not ideal, it's not the end of the world. We'd like to\\nlimit the impact of this scenario by merging in information from prior\\nmigrations if available.\\nThis stance attempts to balance the desire for correctness with the\\nability to have schemas that don't grow indefinitely (by deleting retired\\nsplits), as well as migration files that can be culled after they reach\\na certain age to cut down on cognitive overhead for app maintainers.\\n","Decision":"When reviving a retired split, the variants of the last creation of that\\nsplit will be merged into the set of variants reflected in the new\\nschema.\\n","tokens":158,"id":5046,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"testtrack-cli\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBecause we're deprecating fully-loaded local TestTrack server instances\\nin favor of the testtrack CLI, client-side validations are the only way\\nof ensuring that migrations will apply cleanly in production.\\nWe're entering a world where developers will likely not have local\\ncopies of all the app repositories that might contribute splits to\\nTestTrack's configuration fully updated at all times, making validating\\nsplit names locally across apps impossible and undesirable to attempt.\\nSo we're seeking to find a balance that can validate as much as we can\\nwhile accepting the fact that cross-app split dependencies won't be\\nvalidatable locally.\\n","Decision":"feature_completions and remote_kills of splits defined by other apps\\nwill not be validated, but you'll have to opt out in one of a few ways\\nin order to skip validation:\\n* Choose a split name prefixed with another app's name (a new-style\\nsplit), which indicates that validation of split presence is\\nimpossible.\\n* Specify a legacy non-prefixed split name with the `--no-prefix`\\noption.  Non-prefixed splits names will not be validated for presence\\nin the schema because they are not obviously tied to any app in\\nparticular, so even though a non-prefixed split might belong to our app,\\nit's not a certainty, and impossible to validate.\\n* Specify that you know that a split name doesn't appear in the schema\\nand you want to write a migration referring to it anyway via the\\n`--force` option. This is important in the case of\\ncreating\/modifying\/destroying remote kills or feature_completions for\\nretired splits which no longer appear in the schema file due to their\\nretirement.\\n","tokens":130,"id":5047,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"testtrack-cli\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are transitioning from Rails migrations and a legacy TestTrack schema\\nformat to testtrack CLI. In order to have confidence that migrations\\nwill apply cleanly in production, we need to validate as much as we can,\\nand we have a path to full information on extant splits.\\n","Decision":"We will not allow retirements of splits missing from the schema. We will\\ninstead import all legacy splits and validate against the schema.\\n","tokens":61,"id":5048,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0001-pipenv-for-package-and-dependency-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe discussion about which package & dependency manager to use has come up\\nrepeatedly over time. The main contenders are `pip`, `pipenv` and `conda`.\\nAll of them have been used (and even been mixed) in different Flowminder\\nprojects and deployments.\\nEach of them has different pros\/cons depending on the context. Here is a\\nbrief summary given our context.\\n(a) pip:\\n- Pros: pure Python, no extra dependencies, [PyPA recommended](https:\/\/packaging.python.org\/guides\/tool-recommendations\/)\\n- Cons: installing packages can be a pain if they involve C extensions or other compiled code (and no wheels are provided); even more so for packages that depend on external libraries\\n(b) [pipenv](https:\/\/docs.pipenv.org\/):\\n- Pros: same as `pip`; comes with support for managing virtual environments; improved dependency management over pure `pip`\\n- Cons: same as `pip`\\n&#40;c) [conda](https:\/\/conda.io\/miniconda.html)\\n- Pros: supports packaging of non-Python dependencies (e.g. third-party libraries), which is advantageous if the user doesn't have full control over the system (e.g. installation in a non-admin environment)\\n- Cons: introduces a third-party dependency; not always clear which conda channels provide which packages\\n","Decision":"We will use pipenv for dependency management.\\n","tokens":290,"id":5049,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0007-mapbox-for-worked-examples.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe tried three different visualisation libraries for producing the maps in the worked examples notebooks (Folium, GeoViews and Mapbox). Another possibility is `deck.gl`\/`kepler.gl` for Jupyter notebooks (https:\/\/github.com\/uber\/deck.gl\/issues\/2929, https:\/\/github.com\/keplergl\/kepler.gl\/issues\/331), but this has not yet been released at time of making the decision.\\nBelow is a summary of the current pros\/cons of each. All three packages are continuing to develop, so pros and cons are likely to change in the near future.\\n### [Folium](https:\/\/python-visualization.github.io\/folium\/)\\nPros:\\n- Has been around longer than mapboxgl-jupyter, so there are more plugins and additional features available.\\nCons:\\n- Poor performance for large datasets.\\n- Large choropleths do not appear in Chrome.\\n### [GeoViews](http:\/\/geo.holoviews.org\/)\\nPros:\\n- Simple to produce maps with drop-down selectors and time sliders, with minimal code.\\n- Part of the PyViz project, so designed to work nicely with many other packages in the Python data science ecosystem.\\nCons:\\n- Dependency on `Cartopy`, which has non-Python dependencies `GEOS` and `proj` (and a potential DLL conflict with the GEOS version included with `shapely`), which makes it very difficult to install with pip\/pipenv (although it's straightforward with conda).\\n- Doesn't have an equivalent of the \"heatmap\" visualisations offered by `Folium` and `Mapbox`.\\n- Very difficult to embed the resulting maps in `nbconvert` markdown output (although it works fine in `nbconvert` html output).\\n### [Mapbox GL](https:\/\/mapbox-mapboxgl-jupyter.readthedocs-hosted.com\/en\/latest\/)\\nPros:\\n- Better performance than Folium.\\n- Maps are easily embedded in the docs pages, and appear in all browsers checked (Chrome, Firefox, Safari).\\nCons:\\n- Currently no support for multi-layered maps or time sliders.\\n- Requires an API token to produce visualisations.\\n","Decision":"We will use Mapbox to produce visualisations in the worked examples. As all packages considered continue to develop, we may decide a different package better suits our needs in the future.\\n","tokens":471,"id":5050,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0004-http-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to wrap the FlowKit toolkit in a single HTTP API, an HTTP framework is required. There are a variety of options for this purpose, e.g. Flask and Django.\\nBoth Flask & Django offer a significant plugin ecosystem, and are 'battle-tested'. However, both are on the heavy side and are bound by legacy design. Of the two, Flask has much less boilerplate overhead.\\nAn alternative option is Quart, which is considerably newer. Quart is compatible with the Flask ecosystem plugin, and built to follow the newer ASGI standard. It is lightweight, offers impressive performance, and takes full advantage of the recent addition of asyncio to Python.\\nQuart also supports websockets, which, while not an immediate priority, are likely to be very useful for future, more dynamic iterations of the API.\\nShould Quart become defunct, the close mapping to the Flask API provides a low-impact exit.\\n","Decision":"The FlowKit API will make use of the Quart framework.\\n","tokens":190,"id":5051,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0010-prefect-for-autoflow.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe first prototype of AutoFlow used [Apache Airflow](https:\/\/airflow.apache.org\/) (as used in FlowETL) to define and execute workflows. However, this proved to be problematic in some respects - Airflow has limited support for parametrising DAG runs and sharing data between tasks, and re-running a DAG for an execution date for which it has already run is complicated.\\n[Prefect Core](https:\/\/docs.prefect.io\/) is an alternative open-source workflow engine, which allows DAGs to be parametrised and run simultaneously for multiple sets of parameters, and allows data exchange between tasks. Prefect also allows the creation of dynamically-generated tasks mapped over the outputs from running another task, which makes it easier for AutoFlow to spawn multiple runs of a workflow when the sensor finds multiple days of data for which the workflow has not previously run.\\n","Decision":"AutoFlow will use Prefect to define and run workflows.\\n","tokens":179,"id":5053,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0008-jupyter-notebooks-for-autoflow.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAutoFlow aims to provide a method for automating workflows involving FlowAPI queries. As executable documents, [Jupyter notebooks](https:\/\/jupyter.org\/) provide a medium for users to define workflows (using FlowClient to communicate with FlowAPI) in a familiar analysis environment, and supply these workflows to AutoFlow to be scheduled and executed. Additionally, Jupyter's markdown cells and inline display of outputs (including images and markdown) make Jupyter notebooks suitable candidates for producing static reports (using [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/)).\\nAnother advantage of using Jupyter notebooks for automated workflows is that if any errors occur during execution, these errors will be displayed inline within the notebook, aiding debugging.\\nWith the decision to automate running notebooks comes the need to parametrise the notebooks at execution time (otherwise we would be repeatedly running an identical notebook and getting the same output). AutoFlow will use [Papermill](https:\/\/papermill.readthedocs.io) to parametrise and execute Jupyter notebooks. Additionally, [scrapbook](https:\/\/nteract-scrapbook.readthedocs.io) (formerly a part of the papermill library) can be used to persist data in a notebook so that it can be re-used in a later notebook. This allows for building workflows from multiple notebooks, for example to produce a daily PDF report that compiles results from multiple separate analyses.\\n","Decision":"AutoFlow will use Jupyter notebooks as the central user-defined components of workflows (both for defining queries to be run and for producing the content of output reports), and will use Papermill to parametrise and execute the notebooks. Scrapbook will also be installed in an AutoFlow deployment, so that users can share data between several notebooks in a workflow.\\n","tokens":292,"id":5054,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0011-redaction-strategy-for-labelled-aggregates.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt present, any rows in a spatial query are dropped if they return an aggregate of 15 subscribers or less. With new\\nlabelling disaggregation queries being added to Flowkit, there is an increased risk of deanonymization attacks\\nusing the increased resolution of information - we need to consider further redaction strategies to mitigate this.\\n","Decision":"For any labelled spatial aggregate queries, we drop any aggregation zone that contains any disaggregation less than 15\\n(for consistency with the rest of the dissagregation strategy).\\n","tokens":71,"id":5055,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0003-http-api.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOriginally, the software tool that became FlowKit was designed to be used as an extensible library, which connected to one shared database. Users would extend their own copy of the library to add query types, or even modify existing ones.\\nThis introduced considerable difficulties, e.g.:\\n- No guarantee that an analysis written by one person could be run by another, or by the same person in future.\\n- All users required highly privileged access to the database\\n- No way to manage usage of shared resource\\n- No way to ensure that upstream changes were in use\\n- Difficult to effectively exploit the ability to reuse already computed results between analysts\\n- Significant complexity and blurred functional boundaries in the main library\\n- Very difficult to use the tool outside the Python ecosystem\\n- Substantial challenges in logging access and activity\\nThis motivates the revised design, where there is a _single_ copy of the library responsible for constructing and running queries, accessed through a language neutral HTTP API. This facilitates some significant improvements:\\n- Easy to produce clients for multiple language ecosystems\\n- Can make substantial changes to the enclosed code, database structure etc. with very little disruption\\n- Supports granular access control\\n- Enables more secure storage of raw data, by removing direct access to the data\\n- Allows for much more efficient sharing of resources\\n- Supports comprehensive logging\\n- Much clearer 'seams' between functional parts, and simpler codebase\\n- Simpler code, because scheduling of query runs is controlled by a single point.\\n- Considerable opportunities to be more efficient in scheduling runs and caching of queries.\\n","Decision":"FlowMachine will be wrapped by an HTTP API.\\n","tokens":328,"id":5056,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0002-pytest-for-testing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTesting is an important component of a fast changing software. Without testing\\nit is not possible to guarantee that new components or changes to the current\\ncodebase do not break pre-existing functionality. The purpose of unit testing in\\nparticular is to ensure that isolated parts of the codebase work as expected by\\nthe developer. Tests should provide enough evidence to the developer that his\\ncontributions are working. It also aids other developers to understand the\\npurpose of each and every component.\\nIn order to reduce the burden of writing tests, we need good test management\\ntools that play well with `python` which is the main scripting language for this\\nproject.\\nThere are many testing framework for Python, including:\\n[`nosetests`](https:\/\/nose.readthedocs.io\/),\\n[`unittest`](https:\/\/docs.python.org\/3\/library\/unittest.html) and\\n[`pytest`](https:\/\/docs.pytest.org\/).\\nWe were previously using `nosetests`, which makes use of classic xunit-style\\nsetup whose main strength is easily declaring setup and teardown methods for\\neach testing function. While these methods are simple and familiar to those\\ncoming from a unittest or nose background,  pytest\u2019s more powerful fixture\\nmechanism leverages the concept of dependency injection, allowing for a more\\nmodular and more scalable approach for managing test state, especially for\\nlarger projects and for functional testing.\\nIn our particular case, we can leverage the power of fixtures to manage docker\\nimages and to define connection procedures. Ideally, all of the testing\\nrequirements are defined within the testing framework. Fixtures are really\\npowerful in providing powerful abstractions for resource allocation.\\n","Decision":"We will use pytest for testing Flowkit components.\\n","tokens":351,"id":5058,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"http-transport\/adr-002-middleware.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[Flashheart](https:\/\/github.com\/bbc\/flashheart), although useful, has become difficult to maintain and extend due to features being coupled into the same client. HttpTransport mitigates this by using `middleware` to extend the Rest clients behaviour. Middleware is comparable to a `plugin` based artitecture. This allows users to add or change behaviour without having to make changes to the core client. This conforms to the [open\/closed principle](https:\/\/en.wikipedia.org\/wiki\/Open\/closed_principle).\\n","Decision":"We have decided to use [Koa](https:\/\/github.com\/koajs\/koa) middleware via the [Koa compose](https:\/\/github.com\/koajs\/compose) library, rather than creating our own custom implementation. We opted to use this library because:\\n* It's a well tested library used extensively in production environments\\n* Aids the implementation of caching layers (see example above)\\n* Familiar syntax (express\/Koa)\\n* Supports async\/await\\n### Example middlware stack\\n```js\\nasync function middleware1(ctx, next) {\\nconst req = ctx.res \/\/ handle request\\nawait next();       \/\/ invokes the next middleware\\nconst res = ctx.res \/\/ handle response\\n}\\n\/\/ etc ...\\nasync function middleware2() {}\\nasync function middleware3() {}\\n\/\/ register using `.use`\\nhttpTransport\\n.use(middleware1);\\n.use(middleware2);\\n.use(middleware3);\\n```\\nThis would unwind the `stack` in the same way as Express\/Koa does:\\n```\\n1st middleware ---> 2nd middleware ---> 3rd middleware ---> HTTP request\\n|\\n|\\nv\\n1st middleware <--- 2nd middleware <--- 3rd middleware <--- HTTP response\\n```\\nThis aids with modules such as caching with transformations in between:\\nCaching middleware:\\n```js\\nfunction modifyHeaders(req, res, next) {}\\nfunction redisCache(req, res, next) {}\\nhttpTransport.use(modifyHeaders)\\n.use(redisCache);\\n```\\nMiddleware execution order:\\n```\\nmodifyHeaders ---> redisCache ---> HTTP request\\n|\\n|\\nv\\nmodifyHeaders <--- redisCache <--- HTTP response\\n```\\nThis ensures the Caching module caches the request as it enters the pipeline and requires the minimum amount of processing to recreate the cache key despite the transport modifying it further.\\n### Terminating the middleware chain\\nTerminating a chain is achieved by suppressing the call to `next()`\\n```js\\nasync function cachingMiddleware(ctx, next) {\\nconst req = ctx.res\\nif (isCached(req)) {\\nreturn;\\n}\\nawait next(); \/\/ call next middleware, allowing chain to continue\\nconst res = ctx.res\\n\/\/ handle setting caching response\\n}\\n```\\n","tokens":107,"id":5060,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"google-cloud-cpp-common\/2018-06-19-do-not-duplicate-server-side-validation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**: many APIs impose restrictions on their parameters. Names may have\\nlength limits, or strings may need to conform to specific formats, or ranges\\nmust be non-empty.  It is tempting to validate these arguments in the\\nclient-side, since we *know* the API will fail if it is allowed to continue\\nwith the invalid parameters. However, any such validation is redundant, since\\nthe server is going to validate its inputs as well. Furthermore, any such\\nvalidation is wasteful: most of the time the application will pass on valid\\narguments, checking them twice is just wasting client-side CPU cycles. Moreover,\\nthe server-side is the source of truth, so having the validations performed on\\nthe client side wil require extra work if the server restrictions are ever\\nmodified.\\n**Decision**: the client libraries do not validate any of the request\\nparameters beyond what is required to avoid local crashes.\\n**Consequences**: the main downsides are:\\n- The error messages could be better if generated locally.\\n- The server must validate all inputs correctly.\\n- Sometimes, we will need to validate inputs to avoid library crashes, and\\nthat will be confusing because it will seem like a violation of this ADR.\\n","Decision":"parameters beyond what is required to avoid local crashes.\\n**Consequences**: the main downsides are:\\n- The error messages could be better if generated locally.\\n- The server must validate all inputs correctly.\\n- Sometimes, we will need to validate inputs to avoid library crashes, and\\nthat will be confusing because it will seem like a violation of this ADR.\\n","tokens":253,"id":5062,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"google-cloud-cpp-common\/2018-06-13-storage-always-retries.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":5064,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"google-cloud-cpp-common\/2019-01-04-error-reporting-with-statusor.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must\\nwork for all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":5065,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"google-cloud-cpp-common\/2018-06-18-storage-request-parameters-are-function-arguments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n\/\/ Create a new bucket handle that applies the given optional parameters to\\n\/\/ all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","Decision":"that needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket \/* or Object as applicable *\/ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n","tokens":498,"id":5066,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"phpadr\/0001-documenting-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nRecord certain design decisions for the benefit of future team members as well as for external oversight.\\n","Decision":"Use Architecture Decision Records (ADR), that is a technique for capturing important architectural decisions, along with their context and consequences as described by [Michael Nygard](https:\/\/twitter.com\/mtnygard) in his article: [Documenting Architecture Decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":22,"id":5067,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"phpadr\/0002-develop-tool-to-manage-adrs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEncourage and facilitate the use of documentation in agile projects in world of evolutionary architecture using the technique Architecture Decision Records (ADR).\\n","Decision":"Develop a command-line tool to manage the Architecture Decision Records (ADR) that will be stored in version control along with the project source code.\\n","tokens":29,"id":5068,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"phpadr\/0006-yaml-as-configuration-file.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to use a custom ADR template, it must be possible to configure the path to it. The same template must be used so that all ADR`s are structured in the same way.\\n","Decision":"The template path can be defined via a [YAML](http:\/\/yaml.org\/) configuration file.\\n","tokens":44,"id":5069,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"phpadr\/0003-php-as-scripting-language.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe tool must be cross-platform and developed using a open source programming language in command-line interface to be used in any project independent of the tech stack.\\n","Decision":"Develop using [PHP](http:\/\/php.net\/) (PHP Hypertext Preprocessor) to work command line scripting.\\n","tokens":34,"id":5071,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"phpadr\/0004-composer-as-dependency-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nManaging dependencies manually in any programming language is hard work, then we will need to use a dependency management tool for that project.\\n","Decision":"It will be used the [Composer](https:\/\/getcomposer.org\/) as tool for dependency management.\\nThis project can also be installed with Composer using the following command:\\n```\\ncomposer require globtec\/phpadr --dev dev-master\\n```\\n","tokens":29,"id":5072,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docker-texlive\/0002-provide-all-packages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nShould the Docker image include all packages or a subset of the packages?\\n","Decision":"Chosen option: \"Provide all packages\", because\\n* texliveonfly does not work on all packages\\n* speeds-up compilation time (because no additional download)\\nWe accept that the final image is ~2GB of size.\\n","tokens":21,"id":5073,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"docker-texlive\/0001-do-not-base-on-any-tex-image.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe Docker image can be made from scratch or base (`FROM`) on an existing one.\\nWhen choosing an existing one, which one should be taken?\\n## Decision Drivers\\n* Self-maintaining an image (from scratch) is hard and one repeats the mistakes, others have done\\n* Patching an existing image (via PRs) might lead to rejections\\n","Decision":"* Self-maintaining an image (from scratch) is hard and one repeats the mistakes, others have done\\n* Patching an existing image (via PRs) might lead to rejections\\nChosen option: \"Do not base on any tex image\", because\\n* breaking changes on \"base images\" could come in\\n* could become unmaintained (e.g., <https:\/\/github.com\/adinriv\/docker-texlive>)\\n* the `install-tl-ubuntu` script could get unmaintained\\nWe accept that\\n* We are based on debian sid, which constantly changes\\n* We will have to monitor the upstream repository if texlive 2019 is released and possibly adapt our Dockerfile.\\n* We get a large image - more than 4 GB.\\n* We have to install font packages separatetly for each font (e.g., [fonts-texgyre](https:\/\/packages.debian.org\/sid\/fonts\/fonts-texgyre))\\n","tokens":82,"id":5074,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-5.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nGiven the unique use case of having six classifiers on a single interface, there will likely be times the classifiers will need to communicate with one another and the app on a global scale. This is most apparent when considering notifications: where one classifier will have to send messages, subjects, and answers to another classifier. It simply isn't enough to have one parent view model communicate with a child and vice-versa. There should be a pub\/sub technology used to remove some of the complexity in architecture.\\n### Decision\\nAlthough some WPF frameworks come shipped with a messenger, I will build my own simple messenger to better understand what is happening behind the scenes with a messenger. Building a messenger from scratch while also remove some reliance on a third-party framework.\\n### Status\\nAccepted\\n### Consequences\\nWhile a messenger does fulfill some needs in the app (inter-classifier communication), it is easy to become too dependent on a messenger. That said, a messenger should be used sparingly and only when there is no other way for the view models (or other components) to communicate with one another.\\n_In Retrospect:_ The messenger is incredibly useful for a multi-touch table. While I initially relied too much on the messenger, some later refactoring removed that dependence in lieu of more parent-child communication in the view models. I also found the messenger was helpful when some of the view models needed to initiate animation in a view due to an event firing.\\n","Decision":"Although some WPF frameworks come shipped with a messenger, I will build my own simple messenger to better understand what is happening behind the scenes with a messenger. Building a messenger from scratch while also remove some reliance on a third-party framework.\\n### Status\\nAccepted\\n### Consequences\\nWhile a messenger does fulfill some needs in the app (inter-classifier communication), it is easy to become too dependent on a messenger. That said, a messenger should be used sparingly and only when there is no other way for the view models (or other components) to communicate with one another.\\n_In Retrospect:_ The messenger is incredibly useful for a multi-touch table. While I initially relied too much on the messenger, some later refactoring removed that dependence in lieu of more parent-child communication in the view models. I also found the messenger was helpful when some of the view models needed to initiate animation in a view due to an event firing.\\n","tokens":295,"id":5075,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-4.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nWhile being a novice to C# and WPF, I thought it might be helpful to adopt a popular framework to make the development process easier and remove some of the unnecessary \"gotchas\" of writing in a new language. Using a framework might also quicken the pace of development and remove the need to write many helper classes.\\n### Decision\\nThe Galaxy Zoo touch table app will not use a framework. Adopting a framework seems like a shortcut to learning the basics of C#, and I want to focus more on _how_ the language works rather than than simply getting something working. Also, a framework might cause a lot of overhead if only a few aspects of the framework are being used. Learning a new framework along with learning C# and .NET might be overwhelming.\\n### Status\\nDeclined\\n### Consequences\\nWhile some frameworks are well documented (MVVM Light), others are quickly becoming deprecated (Silverlight), so there is a worry that any framework could soon lose favor in the .NET community. By declining the use of a framework, there is the potential of adding more dev time that will be spent creating a messenger, view model base, and other components often found in a WPF framework.\\n_In Retrospect:_ After becoming more familiar with WPF, I wish I had chosen a framework (perhaps MVVM Light) to achieve some of my needs for this app. For example, although I created my own messenger based on a Stack Overflow post, I often ran into issues with the messenger and was unable to find advice on Stack Overflow as most of those answers reference the MVVM Light messenger. I don't think MVVM Light would've been a huge load on the code. It's likely using a framework could've cleaned up some sections of code. I can see the merits of declining a framework for novices to WPF, but I don't think much is gained from going without.\\n","Decision":"The Galaxy Zoo touch table app will not use a framework. Adopting a framework seems like a shortcut to learning the basics of C#, and I want to focus more on _how_ the language works rather than than simply getting something working. Also, a framework might cause a lot of overhead if only a few aspects of the framework are being used. Learning a new framework along with learning C# and .NET might be overwhelming.\\n### Status\\nDeclined\\n### Consequences\\nWhile some frameworks are well documented (MVVM Light), others are quickly becoming deprecated (Silverlight), so there is a worry that any framework could soon lose favor in the .NET community. By declining the use of a framework, there is the potential of adding more dev time that will be spent creating a messenger, view model base, and other components often found in a WPF framework.\\n_In Retrospect:_ After becoming more familiar with WPF, I wish I had chosen a framework (perhaps MVVM Light) to achieve some of my needs for this app. For example, although I created my own messenger based on a Stack Overflow post, I often ran into issues with the messenger and was unable to find advice on Stack Overflow as most of those answers reference the MVVM Light messenger. I don't think MVVM Light would've been a huge load on the code. It's likely using a framework could've cleaned up some sections of code. I can see the merits of declining a framework for novices to WPF, but I don't think much is gained from going without.\\n","tokens":391,"id":5077,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-7.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nThe app needs a convenient way to deploy to multiple devices. However this is done, installing and updating the app should be effortless for users. Ideally, the developer would make any necessary changes to the app, publish those changes to a location, and someone on the other end would only need to double click a download\/update link.\\n### Decision\\nOlder WPF and WinForm applications used a separate setup application, which was responsible for guiding someone through the process of installing an app. A user first downloaded the setup application, which would be responsible for installing the separate application on a system. We abandoned this route early in the development process due to the frequent errors that occurred during the install process.\\nClickOnce is the standard publishing method for WPF applications. Visual studio even references ClickOnce under the \"Security\" tab of the properties window. Documentation on ClickOnce is also quite robust. Because of this, we will accept ClickOnce as our method of choice when publishing.\\n### Status\\nAccepted\\n### Consequences\\nThere aren't many options to choose from when it comes to deploying a WPF application. Unfortunately, if ClickOnce doesn't easily suit our needs, we'll likely have to spend a good bit of time troubleshooting.\\n_In Retrospect:_ Unfortunately, it is recommended that ClickOnce deploys to a CD-Rom, a website, or a file share\/UNC path. This seems particularly dated, especially with the CD-Rom reference. Also, the Adler Planetarium is deprecating their local file share system in lieu of using Google Drive. This is why we use Google Drive File Stream (see wiki) when publishing, which works _fine_. As WPF isn't necessarily cutting-edge technology, I don't see the publishing process evolving much in the future. While ClickOnce _usually_ works well, we'll have to deal with any errors in deployment as we have our hands tied with using ClickOnce.\\n","Decision":"Older WPF and WinForm applications used a separate setup application, which was responsible for guiding someone through the process of installing an app. A user first downloaded the setup application, which would be responsible for installing the separate application on a system. We abandoned this route early in the development process due to the frequent errors that occurred during the install process.\\nClickOnce is the standard publishing method for WPF applications. Visual studio even references ClickOnce under the \"Security\" tab of the properties window. Documentation on ClickOnce is also quite robust. Because of this, we will accept ClickOnce as our method of choice when publishing.\\n### Status\\nAccepted\\n### Consequences\\nThere aren't many options to choose from when it comes to deploying a WPF application. Unfortunately, if ClickOnce doesn't easily suit our needs, we'll likely have to spend a good bit of time troubleshooting.\\n_In Retrospect:_ Unfortunately, it is recommended that ClickOnce deploys to a CD-Rom, a website, or a file share\/UNC path. This seems particularly dated, especially with the CD-Rom reference. Also, the Adler Planetarium is deprecating their local file share system in lieu of using Google Drive. This is why we use Google Drive File Stream (see wiki) when publishing, which works _fine_. As WPF isn't necessarily cutting-edge technology, I don't see the publishing process evolving much in the future. While ClickOnce _usually_ works well, we'll have to deal with any errors in deployment as we have our hands tied with using ClickOnce.\\n","tokens":394,"id":5078,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-9.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nNow that the touch table is on the museum floor, we expect new errors to crop up, especially as the table is now used throughout the day by multiple people at a time. The main issue we had on the first week is the table frequently crashing after short bursts of activity. It is not the most productive approach to keep an eye on the table, discover crashes by checking the event viewer, reboot the app, and hope it works while a patch is being made.\\nIdeally, the table will automatically report errors as they occur so developers can debug remotely without having to keep constant watch on the table.\\n### Decision\\nOne huge issue here is the table crashing and leaving a bare Windows desktop on the museum floor. In response to this, a batch script is running on the table's command prompt that restarts the application if it detects app closure from a crash.\\nAs for automated errors reporting, we will implement Sentry to send an email whenever the table experiences a new crash. Although many of the small bugs were addressed the first week, it's likely new bugs will appear as the table stays on the floor. This way, even though the app will restart on a crash, developers will know when a bug needs to be patched.\\n### Status\\nAccepted\\n### Consequences\\nThe application is more or less running on autopilot using both Sentry and a batch script. Hopefully, this does not cause lack of urgency when a bug does appear. Sentry mentions where the app crashes and provides some breadcrumbs to trace back the bug, and this should be sufficient for remote debugging.\\n","Decision":"One huge issue here is the table crashing and leaving a bare Windows desktop on the museum floor. In response to this, a batch script is running on the table's command prompt that restarts the application if it detects app closure from a crash.\\nAs for automated errors reporting, we will implement Sentry to send an email whenever the table experiences a new crash. Although many of the small bugs were addressed the first week, it's likely new bugs will appear as the table stays on the floor. This way, even though the app will restart on a crash, developers will know when a bug needs to be patched.\\n### Status\\nAccepted\\n### Consequences\\nThe application is more or less running on autopilot using both Sentry and a batch script. Hopefully, this does not cause lack of urgency when a bug does appear. Sentry mentions where the app crashes and provides some breadcrumbs to trace back the bug, and this should be sufficient for remote debugging.\\n","tokens":321,"id":5079,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-6.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nThe app needs tests. This is a must. Although test driven development is best, the app added tests after development began to test logic and encourage future changes do not break functionality. It would be best to test all aspects of the MVVM architecture, but view models are where the majority of the app logic lives.\\n### Decision\\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n","Decision":"Several testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n","tokens":434,"id":5080,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-8.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nThe application runs well when using Panoptes for subject retrieval and Caesar for classification counts. Although the touch table has never lost internet connection (using WiFi) and internet connection on the museum floor is quite reliable, we should be prepared for internet outages, however abrupt or short. Although Caesar works well as a new service, we should be prepared for a situation where Caesar is unresponsive.\\n### Decision\\nWe will use a local database in the case our external dependencies (Panoptes and Caesar) are unresponsive. The local database will be responsible for holding subject information from Panoptes and classification counts from Caesar. Furthermore, we will hold subject images locally so we will not have to wait for a potentially large array of subject images returning from the api.\\nWe will also use a classification queue similar to the one used in PFE, whereas a new classification is kept in a queue if unable to be created through the Panoptes api (eg. in the case of lost internet connection).\\n### Status\\nAccepted\\n### Consequences\\nThis will require a good amount of overhead to choose new technologies and write the logic to keep subjects locally. This will introduce a new part of the app to maintain while making sure the table has the same subjects locally as well as on the project builder. Things could get hairy on the table if a keen eye isn't kept on keeping the local database and subject set up to date.\\nMy main worry is that using a local database will add another layer of complexity to the app that will make it a bit more difficult for a new developer to maintain the app. In this case, we will have to rely on strong documentation to ensure this doesn't happen.\\n_In Retrospect:_ Overall, I'm very satisfied with this decision. The app really only _needs_ internet connection to create a classification. Also, the app should perform well even if local subject images aren't available. The table will keep track of classification counts correctly in the case Caesar is down. Caesar is mainly used to provide accurate classification counts in case the table counts get out of sync. I strongly recommend using a local database on any future development.\\n","Decision":"We will use a local database in the case our external dependencies (Panoptes and Caesar) are unresponsive. The local database will be responsible for holding subject information from Panoptes and classification counts from Caesar. Furthermore, we will hold subject images locally so we will not have to wait for a potentially large array of subject images returning from the api.\\nWe will also use a classification queue similar to the one used in PFE, whereas a new classification is kept in a queue if unable to be created through the Panoptes api (eg. in the case of lost internet connection).\\n### Status\\nAccepted\\n### Consequences\\nThis will require a good amount of overhead to choose new technologies and write the logic to keep subjects locally. This will introduce a new part of the app to maintain while making sure the table has the same subjects locally as well as on the project builder. Things could get hairy on the table if a keen eye isn't kept on keeping the local database and subject set up to date.\\nMy main worry is that using a local database will add another layer of complexity to the app that will make it a bit more difficult for a new developer to maintain the app. In this case, we will have to rely on strong documentation to ensure this doesn't happen.\\n_In Retrospect:_ Overall, I'm very satisfied with this decision. The app really only _needs_ internet connection to create a classification. Also, the app should perform well even if local subject images aren't available. The table will keep track of classification counts correctly in the case Caesar is down. Caesar is mainly used to provide accurate classification counts in case the table counts get out of sync. I strongly recommend using a local database on any future development.\\n","tokens":439,"id":5081,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/create-react-app.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nIt would be nice to have `babel-plugin-jsx-remove-data-test-id` but I don't know of a way to add it to Create React App.\\nit would be possible to fork CRA and adjust the config there. However, that seems to involved the following:\\n1. Fork CRA\\n2. Add plugin to babel-preset-react-app\\n3. Publish developerdavo-babel-preset-react-app\\n4. Point to developerdavo-babel-preset-react-app in react-scripts\\n5. Publish developerdavo-react-scripts\\nNot to mention, local development requires package linking and updating dependencies in developerdavo-babel-preset-react-app would require publishing two packages.\\n### Decision\\nIt doesn't seem to be worth the maintenance cost\\n","Decision":"It doesn't seem to be worth the maintenance cost\\n","tokens":156,"id":5083,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/stylelint.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\n- linting for CSS\\n- After running it, it didn't pick up a single error\\n### Decision\\nDon't start using stylelint\\n","Decision":"Don't start using stylelint\\n","tokens":33,"id":5084,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/sentry.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\n- error tracking\\n### Decision\\nI don't think it will be easy to justify to a client because it is not yet mainstream in enterprise development and I'm not super passionate about it. So I'm not going to invest the time learning it now.\\n","Decision":"I don't think it will be easy to justify to a client because it is not yet mainstream in enterprise development and I'm not super passionate about it. So I'm not going to invest the time learning it now.\\n","tokens":55,"id":5085,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/react-hooks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\n- new way of writing React code\\n### Decision\\nSince it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\n","Decision":"Since it is still considered new and will be overwhelming for new developers, I don't think it's worth starting yet.\\n","tokens":39,"id":5086,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/prettier.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/classnames.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nThe `classnames` library makes it easier to combine class names.\\n### Decision\\nStart using\\n","Decision":"Start using\\n","tokens":23,"id":5088,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/typescript.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nTypescript offers better IDE integration and type checking.\\n### Decision\\nStart using Typescript.\\n","Decision":"Start using Typescript.\\n","tokens":23,"id":5089,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/styled-components.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/cypress.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\n- e2e\/UI testing framework\\n- Was not able to get it to run on Netlify\\n### Decision\\nI won't be able to start using it until I can get it working with a CI tool.\\n","Decision":"I won't be able to start using it until I can get it working with a CI tool.\\n","tokens":48,"id":5091,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"my-notes\/react-testing-library.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.### Context\\nIt's annoying when tests break when refactoring.\\n### Decision\\nStart using React Testing Library\\n","Decision":"Start using React Testing Library\\n","tokens":23,"id":5092,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"apim-blue-green-deploy\/0002-allowed-search-api-versions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe REST API for Azure Search requires the API Version identifier to be passed as the query parameter `api-version` for all calls<sup>[1]<\/sup>.\\nAt the time of writing the General Available (GA) version is '2017-11-11' and the Preview version is '2017-11-11-Preview'. Attempting to retrieve an index definition for an index which includes Preview features with a query using the GA version in the query parameter `api-version` results in a HTTP error.\\nAs a result of this we need to support the parametrisation and validation of the search API version. Validation could be against values held in environment variables or against config values within the code itself.\\n","Decision":"* In order to maintain backwards compatibility the API Version identifier is an optional property in the payload. The default value is the current GA version.\\n* The validation will be based on config values **_**within**_** the code to check the api version parameter against a range of allowed values. The reason we have adopted this approach over using environment variables is that the code has been written to support specific versions of the API and therefore the responsibility for establishing this relationship rests with the code not the environment.\\n","tokens":150,"id":5093,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"apim-blue-green-deploy\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5094,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"postfacto\/0002-replace-p-flux-with-redux.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCurrently browser state is managed by [p-flux](https:\/\/github.com\/pivotal-cf\/p-flux). This tool hasn't been updated for some time and causes a number of issues that make it harder to work with the code:\\n1. Static dispatch of actions\\n* This makes it harder to test as evidenced by the complex global set up in `application_globals.js` that each test has to hook into.\\n2. Data segregation\\n* This isn't entirely caused by p-flux but at the moment, the entire store's state is passed down to every component as there is no way to connect specific components to the store.\\n3. Hard to see and reason about state\\n* We log each action to the console but other than that we have no way really understand and debug how and when state changes.\\nIn terms of alternatives, Redux seems to be winning as the most common, go to state, management system.\\n","Decision":"The reason this hasn't been done before is because the p-flux patterns and components are woven throughout the code, so changing it is complex and risks. However, this proposal suggests that there is a way to incrementally make those changes as such:\\nFor each piece of state in the store (i.e retros, retro, session)\\n1. Define redux reducer to store and manipulate state in all the ways main-dispatcher.js does\\n2. Create a 'boundActionCreator' that can dispatch the events identified 1.\\n3. Pass the action creator to main-dispatcher as a dependency\\n4. Call action creator whenever state is modified in main-dispatcher\\n5. Replace state provided to router from p-flux with redux by connecting router to store\\nDoing this for each piece of state means we will have remove p-flux from the storage of state, however we will still be relying on it for action dispatch and will still have poor data segregation.\\nAt this point, we can start pushing state down to child components by connecting them directly rather than passing them down. Once we have done this, we can swap the static dispatch calls from those components to use redux's mapDispatchToProps approach, also replacing `api-dispatcher` with redux middleware.\\nAt this point most of p-flux will be gone with a couple of exceptions such as the `analytics-dispatcher` which should probably be refactored into either dependencies or redux middleware.\\n","tokens":195,"id":5095,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"postfacto\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5096,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0002-secure-JSON-output.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe are working on a new feature of Form Builder where a form creator (user) can\\nopt to have submissions (citizen answers) sent as JSON to an endpoint of their choosing.\\nThese endpoints can be any url, but will likely be CMS or case management systems.\\nThe first use case of this is the HMCTS complaints form. This form requires an\\nintegration to HMCTS' Optics system. We will be creating a custom adapter that receives\\nthe JSON output from Form Builder, and then generates the complaint in Optics. This\\nadapter will live outside the Form Builder infrastructure. In the future, the hope\\nis that teams will create their own adapters in order to integrate Form Builder\\nwith various external systems.\\nThis ADR is regarding how to secure this JSON output from Form Builder to the\\nendpoint set by the user.\\nThreats\\n- Payload may be intercepted, exposing sensitive user answers.\\n- Payload may be modified in transit, with potentially damaging consequences.\\n- POST requests could be made to the receiver (adapter) from sources other than Form Builder,\\ncreating records in their systems that don't reflect user submissions.\\nRisks\\n- Reputational damage due to data breaches\\n- Security threats from modified payloads\\nMistakes from the User\\n- Users could potentially enter an incorrect endpoint, which would deliver sensitive\\ndata to an incorrect endpoint.\\n- Users could share security information accidentally, allowing malicious actors\\nto receive payloads.\\n","Decision":"### TLS over HTTP\\nUse TLS over HTTP (HTTPS). This would ensure\\nencrypted data in transit, which [satisfies MOJ standards](https:\/\/ministryofjustice.github.io\/security-guidance\/standards\/cryptography\/#cryptography).\\nWe came to the conclusion that mutual TLS doesn\u2019t really bring any benefits and\\ninvolves way too much management overhead in this context, so agreed on normal\\nTLS + payload encryption with a pre-shared secret.\\n### Encrypted JSON Payload\\nEncryption of the payload is possible with JWE (JOSE toolkit). This would require\\ndecryption at the other end. Payload encryption is preferred over TLS alone as TLS\\nis often terminated at the edge of a large network with communications travelling in the clear\\ninside that network. Given the sensitivity of the submitted data for some forms\\nand that it may need to be relied on in court, we need more confidence that only\\nthe intended recipient can read the data and that other actors on that network\\ncan\u2019t impersonate FB by sending other requests to the endpoint.\\n### Shared Secret\\nThe shared secret will be set as an ENV var in the publisher to be consumed by\\nthe form's runner instance. The shared secret will be used to encrypt \/ decrypt\\nthe payload. This shared secret would ideally be system generated rather than user\\ngenerated to ensure that it is appropriate for the encryption method we choose.\\n### Certificates vs Shared Secret\\nBoth certificates and shared secrets can be used for signing and de-serialising the payload.\\nGiven users may not be technical, a shared secret would be preferred as requiring users\\nto generate and upload certificates may be too much to ask.\\n### Validation of remote endpoint\\nThe Form Builder system should validate the endpoint that is entered by the user.\\nAt minimum this should be an HTTPS endpoint and `*.gov.uk`.\\n### Overview of Solution\\nForm Builder:\\n- Connects with adapter via HTTPS using a ruby library such as [Net:HTTP](https:\/\/ruby-doc.org\/stdlib-2.6.3\/libdoc\/net\/http\/rdoc\/Net\/HTTP.html).\\n- Connection with TLS using `Net::HTTP` gem or similar.\\n- Encrypts JSON payload using a shared secret and JWE protocol.\\n- Sends as POST request.\\nAdapter:\\n- Receives HTTPS POST request from Form Builder.\\n- Decrypts JSON payload using shared secret.\\n","tokens":304,"id":5097,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0005-integration-with-github.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe decision to use Github, and git itself, as the versioning system for the forms was not recorded at the time so this is in place of that.\\nThe Form Builder platform needs a place to store the JSON metadata that the editor generates. This metadata is used by the Runner Node's in order to create the forms upon deployment.\\n","Decision":"The main reasons for choosing git and Github:\\n- Transparency. All the forms are published in the open for the public to see.\\n- Versioning. Git itself is a versioning tool and Github provides a user friendly view into that.\\n- Collaboration. Github provided a built in approach to collaboration that was felt to be necessary for the future users of the editor.\\n- It was low effort to integrate on the premise that the initial users of the editor would be either developers or those with some background of git.\\n","tokens":71,"id":5100,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0009-editor-online.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPart of MoJ Online\u2019s toolkit is the Editor. This software allows a user (form owner) to build and test a form. The Editor is actually made up of several pieces of software, but mainly: the Editor and the Console Application.\\nThe Console Application is an electron application which has 3 functions:\\n* Installs the Editor\\n* Runs the Editor (previews the form using the runner)\\n* Creates a github repository\\nThe Editor does not interact with any of the services instead it produces a collection of files which is finally stored into Github. A link to Github is then used by the Publisher to obtain the files and create the form online.\\nCurrently the editor application is managed in the electron console application, each time either you edit a form or create a form the console will start a web server on an unused port and open the editor in the hosts browser. Therefore you could have more than one form editing at the same time. This application is only available for Macs and is signed, notarised and stapled by Apple to run.\\nIt was decided to redesign the Editor to be more self service and to this end the method of storing the form metadata was changed - see [ADR-00?](0007-replacing-the-storage.md).\\n","Decision":"After evaluating the different options and including the direction of the product, it was decided to rebuild the editor using Ruby on Rails and custom javascript for the front end.\\n","tokens":260,"id":5101,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0004-microservice-authentication-method.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere are currently 3 different ways that microservices talk to each other in the form builder platform JWT, JWE and API keys.\\nConsistency can benefit the service by keeping a minimum of protection across all endpoints and help developers better understand the system.\\n**Constraints**:\\n- Backend services should not be allowed to make arbitrary calls with each other without the authorisation of a `runner` form submission.\\n- The `runner` should be the only service able to authorise new submissions.\\n","Decision":"JWT supports the use of tokens that are signed with a private key and verified with a public key.\\nThe frontend `runner` applications possess the private key needed to sign a JWT token.\\nA token is in the header in every request.\\nBackend applications can then use the preexisting `token API` to get the public key of that service and use it to verify the JWT tokens signature.\\nJWT verification allows for all backend APIs to check the origin of a request is a currently active form.\\nApplications can use the token given to them from a running form to make new requests to sibling APIs on its behalf.\\nTherefore APIs do not possess private keys (which would allow them to make arbitrary calls)\\nUser data continues using its own layer of protection using encrypted user tokens however this is now incorporated into JWT in the `sub` (subject) claim.\\n","tokens":105,"id":5103,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0007-replacing-the-storage.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe center of the Form Builder Platform is the form metadata, which is a collection of json files. These files are created by the Editor application and stored into Github using Git to version and transfer. The Editor does not interact with any of the services instead it produces a collection of files which is finally stored into Github. A link to Github is then used by the Publisher to obtain the files and create the form online.\\nGithub was chosen due to it giving transparency, versioning, some form of collaboration, data ownership and it could be integrated to the local services easily being built. Another reason was that the user base for Form Builder would be Developers who would have an understanding of version control and git and therefore be beneficial.\\n### Why the change\\nThere is a shift in the product proposal moving from a supported model to a full self service model, this has shown that using git and github is a blocker or at least friction point for new users. Also, to facilitate this move to being a true Self Service platform the method of user intervention between the editor and the publisher.\\n","Decision":"Remove the use of git and github to store the form metadata.\\n","tokens":219,"id":5104,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"form-builder\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will keep ADRs in the `decisions` directory of this repository.\\nDecisions will be written using Markdown and named `adr-NNN-AAAA.md`.\\n","tokens":16,"id":5105,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"afterwriting-labs\/003-release-process.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTravisCI is currently used for building feature branches, pull requests and releases.\\nThe way releases, however, are performed is a bit convoluted and not flexible. By default minor version is updated\\nand it's not clear how to do a major or patch release. Due to the way TravisCI builds feature branches vs pull requests\\nit requires hacks in travis config.\\n","Decision":"Solution implemented in https:\/\/github.com\/ifrost\/starterkit have worked quite well so far. It's based on adding\\nlabels to commit messages when merging a pull request to master.\\nIt's way easier to do major\/minor\/patch or hotfix release and publish to npm separately.\\n","tokens":80,"id":5106,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"afterwriting-labs\/001-code-conventions--in-progress.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe whole codebase should be easy to read.\\n","Decision":"Each file should look like it was created by one person to make it easier to follow. Defined standards should help\\nmaking decision of where code should be added and how it should we written.\\nCode conventions will be added in separate files, modified accordingly to need. A rule should become a convention if:\\n* It helps to find the right place to put code\\nexample: Controllers should be responsible for business logic, hence they should never manipulate DOM\\n* or it makes code more readable\\nexample: All variables should have meaningful names\\n* or it helps prevent from bugs\\nexample: Always add \"use strict\" to each file\\nIf a rule does not support neither, should not be a convention (e.g. \"Always use.forEach instead of for\" does not support\\nany of above)\\n","tokens":14,"id":5107,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"afterwriting-labs\/002-technical-debt.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTechnical Debt is something that emerges as trade-off between short-term benefits and long-term value. It's OK to have\\nTechnical Debt as long as it measurable and manageable. At any point of time it should be possible to say how big the\\ndebt is, what parts of code are affected and how critical the debt is. It's required to help to plan and to estimate\\nnew features.\\n","Decision":"A little twist will be added to old good TODOs that all developers hate. Here it goes:\\n* When debt is identified a TODO note is added to file\/function\/code fragment depending on the scope;\\n* Debt Markers are added to the description (a debt marker is \"+\");\\n* Each time code is modified, the related debt should be:\\n* paid by fixing TODO,\\n* or increased by adding another Debt Marker;\\n* Debt cannot be greater than 5 (\"+++++\"). When debt reaches 5, no code can be modified before fixing the TODO;\\n* If debt for a code fragment is increased, the debt for the function and file also increases;\\nIt should assure that Technical Debt is tracked. In places of code that is not used frequently, debt will not increase too\\noften. There's no need to pay the debt for unused code.\\nIf code, file, function is used frequently, any related debt should be dealt with quickly. Debt markers will make sure\\neach time code in debt is used, the debt increases, forcing to pay it sooner.\\nIn cases of critical debt, a TODO note can be created with more than one initial Debt Marker.\\nEvery TODO must have at least one Debt Marker.\\nExample todo note:\\n\/**\\n* TODO: Improve performance from O(n^2) to O(n). (+++)\\n*\/\\nfunction foo() {\\n\/\/ ...\\n}\\nIf a TODO has multiple lines, Debt Markets must be added at the same line to make it easier to parse. Markers can be placed\\nat the beginning, e.g.:\\n\/\/ TODO: (++) Move method to presentation controller\\n\/\/ and add unit tests.\\nfoo: function() {\\n...\\n}\\n","tokens":85,"id":5108,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhile developing APIs which store metadata about datasets, versions, filter jobs\\nand import jobs using Postgres, it has been found that some updates required\\ncomplex transactional updates across multiple tables.\\nWe need to assess our data model and decide if a relational database\\nbest fits this use case, or what alternative would be best.\\n","Decision":"MongoDB will be used to store metadata for datasets, versions, filter and import\\njobs. This allows us to prioritise the read heavy interactions, storing the\\ndata in the same JSON format it will be presented to users.\\n","tokens":70,"id":5109,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0005.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAdditional screens for the publishing workflow were designed given all the\\npossible fields regardless of how they are retrieved or edited from the API.\\nThese fields were broken down into suggested logical groupings for users to edit.\\nThis separation means multiple API calls could be needed to save updates on a\\nsingle screen, and that datasets will have to be added with versions depending\\non which fields are updated (going against [decision 0004](0004.md)).\\n","Decision":"Screens should be separated to update either the dataset or version metadata\\nbut not both in one.\\n","tokens":96,"id":5110,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0007.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFilters originally contained all details defining the selection and the static\\ncontent the selection generated. This was split, so the resource for defining\\nselections was separate to the outputs it created, which means a step is needed\\nto create the output resource on submission.\\nThis could be achieved through a query parameter on the filter endpoint, so that\\nas a user is updating their filter they update it to submitted. This could also\\nbe achieved by creating a new endpoint, which a user POSTs to and provides an\\nID for the filter they wish to submit. Both options have the same limitation that\\nthe filter may have been changed by another user between the last GET and when\\nit is submitted.\\nThe new endpoint solution is considered more RESTful.\\n","Decision":"The most RESTful solution of creating resources via a POST is preferable\\n","tokens":156,"id":5111,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide whether the Customise My Data beta will be hosted\\nas part of the existing ONS website and infrastructure, or whether\\nit will be hosted independently.\\n","Decision":"The beta will be hosted on beta.ons.gov.uk, on its own dedicated infrastructure.\\nThis reduces the risk of CMD-specific changes impacting on our existing service.\\n","tokens":39,"id":5112,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0006.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to create a filter, a user must provide a unique way of referencing\\na specific version\/instance document. Instance IDs uniquely identify the document\\nbut it may not be clear to users what this ID represents.\\nIt is likely that users understand which version, of which edition, of which\\ndataset they are interested in - and this also uniquely identifies the document.\\n","Decision":"Filters should be created using a combination of dataset\/edition\/version IDs\\n","tokens":80,"id":5113,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0004.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen designing APIs to handle metadata about datasets, editions and versions\\nno particular limitations were placed on which order these documents should be\\nedited in.\\nIn the existing publishing workflow, a single item can be added to only one\\ncollection at a time. As the APIs have separated formerly joined concepts, it\\nis necessary to understand whether datasets can be edited or published in\\nisolation from versions.\\n","Decision":"Adding a version to a collection does not prevent the dataset being edited in a different collection\\n","tokens":85,"id":5114,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0003.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen building new micro-services defaults were given to some configuration values so\\napplications would work locally 'out of the box'. This was not done for all values\\nor in all services.\\nThere was discussion around whether it would be better for the service to\\nerror\/exit if any configuration value was not provided on startup, to be clear\\non what needs to be set.\\n","Decision":"All services will be built with default configuration values set - with defaults\\nthat will most likely work when running services locally.\\n","tokens":80,"id":5115,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0010.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nThe list of codes in the code-list may not exactly match the list of codes in\\nthe dimension options, due to sparsity or how datasets are broken down.\\nIn order to build a list of datasets which use each code, a relationship needs\\nto be built between the code-list\/code graph and the instance\/dimension-option\\ngraph. There was a question as to whether this should link:\\n- **A) From: code-list, To: instance** -  As instances may contain sparse\\nsubsets of a code-list, this option would still require extra work to know\\nwhich codes specifically existed per instance.\\n- **B) From: code, To: instance** - This effectively duplicates the existing relationships to an instance, as it is the same as the dimension options.\\n- **C) From: code, To: dimension option** - The extra step to traverse back up\\nto the instance decreases the performance.\\n- **D) Don't build a relationship at all, just query for it on request** -\\nThis is very un-performant, and something that can easily be optimized by\\nbuilding the relationships.\\n","Decision":"A relationship should be created between a code node and each instance node which\\nfeatures the code as a dimension option. (Option B)\\n","tokens":288,"id":5116,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0008.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. This dimension metadata is stored in APIs.\\nAPIs which hold secure or market sensitive data must meet different NFRs with\\nregards to security and publishing. It is important to understand what data is\\nsubject to these NFRs or not.\\nIt was suggested that hierarchies and code lists may not be considered sensitive,\\nand so the APIs which control access to this data would not be subject to the\\nsame NFRs.\\n","Decision":"Hierarchies and code lists do not need to be published, and do not need a\\npreview option.\\n","tokens":134,"id":5117,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"dp\/0009.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ikforth\/0002-32-bits-code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n32-bits systems were all the rage when this project was concieved back in 1999.\\n64-bits systems were not available yet.\\n16-bits and 8-bit systems were not interesting.\\n","Decision":"IKForth is implemented as 32-bits code with 32-bits CELL size.\\n","tokens":47,"id":5119,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ontrack\/0002-extension-indicators-identifiers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nExample: take principles from an asciidoc source.\\nFor sync, having uuid might be an issue unless we use a \"source\" optional\\nattribute. That might make things more complex...\\nA source can be empty (when created from GUI) or set to the FQCN of the\\nprovisioner.\\nEven with a source attribute, having UUID does not make the automatic\\nidentification of a category or a type, based on name only, easy.\\nHence the need for a semantic id.\\nRisks of collision for provisioners remains and provisioned items will have\\nto prepend the id of their provisioner to their id.\\nA qualified source attribute still make sense to identify items which\\ncannot be edited. To fit with the usual way of Ontrack, the source must\\nbe a FQCN which maps to an `Extension`.\\n","Decision":"* do not use UUID for indicator items (categories, types & protfolios)\\n* for provisioned items, prepend the id of the provisioner to the item id\\n* introduce a source attribute and an indicator source model\\n","tokens":184,"id":5121,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0003-combined-codebase.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHistorically Alfresco has had a bad experience of having a monolithic codebase in SVN. The main issue with this was the\\ninability to easily work on feature branches, resulting in frequent conflicting changes. To make matters worse this was\\nseen as hard to move to git, since GitHub has\/had a maximum limit on repository size and there were several large\\nbinary artifacts stored in the Alfresco history [[1]].\\nMore recently the Alfresco codebase has been split into a large number of small git repositories [[2]].\\nThis proposal is to merge the various git repositories together. By doing this we aim to:\\n1. Remove the effort of creating releases of library projects.\\n1. Allow tests to be written on the same branch (in the same repository) as the production code (i.e. facilitate TDD).\\n1. Make it easier for everyone (especially people outside the team and new starters) to find the 'right bit' of code.\\n[1]: https:\/\/community.alfresco.com\/community\/ecm\/blog\/2015\/04\/01\/so-when-is-alfresco-moving-to-github\\n[2]: https:\/\/ts.alfresco.com\/share\/proxy\/alfresco\/api\/node\/content\/versionStore\/version2Store\/a0c2492f-6354-4b98-adfc-e63d5c2209f5\/SearchCodeBase.png\\n","Decision":"We will merge the search-related repositories together and preserve their history.  We will not attempt to merge code\\nthat other teams also need (for example the TAS test utilities or the alfresco-data-model projects).\\n","tokens":290,"id":5124,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0006-release-synchronisation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the Search Services and Insight Engine\\nrepositories.  In [ADR 4: \"Community Mirror\"](0004-community-mirror.md) we discussed how we would set up a build job to\\nensure community code is available to the community. In particular we decided:\\n> We will mirror `master` and all branches starting with `release\/` to a branch with the same name on GitHub.\\n","Decision":"During the combining of the Search Services and Insight Engine codebases we will create a branch `master` by merging\\nthe existing `master` branches of those two projects. We will update the version of Insight Engine to match Search Services - that\\nis 1.4.0-SNAPSHOT.\\n","tokens":108,"id":5126,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0009-message-driven-content-tracker-next-gen.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis is a second iteration of content tracking via message bus design. See [previous version](0007-message-driven-content-tracker.md).\\nNew content tracker implementation will be based on new Search Services architecture (SS v2.0 or next gen). Main context behind this decision is almost the same as for v1 - get more throughput by leveraging new Transform Service.\\n","Decision":"The decision is based on [version 1](0007-message-driven-content-tracker.md). The main differences are:\\n* Shared File Store may not be the right option as it is only available for Enterprise. Alternatively the URL to content can point to other locations. (TBC)\\n* The change in behaviour requires a major release of Search Services, most likely version 2.0.\\n* The changes in Content Repository will be available from version 6.3.\\n* The synchronous transformation APIs will remain functional until 7.0.\\nDetails of the architecture to be clarified (WIP).\\n","tokens":77,"id":5127,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0005-merge-tests-and-production-code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the production and end-to-end test\\nrepositories. In [ADR 4: \"Community Mirror\"](0004-community-mirror.md) we discuss setting up a mirror for the community\\ncode.\\n","Decision":"We will separate the end-to-end test code in half so that any code solely related to Insight Engine won't be mirrored.\\nWe will remove the existing test groups for the different versions of Search Services and Insight Engine, and instead\\ndelete any tests from branches where they should not be run.\\n","tokens":68,"id":5129,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0004-community-mirror.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the Search Services and Insight Engine\\nrepositories.  Since we want to enable the community to submit pull requests to the Search Services project we need a\\nway to keep this code up to date on GitHub.\\n","Decision":"We will mirror `master` and all branches starting with `release\/` to a branch with the same name on GitHub.  We will\\nexclude the alfresco-insight-engine-parent directory. We will include these commands as part of our build to do this:\\n```\\n# This avoids making changes to the original branch.\\nget checkout -b tempBranch\\n# This strips all enterprise changes (in a reproducible way) and pushes any updates to the mirror.\\ngit filter-branch -f --prune-empty --index-filter 'git rm -r --cached --ignore-unmatch alfresco-insight-engine-parent'\\ngit push out HEAD:$branch\\n# This resets us back to where we were before the filtering.\\ngit checkout $branch\\n```\\n","tokens":69,"id":5130,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0007-event-content-tracker.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe current approach of the *Content Tracker* in *Search Service* is to query SOLR for any `dirty` documents which it then fetches from Alfresco. Once the content is successfully obtained from Alfresco, it marks it `clean` which eventually get committed to the index. This approach will need to be modified as it applies pressure on Alfresco on every call to get the content. Taking an event based approach where the *Content Tracker* subscribes to a topic with policy the specific behaviour will allow to get the extracted content when ready.\\nDifferent alternatives have been evaluated at [Event Based Content Tracker Spike](https:\/\/github.com\/Alfresco\/SearchServices\/blob\/master\/alfresco-search\/doc\/architecture\/event-trackers\/event-based-content-tracker-spike.md)\\nThis proposal is to develop a new Content Tracker that consumes content based on events.\\n","Decision":"We will use an Event Oriented Content Tracking based in Apache Kafka. This product supports recovering from a previous event, so catching up with the Repository and re-indexing operations are granted.\\n","tokens":177,"id":5131,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"SearchServices\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCapture and record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":15,"id":5132,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"content-data-admin\/adr-000-document-architectural-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe aim to:\\n- Make it easier to understand the codebase and its status\\n- Reduce the number of meetings to handover information across teams\\n- Facilitate team rotations across GOV.UK\\n","Decision":"Track architectural decision that impact the status of the Content Data Admin, [following a lightweight format: ADR][1]\\n","tokens":46,"id":5133,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"university-manager-mock-frontend\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSince both developers will be working in paralel, and time is very limited,\\nan architecture that allows paralel work is highly beneficial.\\n","Decision":"We will implement the backend using REST API principles, allowing both modules to work independently.\\n","tokens":33,"id":5134,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"university-manager-mock-frontend\/adr-002.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nSince both developers will be working in paralel, and time is very limited,\\nan architecture that allows paralel work is highly beneficial.\\n","Decision":"We will implement the backend using REST API principles, allowing both modules to work independently.\\n","tokens":33,"id":5135,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"profiles-db-elastic\/0003-use-elastic-co-docker-image.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[ElasticSearch](https:\/\/www.elastic.co\/products\/elasticsearch) is a Java based search engine built on top of technologies like Lucene and Solr. It is open source software which can be freely [downloaded](https:\/\/github.com\/elastic\/elasticsearch) and installed on a number of operating systems. It is also freely available from elastic.co as a Docker image and as a subscription based SaaS offering. The SaaS option is discounted in ADR [0002](0002-use-docker-image-for-elastic.md).\\n","Decision":"We will use the official Docker image provided by elastic.co.\\n","tokens":112,"id":5136,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"profiles-db-elastic\/0004-bulk-load-gp-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOther work within the same domain has created a JSON file containing all GP practices that need to be searched and exposed via ES. This datafile was originally created to be loaded into MongoDB. Whilst Elastic Search (ES) can import the JSON document in the existing format it takes considerable time. Re-shaping the data to that required by the bulk import API of ES means the import can happen in a matter of seconds rather than many minutes otherwise.\\n","Decision":"We will use the existing data file but reshape the data specifically to suit the needs of ES.\\n","tokens":92,"id":5137,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"profiles-db-elastic\/0002-use-docker-image-for-elastic.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[ElasticSearch](https:\/\/www.elastic.co\/products\/elasticsearch) (ES) is a Java based search engine built on top of technologies like Lucene and Solr. It is open source software which can be freely [downloaded](https:\/\/github.com\/elastic\/elasticsearch) and installed on a number of operating systems. It is also available from elastic.co as both a Docker image and as a cloud service.  The SaaS offering from elastic.co has a cost approaching $100\/month for a production set up. Our default approach to provisioning is to use containerisation in general, and Docker in particular. There are no compelling reasons not to use Docker containers to provision and deploy Elastic Search.\\n","Decision":"We will use Docker containers to provision and deploy Elastic Search.\\n","tokens":144,"id":5138,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"rbac\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe definitive schema that it will be used is represented in the next link:\\n![Database schema](.\/database_schema_adr-001.svg \"Database schema\")\\nIn the definition of the schema, some decisions were made to adapt it to the proper context of this library.\\n","Decision":"1. The `user` table won't exist as part of the database schema of the library. The `user_id` column in `rbac_role_assignment` and `rbac_super_admin` tables will store the values of the primary key of the existing application database \"user table\". As the name of the table is unknown to the library, the `user_id` column won't be defined as a foreign key.\\n2. If a user id exists in the `rbac_super_admin` table, further checks won't be performed and the permission will be considered as `granted`.\\n3. The `rbac_role_assignment` table will relate a user, with a role in a context.\\n4. The `rbac_context_type` table describes the different types of resources that will exist in an application. The values in that table are totally attached to the application domain and will be pre-defined by the application itself, not the library.\\n5. The `rbac_context` table will store each and every instance of the resources present in the application, for each type of `rbac_context_type`s. Hierarchical relationships between resources will be defined by the `parent_id` parameter.\\n6. The `rbac_role_permission` table will store the list of permissions granted or denied to a role.\\n7. When checking if a permission is granted or denied in a given context, the full hierarchy of the context will be used for the check. This means that once the permission is denied in a higher context in the hierarchy, it will be considered denied in the lower contexts automatically. That is, permission denial completely overrides permission grants.\\n8. The `rbac_permission` table relates permissions with context types. A permission will only make sense for a certain `rabc_context_type` and this relation will be defined in this table.\\n9. For now Optional\/Advanced functionality won't be developed.\\n","tokens":60,"id":5141,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nContext for rate limiting pattern\\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"adr-generation-tool\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5143,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"commcare-cloud\/0002-back-service-elasticsearch-with-custom-implementation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe `service` command is has become a useful way for us to start, stop, restart,\\nand check the status of the many service processes behind a commcare cluster.\\nFor the most part, these commands map very predictably to a start\/stop\/restart\/status\\ncommand on either [`service`](https:\/\/linux.die.net\/man\/8\/service) or `supervisorctl`.\\nRecently and in the past, we have found that `elasticsearch` does not stop or restart\\nreliably. In particular, after running `service elasticsearch stop`,\\nusing `ps aux | grep elasticsearc[h]` may still show a running Elasticsearch\\nprocess, even if `service` is reporting a stopped status. This then requires\\nkilling the rogue process in order for a `start` to bring up a genuinely new process.\\nBefore the addition of the `service` command, we already had the `restart-elasticsearch`\\ncommand, which is conceptually (though currently not functionally) equivalent to\\n`... service elasticsearch restart`. It does not address the above issue of rogue\\nprocesses, but it does address some other issues, including a reminder to stop all pillows,\\nas well as theoretically providing rolling (and thus no-downtime) restart,\\nwhich we currently do not benefit from because we do not store data in duplicate.\\n","Decision":"We will be changing `... service elasticsearch ...` from using the default\\nbehavior to performing a more targeted set of actions that are guaranteed to achieve the\\nstop\/start\/restart\/status action, and we will also be deprecating the `restart-elasticsearch`\\nentry point. In particular, `... service elasticsearch stop` will\\n- stop pillows\\n- guarantee the process has stopped and `kill -9` it after a period of time if it has not\\nand `start` will\\n- start pillows\\nand `restart` will be functionally a `stop` followed by a `start`.\\nPros:\\n- Someone with little context will be able to run a `... service elasticsearch ...`\\nand have it have the intended overall consequence\\n- The tool will incorporate what is currently heard knowledge\\nCons:\\n- A \"power\" user may think of the command as executing a very specific other command,\\nand may be taken aback by the tool trying to outsmart them.\\nTo mitigate this con, the command will also explain itself beforehand,\\nand print out the command you could run to execute the narrower command,\\nand ask for confirmation before continuing.\\nSee https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/1721#discussion_r187730933\\nfor a short exchange that demonstrates this trade-off.\\n","tokens":285,"id":5144,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"commcare-cloud\/0003-implement-one-command-to-deploy.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDuring some refactoring of our static file versioning code, there existed several issues which caused static files to be served incorrectly.\\nThese errors propagated in several different ways across the site, but the root issue was always that there was a bug around how JS files were being served.\\nFull context can be found in the following PRs:\\n- Initial refactoring: https:\/\/github.com\/dimagi\/commcare-hq\/pull\/24938\\n- Failed attempts at fixing `force_update_static`:\\n- https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/3148\\n- https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/3186\\n- Removal of `hotfix_deploy` and `force_update_static`: https:\/\/github.com\/dimagi\/commcare-cloud\/pull\/3204\\n- Simplification of the deploy static file process: https:\/\/github.com\/dimagi\/commcare-hq\/pull\/25284\\n","Decision":"We are removing the previous commands `hotfix_deploy` and `force_update_static`.\\nAny future deploys must go through `commcare-cloud <env> deploy`\\n","tokens":202,"id":5145,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"skypy\/adr-01.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"skypy\/adr-02.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"api-docs\/0006-version-based-directory-and-path.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nOld or specific versions of a documentation can not be accessed.\\nOnly the actual version exists online. Every project's documentation\\nis in the same directory, no structure.\\n","Decision":"Create project specific directories under the `\/_posts` directory and\\ncreate version numbered directories e.g.: `\/_posts\/android\/1.0`,\\n`\/_posts\/ios\/2.1`. Put each documentation file (`README.md`, `CHANGELOG.md`, ...)\\ninside the version numbered directory.\\nCreate one YAML Front Matter index file for each product which points\\nto the latest version, see example below where latest version is `0.5`.\\nUse the `permalink: \/player-sdk\/android\/latest` key to specify where\\nthe latest documentation is accessible online, like:\\nhttp:\/\/developers.ustream.tv\/player-sdk\/android\/latest\\n```\\n\/_posts\/android\/2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5)\\nweight: 30\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 1\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/latest\/\\n---\\n{% include_relative 0.5\/README.md  %}\\n```\\nChange the `include_relative` setting to include the latest version.\\nUse the `categoryItemIsShown: 1` setting to direct Jekyll to show\\nthis document when listing category contents.\\nCreate a YAML Front Matter index file in each version numbered directory\\nin which the version is specified, example:\\n```\\n\/_posts\/android\/0.5\/2017-02-02-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5.x)\\nweight: 3\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/0.5\/\\n---\\n{% include_relative README.md  %}\\n```\\nUse the `categoryItemIsShown: 0` to hide this version from category listing,\\nas only the latest should be listed.\\nPrevious version can be accessed online using urls like:\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.4\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.5\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/latest\/ - which only points to `0.5`\\n","tokens":38,"id":5150,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"api-docs\/0005-using-markdown-in-jekyll.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nJekyll supports the markdown syntax by default. Files must be prefixed\\nwith a YAML Front Matter section for the engine to categorize, format\\nand display the blog post\/page. Project documentation files (e.g.: `README.md`)\\ntaken from a project release bundle must not have YAML Front Matter section\\nor any Jekyll or third-party specific formatting but pure markdown syntax.\\n","Decision":"Create YAML Front Matter index files in the `\/_posts` directory,\\nlike `2017-01-01-readme.md` and use the `include_relative`\\n[Jekyll command](https:\/\/jekyllrb.com\/docs\/includes\/) to include\\nthe unmodified `README.md`. Create a new layout format in Jekyll\\nnamed `markdown.html` and use the `markdown` value specifying\\nthe `layout:` in the YAML Front Matter index file, see example:\\n```\\n2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Sample title here\\nweight: 3\\ncategory: a-category-here\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\n---\\n{% include_relative README.md  %}\\n```\\nThe `markdown.html` should have the same contents as the `post.html`\\nwith an extra `article`, `section` tags surrounding the `{content}` tag\\nto keep formatting consistent.\\n","tokens":81,"id":5151,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"api-docs\/0004-use-markdown-as-documentation-format.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProject documentation is formatted using HTML. Documentation is accessible only online.\\nA browser is needed to read the pages. Too much noise around the important text.\\nDocumentation can differ from the version bundled with the project,\\nusually not updated online. Update needs HTML editing\/formatting\/reformatting.\\nNot portable.\\n","Decision":"Use the [markdown syntax](https:\/\/daringfireball.net\/projects\/markdown\/syntax)\\nto format documentation.\\n","tokens":67,"id":5152,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"api-docs\/0002-use-jekyll-to-serve-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nProject documentation must be published on the internet.\\n","Decision":"Use [Jekyll](https:\/\/jekyllrb.com\/) on our servers to serve these documents just like GitHub Pages do.\\n","tokens":13,"id":5153,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"api-docs\/0003-blog-post-as-a-documentation-page.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA document must be inserted into Jekyll.\\n","Decision":"For each project document, create a new blog post in Jekyll.\\n","tokens":13,"id":5154,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reaction-docs\/0002-12factor-config-from-env.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFrom [12factor.net](https:\/\/12factor.net\/config):\\n#### Store config in the environment\\nAn app\u2019s config is everything that is likely to vary between deploys (staging,\\nproduction, developer environments, etc). This includes:\\n* Resource handles to the database, Memcached, and other backing services\\n* Credentials to external services such as Amazon S3 or Twitter\\n* Per-deploy values such as the canonical hostname for the deploy\\nApps sometimes store config as constants in the code. This is a violation of\\ntwelve-factor, which requires strict separation of config from code. Config\\nvaries substantially across deploys, code does not.\\nA litmus test for whether an app has all config correctly factored out of the\\ncode is whether the codebase could be made open source at any moment, without\\ncompromising any credentials.\\nThe twelve-factor app stores config in environment variables (often shortened\\nto env vars or env). Env vars are easy to change between deploys without\\nchanging any code; unlike config files, there is little chance of them being\\nchecked into the code repo accidentally; and unlike custom config files, or\\nother config mechanisms such as Java System Properties, they are a language-\\nand OS-agnostic standard.\\nAnother aspect of config management is grouping. Sometimes apps batch config\\ninto named groups (often called \u201cenvironments\u201d) named after specific deploys,\\nsuch as the development, test, and production environments in Rails. This\\nmethod does not scale cleanly: as more deploys of the app are created, new\\nenvironment names are necessary, such as staging or qa. As the project grows\\nfurther, developers may add their own special environments like joes-staging,\\nresulting in a combinatorial explosion of config which makes managing deploys\\nof the app very brittle.\\nIn a twelve-factor app, env vars are granular controls, each fully orthogonal\\nto other env vars. They are never grouped together as \u201cenvironments\u201d, but\\ninstead are independently managed for each deploy. This is a model that scales\\nup smoothly as the app naturally expands into more deploys over its lifetime.\\n","Decision":"Follow the twelve-factor advice and use configuration from the environment.\\nWe will centralize the interface for the environment into a single location so\\nthat we can enforce strict requirements on vars. For example, the application\\nshould fail fast if a requirement is not valid. A `PORT` environment variable\\nshould be validated that it is positive number within the accepted port values.\\n#### Strictness\\nWe're currently using the npm package [`envalid`][1] to provide this\\nfunctionality. There are several libraries that serve this purpose.\\n[`envalid`][1] was specifically chosen because it doesn't have any\\nfunctionality that would allow developers to break this decision.\\nSome libraries allow merge of config files. Some libraries allow usage of\\n`NODE_ENV`. These features allow (if not encourage) developers to do the wrong\\nthing.\\n","tokens":449,"id":5156,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0001-user-ms-dynamics-for-contact-case-document-management.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHandling pre-applications for licenses requires opening cases, attaching relevant supporting information (documents) and recording the decisions & communications made by operators and SEPA staff.\\nSEPA also needs to be able store information about the operator contacts.\\n","Decision":"MS Dynamics offers the best value in terms of \u2018out the box\u2019 functionality, interoperability with other cloud services & existing SEPA systems (CLAS) and has an existing knowledge and resource base in house.\\n","tokens":50,"id":5158,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0010-azure-blob-storage-file-chunking-containers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAzure Blob Storage has been proposed as the storage mechanism for CCP case & supporting files, because of the interoperability with other Azure & Microsoft (Dynamics) services.\\nHow the documents are uploaded & stored needs to be considered taking into account the following:\\n1. How will large file uploads be supported that can be restarted in the case of connection error, or resumed in the case of a user closing their browser?\\n2. How can the files be grouped & related to the Dynamics case (permit), in isolation within the Azure Blob Storage service?\\n","Decision":"Azure Blob storage supports file uploads via block 'chunks' where a file is split into smaller parts that are uploaded and recommitted into the full file once all\\nblocks have uploaded.  This enables the following:\\n* browser upload limits are avoided as the file is usually split into 4-5 MB chunks\\n* more efficient uploads as multiple chunks can be uploaded in parallel\\n* in the case of the failed or interrupted uploads, the number of successfully uploaded blocks for a file can be requested from Azure.  Using this information the application can then resume an upload by sending only the remaining blocks, before committing the entire file.\\nAzure Blob Storage supports grouping files together into 'containers', essentially a folder structure.  SEPA should create a container for each permit application naming it with the CLAS CAR reference number.\\nAzure Blob Storage also supports file metadata and this should be used to store additional user information (user id) and the CAR reference.\\n","tokens":115,"id":5159,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0007-document-permissions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAzure Blob Storage will be used to store all files associated with permit application.  Both front stage and back stage users need to be able to read and upload documents to a 'case file'.\\n## Decision Drivers\\n* Back stage users should be able to see all files relating to a permit\\n* Front stage users should only be able to view and download files that are relevant to them.\\n* Sensitive data privacy \/ security requirements mean access to certain documents should be restricted for both front stage and back stage users.\\n","Decision":"* Back stage users should be able to see all files relating to a permit\\n* Front stage users should only be able to view and download files that are relevant to them.\\n* Sensitive data privacy \/ security requirements mean access to certain documents should be restricted for both front stage and back stage users.\\n[Option 2] Using a combination of Active Directory groups and SAS tokens provides the greatest flexibility to manage document access.\\n### Positive Consequences\\n* Front stage users are only able to access documents that are relevant to them, this would be managed by the frontend application & the document type attribute stored in the CRM \/ document metadata.\\n* Using SAS tokens for sensitive documents also simplifies the private access.\\n### Negative Consequences\\n* SAS tokens would potentially need be generated for each file download.\\n","tokens":112,"id":5160,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0009-custom-frontend-application-backstage-users.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nMicrosoft Dynamics CRM has been proposed as the system to manage contacts, accounts & permits (cases) for the CCP prototype.\\nThe interface used by back stage users (SEPA modellers, registry, officers, regulators) to process tasks and actions against permits should be sufficiently customisable to support a good user experience.\\n## Decision Drivers\\n* Data is stored in the CRM so all interactions must be compatible with the data entities available.\\n* A user-centered design approach requires the technical solution to support custom journeys and styling as much as possible.\\n* A custom frontend application has also been proposed for the front stage users (applicants).\\n","Decision":"* Data is stored in the CRM so all interactions must be compatible with the data entities available.\\n* A user-centered design approach requires the technical solution to support custom journeys and styling as much as possible.\\n* A custom frontend application has also been proposed for the front stage users (applicants).\\n[Option 1] Reusing the application's components and functionality built for the front stage users would provide both a unified best practice user experience, and a decoupled frontend application that minimises the reliance upon and customisations required to the Dynamics CRM.\\n### Positive Consequences\\n* SEPA creates & relies upon a reusable set of back stage components that can be used by other project's requiring a web interface.\\n* Least reliance \/ lock in to CRM vendor, both in terms of customisations but also in terms of the domain specific resources required.\\n* Best support for evolutionary design & architecture - changing the standalone frontend application would be less costly compared to changing the CRM interface.  Facilitates hypothesis driven development.\\n### Negative Consequences\\n* The Dynamics UI would retain it's default labelling, which could sit out of the context of 'permitting'.\\n* Additional resources required to maintain the standalone frontend application.\\n* A custom backstage needs to be considered in the wider SEPA architectural context.\\n","tokens":134,"id":5161,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0005-user-groups-roles-permissions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe application needs to control access to permits & their supporting documents based on user roles.\\n## Decision Drivers\\n* Applicants and operators will be grouped by 'account' in the CRM, however SEPA should be able to differentiate between an organisation's users & it's lead contacts  (administrators).\\n* SEPA staff will potentially require different access levels depending on the permit application; sensitive data requirements means certain documents should only be accessible to users with specific privileges.\\n","Decision":"* Applicants and operators will be grouped by 'account' in the CRM, however SEPA should be able to differentiate between an organisation's users & it's lead contacts  (administrators).\\n* SEPA staff will potentially require different access levels depending on the permit application; sensitive data requirements means certain documents should only be accessible to users with specific privileges.\\n[Option 1] SEPA staff should be assigned to Active Directory groups based on their roles, the following are suggested but need to be reviewed based on a more thorough investigation of roles.\\n### Positive Consequences\\n* This option maintains a basic user hierarchy that would be available outside of the Dynamics system that would potentially be useful if the accounts are used in other applications\\n### Negative Consequences\\n* Potentially more overhead in terms of group management & to an extent duplication in terms of the CRM account to contact grouping.\\n","tokens":100,"id":5162,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0003-authenticate-frontstage-users-azure-b2c.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application.\\nActve Directory B2C (business to consumer) provides existing 'user flows' and allows for creation of custom policies that configure & customise the authentication process.  Which option is most suitable needs to be considered taking into account security & support for the custom UX.\\n## Decision Drivers\\n* Identity management must allow for permission based access to MS Dynamics 365 & file storage solutions.\\n* The level of customisation available for the sign up & sign in process should support the user journey being prototyped as much as possible.\\n","Decision":"* Identity management must allow for permission based access to MS Dynamics 365 & file storage solutions.\\n* The level of customisation available for the sign up & sign in process should support the user journey being prototyped as much as possible.\\n[Option 2] Using Azure B2C with an existing 'Sign up and sign in' policy leverages the best out of the box authentication.  Users register and login using an Azure hosted service, users are redirected back to the application where a valid OAuth JWToken identifies who is currently logged in.\\nA custom domains would be best option, but this seems to be unavailable at the time of writing.\\n### Positive Consequences\\n* Leverages best practice for auth now and in the future.\\n### Negative Consequences\\n* Additional policy customisations would be required to fully customise the flow of the registration and login process (see links).\\n* Standalone HTML\/CSS files will be need to be managed and uploaded to \/ updated in the Azure Directory\\n","tokens":161,"id":5164,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0008-custom-frontend-application-frontstage-users.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nMicrosoft Dynamics CRM has been proposed as the system to manage contacts, accounts & permits (cases) for the CCP prototype.\\nUsers should be able to recognise that the service is provided by SEPA (styling) and the interface should be sufficiently customisable\\nto support a good user experience.\\n## Decision Drivers\\n* Data is stored in the CRM so all interactions must be compatible with the data entities available.\\n* A user-centered design approach requires the technical solution to support custom journeys and styling as much as possible.\\n","Decision":"* Data is stored in the CRM so all interactions must be compatible with the data entities available.\\n* A user-centered design approach requires the technical solution to support custom journeys and styling as much as possible.\\n[Option 1] A custom web application, combining a modern reactive frontend framework with a backend API gateway to interact with Dynamics and Azure services via restful webs APIs\\nwould provide the best support for optimising the user journey.\\nAvoiding the use of a hosted web application (MS Portals), by decoupling the frontend, reduces the reliance on a single platform.\\n### Positive Consequences\\n* Complete control over the user interface and user journey.\\n* SEPA can reuse & build upon shared web patterns & components.\\n### Negative Consequences\\n* Additional overhead required for maintenance, testing and deployment.\\n","tokens":113,"id":5165,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"sepa-customer-platform\/0002-use-azure-blob-storage-store-large-supporting-data-files.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA cloud storage solution is required to allow operators and SEPA staff to upload the large supporting data files required to process an application.\\nThe storage solution would need to integrate with the case management tool (Microsoft Dynamics 365)\\n","Decision":"MS Dynamics offers close integration with Azure Cloud services, therefore Azure Blob Storage has been selected.\\n","tokens":48,"id":5166,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"Corvus.Retry\/0001-centralised-access-to-isleepservice.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen implementing nullability we found that the code around `ISleepService` did not fit well with nullable references. There were static properties which never returned `null` but could usefully be set to `null` at the end of a unit test (to revert to the default behaviour).\\nWe began to update the implementation so that the intent of the property was clearer (using methods for setting and unsetting the property) but found that there were many places within the codebase which had their own versions of the `ISleepService`.\\n","Decision":"We centralised the access to `ISleepService` by implementing a static class `SleepService` which provided access to an instance of `ISleepService` which defaulted to the non-test behaviour but can be set to override that behaviour.\\nWe made this new behaviour `internal` because it's purely for testing purposes and we allowed the `Corvus.Retry.Specs` project to access internals within `Corvus.Retry`.\\n","tokens":113,"id":5167,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"identity-site\/0003-remove-basscss.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to seamlessly incorporate the Login.gov and US Web Design System (USWDS), we need to remove BassCSS\\n","Decision":"We will remove BassCSS FROM THE `identity-site` repo.\\n","tokens":28,"id":5168,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"identity-site\/0002-add-netlifycms.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe team would like more control in editing Login.gov content instead of having an engineer edit a large YML file. The goal is to not have to know code in order to make edits to Login.gov.\\n","Decision":"We will add Netlify CMS (Content Management System) to the architecture of the Login.gov.\\n","tokens":45,"id":5169,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0003-pace-projects-will-have-independent-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPACE contains a number of independently developed and released projects with separate source and build pipelines.\\nCoordination of documentation from projects using three different primary languages (MATLAB, Python, C) with varying release cycles will create tight coupling between the projects and their builds.\\n","Decision":"Each project will maintain its own GitHub pages documentation that will be updated with the project's build-release-deploy pipeline.\\n","tokens":56,"id":5173,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0013-light-python-wrapper-implementation-detail.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs described in [ADR #12](0012-matlab-python-wrapper.md),\\nPACE aims to provide both a Python and a Matlab user interface (UI) for all its constituent programs.\\nIn addition, these programs should be able to interact with each other regardless of the user's environment.\\nThat ADR decided that the Matlab UI for Brille and Euphonic (which currently only have Python UIs)\\nwould use an interface library called [`light_python_wrapper`](https:\/\/github.com\/pace-neutrons\/light_python_wrapper).\\nA meeting between the developers of these programs on Jan 7 2021 also agreed the implementation details given here.\\nIn addition, the meeting also decided that any computation or logic required for interfacing Brille\/Euphonic\\nwith other PACE programs should be written in *Python* so as to be accessible to both the Matlab and Python UI\\nwithout the need for compiled Matlab.\\n","Decision":"* Any computation required for use with Horace or other Matlab programs would be implemented in *Python*.\\n* This Python code would be distributed as a separate module, to be installable using `pip`.\\n* The Matlab UI itself would be a pure interface layer (very little computation)\\nand be provided mainly by `light_python_wrapper`.\\n* `light_python_wrapper` will be moved to a separate repository, and used as a submodule in\\n[`horace_euphonic_interface`](https:\/\/github.com\/pace-neutrons\/horace-euphonic-interface)\\nand [`brillem`](https:\/\/github.com\/brille\/brillem).\\n* The continuous integration (CI) system would be updated to pull in the submodule for tests\\n* The CI system would also be used to build a [\"Matlab Toolbox\"](https:\/\/uk.mathworks.com\/help\/matlab\/creating-help.html)\\nfor distribution which would include `light_python_wrapper` (so users do not have to install it separately).\\n* Using this Matlab Toolbox `mltbx` file and uploading it to the github release allows it to be\\n[automatically published](https:\/\/www.mathworks.com\/matlabcentral\/about\/fx\/#Why_GitHub) by the File Exchange.\\n","tokens":196,"id":5174,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0006-store-built-documentation-in-branch.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGitHub Pages support two [publishing sources](https:\/\/help.github.com\/en\/github\/working-with-github-pages\/configuring-a-publishing-source-for-your-github-pages-site) for project documentation:\\n- the `docs` folder on the `master` branch\\n- the root folder of the `gh-pages` branch\\na third option is to store the built project documentation in\\n- documentation GitHub project.\\nIf the built documentation is stored on the `master` branch:\\n- the codebase includes built artifacts\\n- any release tag needs to be created on the commit after that from which the build was executed\\n- requires the CI server to make commits to the `master` branch\\n- it's straightforward to manage document builds from branches other than `master`\\nWhere the build artifacts on a separate branch:\\n- harder to compare source with the built documentation and manage the build artifacts from branches,\\n- cleanly separates build artefacts from source code\\nStoring built documentations in a separate GitHub repository:\\n- offers clear separation between source and built documentation\\n- carries an overhead or managing another repository\\n- documentation will served at a URL that doesn't match the source: e.g. `pace-neutrons.github.io\/horace-docs`\\n","Decision":"Built documentation will be stored on the `gh-pages` branch.\\n","tokens":259,"id":5175,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0010-use-compiled-matlab-for-python.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPACE is a collection of programs written in various languages including Matlab\\nand Python, but the aim is that *users* should be able to use either Python\\n*or* Matlab to run PACE programs.\\nMatlab has [built-in functionality](https:\/\/www.mathworks.com\/help\/matlab\/call-python-libraries.html)\\nto call Python programs however the reverse is not true - principally because\\nMatlab requires a (paid-for) license to use.\\nThus to allow Python users to call the current Matlab PACE codes\\n([Horace](https:\/\/github.com\/pace-neutrons\/Horace\/)\/[Herbert](https:\/\/github.com\/pace-neutrons\/Herbert\/))\\nit should either be translated to Python or C++ and a wrapper to Python and Matlab created,\\nor the Matlab Compiler Runtime (MCR) toolbox should be used to \"compile\"\\n`horace`\/`herbert` for distribution as a Python package, as detailed\\n[here](..\/..\/python_interface\/design\/01_pace_python_high_level_discussion.md).\\nUsing the MCR would also allow users to use Horace without a Matlab license,\\nwhilst enabling us to leave the Horace code mainly in the Matlab language.\\n","Decision":"The decision was made to use a compiled Matlab library for Python users to\\nrun PACE.\\nThis is because it was judged to be unfeasible to translate the `horace`\/`herbert`\\ncodebase to C\/C++.\\n","tokens":259,"id":5180,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0004-pace-documentation-link-to-projects.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe PACE project is a collection of independently developed and released tools. These tools are built in different technologies.\\nCreating a single \"global project documentation\" release would introduce a tight coupling between the projects and create complexities around supporting multiple versions of documentation at a project level.\\nGitHub pages provide an \"organization\" level documentation page. For PACE this will be at https:\/\/pace-neutrons.github.io. The documentation is displayed from a single folder or branch in an associated GitHub repository.\\n","Decision":"The organization-level documentation will provide a \"high-level\" overview of the project and contained tools, with links to the project-specific documentation URLs.\\n","tokens":102,"id":5181,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"pace-developers\/0009-brille-integration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n[brille](https:\/\/github.com\/brille\/brille) is a library for computing symmetry operations\\nand linear interpolation within an irreducible part of the first Brillouin zone.\\nWhilst its symmetry operations functionality can be used stand-alone,\\nthe interpolation functionality should be integrated with codes\\nwhich compute quantities in reciprocal space\\n(such as [euphonic](https:\/\/github.com\/pace-neutrons\/euphonic) and [spinW](https:\/\/github.com\/spinw\/spinw))\\nto make these programs more user friendly.\\nAt present there are separate projects, [brilleu](https:\/\/github.com\/brille\/brilleu\/)\\nand [brillem](https:\/\/github.com\/brille\/brillem\/) to achieve this integration.\\nIn both cases brille\/X interface constructs a brille object from X and handles calling the X method(s)\\nto determine the information required for brille's interpolation.\\nThis relationship could be flipped if each X constructs its own brille object and\\nthen uses it to perform interpolation.\\n","Decision":"A [meeting](https:\/\/stfc365.sharepoint.com\/:b:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/20201001_brilleX_Xbrille.pdf?csf=1&web=1&e=9XBRUe)\\nwas held and the decision was made that the integration of the interpolation functionality of brille\\nshould not be done by the external projects `brilleu` and `brillem`.\\nInstead, the interface between the calculator X and brille should be embedded within X which will\\nconstruct is own brille object and then use this to perform the interpolation.\\n","tokens":230,"id":5183,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"umbrella\/0002-component-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe components provided by this package SHOULD primarily be designed\\nwith the following aims:\\n### Stateless\\n@thi.ng\/hdom provides no guidance or opinion about how component state\\nshould be handled. However, stateless components are generally more\\nreusable (same component function can be used multiple times) and easier\\nto test and reason about.\\n### Composable\\nThe components provided by this package are often meant to be used as\\nbuilding blocks for larger, more complex components. This often requires\\nextensive configuration points.\\n### Configurable\\nComponents should be designed such that both their behavior and styling\\ncan be configured as much as possible\/feasible. At the same time, this\\nflexibility SHOULD NOT have too great an impact on user code.\\nFurthermore, the configuration process SHOULD be as uniform as possible\\nfor all provided components.\\n### Unstyled, but skinnable & themable\\nThe last point deals with the multi-step process and separation of:\\n1) configuring a raw component using a specific set of behavioral &\\naesthetic rules (usually via some form of CSS framework) and\\n2) application or instance specific theming as an additional\\ncustomization step\\nNeither of these steps SHOULD be in direct scope of the\\n@thi.ng\/hdom-components package, but the raw components themselves MUST\\nsupport these use cases for practical, real world usage.\\nIt also worth pointing out that skinning and theming MIGHT not always be\\nseparate steps and will be specific to the CSS framework used at\\nruntime.\\n","Decision":"Define all suitable components in a way which enables this uniform\\nworkflow:\\n### Raw component with configuration options\\nWhere required, components SHOULD be pre-configured via an higher order\\nfunction accepting a configuration object with component-specific\\noptions.\\nWhenever possible, the component SHOULD only require partial options and\\nmerge them with its own defaults. Each option MUST be documented using\\nJSDoc comments. Likewise the HOF component function MUST be documented,\\nspecifically to explain which runtime arguments are expected\/accepted by\\nthe returned function.\\n#### Runtime argument handling\\nIf there are more than 2 runtime args and \/ or the majority of them is\\noptional, the returned component function SHOULD accept those args as\\noptions object.\\nIf the component can take child elements as arguments, these SHOULD be\\naccepted as varargs and NOT as part of the options object.\\n#### Example component\\nThe following example button component demonstates these approaches.\\nBtw. It's the actual implementation of the [hdom-components button\\ncomponent](..\/src\/button.ts).\\n```ts\\n\/\/ button.ts\\nimport { IObjectOf } from \"@thi.ng\/api\";\\nexport interface ButtonOpts {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Attribute object to use for enabled buttons.\\n* Default: none\\n*\/\\nattribs: any;\\n\/**\\n* Attribute object to use for disabled buttons.\\n* Default: none\\n*\/\\nattribsDisabled: any;\\n\/**\\n* Flag to indicate if user supplied `onclick` handler\\n* should be wrapped in a function which automatically\\n* calls `preventDefault()`.\\n* Default: true\\n*\/\\npreventDefault: boolean;\\n}\\nexport interface ButtonArgs {\\nattribs: IObjectOf<any>;\\nonclick: EventListener;\\ndisabled: boolean;\\n}\\n\/**\\n* Higher order function to create a new stateless button component,\\n* pre-configured via user supplied options. The returned component\\n* function accepts the following arguments:\\n*\\n* - hdom context object (unused)\\n* - partial `ButtonArgs` object (extra attribs, onclick, disabled)\\n* - body content (varargs)\\n*\\n* Any `attribs` provided as arg via `ButtonArgs` are merged with the\\n* default options provided to the HOF. The `disabled` arg decides which\\n* button version to create. The button can have any number of body\\n* elements (e.g. icon and label), given as varargs.\\n*\/\\nexport const button = (opts?: Partial<ButtonOpts>) => {\\n\/\/ init with defaults\\nopts = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\npreventDefault: true,\\nattribs: {},\\n...opts\\n};\\n!opts.attribs.role && (opts.attribs.role = \"button\");\\nreturn (_: any, args: Partial<ButtonArgs>, ...body: any[]) =>\\nargs.disabled ?\\n[opts.tagDisabled, {\\n...opts.attribsDisabled,\\n...args.attribs,\\ndisabled: true,\\n}, ...body] :\\n[opts.tag, {\\n...opts.attribs,\\n...args.attribs,\\nonclick: opts.preventDefault ?\\n(e) => (e.preventDefault(), args.onclick(e)) :\\nargs.onclick\\n}, ...body];\\n};\\n```\\n### Create pre-configured components\\nTo use the raw component, instantiate it via supplied options. Since the\\ncomponent is stateless, the same instance can be used multiple times\\nfrom user code. Furthermore, this approach enables the publication of\\ndedicated packages, providing pre-defined, themed components, ready to\\nuse without further pre-configuration.\\nIn this example, we use [Tachyons](https:\/\/tachyons.io) CSS classes to\\nprovide themed versions of the above raw button component. However, a\\nmore \"traditional\" approach could inject CSS rules via the `style`\\nattribute. Also see\\n[@thi.ng\/hiccup-css](https:\/\/github.com\/thi-ng\/umbrella\/tree\/develop\/packages\/hiccup-css)\\nfor this purpose.\\n```ts\\n\/\/ themed-button.ts\\nimport { button as rawButton } from \".\/button\";\\n\/\/ predefine skinned buttons\\n\/\/ here using Tachyons CSS classes as example\\nexport const primaryButton = rawButton({\\nattribs: {\\nclass: \"dib ph3 pv2 mb2 mr2 br-pill link bg-blue hover-bg-black bg-animate white\",\\nhref: \"#\"\\n},\\nattribsDisabled: {\\nclass: \"dib ph3 pv2 mb2 mr2 br-pill bg-gray white\"\\n}\\n});\\nexport const button = rawButton({\\nattribs: {\\nclass: \"dib ph3 pv2 mb2 mr2 link dim br2 ba blue\",\\nhref: \"#\",\\n},\\nattribsDisabled: {\\nclass: \"dib ph3 pv2 mb2 mr2 br2 ba gray\"\\n}\\n});\\n```\\n### Usage & composition\\nUser code just needs to import pre-configured components and can further\\ncustomize them, e.g. to create an icon button (here using [Font\\nAwesome](https:\/\/fontawesome.com)).\\n```ts\\n\/\/ user.ts\\nimport { start } from \"@thi.ng\/hdom\";\\nimport { button, primaryButton } from \".\/themed-button\";\\n\/\/ derive icon buttons only accepting custom event handler arg\\nconst confirmButton = (_, onclick) =>\\n[primaryButton, { onclick }, [\"i.fas.fa-check.mr2\"], \"Confirm\"];\\nconst cancelButton = (_, onclick) =>\\n[button, { onclick }, [\"i.fas.fa-times.mr2\"], \"Cancel\"];\\nstart(\"app\",\\n[\"div\",\\n[primaryButton, { onclick: () => alert(\"bt1\") }, \"bt1\"],\\n[primaryButton, { onclick: () => alert(\"bt3\"), disabled: true }, \"bt2\"],\\n[button, { onclick: () => alert(\"bt3\") }, \"bt3\"],\\n[button, { onclick: () => alert(\"bt4\"), disabled: true }, \"bt4\"],\\n\/\/ icon buttons\\n[confirmButton, () => alert(\"confirm\")],\\n[cancelButton, () => alert(\"cancel\")],\\n]\\n);\\n```\\n","tokens":324,"id":5184,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"umbrella\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5186,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0017-migrate-testnet-devnet-to-use-std-stellar-wallet.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nStellar testnet network is overloaded recently and causes a lot of delay and errors\\n","Decision":"Using Stellar Mainnet network (STD TFT). But prices will vary according to the explorer type, example: (devnet would cost 1% from mainnet, testnet would cost 10% from mainnet price)\\n","tokens":20,"id":5187,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0012-use-poetry-lock-file-to-install-instead-of-poetry-update.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDependancies versions update can lead to failure. They needed to be tested well before using them. Poetry update bumps version to latest each time to execute and takes a long time\\n","Decision":"- Use poetry lock file to install dependancies using specific versions instead of executing poetry update each time\\n- Updating versions will be by the repository maintainers after making sure no compitablity issues\\n","tokens":39,"id":5189,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0018-change-domain-names-to-shorter-ones.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDomain names in deployed 3bots and solutions are too long\\n","Decision":"Change domain names to shorter ones, and in case of duplication it will add a random number appended to the domain\\n","tokens":16,"id":5190,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0010-update-3bot-deployer-image-to-use-ubuntu-20-04.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nUbuntu 19.10 has become deprecated and poetry version is old which causes poetry failures in 3Bot deployer after removing poetry update and sticking to peotry install using lockfile\\n","Decision":"Update base image (phusion) to use ubuntu 20.04 instead of ubuntu 19.10 and use it in 3Bot deployer\\n","tokens":41,"id":5191,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0009-add-logs-in-trc-and-nginx-container.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAdd feature to stream logs from trc and nginx container for better debugging\\n","Decision":"Update the nginx and trc flists to use zinit and redirect logs to stdout to be streamed from redis\\n","tokens":18,"id":5192,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0011-nginx-expose-option.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMinio solution listens for HTTP only. To safely expose it and use it for backups with restic for example, it needs to use HTTPS and use a valid certifcate.\\n","Decision":"Add option to solution expose chatflow to expose solutions using nginx reverse proxy to terminate ssl connections.\\n","tokens":40,"id":5193,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0006-add-preinstall-to-packages.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nFailed to install weblibs package on fresh installation\\n","Decision":"Add pre-install method to packages. This method will have the code that will be executed once before installation and package can't go without and that will seperate using install with kwargs for configuring the package and pre-install to dependancies like git clone.\\n","tokens":14,"id":5194,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0004-marketplace-to-automate-payment.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nMarketplace demos website to work on testnet only and automate the payment\\n","Decision":"- use a defined wallet `demos_wallet` for payments\\n- the wallet will have lots of TFTs to use\\n- solutions to live max 3 hours\\n","tokens":18,"id":5195,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0020-dynamically-get-branch-for-threebot-deployer.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn threebot deployer we hardcode the branch we deploy from in the code, this will be annoying if we need to deploy from another branch.\\n","Decision":"Getting the branch dynamically from the active branch of js-sdk repository\\n","tokens":34,"id":5196,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0007-payment-for-3bot-deployer.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe deployment of 3Bot is not guaranteed to succeed due to network failures or misbehaving of nodes on the grid. Users who pay for a 3Bot reserve capacity and in case of failing to initialize the solution they lose their money (and that capacity won't be usable after payment)\\n","Decision":"- 3Bot deployer to start with a funded wallet\\n- when the user wants to create an instance, we create a pool for this instance using the funded wallet for 15 mins (that should be enough for the initialization step).\\n- When we manage to deploy and initialize the 3Bot, we ask the user to extend the lifetime of the 3Bot in the same chatflow\\n- In case of pool extension failure they will be refunded from the explorer\\n","tokens":63,"id":5197,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0014-managed-domain-verification-and-blocking-in-apps.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere is no validation for the managed domains specified in the gateways information on the explorer side. Some of these domains are not delegated properly to the gateway's name server so all subdomains we create using these domains are not populated and not resolvable.\\n","Decision":"Create a test subdomain of each managed domain and verify that the subdomain is resolvable and block managed domains that fail this check for a certain amount of time.\\n","tokens":54,"id":5199,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0015-parameterize-zos-sal-in-identity.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nParameterize zos sal in identity so if we want to switch identity at certain point we can for example to deploy workloads with that identity\\n","Decision":"- Make zos sal paramertized with specific identity. If not passed it will use the default identity\\n","tokens":32,"id":5200,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0013-update-cryptpad-flist.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nCryptpad solution image used in docker was deprecated and flist doesn't make use of volumes\\n","Decision":"- Update flist to make use of volumes\\n- Update base image in docker file to be able to maintain\\n","tokens":22,"id":5202,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0019-use-custom-activation-wallet-for-threebot-deployers.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nActivation service fails a lot and that lead to starting online threebot without a wallet\\n","Decision":"Add option to activate the 3bot wallet via custom activation wallet on the deployer in case threefold service activation fails and pass the secret in the secret env\\n","tokens":20,"id":5203,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0021-wait-for-pool-payments.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nPools payment can fail due to stellar transaction submission timeout.\\n","Decision":"Since the endpont `https:\/\/<explorer-url>\/api\/v1\/reservations\/pools\/payment\/<pool-id>` is deployed now, A wait method is added to wait for the payment info until it's done successfully\\n","tokens":16,"id":5204,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0005-3bot-to-start-with-a-testnet-funded-wallet.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTo reduce overhead of users interaction with the Grid and the hassle with the money flow\\n","Decision":"Every 3bot starts with testnet wallet funded from faucet with 1K TFT\\n","tokens":20,"id":5205,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0002-block-misbehaving-nodes.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nBecause of failures on zos side e.g [zdb address in use](https:\/\/github.com\/threefoldtech\/zos\/issues\/916) and [failed to retrieve owner of vollume](https:\/\/github.com\/threefoldtech\/zos\/issues\/919) We need a feature to block specific nodes or specific farms that acting weird or with low performance\\n","Decision":"When deploying every threebot is aware of failing nodes, and maintains a disallow list typically blocking failing nodes for 4 hours.\\n","tokens":76,"id":5206,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"js-sdk\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5207,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0002-use-sentry-for-application-monitoring.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHackney have a preference for using this tool across their digital services: https:\/\/github.com\/LBHackney-IT\/API-Playbook#centralised-exception-logging\\n","Decision":"Sentry will be the third party service we use for collecting and managing the exceptions for all environments.\\n","tokens":39,"id":5208,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0005-use-docker.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHackney have a preference for using containers across their digital services: https:\/\/github.com\/LBHackney-IT\/API-Playbook#containers\\n","Decision":"We will build and run this service using Docker and Docker Compose. Heroku container hosting will be used.\\n","tokens":33,"id":5209,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0014-use-heroku-scheduler.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere is a user need to receive daily emails around a certain time.\\nHeroku Scheduler is a free service created by Heroku that lets you schedule\\ntasks to run at specific times.\\nHackney have already used Heroku scheduler in the [Hackney Repairs project](https:\/\/dashboard.heroku.com\/apps\/hackney-repairs-production\/scheduler).\\n","Decision":"Since there is an established pattern of using Heroku Scheduler in other projects\\nwe should continue this pattern by adopting it in this project.\\n","tokens":76,"id":5210,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0013-use-s3.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere is a user need to store evidence in the form of images, videos, and PDF\\ndocuments within the Report a Defect system.\\nThis is a common need across Hackney projects and S3 is already in use across\\nthe organisation. As such there is a project in flight to wrap S3 functionality\\nin an API that services can use.\\nAt the time of writing, this API project is not ready nor is there an agreed\\ntimeframe for when it will be ready.\\nThere is no user need to be able to edit an image once uploaded, as such the S3\\nbuckets will not need to have versioning enabled.\\n","Decision":"Since there is an established pattern of using S3 for asset storage, we have\\ndecided to continue this pattern and use S3 for this project.\\nWe will be requesting that two buckets get made, one for the staging environment\\nand one for the production environment.\\n","tokens":139,"id":5212,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0003-use-new-relic-for-performance-monitoring.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHackney have a preference for using this tool across their digital services: https:\/\/github.com\/LBHackney-IT\/API-Playbook#centralised-exception-logging\\n","Decision":"New Relic will be the third party service we use for collecting and managing performance data for all environments.\\n","tokens":39,"id":5214,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0009-use-sidekiq-and-redis-for-asynchronous-tasks.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\n- This service is sending multiple notifications by email and SMS to various users and is a core part of the value it provides\\n- There are no services or strategies described in the API documentation as of today https:\/\/github.com\/LBHackney-IT\/API-Playbook.\\n- Another Hackney Rails service is using a Shopify gem called DelayedJob with ActiveRecord https:\/\/github.com\/LBHackney-IT\/repairs-management\/blob\/develop\/Gemfile#L56\\n- I haven't used DelayedJob before\\n- I have used Sidekiq and Redis quite a lot\\n- Choosing Redis and not using Postgres for the data store will offer more advantages in the long run, such as using it for feature flagging and short term counters. Decoupling from Postgres also allows ensures that we don't go over our Heroku database usage by accident\\n- There is only 4 weeks left of this project and we've only carved out a small time to make these synchronous tasks asynchronous so time is a big factor at the moment\\n- Sidekiq and Redis are well known services and tools in the Rails community\\n","Decision":"Use Sidekiq and Redis for Asynchronous tasks\\n","tokens":233,"id":5215,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0012-use-postgres-hobby-basic.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nAs the application goes live, we need to ensure that it is using a database with\\nsufficient capacity.\\nThe production app is currently using the Heroku 'Hobby Basic' Postgres add-on,\\nwhich has a row limit of 10 million records, and costs $9\/month. The staging\\nversion is using the 'Hobby Dev' add-on, which has a row limit of 10 thousand,\\nand is free.\\nThe fastest-growing table in this application is most likely `activities`, which\\nrecords every CRUD operation made to the `defects` table. `defects` is expected\\nto grow at a rate of a few thousand records per year.\\nAs the application has just gone into production, we do not have data on which\\nto project the expected growth rate for the `activities` table, but the current\\nPostgres plan would allow for 1,000 `activities` records per defect over a year\\nbefore we hit the row limit.\\nThe next level up in Heroku's Postgres plan is 'Standard 0', which has no row\\nlimit but a storage limit of 64 GB, and costs $50\/month.\\n","Decision":"We will keep the production app on the Hobby Basic plan.\\n","tokens":245,"id":5216,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0007-use-postgres-search.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe New Build Team need to be able to perform basic search over properties for finding properties to either report defects against or to manage existing defects.\\n","Decision":"To implement search we will use the built in Postgres Search rather than adding a new dependency on another service like ElasticSearch.\\n","tokens":32,"id":5217,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0004-use-pingdom-for-uptime-monitoring.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nHackney have a preference for using this tool across their digital services: https:\/\/github.com\/LBHackney-IT\/API-Playbook#centralised-exception-logging\\n","Decision":"Pingdom will be the third party service we use for collecting and managing performance data for all environments.\\n","tokens":39,"id":5218,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0010-communal-defect-abstraction.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe New Builds Team manages defects for individual properties as well as those for communal areas like shared lifts or parking areas. Before this decision the service only supports creating defects against individual properties.\\nThe data we have is that 70% of defects are at the property level and 30% are for communal areas.\\nThe primary user need is to allow the New Build Team to create communal defects which provide the contractor with a location\/address and access information so that they can arrive at the right location first time and fix the defect.\\nThe secondary business need is that Hackney can repurpose all defect data with other digital services so they can make provide a better experience to residents in terms of repair management and future purchasing decisions.\\nThe new build team have to manually create a minimal viable data hierarchy within the service of schemes and properties. This replicates what they were doing previously in Google Spreadsheets and is a step forward towards setting up an integration with can automate this effort.\\nWe have found that the existing data hierarchy for the concepts of estates, schemes, cores, blocks and sub-blocks is inconsistent between Universal Housing, the Property API and the previous workflow that the New Build Team used. We were unable to find consistent and clear definitions for which to use in each scenario.\\nWe learnt that data hierarchy is being reviewed with the potential for the Addresses API to become responsible for this as a strategic decision in becoming less dependent on Universal Housing.\\nWe have learnt that an older Hackney service called the Repairs Hub has had difficulty in trying to solve the same problem in figuring out the right hierarchy and instead of using the Property API has used the Repairs API as a proxy to get meaningful data on properties: https:\/\/github.com\/LBHackney-IT\/repairs-management\/blob\/develop\/app\/models\/hackney\/property.rb#L12. We don't believe that coupling the defect service to property information in the same way is beneficial as it would increase technical debt. We presume there is an ambition that property data should be made available only through either the Property API or the Addresses API, and that there will be less cost in Hackney only having to have to migrate 1 service instead of 2.\\n","Decision":"We have decided to abstract the concept of communal hierarchy into a single flexible concept called 'Communal areas'.\\nA Scheme will have many properties and many communal areas, each can have many defects.\\nTo solve the primary user need communal areas require:\\n- a `name` for the NBT to manage defects with a consistent grouping\\n- a `location` that will allow the contractor to receive a single address or set of addresses via email\\n","tokens":445,"id":5219,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"radar-upload-source-connector\/0001-component-separation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nGiven the need to upload files, process them and commit them to Kafka, how do we set up component separation\\n## Decision Drivers\\n* Usability \u2013 users should not have to wait too long, get a good overview of progress made, and understand and indicate how the data that they upload will be processed.\\n* Reliability \u2013 uploaded data should always be committed to Kafka or be marked erroneous\\n* Performance \u2013 data processing should be scalable.\\n","Decision":"* Usability \u2013 users should not have to wait too long, get a good overview of progress made, and understand and indicate how the data that they upload will be processed.\\n* Reliability \u2013 uploaded data should always be committed to Kafka or be marked erroneous\\n* Performance \u2013 data processing should be scalable.\\nChosen option: \"Frontend, backend and connector with converters\", because it keeps the backend free from any issues with the connector. The connector can scale at wish. The setup is similar to \"Frontend, backend, converter job system, connector\" except the job system is integrated into the connector. This makes the connector a bit heavier on code and internal management but it keeps the amount of setup and configuration low.\\n### Positive Consequences\\n* Users have a responsive backend that gives them status updates because data is not processed in the backend\\n* Connector is in charge of conversion and Avro schema handling\\n* Connector can restart without any interruptions to the user interface.\\n* Severe conversion errors will not cause the backend to fail.\\n### Negative Consequences\\n* Converters do not reside in the backend, but the backend does need to know what converters are available. This makes the configuration a bit heavier.\\n* The connector configuration will become more involved with configuration for each converter.\\n","tokens":96,"id":5221,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"radar-upload-source-connector\/0002-converter-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nEach converters may take some configuration. Where do we store this information?\\n## Decision Drivers\\n* Usability \u2013 The configuration should not be duplicated too much.\\n* Flexibility \u2013 per-project configuration of converters should be possible to cater for the needs of different projects\\n","Decision":"* Usability \u2013 The configuration should not be duplicated too much.\\n* Flexibility \u2013 per-project configuration of converters should be possible to cater for the needs of different projects\\nChosen option: \"Configuration in backend\", because configuration options of the backend are much more flexible. It also avoids any duplicate and possibly conflicting configuration.\\n### Positive Consequences\\n* Flexible configuration.\\n* Duplication is avoided.\\n### Negative Consequences\\n* Unconfigured converters may not function correctly.\\n* Hard-coded configuration in the converter may conflict with settings in backend.\\n* Updates to the backend converter config have to be propagated to the connector.\\n","tokens":60,"id":5222,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"glam\/0001-auth-via-nginx.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nGLAM requires authentication to protect sensitive metrics, make some metrics and\\nviews visible to non-Mozillians, and in the future to potentially add\\nuser-oriented features. Currently we handle the authentication in the app layer,\\nalbeit with some issues. This ADR attempts to resolve these issues.\\n## Decision Drivers\\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\n","Decision":"- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\nIn the GLAM meeting, nginx was chosen as the preferred option for the following\\nreasons:\\n- It removes code complexity and maintenance from the app layer.\\n- It would remove a necessary security review by using something that has\\nalready been reviewed.\\n- It offers the quickest path to fix the authentication issues we are facing.\\n- In the future it satifies the requirement of offering user-facing features.\\n- In the future it satifies the requirement of optional non-authenticated\\nendpoints.\\n","tokens":136,"id":5223,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"glam\/0003-file-based-data.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThe GLAM data consists mainly of a histogram, percentiles, and counts for each\\nset of dimensions. This can be stored in a file whose path and name can\\nrepresent the dimensions and whose content can contain the actual data.\\n## Decision Drivers\\n- The data should be efficient to look up and read for the front-end\\n- The data should be efficient to write for the ETL\\n- The data should be cost efficient to write and maintain\\n","Decision":"- The data should be efficient to look up and read for the front-end\\n- The data should be efficient to write for the ETL\\n- The data should be cost efficient to write and maintain\\nMoving the data into Postgresql was chosen, as outlined in the pros\/cons below.\\nTo summarize, the need to update the file as new aggregation data was produced\\nfor the same set of dimensions causes a file based approach to be more complex\\nand something databases were created to solve.\\nThe SQL approach of `INSERT ... ON CONFLICT DO UPDATE ...` accomplishes this\\nwell.\\n","tokens":99,"id":5224,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"glam\/0002-data-exploration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nGLAM is designed to answer a general set of questions against a limited set of\\ndimensions. Sometimes the underlying data can tell us more than GLAM allows. For\\nthose users who want to dig deeper into the data, GLAM should provide jump off\\npoints to do so.\\n## Decision Drivers\\n- What is best to provide to end users to get their questions answered?\\n- What can we accomplish with our current resources for the generalized case,\\nbut also help enable non-generalized use cases?\\n","Decision":"- What is best to provide to end users to get their questions answered?\\n- What can we accomplish with our current resources for the generalized case,\\nbut also help enable non-generalized use cases?\\nChosen option: 2. Copy-paste-able SQL query\\nIt was decided that option 2 was the best and most useful option. Option 1 has\\nnot been requested thus far. Option 3's tight integration with Redash was deemed\\nunwise as Redash has an unknown future at Mozilla and the SQL query could be\\neasily copied there as well as the bigquery console.\\n","tokens":112,"id":5225,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"metadata-registry\/0001-entity-metadata.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nIn order to support distributing additional entity metadata which is not\\notherwise required for on-chain operation we introduce an entity metadata\\nregistry containing self-certifying entity statements.\\n","Decision":"At a high level, this proposal introduces a new metadata registry provider\\ninterface currently containing only a single method:\\n* `GetEntity(id) -> EntityMetadata`\\nThe method is given an entity identifier (currently an Ed25519 public key) and\\nreturns entity metadata from the registry after performing the necessary\\nverification.\\n### Entity Metadata Statements\\nThe entity metadata structure is a document encoded using [canonical CBOR as\\ndefined by RFC 7049] with the following fields:\\n* `v`: format version (uint16, required, must be `1`)\\n* `serial`: the serial number of the entity metadata statement where the highest\\nserial number should be treated as the most recent (uint64)\\n* `name`: an entity name (string, optional, max 50 characters)\\n* `url`: an URL associated with the entity (string, optional, max 64 characters,\\nmust be a valid URL using the scheme `https` without any query or fragments)\\n* `email`: an e-mail address associated with the entity (string, optional, max\\n32 characters, must be a valid e-mail address)\\n* `keybase`: a keybase.io handle (string, optional, max 32 characters, must\\nmatch the regular expression `^[A-Za-z0-9_]+$`)\\n* `twitter`: a Twitter handle (string, optional, max 32 characters, must match\\nthe regular expression `^[A-Za-z0-9_]+$`)\\nThe entity metadata document is signed using the [signed envelope] used by Oasis\\nCore with the following [domain separation context]:\\n```\\noasis-metadata-registry: entity\\n```\\nThe maximum size of the signed statement encoded in JSON is 16384 bytes.\\n<!-- markdownlint-disable line-length -->\\n[canonical CBOR as defined by RFC 7049]: https:\/\/tools.ietf.org\/html\/rfc7049\\n[signed envelope]: https:\/\/pkg.go.dev\/github.com\/oasisprotocol\/oasis-core\/go\/common\/crypto\/signature?tab=doc#Signed\\n[domain separation context]: https:\/\/github.com\/oasisprotocol\/oasis-core\/blob\/master\/docs\/crypto.md#domain-separation\\n<!-- markdownlint-enable line-length -->\\n### Filesystem Provider\\nThis proposal also standardizes a filesystem-based provider that uses a\\nfilesystem implementation for storing the signed metadata statements.\\nWe assume the filesystem base directory is `\/`. The top-level directory must be\\nlocated `\/registry`.\\nEntity metadata statements must be stored as a subdirectory named `entity`,\\ngiving the relative path `\/registry\/entity`.\\nEach entity metadata statement must be stored in its own file named based on\\nthe hexadecimal encoding of the entity public key that signed the statement, for\\nexample:\\n```\\n\/registry\/entity\/5fb36d105a6c85a21542abdd712c292ae37425e842b38db450970cdef0780bd8.json\\n```\\nThe signed statement must be the [signed envelope] encoded in JSON format.\\n### Git Provider\\nAs an extension of a filesystem-based provider this proposal standardizes Git\\nrepositories as a way to distribute signed statements. The conventions defined\\nfor the filesystem provider are used.\\n","tokens":36,"id":5226,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reactive-interaction-gateway\/0005-maintain-changelog.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to track what changes have been made to RIG and make it clear to everyone. It should also help with identifying of issues that may be caused by changes in the past.\\n","Decision":"We decided to use `CHANGELOG.md` to have it as a single source of truth to what happened in RIG. Every submitted Pull Request will contain update to this file as well. By this everyone can clearly see which version has which features or fixes.\\n","tokens":41,"id":5228,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reactive-interaction-gateway\/0002-don-t-check-for-functionclauseerror-in-tests.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nTypically, a module API filters possible inputs implicitly by making use of pattern matching. For instance, a GenServer that only handles `:test` messages might have a method similar to `handle_call(:test, _, _)`.\\nWriting tests that assure that pattern matching works with the given signature (e.g., by checking that `handle_call(:illegal, 0, 0)` fails) has drawbacks:\\n* Asserting that the process `exit`s with a `FunctionClauseError` is not straight-forward.\\n* Arguably, one of the ideas of pattern matching in function signatures is to save on testing negative cases in the first place.\\n* Often, testing for missing function clauses make tests needlessly brittle.\\nStill, there is the case of regression testing, e.g., making sure that there will never exist a handler for other messages. We think that such a restriction is rarely required, though.\\n","Decision":"Except for regression tests, tests should not aim at triggering `FunctionClauseError`s.\\n","tokens":191,"id":5229,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reactive-interaction-gateway\/0004-use-rig-config-for-global-configuration.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThe previous way of handling application configuration did not allow for a clean way\\noverride values using environment variables. Such overrides are necessary, because\\n`config.exs` is evaluated at compile-time only. For instance, changing RIG's HTTP port\\nshould not require the corresponding Docker image to be recompiled.\\n","Decision":"We use [Confex](https:\/\/hexdocs.pm\/confex\/Confex.html) to make this more flexible.\\nConsider the following example for use in `config.exs`:\\n```elixir\\nconfig :rig, RigInboundGateway.ApiProxy.Base,\\nrecv_timeout: {:system, :integer, \"PROXY_RECV_TIMEOUT\", 5_000}\\n```\\nNote that default values are given in `config.exs` and _never in the implementation_.\\nThe keyword list that supplied to the macro specifies the _required_ keys; that is,\\ntheir presence will be checked. Note that you can also validate the configuration\\nvalues yourself by using `:custom_validation`, for example:\\n```elixir\\ndefmodule Rig.Kafka.MessageHandler do\\nuse Rig.Config, :custom_validation\\ndefp validate_config!(nil), do: validate_config!([])\\ndefp validate_config!(config) do\\n{target_mod, target_fun} = Keyword.fetch!(config, :user_channel_name_mf)\\n%{\\nmessage_user_field: Keyword.fetch!(config, :message_user_field),\\nuser_channel_name: fn user -> apply(target_mod, target_fun, [user]) end\\n}\\nend\\n...\\nend\\n```\\n","tokens":66,"id":5230,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"reactive-interaction-gateway\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard in his article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nIn order to make this easier to handle, [adr-tools](https:\/\/github.com\/npryce\/adr-tools) should be used to create new entries, as well as to supersede existing ones.\\nIn a nutshell:\\n* ADRs are basically lightweight RFCs.\\n* We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\n* The files are similar to this one (plaintext, same structure).\\n* ADRs are numbered sequentially. Numbers will not be reused.\\n* A decision will be considered *immutable* (except for the status). Amendments go to a new ADR that supersedes the old one.\\n* If a decision is reversed or changed, the original file is kept around, but marked as superseded. Use `adr` for doing that -- e.g., `adr new -s 12 Use PostgreSQL Database` (see `adr help new`) -- in order to do this with a consistent style (the ADR's status is changed and links are inserted).\\n","tokens":16,"id":5231,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"expound\/adr-001.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Problem\\nThe default value printer omits irrelevant values. While this compacts large values and reduces noise, it can also obscure the location of data by omitting helpful context about the \"siblings\" of the bad data.\\n## Context\\nhttps:\/\/github.com\/bhb\/expound\/issues\/18\\nMore generally, there is some demand for completely different output that would be suitable for showing an outside user (say, a consumer of an API)\\nhttps:\/\/clojurians-log.clojureverse.org\/clojure-spec\/2017-07-17.html#inst-2017-07-17T18:09:19.667294Z\\nThe current printer was built for finding problems in large data structures coming over the wire. However, in the case of instrumentation, values are often not as deeply nested or as large.\\n","Decision":"Add a new \"printer builder\" function.\\n","tokens":171,"id":5232,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"php-docker-template\/0003-default-php-extensions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nGiven the premise that those images are designed to run in production, in a clustered environment and managed by an orchestrator like Kubernetes or Docker Swarm, it becomes necessary to shape the PHP installation towards those needs.\\nFor instance, images that ship without any PHP extensions (like the official ones) are not able to handle posix signals (like `SIGTERM` or `SIGINT`) from an orchestrator. Moreover they lack in-memory user cache, which can be useful for projecting data into a service.\\n","Decision":"Ship the Docker images with extensions that contribute to the vision of the microservices use case, being:\\n- `PCNTL` in order to deal with user signals. [PCNTL Manual page](http:\/\/php.net\/manual\/en\/book.pcntl.php)\\n- This is meant for long running non-interactive php applications. `fpm` is not impacted, since it [can deal with these signals natively](https:\/\/linux.die.net\/man\/8\/php-fpm).\\n- `APCU` in order to provide a user in-memory caching namespace. [APCU Manual page](http:\/\/php.net\/manual\/en\/book.apcu.php)\\n- `OPcache` for PHP bytecode cache, given the Docker image immutability principle it's good not to re-parse the code. [OPcache Manual page](http:\/\/php.net\/manual\/en\/book.opcache.php)\\n","tokens":104,"id":5234,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"php-docker-template\/0002-nginx-configuration-is-shaped-for-php-needs.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nNginx is a webserver which can work on many ways, from proxy, to reverse proxy, load balancer and traditional web server.\\nWe need a configuration which suits the need for PHP fpm under sockets\\n","Decision":"The default configuration and custom variables will be shaped for PHP fpm needs, allowing an easy plug-and-play for these kind of projects.\\nThis includes using a shared PHP fpm socket under `\/var\/run`.\\n","tokens":48,"id":5236,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"php-docker-template\/0005-define-a-policy-for-supported-versions-and-upgrades.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nDoubts have appeared as we decide which versions to drop from our pipeline and upgrade internal dependencies. In order to solve this we decided to be explicit with users on how we make those decisions.\\n","Decision":"We have defined the following policy, to be mirrored in the README.\\n- We will provide continuous new builds for images while their versions are actively supported.\\n- PHP versions will be supported until no longer in [Security Support](https:\/\/php.net\/supported-versions.php).\\n- alpine versions will be supported until they reach [EOL](https:\/\/alpinelinux.org\/releases).\\n- nginx versions will be supported until they are classified as [legacy](https:\/\/nginx.org\/en\/download.html).\\n- Past images will remain hosted on DockerHub as long as possible.\\n- Once DockerHub limits are reached, images will become unavailable chronologically, unless they are still actively receiving new builds.\\n- Packages focused on development will only be enabled in the `dev` image, and be updated in reasonable timeframes.,\\n- For example, Xdebug will only be shipped on `dev` versions and will be updated to the latest version as fast as possible.\\n","tokens":43,"id":5238,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-api\/2019-07-16-database-stack.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nReplacing the sample JSON files with a fully-fledged database, to scale with the DPE app.\\nBuilding off of [the database schema ADR](https:\/\/github.com\/bbc\/digital-paper-edit-api\/docs\/ADR\/2019-04-29-SQL-database-schema.md) - created via [DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\nDeciding on RDS as a service and Postgres as an engine done [prior](https:\/\/github.com\/bbc\/digital-paper-edit-infrastructure\/pull\/9).\\nThings to decide:\\n* Whether to spin up local RDS instance for testing\\n* How to build up queries and DB logic, replacing JSON manipulation in routes\\n* How to handle migrations\\n* Seeding DB with existing JSON sample data to help development\\n","Decision":"<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\nOption 3 was chosen as part of the spike and the fact that we have parameterised environments. This enabled speed for the spike with minimal overhead of extra Docker setup \/ db binaries.\\nUsing `pg` in conjunction with `knex` allowed us to easily handle migrations and seeding with sample data. Found [Objection](https:\/\/vincit.github.io\/objection.js\/) and [Sequelize](http:\/\/docs.sequelizejs.com\/) to be unnecessary for the queries we require. Objection is easily implementable later, if an ORM is required down the line.\\n","tokens":232,"id":5239,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-06-20-environment-injection.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThis addresses the fact that we could not configure different environments to point to different API URLs as necessary (across Int, Test, and Live envs in Cosmos).\\nAt the point of deployment, `npm run build` is called in Jenkins to build the static  bundle and minify etc. Environment config (`REACT_APP_SERVER_URL` etc. variables declared in `.env`) get baked into the bundle and obfuscated at this point.\\nWe would need different Jenkins jobs or parameters to bake in the environment config (eg. the correct server URL) multiple times, which goes against the idea of having an _unconfigured \/ environment agnostic_ RPM in Cosmos.\\n## Decision Drivers <!-- optional -->\\n* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection \/ DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\n","Decision":"* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection \/ DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\nThis is a common problem with create-react-app and the need to configure it. We came across [this solution](https:\/\/github.com\/facebook\/create-react-app\/issues\/578#issuecomment-277843310) which detailed a simple way of consuming config with the `window` global.\\nIn source code this remains localhost:8080 as a local development fallback and it is passed in via Cosmos environment config.\\n","tokens":192,"id":5240,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-12-02-nosql-schema-security.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n[Security rules](https:\/\/firebase.google.com\/docs\/firestore\/security\/get-started) in Firebase is glob-like.\\n```c++\\nservice cloud.firestore {\\nmatch \/databases\/{database}\/documents {\\nmatch \/<some_path>\/ {\\nallow read, write: if <some_condition>;\\n}\\n}\\n}\\n```\\nWe need to add user authentication ID to the conditional statement above to tighten security for who can access our data. This means that we might need to rethink how we want to organise the data, as it was initially thought to be flat:\\n```text\\n\/users\/{uId}\\n\/projects\/{pId}\\n\/transcripts\/{trId}\\n\/paperedits\/{peId}\\n\/uploads\/{upId}\\n```\\nIf we retained flatness, it will be easy to access the data - however we need to complicate our data structure by retaining the user ID in each of the records.\\n## Decision Drivers <!-- optional -->\\n- Security\\n- Ease in implementation (DB)\\n- Ease in implementation (Security Rules)\\n- Extensibility\\n","Decision":"- Security\\n- Ease in implementation (DB)\\n- Ease in implementation (Security Rules)\\n- Extensibility\\nChosen option: \"2 collections\", because when thinking about membership and extensibility, we'd want to be able to have multiple `users` in a single `project` and every asset (`transcripts`, `paperEdits`) should be associated with a `project`.\\nWe will authenticate users based on a field in projects called `users`. Each project will have several users like so:\\n- `projects\/{id}\/users` = []\\nA security rule like below will be able to test authentication and membership of the project on users.\\n```js\\nfunction isOnProject() {\\nreturn (\\nrequest.auth.uid in\\nget(\/databases\/$(database) \/ documents \/ projects \/ $(pid)).data.users\\n);\\n}\\n```\\n### Uploads (Storage and Firestore)\\nThe `uploads` subcollection has been added to the `users` for security and convenience: Storage rules are a lot easier to implement based on users, and we want to make things symmetric as possible.\\n| Firestore                   | Storage                     |\\n| --------------------------- | --------------------------- |\\n| `users\/{id}\/uploads\/{upId}` | `users\/{id}\/uploads\/{upId}` |\\nTo keep consistency, we also use the same `id` for `transcript` and `uploads` in Firestore\\n| Firestore                            | Firestore                     |\\n| ------------------------------------ | ----------------------------- |\\n| `projects\/{id}\/transcripts\/{itemId}` | `users\/{id}\/uploads\/{itemId}` |\\n","tokens":236,"id":5241,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-05-01-security.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":5242,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-05-22-exporting-video-preview.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nWe are using BBC `VideoContext` to display a preview of the paper-cuts in a program script.\\nMain export is an EDL or XML to import into audio or video editing software to continue from rough cut to fine cut.\\nHowever it be useful to be able to export the \"rough cut preview\" as a video to remove extra steps if users need to share it as is.\\n## Decision Drivers <!-- optional -->\\n* Easy to reason around\\n* Light on the client, for low performance PC running IE (for web version)\\n","Decision":"* Easy to reason around\\n* Light on the client, for low performance PC running IE (for web version)\\n<!--\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC, most likely 1 - `ffmpeg-remix`_\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026  -->\\n","tokens":119,"id":5245,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-exporting-placeholder-media.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nPeople have mentioned they want to be able to insert placeholders. The placeholders will then be also present in their editing tool of their choice, i.e. Sadie (ADL, EDL) or FCP. Since this is not a prioritised feature (it is a nice-to-have), it has been placed out-of-scope for now. Also \"Placeholders\" features are sometimes not used at all by content producers.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* User Experience\\n* Demand of feature\\n","Decision":"* Ease of implementation\\n* User Experience\\n* Demand of feature\\nThis implementation needs investigation from a technical and UX point of view.\\nWe need to clear up assumptions about how things work technically by trying out the placeholder features of each editing tool.\\nWe also need to clear up assumptions about how people use the placeholders, if it needs to be presented in a certain way in the Workspace view.\\nIdeally I would say Option 1, but we will need to investigate further to conclude properly. This document needs updating once investigations have been done.\\n### Positive Consequences <!-- optional -->\\n* Placeholder works out of the box for specific editing tools\\n### Negative consequences <!-- optional -->\\n* Will require reverse engineering to understand specific editing tools\\n","tokens":115,"id":5246,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-05-20-previewing-papercuts.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs part of the Paper-edit view\\n![Screen Shot 2019-04-18 at 13 54 26](https:\/\/user-images.githubusercontent.com\/4661975\/56362368-8fd2bf00-61e1-11e9-9e87-edf71c0da030.png)\\n> As a user, I want to be able to get (watch or listen to) a preview of the paper-edit\/program script so that I can see how my program's paper-cuts (transcripts text selections) will \"render\" as audio and\/or video.\\nIn the sketch, this would be the \"canvas\" - _borrowing terminology from to the NLE video editors distinction between preview and canvas_\\nA React component that takes a sequence of audio or video clips as an input, with start and end time, and displays a player that can\\n- [ ] Seamlessly play this EDL (edit decision list) without buffer or load time in between clips\\n- [ ] Has player controls, such as\\n- [ ] progress bar\\n- [ ] play\/pause\\n- [ ] stop\\nNote that the component should be able to generate the preview if the sequence is made of audio, video or a mix of audio and video files.\\nIn first version it could just deal with sequences of media, but it be good if in subsequent versions it could handle things like text, titles (as a possible way to display place holders for voice over text?) - this second part needs to be flashed out more. But the general thing to keep in mind is the extensibility of the elements to display, if that makes sense.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* performant\\n","Decision":"* easy to reason around\\n* performant\\nThere are other examples and indications in the repo on how to enable, play, pause, and a progress bar.\\nI think under the hood, Video Context, used the HTML5 canvas to concat the videos in the sequence and then provides a unified interface to treat it as a single media.\\nChosen option: option 5 `bbc\/VideoContext`, because it seems to be a performant way to display an EDL\/playlist of clips with start and end times options. It is also currently being mantained by BBC R&D.\\n","tokens":366,"id":5247,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"digital-paper-edit-firebase\/2019-05-09-modular-architecture.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context and Problem Statement\\nAs this is an opensource project, we might want to separate out the projects so they are more digestible.\\n## Decision Drivers\\n- opensource means removing BBC specific code\\n- separating infrastructure\\n- electron and cep (future integration)\\n","Decision":"- opensource means removing BBC specific code\\n- separating infrastructure\\n- electron and cep (future integration)\\nWe opted for Option 4 after another iteration of the architectual flow.\\nInitially we went for option 3 because we wanted to have a uniform approach for possible future users.\\nHowever, after much consideration, we collectively agreed to remove NPM packaging for the API (and the microservices).\\nSimilarly to option 3, we are still keeping the AWS stacks separate in the infrastructure repository.\\nWe will be using Travis CI for the NPM build.\\nWe will be using Jenkins (not publicly accessible) for the RPM build and releasing to Cosmos.\\nWe will be using Speculate, which is an open-source BBC project to generate SPEC files, which adds little overhead in the projects.\\nWe will not automate the release of NPM, but as a manual step which also does testing.\\nWe will retain the public state of each repository and not be concerned with \"ready\" state of our repositories.\\n### Why move out API from NPM? (Cons)\\n- Unknown benefits to external person\\n- Using NPM for versioning (this can be done through other means)\\n- Possibly atypical use of NPM (although this is inconclusive)\\nThey can fork the repository to try out the services.\\n### Reasons for option 3 initially\\nThere is a separation to all the packages, which means it will be simpler to version, test and contribute to individual packages. The API repository will contain different infrastructural flavours of the business logic. There will be a browser (Cloud) package, CEP package, and Electron package that does the same thing. Each is compatible with the UI. To package these similar packages, we can look at `lerna`. The browser flavoured package will have another Express server in the repository to simplify local testing. Infrastructure, which will have UI and API as dependencies will not have an Express server inside.\\nA potential problem here is that during development, the packages will have features not implemented in the other packages. In order to have parallel set of features, we will add BDD tests to automatically test that all the packages have the same features which are functional.\\nWe are also keeping the AWS Infrastructure public, as the generic cloudformation JSON has no confidential or security issues in it.\\n#### Why release API in NPM? (Pros)\\n- Abstraction\\n- Clear separation of concerns (RPM)\\n- Simpler to set up for external people (???)\\n### RPM deployment flow\\n```\\n+-------------------------+\\n| Infrastructure          |           +----------------+\\n|                         |           |                |      +-------+\\n| \/api                    |           |                |      |       |\\n| \/client                 +---------->+    Jenkins     +----->+  RPM  |\\n| - speculate (SPEC gen)  |           |                |      |       |\\n| \/... microservices      |           |                |      +-------+\\n|                         |           +----+------+----+\\n+-------------------------+                ^      |\\n|      |\\n|      |\\n+-------------------------+                |      |\\n| API                     |                |      |    Pulls client from NPM\\n|                         |                |      |    when creating RPM for client\\n| package.json            +----------------+      +--------------+\\n| - speculate (SPEC gen)  |                                      |\\n|                         |                                      |\\n+-------------------------+                                      |\\n|\\n|\\n+-------------------------+        +-----------------+           v\\n| Client                  |        |                 |       +---+---+\\n|                         |        |                 |       |       |\\n|                         +------->+    Travis CI    +------>+  NPM  |\\n|                         |        |                 |       |       |\\n|                         |        |                 |       +-------+\\n+-------------------------+        +-----------------+\\nhttp:\/\/asciiflow.com\/\\n```\\nJenkins will then be used to deploy the RPMs to each BBC Cosmos project.\\nOnce deployed, this will be two separate EC2 instances.\\n```\\n+---------------------------+           +-----------------------------+\\n|Client                     |           |API                          |\\n|   +-------------------+   |           |   +---------------------+   |\\n|   |Express            |   |           |   |Express              |   |\\n|   | +---------------+ |   |           |   |                     |   |\\n|   | |NPM package    | |   |           |   |                     |   |\\n|   | |with static    | |   |           |   |                     |   |\\n|   | |files of Client| |   +---------->+   |                     |   +--------->...\\n|   | |               | |   |           |   |                     |   |\\n|   | |               | |   |           |   |                     |   |\\n|   | +---------------+ |   |           |   |                     |   |\\n|   |                   |   |           |   |                     |   |\\n|   +-------------------+   |           |   +---------------------+   |\\n|                           |           |                             |\\n+---------------------------+           +-----------------------------+\\n```\\n### Repo naming conventions\\nPrefix will be `digital-paper-edit`:\\n- client: `-client`\\n- server: `-api`\\n- electron: `-electron`\\n- cep: `-cep`\\n- infrastructure: `-infrastructure` or `-aws`\\n","tokens":56,"id":5251,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0006-use-jest.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":5252,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0005-use-eslint.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":5253,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":5254,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0007-make-remultiform-configuration-driven.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to build a reusable React library, that generates multipage forms\\nquickly. We need logic that extends beyond simple \"show this element on this\\npage\", such as conditional logic for displaying sections dependant on values in\\nother sections, and to skip pages based on values elsewhere in the journey.\\n","Decision":"We will build remultiform to export a series of small orchestration components.\\nWe will pass the entire multipage form configuration in via props to those\\norchestrators. The configuration should be agnostic of the set of components it\\nmight receive.\\n","tokens":66,"id":5256,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0009-use-typedoc-to-generate-api-documentation.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":5258,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0002-use-typescript.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https:\/\/www.typescriptlang.org\/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https:\/\/github.com\/microsoft\/TypeScript\/network\/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n","Decision":"We will use TypeScript.\\n","tokens":134,"id":5260,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
{"File Name":"remultiform\/0001-record-architecture-decisions.md","Context":"This is an Architectural Decision Record for a software. Give a ## Decision corresponding to the ## Context provided by the User.## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5261,"Error":{"args":["CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}}
