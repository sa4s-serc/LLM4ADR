{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"Prediction":"## Decision\n\n**Move feed-specific logic to ETL services, while keeping generic, reusable logic within CDH.**\n\n**Rationale:**\n\n* **Flexibility and Maintainability:** Feed-specific requirements often necessitate specialized implementations. Placing this logic within ETL services allows for focused development and modification without impacting the core CDH functionality. This approach promotes cleaner separation of concerns and simplifies future updates or onboarding of new feeds.\n* **Scalability and Reusability:** Generic logic, applicable across multiple feeds, benefits from a centralized implementation within CDH. This ensures consistency, reduces code duplication, and allows for easier maintenance and scaling as new features are introduced.\n* **Performance Optimization:**  ETL services are better suited for handling the intricacies of data transformation and validation specific to individual feeds. This specialization can lead to optimized performance compared to implementing such logic within the broader scope of CDH.\n\n**Consequences:**\n\n* **Increased Complexity:**  Managing logic across CDH and ETL services introduces a level of complexity in terms of development, testing, and deployment. \n* **Potential for Redundancy:** Clear guidelines and communication are crucial to prevent redundant implementations of similar logic in both CDH and ETL. \n* **Dependency Management:**  ETL services may develop dependencies on specific CDH functionalities, requiring careful version control and coordination during updates.\n\n**Mitigation:**\n\n* Establish clear documentation and communication channels between CDH and ETL teams.\n* Implement robust testing frameworks to ensure the integrity of both individual components and their integration.\n* Utilize version control and dependency management tools to streamline development and deployment processes. \n","GenTime":"2024-08-17 18:02:28","text":null}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"Prediction":"## Decision\n\n**Migrate SDK application's CLI key material and metadata storage to utilize platform-specific secret stores where available, providing a fallback mechanism for unsupported platforms.**\n\n**Rationale:**\n\n* **Enhanced Security:** Leveraging platform-native secret stores significantly improves the security posture by utilizing established, secure mechanisms for storing sensitive information. These stores are designed to be isolated from other applications and typically require user authentication for access, mitigating risks associated with plaintext storage.\n* **Improved User Experience:** Utilizing native secret stores can lead to a more integrated and user-friendly experience.  Users benefit from familiar authentication mechanisms and the platform's built-in security features.\n* **Cross-Platform Compatibility:** While aiming for native integration, it's crucial to provide a fallback mechanism for platforms lacking a built-in secret store. This ensures consistent functionality and security across diverse environments.\n\n**Implementation Details:**\n\n1. **Abstraction Layer:** Implement a common interface within the SDK's CLI that abstracts the underlying secret storage mechanism. \n2. **Platform Detection:**  At runtime, the CLI will detect the operating system and prioritize using the platform's native secret store (e.g., Keychain on macOS, Credential Manager on Windows, Secret Service API on Linux).\n3. **Fallback Mechanism:** For unsupported platforms, implement a fallback mechanism using an encrypted database stored in the user's home directory. This database should employ strong encryption algorithms (e.g., AES-256) and require passphrase entry for decryption.\n4. **Migration Strategy:**  Provide a seamless migration path for existing users to transition their keys and metadata from the plaintext storage to the new secure storage mechanism. This may involve prompting users to set up a passphrase for the fallback mechanism if their platform isn't supported.\n\n**Consequences:**\n\n* **Increased Development Complexity:** Implementing platform-specific integrations and a fallback mechanism will introduce additional complexity to the project.\n* **Potential Performance Overhead:** Abstracting the storage mechanism might lead to a slight performance overhead compared to directly accessing plaintext data. However, the security benefits outweigh this minor trade-off. \n\n**Overall, migrating to a secure storage solution leveraging platform-native features with a robust fallback mechanism significantly enhances the security of the SDK's CLI without compromising its cross-platform compatibility.** \n","GenTime":"2024-08-19 14:55:17","text":null}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"Prediction":"## Decision\n\nWe will implement a phased migration approach for client-side transaction handling in the Cosmos SDK, moving away from Amino JSON encoding to Protocol Buffers. This approach prioritizes minimizing breaking changes and maximizing compatibility throughout the process.\n\n**Phase 1: Introduce Protobuf Support alongside Amino**\n\n- Implement Protobuf encoding and decoding for all transaction-related messages.\n- Maintain backward compatibility with existing Amino-based transactions. \n- Introduce a new transaction type identifier that signifies Protobuf encoding.\n- Update the CLI and REST handlers to support both Amino and Protobuf transactions.\n- Allow users to opt-in to Protobuf encoding through a configuration option.\n\n**Phase 2:  Transition to Protobuf as Default**\n\n- Once Protobuf support is deemed stable and widely adopted, deprecate the use of Amino for new transaction types.\n- Provide clear migration guidance and tools for users to transition their applications.\n- Explore mechanisms to gradually phase out Amino support in the future, potentially through a governance proposal.\n\n**Rationale:** This approach balances the need for a smooth transition with the long-term benefits of using Protocol Buffers. It allows developers to gradually migrate their applications while ensuring ongoing compatibility for existing users. \n","GenTime":"2024-08-19 14:55:59","text":null}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"Prediction":"## Decision\n\nImplement a dynamically adjusted commission fee structure for validators, where their commission rate increases proportionally to their stake size beyond a predetermined threshold. This approach incentivizes delegation towards smaller validators by effectively passing on the cost of network centralization to larger validators and their delegators. \n\nHere's a breakdown of the implementation:\n\n* **Threshold:** Define a stake threshold beyond which validators are considered \"large\" and subject to increased commission.\n* **Dynamic Adjustment:** Implement an algorithm that dynamically adjusts the commission rate based on a validator's stake size relative to the threshold. The further beyond the threshold, the higher the commission. \n* **Transparency:**  Ensure the commission structure and its calculations are transparent to delegators, allowing them to make informed decisions. \n\nThis approach achieves the following:\n\n* **Discourages Over-Delegation:** Higher commission fees for large validators incentivize delegators to diversify their stake among smaller validators.\n* **Promotes Decentralization:** By making it less economically attractive to delegate to large validators, the network naturally trends towards a more distributed validator set.\n* **Reflects Network Externalities:** The increased commission fee acts as a tangible representation of the negative externalities associated with validator centralization.\n\n## Consequences:\n\n* **Potential for Complexity:** Implementing and maintaining a dynamic commission structure adds complexity to the staking mechanism.\n* **Gaming the System:** Sophisticated actors may attempt to circumvent the system, requiring careful design and monitoring.\n* **Impact on Existing Large Validators:** Large validators may experience a decrease in delegated stake, potentially leading to network instability during the transition phase. \n\nThis approach attempts to address the centralization issue in a market-driven way, promoting a more decentralized and resilient Proof of Stake network. However, careful consideration of the potential consequences and ongoing monitoring are crucial for its success. \n","GenTime":"2024-08-19 14:56:51","text":null}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"Prediction":"## Decision\n\nTo mitigate the denial-of-service vulnerability posed by unbounded denomination growth in user accounts with the introduction of permissionless IBC, we will implement a gas-based mechanism combined with a dynamically adjusted denomination limit at the account level. \n\nHere's how it will work:\n\n1. **Gas Cost for New Denominations:** Each time a new denomination is added to an account's balance, a significant gas cost will be applied. This cost will discourage malicious actors from spamming users with nonsensical tokens as the attack cost becomes prohibitive.\n\n2. **Dynamic Denomination Limit:** Each account will have a dynamically adjusted limit on the number of allowed denominations. This limit will start at a reasonable base value and increase linearly with the amount of gas the account owner is willing to pay. This allows users to receive a reasonable number of denominations for free while providing flexibility for power users who require more.\n\n3. **Denomination Merging:** We will investigate and potentially implement automatic merging of dust denominations into a single, dedicated \"dust\" denomination. This can help manage the number of denominations and reduce storage\/loading costs.\n\n4. **IBC Packet Filtering:** While not a complete solution, we will explore options to filter IBC packets at the protocol level based on configurable criteria such as known malicious denominations or denominations from blacklisted chains. This can act as an initial line of defense against known attack vectors.\n\nThis approach strikes a balance between security, usability, and flexibility. It discourages attacks by making them costly, empowers users to manage their denomination limit based on their needs, and keeps the door open for future optimizations like denomination merging and packet filtering.\n","GenTime":"2024-08-19 14:57:22","text":null}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"Prediction":"## Decision\n\n**Option 1: Keep repositories separate, publish from DPE Client**\n\n* Components are published to NPM from the DPE Client repository.\n* Storybook repository consumes components from NPM.\n* Storybook repository remains a separate entity.\n\n**Rationale:**\n\nThis option provides a clear separation of concerns. The DPE Client repository remains focused on the component library, while the Storybook repository is dedicated to showcasing and documenting these components. This separation simplifies maintenance and allows for independent versioning and release cycles.\n\n**Option 2: Move Storybook into DPE Client repository**\n\n* Components are not published to NPM.\n* Storybook consumes components directly from the DPE Client repository.\n* Storybook exists as a subfolder within the DPE Client repository.\n\n**Rationale:**\n\nThis option simplifies the development workflow by eliminating the need for separate repositories and publishing steps. However, it tightly couples Storybook to the DPE Client repository, potentially making it more difficult to manage and evolve independently.\n\n**Decision:**\n\nWe will proceed with **Option 1: Keep repositories separate, publish from DPE Client**. This approach offers better maintainability, flexibility, and aligns with best practices for component libraries and Storybook integration.\n\n**Consequences:**\n\n* Requires managing two separate repositories.\n* Introduces a publishing step for components.\n* Ensures a clean separation of concerns and promotes maintainability.\n* Allows for independent versioning and release cycles for components and Storybook.\n","GenTime":"2024-08-19 14:57:40","text":null}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"Prediction":"## Decision\n\nWe will adopt MCP as the primary mechanism for configuration distribution within our Istio service mesh. \n\nThis decision is driven by the following factors:\n\n* **Simplified Client Logic:** Migrating to MCP allows us to offload complex configuration logic from individual Istio clients (like Pilot) to a centralized server (copilot). This simplifies client implementations, reduces their footprint, and enables more consistent behavior across the mesh.\n* **Unified Configuration Management:** MCP provides a standardized contract for managing various configuration resources, including Custom Resource Definitions (CRDs) and service discovery information. This unification streamlines configuration management and improves interoperability between different Istio components. \n* **Enhanced Scalability and Flexibility:** By centralizing configuration logic, MCP facilitates easier scaling and modification of the control plane. New features and configuration options can be introduced without requiring updates to every client in the mesh.\n\nWe acknowledge that adopting MCP introduces a dependency on the copilot server and requires careful consideration of its availability and performance characteristics. However, we believe the benefits of simplified client logic, unified configuration, and increased scalability outweigh these potential drawbacks. \n","GenTime":"2024-08-19 14:58:28","text":null}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"Prediction":"## Decision\n\n**Option 1: Configure Envoy to require client certificates for specific domains and hosts.**\n\nThis approach leverages Envoy's built-in TLS capabilities to enforce client certificate authentication. We will configure Envoy to request and validate client certificates for specific domains and hosts defined in the gateway configuration. This allows for granular control over which services require mTLS.\n\n**Option 2: Implement a custom authorization mechanism within Envoy.**\n\nThis approach involves developing a custom Envoy filter or leveraging an existing authorization framework (e.g., External Authorization) to perform client authentication based on the gateway configuration. This provides flexibility in terms of authentication logic but requires additional development and maintenance effort.\n\n**Decision:** We will proceed with **Option 1: Configure Envoy to require client certificates for specific domains and hosts.**\n\n**Reasoning:**\n\n* **Leverages Existing Functionality:** Envoy natively supports client certificate authentication, reducing development effort and complexity.\n* **Granular Control:**  We can specify which domains and hosts require client certificates through the gateway configuration, offering flexibility and security.\n* **Standardized Approach:** mTLS is a well-established and widely used security standard.\n\n**Consequences:**\n\n* **Client Certificate Management:** This solution requires managing client certificates for authorized front-end instances.\n* **Configuration Complexity:** The gateway configuration needs to accommodate the specific domain and host information for mTLS enforcement.\n\n**Alternatives Considered:**\n\n* **Option 2** was considered but ultimately rejected due to increased complexity and development effort.\n\n**Next Steps:**\n\n1. Update the gateway configuration schema to include fields for specifying domains and hosts requiring mTLS.\n2. Implement logic within the gateway to propagate this configuration to Envoy.\n3. Document the process for generating and configuring client certificates for authorized front-end instances. \n","GenTime":"2024-08-19 14:58:36","text":null}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"Prediction":"## Decision\n\n1. **Establish clear communication channels and expectations:**  Before Thabang leaves, define how the team will communicate during his absence (e.g., daily stand-ups via video conference, project management tool updates).  Clearly outline Thabang's availability for questions or urgent matters.\n2. **Prioritize critical tasks:** Identify and prioritize tasks crucial for project progress before the submission deadline. Focus on completing these tasks before April 29th or adjust the schedule to minimize the impact of Thabang's reduced availability.\n3. **Delegate or postpone non-critical tasks:**  If possible, delegate tasks not critical for the immediate submission deadline to other team members. Non-essential tasks can be postponed until after the submission period when Thabang returns to full capacity.\n4. **Provide remote work resources:** Ensure Thabang has access to all necessary resources and tools to work effectively remotely, such as a reliable internet connection, VPN access, and relevant documentation. \n5. **Maintain flexibility and understanding:** Acknowledge that Thabang's productivity might be impacted during this period. Remain flexible with deadlines and offer support where needed. Regularly check in on his workload and adjust plans accordingly. \n","GenTime":"2024-08-19 14:59:07","text":null}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"Prediction":"## Decision\n\n**Option:** Integrate Google Login API as an alternative login method.\n\n**Decision:** Deferring integration of Google Login API until a later stage.\n\n**Rationale:** \n\nWhile using the Google Login API offers convenience and speed for Sprint 1, several factors contribute to delaying its implementation:\n\n* **Security:** Relying solely on a third-party authentication system for our initial login implementation might introduce security risks and dependencies that need thorough investigation before deployment.\n* **Data Control:**  Utilizing Google Login might require us to handle user data differently to comply with their terms of service and privacy policies.  Clarifying these aspects is crucial before proceeding.\n* **Future Roadmap:**  Integrating Google Login might influence the design and implementation of our own authentication system planned for later sprints.  Prioritizing a robust internal solution first ensures better control and flexibility in the long run.\n\n**Instead:** We will focus on building a basic but functional internal login system for Sprint 1, adhering to security best practices and laying a solid foundation for future authentication features, including potential integration with Google Login at a later stage. \n","GenTime":"2024-08-19 14:59:36","text":null}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"Prediction":"## Decision\n\nDespite the technical \"incorrectness\", we will continue to use `$this->assert...()` instead of `self::assert...()` for PHPUnit assertions within our codebase. \n","GenTime":"2024-08-19 15:00:01","text":null}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"Prediction":"You provided a complete Architectural Decision Record (ADR). It's well-structured and informative.  Here's a breakdown:\n\n**Elements of a Good ADR:**\n\n* **Context:** You clearly presented the problem of handling expected exceptions in test cases, along with two possible solutions in PHP code. This sets the stage for the decision.\n* **Decision:** You explicitly stated the chosen approach: utilizing PHPUnit's `expectException*()` API based on authoritative advice from Sebastian Bergmann. The rationale is clear.\n* **Status:** Marking the decision as \"Accepted\" and linking to a relevant pull request (#1090) provides valuable traceability and indicates that the decision has been implemented.\n* **References:** Including links to Sebastian Bergmann's profile, his article on PHPUnit best practices, and the pull request strengthens the ADR's credibility and allows readers to delve deeper.\n\n**Possible Enhancements:**\n\n* **Consequences:** While the ADR is comprehensive, you could consider adding a \"Consequences\" section. Briefly outlining potential downsides (e.g., the effort required to update existing code) or trade-offs considered can enhance future understanding. \n\n**Overall:**  This ADR effectively documents a technical decision, making the reasoning transparent and accessible for the team and future developers. \n","GenTime":"2024-08-19 15:00:09","text":null}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"Prediction":"## Decision\n\nWe will utilize a Kubernetes-native CI\/CD pipeline for Dapr E2E testing. This approach leverages the strengths of Kubernetes and provides a robust, scalable, and reproducible testing environment.\n\n**Test Infrastructure:**\n\n*   **Kubernetes Cluster:** E2E tests will run against dedicated, ephemeral Kubernetes clusters created on demand. This ensures a clean environment for each test run and avoids interference between tests.\n*   **Dapr Installation:** Dapr will be installed on the test cluster using the official Helm chart, ensuring consistency and ease of configuration.\n*   **Test Application Deployment:**  Test applications will be deployed to the cluster using Kubernetes manifests, mirroring real-world deployment scenarios.\n\n**Test Execution:**\n\n*   **CI\/CD Pipeline:**  A CI\/CD platform like GitHub Actions, GitLab CI, or Jenkins will orchestrate the entire testing workflow.\n*   **Test Runner:** A testing framework like Jest or Mocha,  in conjunction with a library like `dapr-client`, will be used to execute E2E tests.\n*   **Parallel Testing:**  Tests will be parallelized to reduce execution time and provide faster feedback.\n\n**Test Reporting:**\n\n*   **Test Results:** Test results will be collected within the CI\/CD pipeline and published to a centralized dashboard.\n*   **Logging and Tracing:** Dapr logs and distributed tracing data will be captured and correlated with test results for debugging and analysis.\n\n**Rationale:**\n\n*   **Kubernetes Native:**  Aligns with Dapr's focus on Kubernetes and leverages its capabilities for managing ephemeral environments.\n*   **Reproducibility:**  Ephemeral clusters and declarative deployments ensure test consistency and eliminate environment-related issues.\n*   **Scalability:**  Kubernetes allows for easy scaling of the test infrastructure to accommodate growing test suites and parallel execution.\n*   **Observability:**  Integrated logging and tracing provide insights into test behavior and facilitate debugging. \n\n**Alternatives Considered:**\n\n*   **Local Docker Environment:**  While simpler, it lacks the scalability and reproducibility of a Kubernetes-based approach. \n*   **Static Test Environments:**  Difficult to maintain and prone to configuration drift, impacting test reliability.\n\nThis approach provides a comprehensive and scalable solution for Dapr E2E testing, ensuring the reliability and stability of Dapr in real-world deployments. \n","GenTime":"2024-08-19 15:00:25","text":null}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\\nAuthenticode signing of binaries.\\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"Prediction":"## Decision\n\n**We will implement Authenticode signing for all production binaries.** This includes executables, libraries, and installers. \n\n**Reasoning:**\n\n* **Increased Security:** Authenticode signing provides a digital signature that verifies the publisher's identity and guarantees that the binary hasn't been tampered with. This significantly reduces the risk of malware injection and increases user trust.\n* **Improved User Experience:** Signed binaries are less likely to trigger security warnings from operating systems and antivirus software, leading to a smoother installation process for users.\n* **Industry Best Practice:** Signing binaries is considered a best practice in software development, especially for software distributed publicly. \n\n**Details:**\n\n* **Implementation:** We will integrate a code signing process into our build pipeline. This will likely involve obtaining and securely storing a code signing certificate from a trusted Certificate Authority (CA).\n* **Certificate Management:** We will establish procedures for securely storing, renewing, and revoking code signing certificates. \n* **Timestamping:** We will implement timestamping for our signed binaries. Timestamping ensures that the signature remains valid even after the code signing certificate expires.\n* **Verification:** We will implement mechanisms for users to verify the authenticity of our signed binaries. \n\n**Consequences:**\n\n* **Cost:** Obtaining and maintaining a code signing certificate will incur a cost. \n* **Complexity:** Integrating code signing into the build pipeline will add some complexity to the development process.\n* **Maintenance:** We will need to maintain the code signing infrastructure and processes.\n\n**However, we believe that the benefits of Authenticode signing, particularly in terms of security and user trust, outweigh these costs.**\n","GenTime":"2024-08-19 15:00:43","text":null}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"Prediction":"## Decision\n\n**Minimum supported Java version for the Dapr Java SDK will be Java 8.**\n\n**Reasoning:**\n\n* **Wider adoption:** While Java 11 is the latest LTS version, Java 8 remains predominantly used within the Java community. Targeting Java 8 ensures broader compatibility and adoption of the Dapr Java SDK.\n* **Reduced barrier to entry:** Choosing Java 11 might exclude a significant portion of developers and projects still operating on Java 8. This could hinder Dapr's adoption rate.\n* **Future-proofing:**  Dapr can always revisit this decision and increase the minimum supported version later as Java 11 adoption grows. \n\n**Consequences:**\n\n* **Potential limitations:**  Using Java 8 might limit leveraging the newest language features and performance improvements introduced in Java 11.\n* **Maintenance burden:** Supporting an older version might require additional effort in the long run as newer Java versions become standard.\n\n**Mitigation:**\n\n* **Monitor Java adoption trends:** Regularly assess the Java ecosystem to determine when transitioning to a higher minimum Java version (like Java 11 or later) becomes feasible and beneficial.\n* **Clear communication:**  Transparently communicate the rationale behind this decision and any potential limitations to the Dapr community.\n* **Future-proofing:** Design the SDK with modularity in mind, allowing for easier adaptation to future Java versions when necessary. \n","GenTime":"2024-08-19 15:00:50","text":null}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"Prediction":"## Decision\n\nWe will implement a structured Docker image naming convention that incorporates the following elements:\n\n**Format:** `<organization>\/<image-name>:<tag>`\n\n**Elements:**\n\n* **`<organization>`:** A short identifier for our organization (e.g., \"acmecorp\"). This provides a namespace for our images.\n* **`<image-name>`:** A descriptive and concise name for the application or service contained in the image (e.g., \"web-server\", \"database\").\n* **`<tag>`:** A string providing specific information about the image version and architecture. This will follow a sub-structure: `<version>-<architecture>` \n    * **`<version>`:**  Utilizing Semantic Versioning (e.g., \"1.2.3\", \"latest\").\n    * **`<architecture>`:** Specifying the target architecture (e.g., \"amd64\", \"arm64\"). \n\n**Examples:**\n\n* `acmecorp\/web-server:1.2.3-amd64`\n* `acmecorp\/database:latest-arm64`\n\n**Rationale:**\n\n* **Organization:** Ensures uniqueness and avoids conflicts with other publicly available images.\n* **Descriptive Names:** Improves readability and understandability.\n* **Semantic Versioning:**  Provides a clear and consistent way to track image versions.\n* **Explicit Architecture:** Enables easy identification and selection of images for specific target environments.\n\n**Implementation:**\n\n* This naming convention will be enforced for all Docker images built and pushed to our repositories.\n* We will utilize CI\/CD pipelines to automate the image tagging process, ensuring consistency across projects.\n* Documentation will be updated to reflect this decision and provide clear guidelines for developers.\n","GenTime":"2024-08-19 15:00:58","text":null}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"Prediction":"## Decision\n\nWe will introduce a new parameter to the binding API, tentatively named `operationType`, to specify the directionality of the operation.  This parameter will accept values such as `input`, `output`, or `both` to clearly indicate whether the binding is used for sending data to the function, receiving data from the function, or both.\n\nThis approach provides several advantages:\n\n* **Explicitness:**  Clearly defines the intended data flow direction for each binding.\n* **Flexibility:** Supports various use cases, including bi-directional communication with external services.\n* **Backward compatibility:** Existing bindings without the `operationType` parameter will default to the current behavior (likely `input`).\n\nWe will need to:\n\n1. Define the exact syntax and allowed values for the `operationType` parameter.\n2. Update the binding implementation for each supported service to handle the new parameter and execute the appropriate actions.\n3. Update documentation to explain the new parameter and its usage with examples.\n\n## Alternatives Considered\n\n* **Separate APIs for input and output:** This would involve creating distinct APIs or methods for input and output bindings. While cleaner from a semantic perspective, it introduces significant complexity and redundancy. \n\n## Tradeoffs\n\nThis approach prioritizes flexibility and backward compatibility while introducing a minor change to the existing API. It may require slightly more configuration compared to separate APIs, but the benefits outweigh the drawbacks. \n","GenTime":"2024-08-19 15:01:04","text":null}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"Prediction":"## Decision\n\nWe will **not** couple CLI versions with runtime versions in Dapr. \n\n## Status\n\nAccepted\n\n## Consequences\n\n**Pros:**\n\n* **Flexibility:** Decoupling allows users to upgrade\/downgrade the CLI independently of the runtime. This is helpful for managing different environments, testing new CLI features with older runtimes, or using specific CLI versions for compatibility reasons. \n* **Independent Release Cycles:**  Allows for faster iteration and releases for both the CLI and runtime components. Bug fixes and features can be shipped without waiting for a synchronized release cycle.\n* **Simplified Versioning:** Separating versioning simplifies the upgrade process for users. They can clearly understand which component (CLI or runtime) a specific version change relates to.\n\n**Cons:**\n\n* **Compatibility Issues:** Potential for compatibility issues if a user is running a very outdated CLI version with a much newer runtime, or vice-versa.  \n* **Increased Testing Complexity:**  The development team needs to ensure compatibility and test across a wider matrix of CLI and runtime versions.\n\n## Mitigation\n\nTo mitigate the potential downsides of decoupling, we will:\n\n* **Clearly document compatibility:**  Maintain comprehensive documentation outlining the compatibility matrix between CLI versions and supported runtime versions.\n* **Implement Robust Version Checking:** The CLI will perform version checks against the target Dapr runtime and issue warnings or recommendations if there are potential compatibility concerns or if an update is recommended.\n* **Provide Clear Upgrade Instructions:** Offer clear and straightforward guidance on upgrading both the CLI and runtime components independently, outlining best practices and potential considerations. \n","GenTime":"2024-08-19 15:01:24","text":null}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"Prediction":"## Decision\n\nWe **will introduce different manifest types** for Bindings, State Stores, and Message Buses. \n\n## Reasoning\n\nWhile all three component types share some commonalities, separating them in the manifest offers several advantages:\n\n* **Clarity and maintainability:** Distinct manifest types improve the readability and organization of component definitions. It becomes immediately clear what type of component a manifest represents.\n* **Validation and tooling:**  Separate types allow for more specific validation rules and tailored tooling support. This leads to fewer configuration errors and easier development.\n* **Future flexibility:** As each component type evolves, having distinct manifest types provides the flexibility to introduce changes without impacting other types.  \n\n## Consequences\n\n* **Increased complexity:** Introducing new manifest types increases the complexity of the system, requiring updates to documentation, validation logic, and developer tooling.\n* **Migration effort:** Existing users will need to migrate their component definitions to the new manifest types.  \n\n## Mitigation\n\n* **Clear migration path:** Provide clear documentation and tooling to guide users through the migration process.\n* **Versioning:** Use versioning in the manifest schema to manage backward compatibility and ensure smooth transitions. \n","GenTime":"2024-08-19 15:02:11","text":null}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"Prediction":"## Decision\n\n**Changes to `init`**:\n\n* **Do not merge default configurations with existing ones:** When running `dapr init` in an environment where Dapr is already installed, the command should not attempt to merge default configurations with existing ones. This prevents accidental overwrites and ensures predictable behavior.\n* **Introduce an explicit `--force` flag:** To allow users to overwrite existing configurations intentionally, a `--force` flag will be added to the `dapr init` command. Using this flag will explicitly signal the user's intent to overwrite existing settings.\n\n**Changes to `uninstall`**:\n\n* **Do not remove the default namespace:**  The `dapr uninstall` command will no longer remove the default Kubernetes namespace (currently \"dapr-system\"). This change prevents accidental deletion of resources in environments where the default namespace might be shared or customized.\n\n**Additional considerations**:\n\n* **Clear communication in documentation and CLI help:** The updated behavior of both commands will be clearly documented in the CLI help and the official Dapr documentation. \n* **Error handling for existing installations:** The CLI will be updated to provide more informative error messages when users attempt to run `dapr init` in an environment where Dapr is already installed without the `--force` flag.\n\n**Rationale**:\n\nThe proposed changes aim to address the concerns raised in the Github issue by preventing accidental overwrites of configurations and accidental deletion of the default namespace. This approach prioritizes the safety and predictability of the CLI for users. By introducing the `--force` flag and clarifying the behavior of `uninstall`, we provide users with more control over their environment and prevent unintended consequences. \n","GenTime":"2024-08-19 15:02:19","text":null}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"Prediction":"## Decision\n\nWe will implement a REST API as the communication layer between the simulator (Java program) and the Nozama web application (Spring framework). \n\n**Justification:**\n\n* **Platform agnostic:** REST APIs use standard HTTP methods (GET, POST, PUT, DELETE) and data formats (JSON, XML) making it compatible with both Java and Spring applications.\n* **Loose Coupling:**  RESTful architecture promotes loose coupling between client (WebApp) and server (simulator) allowing independent development and deployment. \n* **Scalability:**  REST APIs can be easily scaled horizontally to handle increased traffic.\n* **Well-established and widely used:**  REST is a mature architectural style with a vast ecosystem of tools, libraries, and best practices.\n\n**Alternatives considered:**\n\n* **Direct TCP\/IP sockets:** While potentially faster, this approach introduces tight coupling and complexity in managing network connections. \n* **Message Queue (e.g., RabbitMQ, Kafka):**  Suitable for asynchronous communication and event-driven architectures, but might be overkill for simpler interactions.\n\n**Consequences:**\n\n* **Development effort:**  Implementing a REST API will require development effort on both the simulator and web application sides. \n* **Latency:**  Communication over HTTP will introduce some latency compared to direct inter-process communication. However, this is unlikely to be significant for the simulator use case. \n\n**Next Steps:**\n\n1. **Define the API contract**: Specify the endpoints, request\/response formats, and data models for the communication between the simulator and web application.\n2. **Implement the REST API**:  Develop the API endpoints on the simulator side to expose functionalities required by the web application.\n3. **Integrate the WebApp with the API**:  Update the WebApp to communicate with the simulator through the defined REST API.\n4. **Testing**: Thoroughly test the communication layer between the simulator and web application. \n","GenTime":"2024-08-19 15:03:28","text":null}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"Prediction":"## Decision\n\nWe will keep separate locales within the same index and define unique field names for each field which needs to be analyzed under a specific locale. \n\n## Rationale\n\nWhile using separate indices for each locale offers a clean separation and potentially simpler analyzer configuration, it introduces significant complexity in managing multiple indices:\n\n* **Increased operational overhead:** Managing multiple indices adds complexity to backups, restores, reindexing, and monitoring. \n* **More complex querying:** Searching across multiple indices requires additional logic and can impact performance.\n* **Data duplication for shared content:**  Content that is truly language-agnostic (e.g., article IDs, timestamps) would need to be duplicated across all locale indices.\n\nKeeping data within the same index offers several advantages:\n\n* **Simplified index management:**  Fewer indices to manage, simplifying operational tasks.\n* **Efficient querying:** Searching within a single index is generally faster and less complex.\n* **Flexibility:** Allows for easier implementation of features like cross-locale search suggestions in the future.\n\nThe trade-off is the need to define and manage locale-specific field names. However, this can be mitigated with clear naming conventions and programmatic handling within the application. \n\nThis approach strikes a balance between performance, maintainability, and flexibility, making it the preferred option for our current requirements.\n","GenTime":"2024-08-19 15:03:51","text":null}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"Prediction":"## Decision\n\nGiven the complexity and breadth of the project's requirements, encompassing both learning the Clojure ecosystem and building a highly scalable, resilient, and geographically distributed system across multiple cloud providers, we will adopt an iterative and incremental approach. This will involve the following:\n\n1. **Prioritize Learning and Experimentation:** We will dedicate initial sprints to familiarize ourselves with the Clojure ecosystem, focusing on core concepts, relevant libraries, and tooling for building scalable applications. This learning phase will involve hands-on experimentation and prototyping.\n2. **Start Small, Scale Incrementally:**  We will begin by building a Minimum Viable Product (MVP) with a focus on core functionality and a simplified architecture. This MVP will be deployed to a single cloud provider to minimize initial complexity.  \n3. **Embrace Cloud-Native Principles:** We will design our application using cloud-native principles like microservices, containerization (Docker), and orchestration (Kubernetes). This approach enables scalability, resilience, and portability across cloud providers.\n4. **Progressive Cloud Adoption:** After successfully deploying the MVP on one cloud provider, we will gradually introduce multi-cloud capabilities, evaluating different cloud providers and services based on cost, performance, and geographical needs.\n5. **Continuous Integration and Deployment (CI\/CD):**  We will establish robust CI\/CD pipelines to automate testing, building, and deploying our application. This will facilitate rapid iteration and ensure the reliability of our system as we scale.\n\nThis approach allows us to manage risk by breaking down the complex challenge into smaller, manageable iterations. We can adapt our strategy and prioritize features based on learnings and feedback gained at each stage. By embracing cloud-native principles and a multi-cloud strategy, we maintain flexibility and avoid vendor lock-in, ensuring long-term scalability and resilience. \n","GenTime":"2024-08-19 15:04:19","text":null}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"Prediction":"## Decision\n\nWe will **not** be using CockroachDB for persistence. \n\nWhile CockroachDB offers appealing features like distributed architecture and SQL compatibility, the limitations of the Windows version present significant risks:\n\n* **Reduced Functionality:** The Windows version of CockroachDB lacks essential features like distributed transactions and change data capture (CDC) which are crucial for our application's scalability and data integrity. \n* **Operational Complexity:** Managing a production-ready CockroachDB cluster on Windows can be more complex compared to Linux due to limited tooling and community support.\n* **Performance Implications:** The Windows version might exhibit performance differences compared to its Linux counterpart, impacting our application's responsiveness and resource utilization.\n\nThis decision prioritizes long-term stability, performance, and maintainability. We will explore alternative persistence solutions that offer full functionality and robust support on our target operating system. \n","GenTime":"2024-08-19 15:04:24","text":null}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"Prediction":"## Decision\n\nWe will migrate from Vundle to vim-plug for managing our Vim plugins. \n\nThis decision is driven by the following factors:\n\n* **Vundle's development has stalled:**  With the last update being in August 2019, Vundle appears to be no longer actively maintained. This poses a risk in terms of compatibility with newer Vim versions and plugins.\n* **Positive reception of vim-plug:** vim-plug is widely regarded as a fast and reliable plugin manager with active development and support.\n* **Desire for modernization:**  Switching to a more modern plugin manager allows us to benefit from potential performance improvements, new features, and better overall support. \n\nWe will evaluate the migration process and its impact on our existing setup to ensure a smooth transition.\n","GenTime":"2024-08-19 15:04:55","text":null}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"Prediction":"## Decision\n\nGiven the transition to zsh as the default shell in macOS Catalina and later, we will officially support zsh in our software going forward. This decision impacts both our development environment and the end-user experience:\n\n* **Development:** \n    * All new scripts and configuration files will be written using zsh-compatible syntax.\n    * Existing scripts will be reviewed and updated for zsh compatibility if necessary.\n    * Our CI\/CD pipeline will be configured to test the software using zsh.\n* **End-users:**\n    * Documentation and examples will be updated to reflect zsh syntax and usage.\n    * We will provide clear instructions and support for users who may need assistance transitioning from bash to zsh.\n    * We will investigate and address any potential compatibility issues with zsh.\n\nWe will prioritize maintaining compatibility with bash for users who haven't transitioned to newer macOS versions. However, future feature development and optimizations may prioritize zsh compatibility. \n","GenTime":"2024-08-19 15:05:00","text":null}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"Prediction":"## Decision \n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism for capturing important architectural decisions along with their context and consequences.  Each ADR will be a separate Markdown file stored within a dedicated \"adr\/\" directory at the root level of our project repository. \n","GenTime":"2024-08-19 15:05:07","text":null}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"Prediction":"## Decision\n\nWe will use GitHub Actions as the CI system for this repository. \n","GenTime":"2024-08-19 15:05:08","text":null}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"Prediction":"## Decision\n\nWe will incorporate the citerus library as a Git submodule. \n\nThis addresses the user's concerns in the following ways:\n\n* **Ease of updates:**  Git submodules allow for easy synchronization with the upstream citerus repository. Any updates or fixes made to the common project can be easily pulled into the user's project.\n* **Convenient modification:**  The user can directly modify the citerus code within the submodule and commit those changes. This enables rapid prototyping and experimentation with potential fixes or improvements. \n* **Lightweight touch:**  Working within the submodule doesn't require interacting with the main citerus project's maintainers or build system. This provides the user with the autonomy to explore different approaches without the overhead of continuous integration or code reviews from the upstream project.\n* **Familiar structure:** Git submodules maintain the directory structure of the included project. This ensures the citerus code remains under its own directory, which aligns with the user's preference for elements rooted under a common location.\n\n**Important Considerations:**\n\n* While this approach offers flexibility, it's important to note that changes made within the submodule will need to be consciously merged with updates from the upstream citerus repository to avoid conflicts and ensure compatibility. \n* The user should have a plan for contributing valuable changes back to the main citerus project, if desired.  \n","GenTime":"2024-08-19 15:06:02","text":null}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the future architectural decisions made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a mechanism to document significant architectural decisions along with their context and consequences. \n\nThis decision is driven by the need to:\n\n* **Maintain a record of why certain architectural decisions were made**: This is crucial for future reference and can be particularly helpful when onboarding new team members or revisiting decisions later on.\n* **Improve the transparency of the decision-making process**: ADRs make the reasoning behind architectural choices clear to all stakeholders involved.\n* **Enable better architectural governance**: By documenting decisions, we establish a framework for evaluating future changes and ensure alignment with the overall architectural vision.\n\nWe will use a lightweight Markdown format for our ADRs, such as the MADR template, and store them in a dedicated `\/docs\/adrs` directory within our version control system. This ensures versioning and easy access for all team members. \n","GenTime":"2024-08-19 15:06:39","text":null}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"Prediction":"## Decision\n\nWhile adding a `--require` or `-r` parameter offers a generic solution for potentially many prompts, it introduces unnecessary complexity for this specific case. Instead of burdening the user with specifying required fields for every project, lifecorp-init will be modified to **automatically prompt the user for the `path_url` when creating a web1 app**. This ensures the crucial information is gathered upfront while maintaining the ease of use for typical cases where default values suffice. \n\nThis decision prioritizes a smoother user experience for the most common use case (web1 app creation) while acknowledging the potential need for a more general solution like `--require` in the future. \n","GenTime":"2024-08-19 15:06:42","text":null}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"Prediction":"## Decision\n\nTransition the window manager to use Xlib exclusively for display management and interaction with the X server. \n","GenTime":"2024-08-19 15:06:55","text":null}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"Prediction":"## Decision\n\nWe will not formally define a coding style guide at this time. Instead, we will leverage Swiftlint with a configuration file ([`.swiftlint.yml`](.\/..\/..\/.swiftlint.yml)) to enforce a baseline of code style consistency based on widely accepted practices. This approach allows for flexibility and personal preferences in areas where Swift itself doesn't impose strict rules.\n\nAs the project grows and more developers contribute, we will revisit the need for a more formal coding style guide. We acknowledge that relying solely on individual preferences may become less sustainable with a larger team. For now, Swiftlint serves as a lightweight solution to prevent major inconsistencies and promote readability. \n","GenTime":"2024-08-19 15:07:13","text":null}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"Prediction":"## Decision\n\nWe will use GitHub Actions as our CI solution for this repository. \n","GenTime":"2024-08-19 15:07:41","text":null}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"Prediction":"## Decision\n\nWe will adopt a hybrid approach to styling Web Components that leverages ITCSS principles for global styles and component-specific styles:\n\n**1. Global Styles (ITCSS):**\n\n* We will continue using ITCSS for global styles that affect the overall application layout, typography, and common elements outside of Web Component scopes.\n* ITCSS layers (Settings, Tools, Generic, Elements, Objects, Components, Trumps) will remain applicable for styles outside of shadow DOM.\n\n**2. Component-Specific Styles:**\n\n* Each Web Component will include its own stylesheet scoped within its shadow DOM. This will encapsulate styling specific to the component's internal structure and appearance.\n* Within the component's stylesheet, we will mirror relevant ITCSS layer concepts to maintain consistency and avoid specificity issues within the component:\n    * **Component Settings:** Define component-specific design tokens, variables, or mixins.\n    * **Component Elements:** Style basic HTML elements within the component context.\n    * **Component Objects:** Define layout patterns and reusable styles for component elements.\n    * **Component Trumps:**  Handle specific overrides and state-based styles. \n\n**3. Communication with Parent Components:**\n\n* For situations where a parent component needs to influence the styling of a child Web Component, we will utilize CSS Custom Properties (CSS variables). \n* Parent components will define custom properties that can be consumed within the child component's stylesheet. This enables a level of theming and customization without breaking encapsulation.\n\n**4. Example:**\n\n```css\n\/* Global styles (ITCSS) *\/\n\/* ... *\/\n\n\/* Web Component - my-component.js *\/\nclass MyComponent extends HTMLElement {\n  \/\/ ...\n  connectedCallback() {\n    const shadowRoot = this.attachShadow({ mode: 'open' });\n    shadowRoot.innerHTML = `\n      <style>\n        \/* Component-Specific Styles *\/\n        :host {\n          \/* Component Settings - Using a CSS Custom Property *\/\n          --main-color: var(--global-primary-color, blue); \n        }\n\n        .my-component {\n          \/* Component Elements *\/\n          background-color: var(--main-color);\n        }\n        \/* ...other component-specific styles *\/\n      <\/style>\n      <div class=\"my-component\">\n        <!-- Component content -->\n      <\/div>\n    `;\n  }\n}\n```\n\n**Rationale:**\n\n* This approach balances the benefits of Web Component encapsulation with the need for global style management and consistency. \n* ITCSS principles are applied at both the global and component levels, ensuring maintainability and scalability.\n* CSS Custom Properties facilitate controlled communication between components without sacrificing encapsulation. \n","GenTime":"2024-08-19 15:08:18","text":null}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"Prediction":"## Decision\n\nInstead of hardcoding the `max-height` property for the SubstepsList animation, we will dynamically calculate and set it using JavaScript. This approach will provide the flexibility needed to accommodate varying content heights within the substeps, including unpredictable heights from elements like `ErrorMessage`.\n\n## Consequences\n\n- **Pros:**\n    -  Eliminates the need for fixed `max-height` values, improving responsiveness and handling of variable content height.\n    -  Provides a more robust and flexible animation solution.\n    -  Maintains the integrity of the progress bar functionality. \n- **Cons:**\n    -  Slightly increased complexity due to the dynamic calculation of `max-height`.\n    -  May require additional JavaScript code for handling the calculation and animation.\n\n\n## Implementation Details\n\n1. **Retrieve SubstepsList Height:** When toggling the visibility of substeps:\n    - If showing substeps, use JavaScript to get the computed height of the SubstepsList element (including the `ErrorMessage` height if present).\n    - If hiding substeps, set the target `max-height` to `0`.\n2. **Apply Calculated Height:** Set the calculated height as the `max-height` property of the SubstepsList element before triggering the animation.\n3. **Handle Animation:** Use CSS transitions or a JavaScript animation library to smoothly animate the `max-height` property between `0` and the calculated height, providing the desired show\/hide animation. \n\nThis approach ensures the animation remains smooth and accommodates content height variations, ensuring the progress bar and overall layout function correctly. \n","GenTime":"2024-08-19 15:08:26","text":null}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"Prediction":"## Decision\n\nWe will standardize on **Kotlin** as the primary language for all new Android app development. \n\n## Reasons\n\n* **Official Support and Future:** Kotlin is now the official and recommended language by Google for Android development, signaling strong ongoing support and future development.\n* **Modernity and Conciseness:** Kotlin offers a more modern, concise, and expressive syntax compared to Java, leading to cleaner and potentially less error-prone code. \n* **Interoperability:** Kotlin is fully interoperable with Java, allowing us to leverage existing Java libraries and gradually introduce Kotlin into existing projects if needed.\n* **Developer Productivity:** Kotlin's features like null safety, coroutines, and extension functions are designed to boost developer productivity and reduce boilerplate code.\n* **Growing Community and Resources:** The Kotlin community is rapidly growing, providing ample learning resources, libraries, and support for developers. \n\n## Considered Options\n\n* **Java:**  While a robust and mature language, it lacks the modern features and conciseness of Kotlin. Choosing Java would mean missing out on the advantages Kotlin offers for Android development.\n* **Java and Kotlin (mixed):** While technically possible, using both languages within a single project could lead to inconsistencies in coding style, increased complexity for developers, and potential challenges with code maintenance in the long run.\n\n## Consequences\n\n* **Learning Curve:** Developers unfamiliar with Kotlin will need to allocate time for learning the language and its ecosystem.\n* **Potential Migration:** Existing Java projects will require effort to be migrated to Kotlin if desired in the future.\n\nWe believe the advantages of using Kotlin for new projects, particularly in terms of developer productivity, code quality, and alignment with the future direction of Android development, outweigh the potential drawbacks.  \n","GenTime":"2024-08-19 15:08:34","text":null}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\\nI need a way to document design decisions.\\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as the mechanism for capturing important design decisions within the project. \n","GenTime":"2024-08-19 15:08:35","text":null}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions along with their context and consequences. Each decision will be documented in a separate Markdown file following a consistent template. \n","GenTime":"2024-08-19 15:08:56","text":null}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"Prediction":"## Decision\n\nWe will automate certificate rotation for our Cloud Foundry deployment by integrating Credhub's certificate rotation capabilities. This will involve the following:\n\n* **Leveraging Credhub's built-in rotation features:** We will utilize Credhub's automated certificate generation and rotation mechanisms to handle certificate lifecycles.\n* **Configuring rotation intervals:** We will define appropriate rotation intervals for different types of certificates based on their sensitivity and expiration policies.\n* **Automating rotation triggers:** We will configure Credhub to automatically trigger certificate rotations at predefined intervals. \n* **Integrating with Concourse (optional):** If necessary for specific use cases, we can integrate Credhub's rotation events with Concourse pipelines to automate downstream tasks like service restarts or configuration updates.\n* **Monitoring and Alerting:** We will implement monitoring and alerting mechanisms to track certificate expiration dates, rotation successes\/failures, and notify relevant teams in case of issues.\n\nThis approach eliminates manual intervention, reduces the risk of certificate expirations, and improves the overall security posture of our Cloud Foundry platform. \n","GenTime":"2024-08-19 15:10:02","text":null}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"Prediction":"## Decision\n\nWe will consolidate our CI and Staging environments into a single pre-production environment. This decision aims to streamline our deployment pipeline and accelerate the delivery of changes to production. \n\n**Reasoning:**\n\n* **Reduced Lead Time:**  Merging CI and Staging eliminates a stage in our pipeline, directly reducing the time it takes for a change to reach production.\n* **Simplified Workflow:** Maintaining a single pre-production environment simplifies our workflow, reduces overhead, and minimizes potential points of failure during deployments.\n* **Cost Optimization:** Consolidating environments lowers infrastructure costs associated with running and maintaining separate environments.\n\n**Consequences:**\n\n* **Potential for Regression:**  Merging environments could increase the risk of regressions reaching production if testing in the single pre-production environment is not comprehensive.\n* **Limited Testing Scenarios:** Some specific testing scenarios might be difficult to replicate in a single pre-production environment, potentially requiring alternative solutions like mocking or feature flags.\n\n**Mitigation:**\n\n* **Robust Testing Strategy:** We will implement a comprehensive and automated testing strategy for the single pre-production environment, encompassing unit, integration, and end-to-end tests.\n* **Monitoring and Rollback:** Robust monitoring and a streamlined rollback process will be crucial to quickly identify and address any issues that arise in production.\n\n**Status:** Accepted\n\n**Date:** 2023-10-27 \n","GenTime":"2024-08-19 15:10:33","text":null}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"Prediction":"## Decision\n\nWe will continue to use mutual TLS via routing-release for end-to-end encryption between Gorouter and applications. We will **deprecate the use of IPSec** between Gorouter and Diego cells due to the reported lack of maintenance and potential security risks associated with unsupported software. \n\nThis decision prioritizes the use of a well-maintained and widely adopted encryption method (mutual TLS) over an allegedly unmaintained one (IPSec). \n\n**Further Actions:**\n\n* **Investigate and verify the claim of unmaintained IPSec release.** This should involve researching official documentation, release notes, and community forums to confirm the maintenance status.\n* **Define a timeline for deprecating IPSec.** This should include a communication plan to inform users about the deprecation and provide guidance on migrating to mutual TLS.\n* **Evaluate potential alternatives to IPSec.** While mutual TLS is a robust solution,  exploring other alternatives like WireGuard might be beneficial for future-proofing the platform.\n* **Monitor the security landscape and evolving best practices** for end-to-end encryption to ensure the chosen approach remains secure and relevant. \n","GenTime":"2024-08-19 15:11:03","text":null}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"Prediction":"## Decision\n\nWe will implement a new, simplified, and consistent naming convention for our service plans. This convention will:\n\n* **Eliminate redundant information:** Remove any details already implied by the service or platform (e.g., \"cloud\" if all services are cloud-based).\n* **Focus on key differentiators:**  Highlight the aspects that are truly meaningful to users, such as performance tiers, resource limits, or feature sets.\n* **Use clear and concise language:** Employ straightforward terminology understandable to both technical and non-technical users.\n* **Be extensible:**  Accommodate future service additions and feature variations without compromising clarity. \n\nWe will define a clear mapping between the old and new naming conventions and provide users with ample notification and documentation to ensure a smooth transition. \n","GenTime":"2024-08-19 15:11:33","text":null}
{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"Prediction":"## Decision\n\nRemove the \"Ingroup: <group>\" line from error and warning messages in Elektra.\n\n## Reasoning\n\nThe information about the ingroup (\"kdb\", \"module\", or \"plugin\") where an error occurs is primarily useful for developers debugging Elektra internally.  Regular users generally don't need this information to understand or resolve the error. Removing it reduces the verbosity of error messages, making them clearer and easier to understand for the end user.\n\n## Consequences\n\n* **Improved user experience:** Simpler and clearer error messages for users.\n* **Reduced debugging information:** Developers debugging Elektra will need to rely on other means to determine the source of an error (e.g. debug logs, stack traces).\n* **Potential minor breaking change:** Tools parsing error messages might require adjustments. \n\n## Alternatives considered\n\n* **Keep the \"Ingroup\" line:** This maintains the status quo but does not address the problem of verbose and user-unfriendly error messages.\n* **Provide a verbose mode:** Introduce a flag or configuration option to display the \"Ingroup\" information only when requested. This adds complexity and may not be frequently used.\n* **Improve error messages:** Instead of removing the \"Ingroup\" line, enhance error messages with more context and guidance for the user, making the \"Ingroup\" information potentially useful. This requires more effort and careful design.\n\n## Action Items\n\n1. Remove the code that outputs the \"Ingroup\" line in error and warning messages.\n2. Update unit tests that rely on the \"Ingroup\" information.\n3. Document the change in the release notes.\n4. (Optional) Explore opportunities to improve error messages with more user-friendly information in the future. \n","GenTime":"2024-08-19 15:12:01","text":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n\n##Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"Prediction":"## Decision\n\nWe will be using a direct exchange named based on the intended recipient of the message:\n\n* **For client-to-BabelTower communication:** `babeltower.direct` \n* **For client-to-Connector (direct) communication:** `connector.direct`\n\n**Rationale:**\n\n* **Direct exchanges offer point-to-point messaging:** This aligns with the requirement to have clear communication paths between clients, BabelTower, and the Connector. \n* **Naming clarity:** Using `babeltower` and `connector` in the exchange name clearly indicates the intended destination of the messages.  \n* **Scalability:** This naming convention allows for the addition of further services and communication paths in the future, using a consistent and understandable pattern (e.g., `service_name.direct`).\n\n**Alternatives considered:**\n\n* **Topic exchanges:** While they offer flexibility, they introduce complexity that might not be necessary for the initial architecture. We can revisit this if more intricate routing based on message attributes becomes necessary.\n* **Using a single exchange for all communications:** This would hinder clarity and make it difficult to understand message flow within the system. \n\n**Consequences:**\n\n* **Simplicity:** The chosen approach keeps the messaging architecture straightforward and easy to understand.\n* **Maintainability:** The clear naming conventions contribute to better maintainability of the system.\n* **Potential limitations:**  If the routing requirements become more complex in the future, we might need to revisit this decision and consider alternative exchange types. \n","GenTime":"2024-08-19 15:12:07","text":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n\n\n##Decision\n* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"Prediction":"## Decision\n\nWe will store the configuration in the browser's localStorage. \n\n## Reasons\n\n* **Simplicity:** localStorage offers a straightforward and readily available solution within the browser environment, eliminating the need for a backend database and associated complexities for the pilot phase.\n* **Scope:**  The pilot's requirements suggest a limited scope, making localStorage's storage capacity sufficient. \n* **Speed:** localStorage provides faster read\/write operations compared to network-dependent backend solutions, enhancing the user experience.\n\n## Consequences\n\n* **Data Persistence:** Data stored in localStorage is tied to the browser's origin (domain, protocol, port). Clearing browser data or using a different browser\/device will result in data loss.\n* **Security:** localStorage offers limited security measures. Sensitive information should not be stored using this method.\n* **Scalability:** If the application's scope expands beyond the pilot and requires larger storage, data sharing across devices, or enhanced security, migrating to a backend database solution might become necessary. \n\nWe acknowledge the limitations of localStorage but deem it suitable for the pilot phase due to its simplicity, speed, and the project's limited scope. We will revisit this decision if the application's requirements change in the future. \n","GenTime":"2024-08-19 15:12:13","text":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n\n\n##Decision\nWe will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"Prediction":"## Decision\n\nWe will **not** split the Holochain agent into a front-house\/back-house model as originally envisioned. The security implications and constant communication overhead render this approach infeasible. \n\nInstead, we will implement **two distinct types of agents**:\n\n1. **User Agent:** This agent will be responsible for key management, source chain, app execution (ribosome), and UI interaction. It will reside on the user's device (e.g., web browser).\n\n2. **HoloPort Agent:** This agent will manage DHT shards, routing, metrics, and handle network communication. It will run on HoloPorts with its own set of keys, acting as a proxy for the User Agent with appropriate authorization.\n\nThis approach maintains the desired separation of user data and network functionality while addressing the security and performance concerns of the previous design.\n","GenTime":"2024-08-19 15:12:17","text":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n\n\n##Decision\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"Prediction":"## Decision\n\nAfter considering the tradeoffs of each workflow management tool, we have decided to use **Airflow** for the following reasons:\n\n* **Mature and Stable:** Airflow is a well-established tool with a large community and extensive documentation, offering a stable and reliable platform.\n* **Fits Execution Model:** Airflow's DAG-based approach aligns well with our requirement of managing both ad-hoc and scheduled tasks.\n* **Excellent UI:**  Airflow's intuitive UI provides comprehensive monitoring capabilities, addressing a key concern with other options like Celery and SWF. \n* **Python Integration:** Utilizing Python for both Airflow and our existing codebase ensures seamless integration and minimizes context switching for developers.\n\nWhile Airflow does require managing the scheduler and a cache, and relies on Celery for execution, we believe the benefits, particularly in terms of monitoring and workflow management capabilities, outweigh these drawbacks. \n\nThis decision supersedes the previous recommendation outlined in this ADR. For the latest implementation details and rationale, please refer to **ADR-0019**.\n","GenTime":"2024-08-19 15:12:22","text":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n\n##Decision\nRaster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"Prediction":"## Decision\n\nWe will define a standardized JSON schema for all events recorded in MongoDB. This schema will ensure consistency in data structure, facilitating efficient querying, analysis, and data integration with other systems. \n\n**Schema Definition:**\n\nThe specific schema definition will be determined based on the types of events and the data points relevant to our application. However, a general structure might include:\n\n```json\n{\n  \"eventId\": \"string\", \/\/ Unique identifier for the event\n  \"eventType\": \"string\", \/\/ Type of event \n  \"timestamp\": \"datetime\", \/\/ Timestamp of when the event occurred\n  \"source\": \"string\", \/\/ Source\/origin of the event\n  \"userId\": \"string\", \/\/ Optional - User associated with the event\n  \"data\": { \n    \/\/ Event-specific data points\n  }\n} \n```\n\n**Implementation Details:**\n\n* We will utilize a schema validation library or MongoDB's built-in validation features to enforce the defined schema at the database level. \n* A shared library or module will be created to provide functions for constructing and validating events before they are sent to the database.\n* Documentation for the event schema will be maintained alongside the code to ensure clarity and consistency for developers. \n\n**Benefits:**\n\n* **Consistency:** Enforces a uniform structure for all events.\n* **Data Integrity:** Reduces errors and inconsistencies in data.\n* **Query Efficiency:** Allows for optimized queries and data retrieval.\n* **Integration:** Simplifies data integration with other systems.\n\n**Drawbacks:**\n\n* **Schema Evolution:** Changes to the schema might require data migration or updates to existing data.\n* **Flexibility:**  A predefined schema might limit flexibility for capturing unforeseen event data. \n\n**Alternatives Considered:**\n\n* **Schema-less Approach:** Allows for flexibility but increases the risk of data inconsistency and complexity in querying.\n\n**Decision Outcome:**\n\nA well-defined JSON schema for events in MongoDB is crucial for maintaining data consistency, integrity, and efficient data management in our application.  We acknowledge potential challenges with schema evolution and flexibility, and we will address those with careful planning and design. \n","GenTime":"2024-08-19 15:12:32","text":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n\n\n##Decision\nThe [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"Prediction":"## Decision\n\nWe will use a serverless architecture on Google Cloud, leveraging the following services:\n\n- **Cloud Storage:** For storing the original uploaded images and their various derived versions (different sizes, formats). \n- **Cloud Run:** For running the application backend code responsible for handling image uploads, processing, search indexing, and serving download requests.\n- **Cloud Vision API:** For automatically tagging uploaded images with relevant keywords, improving searchability.\n- **Firestore:** For storing image metadata (title, user-provided tags, upload date, etc.) and user information if authentication is implemented.\n\n## Reasons\n\n- **Speed and Scalability:** Cloud Run scales automatically based on demand, ensuring fast response times even with fluctuating traffic.\n- **Cost-Effectiveness:** The serverless approach eliminates the need for managing servers and reduces costs as we only pay for the resources we consume.\n- **Faster Development:** Cloud Storage and Cloud Vision API offload complex tasks like image storage and processing, allowing us to focus on core application logic and deliver the first version quickly.\n- **Flexibility:** This architecture allows for easy integration of future features and expansions.\n\n## Consequences\n\n- **Vendor Lock-in:** Using Google Cloud services might lead to vendor lock-in. However, the benefits of faster development and scalability outweigh this concern for the initial release.\n- **Learning Curve:** The team needs to learn about the specific Google Cloud services. However, Google provides comprehensive documentation and resources to facilitate the learning process.\n\nThis decision prioritizes speed and scalability for the initial release, aligning with the goal of quickly going online and responding to competitors. Future iterations can address potential drawbacks like vendor lock-in and explore alternative solutions based on usage patterns and evolving requirements. \n","GenTime":"2024-08-19 15:12:39","text":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n\n\n##Decision\nIn order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\\nWe need to choose a frontend framework.\\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"Prediction":"## Decision\n\nWe will use **React** as our frontend framework. \n\nThis decision requires us to also choose a state management solution, for which we will open a separate ADR. \n","GenTime":"2024-08-19 15:12:41","text":"## Context\\nWe need to choose a frontend framework.\\n\n\n##Decision\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"Prediction":"## Decision\n\nUse the GovUK Notify Staging service in the staging environment to test integrations and user journeys with a near-production setup. This will provide a more accurate representation of how the application will interact with GovUK Notify in the live environment. \n","GenTime":"2024-08-19 15:12:42","text":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n\n\n##Decision\nThe GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"Prediction":"## Decision\n\nWe will use the ISO 8601 standard, as described in RFC 3339, for all date and time representations in our JSON objects. This ensures interoperability with other systems and libraries while providing a well-defined and unambiguous format. \n","GenTime":"2024-08-19 15:13:02","text":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n\n\n##Decision\nWe will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"Prediction":"## Decision\n\n**Choose a single-page application (SPA) architecture with server-side rendering (SSR) capabilities.** \n\nThis decision is driven by the need for fast load times in a greenfield project. \n\n* **Fast Load Times:** SPAs can provide a snappy user experience due to client-side rendering and minimal page reloads. SSR further enhances this by pre-rendering the initial view on the server, resulting in near-instantaneous content display for the user. \n* **Greenfield Project:** Starting fresh allows us to fully embrace the SPA architecture and optimize for performance from the outset. This minimizes the risk of performance bottlenecks often encountered when retrofitting SPAs into existing applications.\n* **Quality Attributes:**  While the specific quality attributes are not mentioned, an SPA with SSR inherently lends itself to high performance, responsiveness, and scalability - all crucial for a positive user experience.\n\nThis decision provides a solid foundation for building a performant and user-friendly application. However, further details about specific technologies, frameworks, and trade-offs will need to be considered and documented in subsequent ADRs. \n","GenTime":"2024-08-19 15:13:08","text":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n\n\n##Decision\nWe will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"Prediction":"## Decision\n\nWe will implement a persistence layer using IndexedDB. \n\nThis layer will:\n\n1. **Mediate between the frontend and API:** This decouples the frontend from the API, allowing for flexibility and easier testing.\n2. **Queue data for offline synchronization:**  Unsubmitted form data will be stored in IndexedDB and synchronized with the backend when a connection is available.\n3. **Handle image storage:**  IndexedDB's support for blob data allows us to store uploaded images locally.\n4. **Preserve unsubmitted forms across sessions:** Data stored in IndexedDB persists across sessions, ensuring users can recover unsubmitted work.\n\n**Reasons for choosing IndexedDB:**\n\n* **Wide browser support:**  Ensures compatibility across our target audience.\n* **Offline capabilities:** Aligned with our requirement for offline use.\n* **Blob storage:**  Necessary for handling image uploads.\n* **Persistence:**  Guaranteed data availability between sessions.\n\n**Alternatives considered:**\n\n* **LocalStorage\/SessionStorage:** Limited storage capacity and unsuitable for sensitive data.\n* **WebSQL:** Deprecated and lacking browser support.\n* **Building a custom solution:**  Increases complexity and development time.\n\nThis decision allows us to meet the requirements for offline functionality, data persistence, and image handling while keeping the frontend decoupled from the backend.\n","GenTime":"2024-08-19 15:13:14","text":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n\n\n##Decision\nWe will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"Prediction":"## Decision\n\nWe will utilize environment variables to manage credentials and environment-specific information. \n\n## Rationale\n\nHardcoding credentials or environment-dependent data is insecure and inflexible. Environment variables offer a robust and secure solution for the following reasons:\n\n* **Security:** Environment variables are stored outside the application codebase, minimizing the risk of exposing sensitive information in version control systems or during deployments.\n* **Flexibility:** Environment variables allow for easy configuration changes across different environments (development, testing, production) without modifying the application code.\n* **Portability:** Most cloud platforms and deployment systems provide mechanisms for injecting environment variables, making the application easily deployable in different environments.\n\n## Implications\n\n* **Implementation:**  The application code will need to be adapted to read and utilize environment variables for credentials and environment-specific configurations.\n* **Deployment:** The deployment process must be configured to inject the appropriate environment variables during deployment to each environment.\n* **Security Considerations:**  Secure storage and management of environment variables are crucial. Utilizing platform-specific secrets management solutions or secure vault systems is highly recommended.\n\n## Alternatives Considered\n\n* **Configuration Files:** While configuration files offer some flexibility, they still require storing potentially sensitive information within the codebase or deployment artifacts.\n* **Command Line Arguments:**  This approach lacks flexibility for managing multiple credentials and configurations, especially in automated deployment scenarios.\n\n## Decision Outcome\n\nThis decision promotes secure coding practices, enhances application flexibility across environments, and streamlines the deployment process. We will adopt best practices for managing environment variables securely to mitigate any potential security risks.\n","GenTime":"2024-08-19 15:13:20","text":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n\n##Decision\n* Chosen Alternative: creating scripts\\n"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"Prediction":"## Decision\n\nWe will introduce a new interface called `ProofOperator` to represent merkle proofs in `abci.ResponseQuery` instead of using raw `[]byte`. \n\nThis interface will provide methods to:\n\n1. **Verify:** Validate the proof against a given key, value and root hash.\n2. **Combine:**  Merge the current proof with another `ProofOperator` to create a layered proof.\n\nThis approach offers several benefits:\n\n* **Abstraction:** Decouples the proof format from the underlying implementation, allowing for extensibility and custom proof types.\n* **Layering:** Enables composing complex proofs from simpler ones without the need for decoding and re-encoding.\n* **Flexibility:**  Supports various Merkle proof formats and verification algorithms.\n\nThis decision promotes code clarity, maintainability, and extensibility, paving the way for more advanced and flexible Merkle proof implementations in the future. \n","GenTime":"2024-08-19 15:13:25","text":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n\n##Decision\n### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"Prediction":"## Decision\n\nWe will host the database **[choose one: on-premise, in the cloud with provider X (e.g., AWS, Azure, GCP)]**. \n\n**Rationale:**\n\nWe considered the following factors:\n\n* **[Factor 1]:**  (e.g., Cost, Scalability, Security, Maintenance, Latency)\n* **[Factor 2]:** (e.g., Cost, Scalability, Security, Maintenance, Latency)\n* **[Factor 3]:** (e.g., Cost, Scalability, Security, Maintenance, Latency)\n\n**[ Briefly explain why the chosen option is best suited based on the factors listed. ]**\n\n**Consequences:**\n\n* **Positive:** [List positive consequences of the decision.]\n* **Negative:** [List negative consequences of the decision and how they will be mitigated.] \n\n**This decision will be revisited if:**\n\n* [Condition 1]\n* [Condition 2] \n","GenTime":"2024-08-19 15:13:30","text":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n\n\n##Decision\nWe use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"Prediction":"## Decision\n\nWe will implement a dedicated query language interpreter to support custom dashboards. This will replace the existing filter box form with a more expressive and flexible syntax, allowing users to define complex queries. \n","GenTime":"2024-08-19 15:13:31","text":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n\n##Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"Prediction":"## Decision\n\nWe will implement a **message broker** architecture to facilitate communication between the three independent components of the SDARS application. \n\n## Reasons\n\n* **Flexibility:** A message broker allows for flexible and decoupled communication between components. Components do not need to know about each other directly, only the message format and the queues\/topics they subscribe to. This allows for future changes or additions of components without significant changes to existing code.\n* **Scalability:** Message brokers are inherently scalable. As the application grows and message volume increases, the broker can be scaled independently to handle the load. \n* **Reliability:**  Message brokers offer features like message persistence and acknowledgment mechanisms, ensuring messages are delivered reliably even in case of component failures. \n\n## Consequences\n\n* **Increased complexity:** Introducing a message broker adds another moving part to the system architecture. Developers will need to learn and understand how the broker works and how to interact with it.\n* **Operational overhead:**  Running and maintaining a message broker requires additional operational effort and resources.\n\n## Status\n\nAccepted.\n\n## Notes\n\nSeveral message broker technologies are available (e.g. RabbitMQ, Kafka, ActiveMQ).  A specific technology will be chosen based on the application's specific requirements and constraints (e.g., message volume, need for persistence, real-time requirements).  This selection will be documented in a separate ADR. \n","GenTime":"2024-08-19 15:13:38","text":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n\n\n##Decision\nAdopt Hexagonal Architecture for project.\\n"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"Prediction":"## Decision\n\nTo address the bottleneck and contention issues caused by having a single test Editor, we will implement a dynamic, on-demand provisioning system for Editor instances. \n\nThis approach involves creating new Editor instances specifically for testing purposes as needed, allowing multiple team members to test features concurrently. Once testing is complete, these temporary instances will be automatically decommissioned. This strategy offers several benefits:\n\n* **Increased parallelism and faster feedback loops:** Multiple team members can test features simultaneously, reducing testing time and enabling faster iteration cycles.\n* **Reduced blocking and improved pipeline throughput:** Decoupling testing from the main release pipeline prevents testing bottlenecks and allows for continuous deployment to live production.\n* **Enhanced resource utilization:** By provisioning Editor instances only when needed, we optimize resource usage and potentially reduce costs.\n\nThis solution will require implementing infrastructure-as-code principles to automate the provisioning and configuration of Editor instances. Additionally, we'll need to ensure seamless integration with our existing continuous delivery pipeline and testing frameworks. \n","GenTime":"2024-08-19 15:13:43","text":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n\n\n##Decision\n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"Prediction":"## Decision\n\nWe will migrate to [Jest][jest] as our testing framework. This decision is based on the following factors:\n\n* **Improved TypeScript Support**: Jest offers first-class TypeScript support out of the box, eliminating the need for complex configuration and ensuring accurate code coverage reporting. \n* **Larger Community & Ecosystem**: Jest boasts a significantly larger community than Lab or Ava. This translates to:\n    *  Extensive documentation and resources.\n    * A wider selection of readily available plugins and integrations, fostering greater flexibility and faster development.\n* **Performance**: While Ava has faced criticism for performance issues, Jest is known for its speed and efficiency. While Dodds' review highlights areas for improvement, Jest's overall performance remains strong, especially with its focus on parallel testing and optimized execution.\n\nThis decision prioritizes accurate code coverage reporting, ease of use with TypeScript, and the benefits of a larger ecosystem. Jest's performance, though not perfect, is considered sufficient for our current needs.\n\n[jest]: https:\/\/jestjs.io\/\n[lab-ts]: https:\/\/github.com\/hapijs\/lab\/issues\/800\n[hapi-lab]: https:\/\/hapi.dev\/module\/lab\/\n[ava-typescript]: https:\/\/github.com\/avajs\/ava\/blob\/main\/docs\/recipes\/typescript.md\n[dodds-jest]: https:\/\/kentcdodds.com\/blog\/why-i-never-skipped-jest-tests\n","GenTime":"2024-08-19 15:13:50","text":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n\n\n##Decision\nJest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"Prediction":"## Decision\n\nWe will use a combination of **CSS Modules with a CSS-in-JS solution (specifically, Styled JSX)** for styling our web applications. This hybrid approach leverages the strengths of both methods to address our specific needs.\n\n## Decision Outcome\n\n**CSS Modules** will be used for:\n\n* **Styling reusable components:** This ensures encapsulation and prevents style collisions.\n* **Performance through code splitting:** CSS Modules are bundled by build tools efficiently.\n\n**Styled JSX** will be used for:\n\n* **Page-specific styles and overrides:** Its direct integration within components offers flexibility.\n* **Accessing design system constants:** Styled JSX allows seamless integration with JavaScript, enabling shared constants.\n* **Dynamic styling based on props:**  This provides flexibility for responsive design and component variations.\n\n**Type Safety:** We will utilize TypeScript and PropTypes (depending on the framework) to ensure type safety for both CSS Modules and Styled JSX.\n\n**Responsive Props Pattern:** We will implement a system where atomic layout components accept responsive props, enabling developers to define styles for different screen sizes with ease.\n\n\n## Pros and Cons\n\n**Pros:**\n\n* **Performance:** CSS Modules promote efficient code splitting, while Styled JSX's minimal runtime overhead ensures fast rendering.\n* **Maintainability:** CSS Modules provide style encapsulation, and Styled JSX keeps styles close to their components.\n* **Flexibility:** Styled JSX allows for dynamic styling and theming based on props and global state.\n* **Design System Integration:** Shared constants between JavaScript and CSS through Styled JSX streamline design consistency.\n* **Type Safety:** Both approaches can be statically analyzed with TypeScript or PropTypes.\n\n**Cons:**\n\n* **Hybrid Approach Complexity:** Developers need to understand both CSS Modules and Styled JSX.\n* **Potential for Style Duplication:** Without careful planning, similar styles might be recreated in different components.\n\n## Implementation Notes\n\n* Define clear guidelines for using CSS Modules and Styled JSX.\n* Establish a shared library for design system constants accessible by both CSS and JavaScript.\n* Implement a robust testing strategy to ensure style correctness and prevent regressions.\n\nThis approach offers a good balance between performance, maintainability, and flexibility for our web applications while addressing the specific needs outlined in the context and decision drivers. \n","GenTime":"2024-08-19 15:14:00","text":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n\n##Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"Prediction":"## Decision\n\nWe will use a combination of the following tools and frameworks to ensure our code is well-tested:\n\n**1. Unit Testing Framework:**\n\n* **Choice:** [Choose one:  pytest (Python), JUnit (Java), Mocha\/Jasmine (JavaScript), etc.]\n* **Rationale:**  [Framework Name] is a widely used and well-supported testing framework for [Programming Language] that offers a robust set of features like test discovery, assertion libraries, and reporting tools. \n\n**2. Mocking Library:**\n\n* **Choice:** [Choose one: Mockito (Java), pytest-mock (Python), Sinon (JavaScript), etc.]\n* **Rationale:** [Library Name] will allow us to isolate units of code and test them independently by mocking dependencies and external interactions.\n\n**3. Code Coverage Tool:**\n\n* **Choice:** [Choose one:  SonarQube, JaCoCo (Java), Coverage.py (Python), Istanbul (JavaScript), etc.]\n* **Rationale:** [Tool Name] will be integrated into our CI\/CD pipeline to track code coverage metrics and identify areas of the codebase requiring more testing. \n\n**4. Test Runner (if applicable):**\n\n* **Choice:**  [If using a test runner, specify which one:  Karma (JavaScript), etc.]\n* **Rationale:**  [Tool Name] will help us to easily run tests, generate reports, and integrate with other tools in our development workflow.\n\n**5. (Optional) Mutation Testing Tool:**\n\n* **Choice:**  [If using, choose one:  PITest (Java), Mutmut (Python), Stryker (JavaScript), etc.]\n* **Rationale:**  [Tool Name] will further improve the quality of our tests by introducing small mutations into our code to ensure our tests are effective at catching regressions.\n\n**Factors Influencing This Decision:**\n\n* **Programming Language:** The chosen tools and frameworks are compatible with our primary programming language, [Your Programming Language].\n* **Team Familiarity:**  We have considered the team's existing knowledge and experience with testing tools and frameworks to minimize the learning curve.\n* **Integration with Existing Tools:**  The selected tools can be seamlessly integrated with our existing development environment, CI\/CD pipeline, and code repository. \n\n**Consequences of This Decision:**\n\n* **Positive:**\n    * Improved code quality and fewer bugs.\n    * Increased confidence in deployments.\n    * Facilitates better code design and maintainability.\n* **Negative:**\n    * Initial time investment in setting up the testing infrastructure and writing tests. \n    * Potential slowdown in development speed due to the need to write and maintain tests (though this is outweighed by the long-term benefits). \n\nThis ADR will be revisited and updated as our project and testing needs evolve. \n","GenTime":"2024-08-19 15:14:12","text":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n\n\n##Decision\nWe will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"Prediction":"## **Decision**\n\nWe will use **SNS (Simple Notification Service) + SQS (Simple Queue Service)** as our messaging infrastructure for event-driven communication between microservices. \n\n## **Reasons**\n\n* **Durability and fault tolerance:** While SNS offers a pub\/sub model, it lacks built-in message persistence. Integrating SQS ensures that messages are durably stored and delivered even if the receiving microservice is temporarily unavailable. \n* **Decoupling and asynchronous processing:** The combination allows for true decoupling of microservices. Producers can publish events to SNS without knowledge of consumers. Consumers can then retrieve and process messages from their respective SQS queues at their own pace.\n* **Flexibility:** The SNS+SQS approach offers more flexibility in message handling. We can leverage SQS features like message delay and dead-letter queues for more robust error handling and retry mechanisms.\n* **Cost-effectiveness:** While using both services might appear more expensive at first glance, SQS's polling-based model can be more cost-effective in scenarios with high message volumes compared to SNS's push-based delivery.\n\n## **Consequences**\n\n* **Increased architectural complexity:**  Managing both SNS topics and SQS queues adds a layer of complexity compared to using SNS alone.\n* **Development overhead:** Implementing the integration between SNS and SQS will require additional development effort.\n\n**Mitigation:**\n\n* We will utilize infrastructure-as-code tools like AWS CDK or Terraform to simplify the deployment and management of SNS topics and SQS queues.\n*  We will leverage existing libraries and frameworks for interacting with SNS and SQS to streamline development.\n\nThis decision provides a good balance between simplicity, reliability, and scalability, aligning well with the requirements of our microservices architecture. \n","GenTime":"2024-08-19 15:14:20","text":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n\n\n##Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"Prediction":"## Decision\n\nWe will support both **implicit and explicit loading** of the shared library to cater to different user preferences and use cases.\n\n**Implicit loading:**\n\n* A **global default version** of the library will be configurable. This promotes ease of use by removing the need for explicit imports for most common cases.\n* Users can **override the default version** within a pipeline using the `@Library` annotation with a specific tag, branch, or commit SHA. This provides flexibility for cases requiring specific library versions.\n\n**Explicit Loading:**\n\n* While a global default can be defined, we **will not support omitting the version** in the `@Library` annotation for explicit loading. This ensures clarity and avoids ambiguity regarding the used library version within the pipeline.\n\n**Rationale:**\n\n* Offering both implicit and explicit loading mechanisms caters to a wider range of users and use cases, accommodating both simplicity and explicit control.\n* Enforcing explicit version specification for explicit loading promotes transparency and reduces potential confusion or unintended side effects from implicitly inheriting a global default. \n* While a global default version simplifies common use cases, it should be configurable to allow for updates and project-specific defaults.\n\nThis approach balances ease of use with explicit control, transparency, and flexibility in managing shared library dependencies. \n","GenTime":"2024-08-19 15:14:26","text":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n\n\n##Decision\nWe will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"Prediction":"## Decision\n\n**Option 1: Introduce a Configuration Object**\n\nWe will create a dedicated configuration object to encapsulate the parameters passed to the presenters' initializer. This object will hold all the necessary dependencies and configuration values. \n\n**Option 2: Employ a Dependency Injection Framework**\n\nWe will integrate a dependency injection framework (e.g., Spring, Guice, Dagger) to manage the instantiation and dependency injection of our presenters. \n\n**Decision:**\n\nWe will proceed with **Option 2: Employ a Dependency Injection Framework**.\n\n**Reason:**\n\nWhile introducing a configuration object might seem simpler initially, using a dependency injection framework offers several advantages in the long run:\n\n- **Improved code maintainability and testability:** Decoupling the creation and wiring of dependencies makes the codebase easier to understand, maintain, and test.\n- **Increased flexibility and extensibility:** The framework allows for easy configuration and swapping of dependencies, improving the system's adaptability to future changes.\n- **Standardized dependency management:** Leveraging a well-established framework introduces a standardized approach to dependency management across the project.\n\n**Consequences:**\n\n- **Learning curve:** Adopting a new framework requires an initial investment in understanding its concepts and usage patterns.\n- **Potential performance overhead:** While usually negligible, dependency injection frameworks might introduce a slight performance overhead compared to manual dependency injection.\n\n**Note:**\n\nThe specific dependency injection framework to be used will be determined based on the project's technology stack and team's familiarity with different frameworks. \n","GenTime":"2024-08-19 15:14:33","text":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n\n\n##Decision\nEach module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"Prediction":"## Decision\n\nWe will not implement a `classNames` prop on FDS components. To achieve styling overrides, we will utilize CSS overrides targeted at specific component instances. \n\nThis decision preserves the integrity and intended functionality of FDS components while still allowing for styling flexibility.\n","GenTime":"2024-08-19 15:14:35","text":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n\n##Decision\nChosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"Prediction":"## Decision\n\nWe will adopt a facade pattern for our logging API. This will provide a simple and consistent interface for application developers, abstracting away the complexities of different logging backends, filters, formatters, and forwarding mechanisms.\n\n**Rationale:**\n\n* **Consistency:** The facade provides a unified interface for logging, regardless of the underlying implementation.\n* **Flexibility:**  Different logging backends, formats, filters, and forwarding mechanisms can be implemented and swapped transparently behind the facade.\n* **Simplicity:** Application developers only need to interact with a simple and consistent API, reducing the learning curve and potential for errors.\n\n**Implementation Details:**\n\n1. **Define a common interface:** This interface will expose a set of methods for logging messages at different severity levels (e.g., debug, info, warning, error).\n2. **Implement a facade class:** This class will implement the common interface and delegate the actual logging operations to the configured backend.\n3. **Configure logging backends:** The application will be configurable to use different logging backends (e.g., file, database, remote server).\n4. **Implement filters:** Filters can be applied to the logging pipeline to control which log messages are processed and forwarded.\n5. **Implement formatters:** Formatters will be used to control the format of log messages, including timestamps, severity levels, and other relevant information.\n6. **Implement forwarding mechanisms:** Log messages can be forwarded to different destinations (e.g., log aggregation service, email alerts) based on configurable rules.\n\nThis approach allows us to meet the requirement of a consistent logging API while providing the flexibility needed for various logging needs across different applications and environments. \n","GenTime":"2024-08-19 15:14:42","text":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n\n\n##Decision\nDe-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"Prediction":"## Decision\n\nWe will implement an optimistic update strategy in the UI combined with a projection update status check. \n\n**Optimistic Update:**  Immediately after the user initiates a change, the UI will optimistically update its local state to reflect the expected outcome. This provides immediate feedback to the user, making the application feel more responsive.\n\n**Projection Update Status Check:**\n\n1.  **Unique Operation Identifiers:** Each create\/update operation on a resource will generate a unique identifier (UUID). This identifier will be returned to the UI as part of the operation response.\n2.  **Status Endpoint:**  A dedicated endpoint (`\/projections\/status\/{uuid}`) will be exposed to allow the UI to query the status of a specific update operation. \n3.  **Polling Mechanism:** The UI will poll this endpoint using the operation UUID, providing visual feedback to the user while waiting.\n4.  **Status Resolution:**\n    *   **Pending:** The update is still being processed.\n    *   **Completed:** The projections are up to date. The UI can safely refresh its data.\n    *   **Failed:**  An error occurred during processing. Error details can be provided to the user with potential actions (e.g., retry).\n\n**Error Handling:**  If the projection update fails, the UI will:\n\n1.  Revert to the last known consistent state.\n2.  Clearly communicate the error to the user.\n3.  Provide options for retrying the operation or seeking support.\n\n**Alternatives Considered:**\n\n*   **Blocking UI:** Block the UI until projections are updated. This leads to a poor user experience, especially for long-running updates.\n*   **WebSockets:** Real-time updates via WebSockets were considered but introduce complexity in handling different client states and potential scalability concerns.\n\n**Consequences:**\n\n*   **Improved User Experience:** Users receive immediate feedback and a clear understanding of the system's state.\n*   **Increased Complexity:** Requires implementing status tracking and polling mechanisms.\n*   **Potential for Data Inconsistencies:**  A short window exists where the UI displays optimistically updated data that could be inconsistent with other parts of the system. This is mitigated by the status check and error handling. \n","GenTime":"2024-08-19 15:14:51","text":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n\n\n##Decision\nWe will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"Prediction":"## Decision\n\nWe will implement a mechanism to allow setting individual configuration keys without requiring the user to copy the entire structure of the config file. This can be achieved through one or more of the following approaches:\n\n1. **Command-line Arguments:** Introduce command-line arguments specifically for overriding individual configuration keys. For example:\n   ```bash\n   myapp --config.database.hostname=mydatabase.example.com\n   ```\n\n2. **Environment Variables:** Allow overriding configuration keys using environment variables. A naming convention should be established for mapping environment variables to configuration keys. For example:\n   ```\n   export MYAPP_DATABASE_HOSTNAME=mydatabase.example.com\n   ```\n\n3. **Configuration File Patching:**  Allow users to provide a separate \"patch\" file (e.g., YAML, JSON) that only contains the specific keys they wish to override. The application would then merge this patch file with the main configuration file.\n\n4. **Configuration API:** For applications with a programmatic configuration interface, provide APIs for directly setting or getting individual configuration values.\n\nThe specific approach(es) chosen will depend on the complexity of the configuration structure, the application's deployment environment, and user preferences. \n","GenTime":"2024-08-19 15:14:57","text":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n\n\n##Decision\nSupport holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"Prediction":"## Decision\n\nWe will use the following guidelines for choosing `on_delete` behaviour for foreign keys:\n\n* **`restrict` (Default):** This will be our default choice unless there's a clear reason to use another option. This ensures strong referential integrity and prevents accidental data inconsistencies.\n\n* **`cascade`:** Use this option when a parent-child relationship exists, and deleting the parent should automatically delete the child records. This applies to situations where the child record's existence is entirely dependent on the parent record.  Document this decision clearly, as it has the potential to delete significant amounts of data.\n\n* **`nullify`:** Use this option when the foreign key represents an optional relationship. This means the child record can exist independently of the parent record, even if the parent record is deleted.  Consider adding application-level checks or validations to handle the potential for `NULL` values gracefully.\n\nWe will **not** use the following `on_delete` behaviour:\n\n* **`restrict` for optional relationships:** This could lead to situations where legitimate actions are blocked due to the foreign key constraint.\n\n* **`cascade` without careful consideration:**  Cascading deletions can have unintended consequences and lead to data loss if not implemented thoughtfully. \n\nThese guidelines will be documented in our development standards and applied consistently across the codebase. \n","GenTime":"2024-08-19 15:15:04","text":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n\n\n##Decision\nCurrently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"Prediction":"## Decision\n\nWe will continue to emit unresolved external file references inline when the referenced elements are not present in the base unit's references. \n\n## Reasons\n\n* **Consistency:** This approach maintains consistency with how AMF handles these references during parsing. Inlining the content in both parsing and emitting phases avoids confusion and maintains a unified internal representation.\n* **Simplicity:**  Emitting unresolved references as links would introduce complexity in the unresolved model structure and require additional logic to handle cases where the referenced elements are not explicitly linked. \n* **Self-contained Model:** Inlining the content ensures the unresolved model remains self-contained, facilitating easier analysis and debugging without requiring access to the original external files.\n\n## Consequences\n\n* **Increased Model Size:** Inlining content from external files can lead to larger unresolved model sizes, especially if the referenced files are substantial.\n* **Potential Redundancy:**  If the same external file is referenced multiple times, its content might be duplicated within the unresolved model.\n\n## Status\n\nAccepted\n\n## Notes\n\nWe acknowledge the potential drawbacks of increased model size and redundancy.  However, we prioritize consistency and simplicity in handling unresolved external file references. Future optimizations could explore mechanisms to mitigate these drawbacks, such as deduplication strategies or lazy-loading of inlined content, while maintaining the core decision. \n","GenTime":"2024-08-19 15:15:10","text":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n\n\n##Decision\nIn order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"Prediction":"## Decision\n\nPACE will adopt a plugin architecture where possible, with a clear separation of concerns between core components (Horace\/Herbert) and extension modules (Euphonic, Brille, SpinW, etc.). \n\nTo achieve language interoperability, we will utilize a combination of the following approaches:\n\n1. **Foreign Function Interfaces (FFI):**  Where feasible, we will leverage FFIs to enable direct calls between languages. For instance, Python's `ctypes` or `cffi` can interact with C libraries, and similar mechanisms exist for other languages.\n\n2. **Standardized Data Exchange Formats:**  We will define and utilize standardized data formats (e.g., JSON, HDF5) for exchanging information between components written in different languages. This ensures a consistent and platform-independent way to share data.\n\n3. **Message Passing\/RPC:** For scenarios requiring more complex interactions, we will explore message passing systems or Remote Procedure Call (RPC) frameworks. This allows components to communicate asynchronously and potentially across network boundaries.\n\nThe choice of a specific interoperability approach will depend on the specific requirements of the interaction between components, considering factors like performance, complexity, and development effort.\n\nThis plugin architecture with clear interfaces and well-defined data exchange mechanisms will bring several benefits:\n\n- **Modularity and Extensibility:** New features and analysis tools can be easily added as plugins without modifying the core PACE framework.\n- **Flexibility and Language Agnosticism:** Developers can choose the most suitable language for each plugin, promoting innovation and leveraging existing codebases.\n- **Simplified Maintenance and Testing:**  A clear separation of concerns allows for easier maintenance, updates, and independent testing of individual components.\n\nThis ADR will serve as a guide for future development, emphasizing a well-defined plugin architecture and strategic use of interoperability techniques to ensure seamless integration of various components within the PACE ecosystem.\n","GenTime":"2024-08-19 15:15:18","text":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n\n\n##Decision\nThe guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"Prediction":"## Decision\n\nWe will implement a server-side content analysis step in addition to checking file headers to determine if an uploaded file is a text file. This will involve using a library to analyze the content of the file and identify potential malicious content or file type mismatches. \n\nWhile this approach adds complexity and potential latency, it provides a more robust solution for detecting malicious uploads compared to relying solely on client-provided file headers. \n","GenTime":"2024-08-19 15:15:21","text":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n\n\n##Decision\nInstead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"Prediction":"## Decision\n\nWe will utilize a Java-based technology stack for the development of the DOS (Digital Object Service) component. This decision is based on the following factors:\n\n* **Meeting Non-Functional Requirements:** Java's robust nature, mature garbage collection, and built-in security features align well with our need for a scalable, performant, reliable, and secure system.\n* **Leveraging Existing Ecosystem:** Java boasts a vast and proven ecosystem of libraries and frameworks. This allows us to leverage existing solutions for common functionalities, reducing development time and potential for errors.\n* **Spring Framework for RESTful APIs:** The Spring framework provides excellent support for building RESTful web services, which are crucial for exposing DOS functionality and facilitating integration with other platform components.\n* **Backward Compatibility:** Java's commitment to backward compatibility minimizes the risk of compatibility issues with future versions, ensuring the longevity of our platform.\n\nThis decision will guide the selection of specific technologies for the web server, application container, and database within the Java ecosystem. We will prioritize technologies known for their performance, scalability, and strong community support. \n","GenTime":"2024-08-19 15:15:26","text":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n\n\n##Decision\nThe core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"Prediction":"## Decision\n\nWe will decouple authentication from the `panoptes-javascript-client` and create a new, standalone authentication library. This new library will offer the following benefits:\n\n* **Flexible configuration:**  It will support various configuration methods, including configuration files, environment variables, and programmatic configuration within the consuming application. This approach adheres to the Twelve-Factor App methodology for configuration management.\n* **Client-side compatibility:** The library will be designed for use in both server-side and client-side JavaScript environments, making it suitable for Next.js applications and other modern frontend architectures.\n* **Improved testability:** Separating authentication logic into a dedicated library will improve testability by allowing for independent unit testing of authentication-related functions. \n\nThis decoupling will allow the `panoptes-javascript-client` to focus on its core functionality, while providing a more robust and flexible solution for handling authentication in Zooniverse applications, particularly within the Next.js ecosystem. \n","GenTime":"2024-08-19 15:15:30","text":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n\n\n##Decision\nRewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"Prediction":"## Decision\n\n**Option 1: Auth0 (Recommended)** \n\nWe will utilize Auth0 as the identity and access management solution for the BFI's IIIF Universal Viewer auditing platform. Auth0 is a cloud-based identity platform that offers robust features, ease of integration, and a generous free tier.\n\n**Option 2:  Custom Built Solution**\n\nDevelop a bespoke identity and access management system tailored specifically to the BFI's IIIF Universal Viewer platform. \n\n**Option 3:  Open-Source Solution (Keycloak, etc.)**\n\nDeploy and configure an open-source IAM solution like Keycloak, offering flexibility and control over the platform.\n\n**Decision Justification:**\n\nWe recommend Auth0 (Option 1) as it best addresses the decision drivers:\n\n* **Ease of Deployment & Configuration:** Auth0 excels in this area. Its cloud-based nature and well-documented APIs simplify integration with the auditing platform.\n* **Hosting & Maintenance Costs:** Auth0's free tier accommodates the project's initial scale, minimizing costs. Scaling options are available as usage grows.\n* **Core Feature Availability:** Auth0 provides all necessary features, including user management, authentication (various methods), and role-based authorization, at no extra cost. \n\n**Rationale:**\n\nWhile a custom solution (Option 2) offers the highest level of customization, it comes with significant development, deployment, and ongoing maintenance overheads. Open-source solutions (Option 3) provide flexibility but require more technical expertise for configuration and management.\n\nAuth0 strikes a balance between ease of use, comprehensive features, and cost-effectiveness, making it the optimal choice for the BFI's IIIF Universal Viewer auditing platform.\n","GenTime":"2024-08-19 15:15:38","text":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n\n\n##Decision\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"Prediction":"## Decision\n\nWe will implement a centralized data warehouse to aggregate data from various GOV.UK publishing workflow applications. This data warehouse will:\n\n* **Integrate data:** Consolidate current and historical data from disparate sources related to the GOV.UK publishing workflow.\n* **Support analytical reporting:**  Structure the data in a way that enables efficient querying and analysis for generating reports.\n* **Facilitate performance indicator calculation:** Enable the creation and monitoring of key performance indicators (KPIs) related to the publishing workflow.\n* **Decouple from transactional systems:** Operate independently from transactional applications to avoid impacting their performance. \n\nThis approach addresses the current limitations by:\n\n* **Centralizing data access:** Providing a single source of truth for publishing workflow data.\n* **Enabling comprehensive analysis:** Allowing for cross-application reporting and insights.\n* **Improving data consistency:**  Reducing data discrepancies and inconsistencies arising from siloed information.\n\nFurther architectural decisions regarding specific technologies, data modeling, ETL processes, and integration methods will be documented separately. \n","GenTime":"2024-08-19 15:15:43","text":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n\n\n##Decision\nBuild a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"Prediction":"## Decision\n\nWe will adopt a phased approach for decoupling the repositories, prioritizing stability and minimizing disruption to development and release cycles. This approach will involve:\n\n**Phase 1: Decoupling without breaking changes (Current)**\n\n* **Focus:** Complete the decoupling of the `website` and `js-sdk-toolchain` repositories while maintaining backward compatibility.\n* **Strategy:** \n    * Utilize Git submodules or subtrees to manage dependencies between repositories during the transition. This allows continued development on decoupled code while still being referenceable from the main repository.\n    * Implement rigorous integration testing to ensure that changes in one repository do not negatively impact others.\n    * Maintain existing release processes for all repositories.\n* **Goal:** Minimize disruption to development and release cycles while completing the repository decoupling.\n\n**Phase 2: Independent Release Cycles**\n\n* **Focus:** Enable independent release cycles for each decoupled repository.\n* **Strategy:**\n    * Transition from submodules\/subtrees to package management systems (npm, yarn) for managing dependencies.\n    * Define clear versioning strategies for each repository.\n    * Implement automated build and release pipelines for each repository.\n    * Update documentation to reflect the new repository structure and release processes.\n* **Goal:** Increase development velocity and allow for more frequent releases of individual components.\n\n**Phase 3: Continuous Integration and Delivery**\n\n* **Focus:** Implement continuous integration and continuous delivery (CI\/CD) pipelines for all repositories.\n* **Strategy:**\n    * Integrate automated testing, building, and deployment into the development workflow.\n    * Leverage monitoring and alerting tools to identify and address issues quickly.\n* **Goal:** Further optimize the release process, improve code quality, and enable faster delivery of features and bug fixes.\n\nThis phased approach allows us to reap the benefits of decoupled repositories while mitigating risks and ensuring a smooth transition. Each phase will be carefully planned and executed, with clear communication and collaboration across teams.\n","GenTime":"2024-08-19 15:15:52","text":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n\n\n##Decision\nWe choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"Prediction":"## Decision\n\nWe will use the OCI image format for our container images. \n\n## Reasons\n\n* **Kubernetes Alignment:** Kubernetes shows a clear preference for OCI images and Docker is the default runtime. Using OCI ensures optimal compatibility with our target deployment environment.\n* **Industry Standard:** OCI, based on the Docker v2 image format, is becoming the industry standard. This ensures broader compatibility and access to a larger ecosystem of tools and resources.\n* **Future Proofing:**  With rkt moving to OCI and its growing adoption, choosing OCI future-proofs our decision and avoids potential migration issues later.\n* **Internal Expertise:** Docker enjoys wide adoption and understanding within GDS, facilitating easier adoption and troubleshooting.\n\n## Consequences\n\n* **Potential Vendor Lock-in:** While OCI is becoming the standard, choosing a specific implementation (like Docker) might lead to some level of vendor lock-in. \n* **Alternative Exploration Limited:**  We will not be actively exploring the benefits of ACI or other potential formats.  \n\n**Mitigation:** We will mitigate the vendor lock-in risk by using open-source tools and adhering to OCI specifications as much as possible.  We will also re-evaluate this decision if significant changes occur in the container landscape. \n","GenTime":"2024-08-19 15:15:58","text":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n\n\n##Decision\nWe will build and store OCI images built using Docker.\\n"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"Prediction":"## Decision\n\nWe will adopt a **cloud-native, microservices-based architecture**, leveraging **serverless computing** wherever possible. This approach best addresses our requirements for:\n\n* **Elasticity and Resilience:** Serverless functions scale automatically based on demand, ensuring high availability and fault tolerance. \n* **Pay-as-you-go Pricing:** We only pay for the resources we use, optimizing costs and eliminating upfront investments in idle infrastructure.\n* **Exploiting Economies of Scale:** Cloud providers offer economies of scale, making their services more cost-effective than managing our own infrastructure. \n* **Exploiting Falling Costs of Infrastructure:** We benefit from the continuous innovation and falling costs of cloud infrastructure.\n* **Flexibility and Opportunity Costs:** This approach allows for rapid development and deployment, enabling us to adapt to changing requirements and explore new technologies easily.\n\n**Building Blocks:**\n\n* **Compute:** Primarily leverage **serverless functions** (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) for event-driven workloads. \n* **API Management:** Utilize a dedicated **API gateway** service (e.g., AWS API Gateway, Azure API Management) for security, traffic management, and monitoring. \n* **Data Storage:** Choose from a variety of managed database services (e.g., AWS DynamoDB, Azure Cosmos DB, Google Cloud Spanner) based on specific data needs.\n* **Security:** Implement robust security measures leveraging cloud-native tools for identity and access management (IAM), security information and event management (SIEM), and threat detection.\n\n**Open Source & Build vs. Buy:**\n\nWe will adopt a pragmatic approach, prioritizing managed cloud services for their advantages in scalability, security, and cost-efficiency. However, we will evaluate open-source alternatives where they offer significant benefits in cost, customization, or feature set. We will only consider building components from scratch in exceptional circumstances where no suitable alternative exists and the benefits outweigh the increased development and maintenance overhead. \n\n**Continuous Improvement:**\n\nWe will actively monitor new cloud technologies and industry best practices to continuously improve our architecture and leverage the latest advancements. \n","GenTime":"2024-08-19 15:16:09","text":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n\n\n##Decision\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for capturing important architectural decisions along with their context and consequences. Each significant architectural decision will be documented in a separate Markdown file following a predefined template. \n","GenTime":"2024-08-19 15:16:11","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"Prediction":"## Decision\n\nWe will implement an authorization layer between the OpenFIDO frontend application and the workflow service. This layer will use the user's organization affiliation provided by OpenFIDO to determine resource access permissions.  \n\n**Specifically, this approach involves:**\n\n1. **Organization-based Access Control:**  Define granular access control policies based on user organizations. This could involve assigning roles and permissions to specific organizations within the workflow service.\n2. **API Gateway Integration:** Implement an API gateway that intercepts requests from the frontend application to the workflow service. The gateway will validate the user's JWT token obtained through OpenFIDO and verify their organization against the access control policies.\n3. **Centralized Authorization Service (Optional):** For complex authorization scenarios, consider a centralized authorization service that manages access control policies and provides a unified authorization decision point for multiple services.\n\n**Justification:**\n\n- **Enhanced Security:** By enforcing organization-based access control, we restrict access to the workflow service and GridLabD jobs, ensuring only authorized personnel can execute them.\n- **Simplified Frontend Logic:** The frontend application doesn't need to handle complex authorization logic. It only needs to present the user's organization information obtained from OpenFIDO during API requests. \n- **Flexibility and Scalability:** This architecture allows for flexible access control management and scales well with the introduction of new organizations or changes in access requirements. \n\n**Alternatives Considered:**\n\n- **Role-Based Access Control (RBAC) within OpenFIDO:** This would require managing roles and permissions within OpenFIDO, increasing complexity and potentially creating a single point of failure.\n- **Direct Access from Frontend:** This approach lacks proper authorization control and exposes the workflow service to security risks.\n\n**Consequences:**\n\n- **Development Effort:** Implementing an authorization layer requires additional development effort.\n- **Maintenance Overhead:** Ongoing maintenance will be required to manage access control policies and ensure the authorization layer remains secure and up-to-date.\n\nThis architecture strikes a balance between security, maintainability, and development effort, providing a robust solution for managing access to sensitive resources within the OpenFIDO project. \n","GenTime":"2024-08-19 15:16:21","text":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n\n##Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"Prediction":"## Decision\n\nWe will use a persistent database to store the locations, bookables, and bookings data. This will ensure data durability and availability across multiple instances and deployments. \n","GenTime":"2024-08-19 15:16:23","text":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n\n\n##Decision\n* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"Prediction":"## Decision\n\n**Option 2: Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses.**\n\n## Justification\n\nWhile adding an additional property might seem like a quick fix, it introduces unnecessary complexity and adds more code to maintain.  In the long run, consistent naming conventions will improve code readability and maintainability. \n\nUsing deprecated subclasses ensures backward compatibility with minimal disruption for users. This approach allows for a clean transition to a more consistent API.\n\n## Consequences\n\n**Positive:**\n\n- Improved consistency and readability of the API.\n- Easier to understand and use for developers.\n- Reduced technical debt in the long term.\n\n**Negative:**\n\n- Requires renaming existing classes, which might break existing user code.\n- Introducing deprecated subclasses adds some overhead to the codebase.\n\n**Mitigation:**\n\n- Provide clear documentation on the renaming and deprecated subclasses.\n- Offer automated migration tools or scripts to assist users in updating their code.\n- Clearly communicate the deprecation timeline to users.\n\nThis approach strikes a balance between achieving consistency and minimizing disruption for existing users. \n","GenTime":"2024-08-19 15:16:28","text":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n\n\n##Decision\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"Prediction":"## Decision\n\nWe will adopt `msw` for mocking fetch calls in our unit tests going forward. \n\n## Reasons\n\n* **Alignment with Testing Library philosophy:**  Kent C. Dodds, the creator of Testing Library, advocates for this approach, emphasizing a focus on testing user flows rather than implementation details.\n* **Simplified Test Setup:**  `msw` centralizes mock configuration, reducing boilerplate code in individual test files.  Only overrides for specific test cases are needed.\n* **Improved Maintainability:** Centralized mocking in `msw` promotes consistency and makes it easier to update and manage mocks as the codebase evolves.\n\n## Consequences\n\n* **Migration Effort:** Existing tests using `jest-fetch-mock` will need to be migrated to the `msw` approach.\n* **Learning Curve:**  The team will need to invest time in understanding and implementing `msw`.\n\n## Status\n\nAccepted. \n\n## Action Items\n\n1. **Research and Planning:**  Investigate `msw` thoroughly and determine the best migration strategy for our existing test suite.\n2. **Documentation:** Create internal documentation outlining the usage and best practices for `msw` within our project.\n3. **Gradual Migration:** Start incorporating `msw` into new tests and gradually migrate existing tests from `jest-fetch-mock`. \n\n","GenTime":"2024-08-19 15:16:34","text":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n\n\n##Decision\nThe mocking will be done as much as possible by using the the `msw` package.\\n"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"Prediction":"## Decision\n\nWe will **remove the reading of PLAY_LANG** in the tracking consent Javascript code and **introduce a new data attribute**,  e.g. `data-language`, to determine the language in which to render the banner. \n\nThis decision is based on the following:\n\n* **Future-proof solution:** Using a data attribute offers a consistent approach across all services, including classic services and those not using Play Framework. This avoids reliance on specific cookies and accommodates potential technology changes. \n* **Security best practices:**  While no immediate security concerns exist with PLAY_LANG, promoting the use of HttpOnly cookies where applicable aligns with best practices and avoids future conflicts.\n* **Minimal disruption:**  This solution requires minimal changes to the tracking consent code and avoids teams having to revert HttpOnly settings or add exemptions to security tests.\n* **Consistency:**  Aligns with the direction of other frontend components that utilize data attributes for language settings, promoting consistency across our platforms.\n\nThis approach addresses both the immediate concern of providing a Welsh language option for all services and sets a consistent pattern for future development. \n","GenTime":"2024-08-19 15:16:39","text":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n\n##Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"Prediction":"## Decision\n\n**Alerting logic will be implemented as code and version controlled alongside the applications and infrastructure it monitors.** \n\nThis approach, often referred to as \"Alerting as Code,\" offers several advantages aligned with the decision drivers:\n\n* **Agility and Flexibility:**  Directly modifying code provides the fastest way to silence, un-silence, and re-prioritize alerts. It allows for expressing arbitrarily complex alerting logic, supporting fine-grained control over \"who,\" \"how,\" and \"when\" aspects of notifications, including escalation policies. This flexibility allows tailoring alerts to specific clusters, namespaces, and other contextual factors.\n* **Leveraging Existing Tools and Processes:** Version control systems like Git, already used for managing application code, can manage alerting rules. This integration ensures:\n    * **Traceability and Auditability:** All changes to alerting are tracked, aiding in understanding the evolution of alerting strategies and troubleshooting configuration issues.\n    * **Collaboration and Review:**  Alerting rules can undergo code reviews, fostering knowledge sharing and ensuring quality control.\n    * **Repeatable Deployments:**  Alerting configurations are automatically deployed alongside application and infrastructure changes, reducing configuration drift and ensuring consistency across environments.\n\nThis approach promotes a proactive and iterative approach to alert management, enabling teams to continuously refine their alerting strategies based on evolving needs and feedback. \n","GenTime":"2024-08-19 15:16:45","text":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n\n\n##Decision\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n"}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"Prediction":"## Decision\n\n1. **Assets Folder:** The folder containing assets will be named `assets`.\n2. **ADR Filename Pattern:** ADR filenames will follow the format `XXXX-adr-title-in-kebab-case.md`, where `XXXX` represents a sequentially assigned, four-digit number starting from `0001`. \n3. **ADR Title Format:** ADR titles will be concise and use the imperative mood, for example, \"Use a relational database\" or \"Implement user authentication with OAuth 2.0\".  Capitalize the first letter of each word. \n\nThis decision promotes consistency and clarity in the organization and naming of ADR-related files. \n","GenTime":"2024-08-19 15:16:48","text":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n\n\n##Decision\n### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"Prediction":"## Decision\n\nTo address the performance issues caused by varying blob sizes and access patterns in James's BlobStore API, we will introduce a two-tiered storage system:\n\n**Tier 1: High-Performance Storage (e.g., Cassandra)**\n\n* **Purpose:** Store small, frequently accessed blobs.\n* **Characteristics:**\n    * Low latency reads and writes (sub 10ms)\n    * Suitable for storing data up to a certain size limit (to be determined based on performance testing)\n* **Examples:** Mailbox message headers, frequently accessed metadata\n\n**Tier 2: Object Storage (Existing)**\n\n* **Purpose:** Store large, infrequently accessed blobs.\n* **Characteristics:**\n    * Higher latency for reads and writes (acceptable for infrequent access)\n    * Cost-effective for storing large amounts of data\n* **Examples:** Mailbox message bodies, DeletedMessageVault message headers, large attachments\n\n**Implementation Details:**\n\n* **Blob classification:**  Introduce a mechanism (e.g., configurable size thresholds, metadata tags) to classify blobs as either \"high-performance\" or \"standard\" based on their expected size and access patterns.\n* **API Transparency:** Maintain the existing BlobStore API, abstracting the storage tier selection logic internally. \n* **Data Migration:** Implement a migration strategy for existing data to ensure optimal performance based on the new tiering approach.\n* **Monitoring & Performance Testing:**  Continuously monitor the performance of both tiers and adjust classification rules or storage solutions as needed.\n\n**Trade-offs:**\n\n* **Increased complexity:** Introducing a two-tiered system adds complexity to the storage architecture.\n* **Data consistency:**  Ensure data consistency across tiers, especially during migrations and updates.\n\n**Rationale:**\n\nThis approach allows us to leverage the strengths of both storage solutions while addressing the performance bottlenecks identified in the existing system. By storing frequently accessed data in a low-latency storage solution, we can significantly improve the performance of operations like IMAP FETCH headers and JMAP message listing. \n","GenTime":"2024-08-19 15:16:58","text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"Prediction":"## Decision\n\nWe will adopt the Nginx Ingress controller to secure third-party web applications. This decision is based on the following factors:\n\n* **Kong's limitations:** The open-source version of Kong does not offer the required security features, and contacting them has not yielded any solutions. \n* **Kubernetes-native solution:** The Nginx Ingress controller, available since Kubernetes 1.7, provides a native and integrated approach within our existing Kubernetes infrastructure.\n* **Potential benefits:**  \n    * Leveraging a familiar tool like Nginx potentially reduces the learning curve and integration effort.\n    * Utilizing a Kubernetes-native solution aligns with our existing architecture and may offer better performance and scalability.\n\nThis decision allows us to move forward with securing third-party applications while leveraging existing infrastructure and potentially minimizing development time.\n","GenTime":"2024-08-19 15:17:02","text":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n\n\n##Decision\nWe have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate Markdown file stored in the \/docs\/adr directory of our repository. \n","GenTime":"2024-08-19 15:17:05","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"Prediction":"## Decision\n\nWe will model the ordinal parameter as an enumeration. This enumeration will define named constants for each distinct mode, enhancing code readability and maintainability.\n\nThis approach offers several advantages:\n\n- **Improved Code Clarity:** Using descriptive names instead of magic numbers makes the code self-documenting and easier to understand.\n- **Reduced Error Potential:**  Eliminates the risk of using incorrect ordinal values, as the compiler can enforce the use of defined constants.\n- **Simplified Documentation:** Enumerations provide a clear and structured way to document the different modes and their meanings.\n- **Refactoring Benefits:** Changing the underlying ordinal values in the future becomes easier and less error-prone, as only the enumeration definition needs modification. \n\nWe will implement the enumeration in a central location accessible to all relevant parts of the codebase. Documentation will clearly explain the purpose and usage of each enumerated value. \n","GenTime":"2024-08-19 15:17:10","text":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n\n##Decision\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"Prediction":"## Decision\n\nWe will no longer use jQuery in new GOV.UK Frontend components and will deprecate its use in existing components. \n\nThis decision is based on the following factors:\n\n* **Browser Support:** jQuery's reduced browser support conflicts with GOV.UK Frontend's commitment to accessibility and a wide user base.\n* **Performance:** jQuery's large file size negatively impacts page load times, particularly detrimental for users with slower connections.\n* **Security:** The lack of ongoing security updates for jQuery poses unacceptable risks for our applications and users.\n* **Community Alignment:** The wider web development community is moving away from jQuery in favor of native JavaScript APIs and smaller, more focused libraries. \n\nWe will explore and document alternative approaches for DOM manipulation, event handling, and data interactions using vanilla JavaScript and potentially smaller, more targeted libraries where absolutely necessary. \n","GenTime":"2024-08-19 15:17:15","text":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n\n\n##Decision\nWe\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"Prediction":"## Decision\n\nWe will implement a two-tier architecture for serving `service-stac` where:\n\n1. **Static Assets:** A dedicated web server like Nginx will be responsible for serving all static assets (CSS, images, icons, etc.) directly. This approach provides optimal performance and security for serving static content. \n\n2. **Dynamic Content:** Gunicorn will continue to serve the Django application but will be placed behind Nginx. Nginx will act as a reverse proxy, forwarding incoming requests to Gunicorn. This configuration enhances security by shielding Gunicorn from direct exposure and mitigates slow client and Denial of Service attacks.\n\nThis approach leverages the strengths of both Nginx and Gunicorn:\n\n* **Nginx:** excels in serving static content quickly and efficiently, handling concurrent connections with minimal resource usage.\n* **Gunicorn:** focuses on running the Django application, managing application logic and database interactions.\n\nThis separation of concerns results in a more robust, performant, and secure deployment for `service-stac`. \n","GenTime":"2024-08-19 15:17:20","text":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n\n\n##Decision\nBecause it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"Prediction":"## Decision\n\nWe will utilize a container orchestration platform, specifically Kubernetes, to manage the deployment, scaling, and updating of our application containers. \n","GenTime":"2024-08-19 15:17:22","text":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n\n##Decision\nChosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"Prediction":"## Decision\n\nWe will offer a single set of C++ APIs that do not throw exceptions to indicate errors. Errors will be reported using `Status` or `StatusOr<T>` objects, ensuring consistency for users regardless of their exception-handling capabilities.\n","GenTime":"2024-08-19 15:17:23","text":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n\n##Decision\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"Prediction":"## Decision\n\nWe will use Travis CI for continuous integration and deployment of this application. \n","GenTime":"2024-08-19 15:17:25","text":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n\n##Decision\nUse Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"Prediction":"## Decision\n\nWe will implement a lightweight health check mechanism for the Elasticsearch sidecar container using a dedicated liveness probe. This probe will periodically execute a simple script within the sidecar container. This script will perform the following actions:\n\n1. **Check for the existence and readability of the necessary secrets and environment variables.** This ensures the sidecar has access to the credentials required to communicate with Elasticsearch.\n2. **Attempt a connection to the Elasticsearch instance using the provided credentials.** This verifies network connectivity and authentication. \n3. **Upon successful connection, execute a simple, lightweight Elasticsearch API call (e.g., retrieving cluster health with limited information).**  This confirms the sidecar can successfully communicate with the Elasticsearch API.\n\n**Success Criteria:**\n\n* The liveness probe script exits with exit code 0 if all checks pass, indicating a healthy sidecar.\n* The liveness probe script exits with a non-zero exit code if any check fails, indicating an unhealthy sidecar, which will restart the pod. \n\n**Error Handling:**\n\n* To account for Elasticsearch's initial startup time, the liveness probe will incorporate a configurable initial delay period where failures are ignored. \n* The script will include basic error handling and logging to provide insights into the nature of any failures.\n* Separate alerts will be configured for sidecar and Elasticsearch failures to ensure clear distinction and appropriate responses.\n\n**Rationale:**\n\nThis approach prioritizes simplicity and reliability. By relying on a lightweight script and basic API calls, we minimize the complexity of the health check itself, reducing potential points of failure. \n\n* **Error Distinction:** Achieved by separate alerts for sidecar and Elasticsearch failures.\n* **Error Side Effect:** Limited by ensuring the sidecar health check script is lightweight and does not overload Elasticsearch.\n* **Reliability:** Enhanced by using a simple, robust script and incorporating error handling. \n* **Simplicity:** Maintained through a straightforward script-based approach for the liveness probe.\n\nThis approach allows us to effectively monitor the health of the Elasticsearch sidecar container without introducing unnecessary complexity or significantly impacting the availability of Elasticsearch. \n","GenTime":"2024-08-19 15:17:34","text":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n\n\n##Decision\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"Prediction":"## Decision\n\nWe will implement feature toggles in our Terraform module using a combination of `count` and `ternary` operators, inspired by Capital One's approach ([Building Feature Toggles into Terraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)). \n\nThis approach allows for:\n\n- **Flexible DynamoDB provisioning:**  The module will support deploying without a DynamoDB table by setting a `create_dynamodb_table` variable to `false`.\n- **Optional DynamoDB Autoscaling:** A separate `enable_dynamodb_autoscaling` variable will control whether autoscaling is configured when a DynamoDB table is provisioned. \n\nThis decision prioritizes flexibility and granular control over the module's resources, aligning with its intended use within a larger Data Storage Module. \n","GenTime":"2024-08-19 15:17:38","text":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n\n\n##Decision\nFeature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"Prediction":"## Decision\n\nWe will store the mappings and data transformation logic as Python code within the application codebase. This code will be responsible for:\n\n1. **Defining Elasticsearch mappings:** This includes specifying data types, analyzers, and other mapping parameters for each field in the Elasticsearch index.\n2. **Implementing data transformation functions:** These functions will process the raw JSON data before indexing it into Elasticsearch. This could include:\n    * **Data cleaning and normalization:** Ensuring data consistency and removing duplicates.\n    * **Enriching data:** Adding calculated fields, performing lookups, or integrating with external APIs.\n    * **Geolocation processing:** Converting location data into a format suitable for geolocation searches in Elasticsearch.\n\n**Reasoning:**\n\n* **Flexibility and Complexity:** Python code provides the flexibility to handle complex mapping definitions and data transformations that are difficult to manage through environment variables or configuration files.\n* **Maintainability and Version Control:** Storing the code within the application codebase allows for easier maintenance, version control, and testing alongside the rest of the application logic.\n* **Code Reusability:**  Transformations and mappings can be reused across different parts of the application or for different data sources.\n\n**Alternatives Considered:**\n\n* **Configuration files (YAML, JSON):**  While simpler for basic configurations, they lack the flexibility to express complex transformations or dynamic mapping generation.\n* **Environment variables:** Unsuitable for complex objects and functions, also pose security risks when dealing with sensitive data transformations.\n\n**Consequences:**\n\n* **Increased Code Complexity:** Requires developers to be familiar with both Elasticsearch mappings and Python code.\n* **Potential Performance Overhead:** Executing Python code for each data record might introduce some performance overhead, which needs to be considered during implementation and potentially optimized if necessary. \n","GenTime":"2024-08-19 15:17:46","text":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n\n\n##Decision\nGiven the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"Prediction":"## Decision\n\nWe will avoid the \"flat structure\" for organizing project modules in the `klokwrk-project` and instead adopt a more structured approach called \"strategic structure\". This structure will address the limitations of the flat structure by:\n\n* **Grouping modules into logical layers or components:** This will improve navigability and provide a clearer picture of module relationships.\n* **Introducing a clear dependency direction:**  Modules within a layer\/component should ideally only depend on modules in the same layer or lower layers, reducing coupling and promoting maintainability.\n* **Visually representing abstraction levels:** The structure should convey the abstraction level of each module, with higher-level modules depending on lower-level ones.\n* **Clearly identifying system entry points:** Dedicated layers or components will house entry points for different parts of the system (e.g., API, CLI).\n* **Leveraging directory names and organization to convey information:** Beyond just module names, the directory structure itself will provide valuable insights into the system's architecture.\n\nThis approach aims to create a more scalable, maintainable, and understandable project structure, particularly as the `klokwrk-project` grows in complexity. Further details of the \"strategic structure\" and its implementation will be defined in subsequent ADRs. \n","GenTime":"2024-08-19 15:17:52","text":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"Prediction":"## Decision\n\nWe will deprecate the existing jclouds-based Object Storage implementation in favor of a new, simpler implementation based on the official AWS SDK for Java. This decision is driven by the following factors:\n\n* **Simplification and Alignment with Reactive Style:**  The jclouds library introduces complexity that clashes with our reactive codebase. Using the AWS SDK directly allows for a more straightforward and idiomatic implementation that aligns with our current architectural patterns. \n* **Improved Build Times:**  Removing the jclouds dependency will significantly reduce build times, as it is a large library with many transitive dependencies.\n* **Swift Compatibility through S3 API:**  Leveraging Swift's S3 compatibility layer eliminates the need to maintain separate code paths for S3 and Swift, further simplifying our codebase. \n\nWhile this decision means we will no longer directly support the native Swift API, the benefits of a simpler, faster, and more maintainable codebase outweigh this limitation. \n","GenTime":"2024-08-19 15:17:56","text":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\n\n##Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"Prediction":"## Decision\n\nWe will standardise on a single JavaScript formatting tool and configuration to be used across the entire team. This tool will be integrated into our development workflow to ensure consistent formatting for all JavaScript files. \n\nWe will also enforce this standard automatically, either through a pre-commit hook or as part of our Continuous Integration pipeline. This ensures that only code adhering to the defined standard is merged into the main branch.\n","GenTime":"2024-08-19 15:17:58","text":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n\n##Decision\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"Prediction":"## Decision\n\nWe will develop a data pipeline to extract, transform, and load (ETL) data from Google Sheets to the Hackney data platform. \n\nThis pipeline will:\n\n1. **Extract** data from identified Google Sheets using the Google Sheets API.\n2. **Transform** the extracted data into a consistent format suitable for the data platform. This may involve:\n    - Data cleaning (e.g., handling missing values, standardizing formats)\n    - Data validation (e.g., ensuring data types and ranges are consistent)\n    - Data enrichment (e.g., adding calculated fields, merging with other datasets)\n3. **Load** the transformed data into the data platform. This may involve:\n    - Choosing an appropriate storage location (e.g., database table, cloud storage)\n    - Defining the data schema for the target location\n    - Implementing mechanisms for efficient data loading and updates\n\nWe will prioritize the development of this pipeline to address the most critical datasets first, gradually incorporating others based on their priority and dependency on other datasets. \n","GenTime":"2024-08-19 15:18:03","text":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n\n\n##Decision\nWe will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"Prediction":"## Decision\n\nWe will introduce two new fields to the steps context: `steps.<id>.outcome` and `steps.<id>.conclusion`.\n\n*   **`steps.<id>.outcome`**: This field will store a boolean value indicating the overall success or failure of the step.  A value of `true` signifies success, while `false` indicates failure.\n*   **`steps.<id>.conclusion`**: This field will store a short, human-readable string summarizing the outcome of the step. This could be as simple as \"Success\" or \"Failure,\" or it could provide more context, such as \"File uploaded successfully\" or \"API request failed with error code 404.\"\n\nThis decision enables conditional execution of steps based on the outcome of preceding steps, providing increased flexibility and control over software workflows.\n\n## Reasoning\n\n* **Improved Workflow Control:**  Introducing conditional logic based on step outcomes enables more sophisticated and dynamic workflows.\n* **Clear Success\/Failure Indication:** Separating outcome (`true`\/`false`) from a potentially more detailed conclusion provides a clear signal for conditional logic. \n* **Human Readability:** The `conclusion` field enhances understandability of step execution for debugging and monitoring.\n\n## Consequences\n\n* **Increased Complexity:** Adding new fields introduces complexity to the existing steps context.\n* **Backward Compatibility:**  Existing systems relying solely on `steps.<id>.outputs` will need to be assessed for compatibility and potentially updated. \n* **Standardization:** Clear guidelines and documentation will be required to ensure consistent usage of `outcome` and `conclusion` across the codebase. \n","GenTime":"2024-08-19 15:18:10","text":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n\n\n##Decision\nFor steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"Prediction":"## Decision\n\nWe will use a platform-agnostic analytics abstraction layer to send data to Ophan. This will involve:\n\n1. **Defining a shared schema:** A common schema for events and screen views will be defined, encompassing the required data points for both platforms (iOS and Android).\n2. **Implementing a client-side abstraction layer:** This layer will provide a unified interface for both platforms to track events and screen views. It will handle:\n    - Mapping platform-specific event names and data to the shared schema.\n    - Batching events for efficient network usage.\n    - Persisting events locally for offline scenarios.\n3. **Integrating the Ophan SDK:** Each platform will include the Ophan SDK, and the abstraction layer will communicate with the respective SDK to send the analytics data.\n\n## Rationale\n\nThis approach provides several benefits:\n\n- **Platform independence:** By abstracting away platform-specific details, we maintain consistent tracking logic and data format across iOS and Android. This simplifies analysis and ensures data integrity.\n- **Flexibility and maintainability:** Changes to the Ophan schema or SDK can be managed within the abstraction layer, reducing code duplication and easing maintenance.\n- **Testability:** The abstraction layer can be easily mocked for unit testing, ensuring the accuracy of tracked data.\n\n## Consequences\n\n- **Initial development effort:** Implementing the abstraction layer requires an upfront investment.\n- **Potential performance overhead:** The abstraction layer introduces a slight performance overhead compared to directly using the Ophan SDK. However, this is mitigated by efficient batching and caching mechanisms.\n\n## Status\n\nAccepted\n\n## Open Questions\n\n- What specific data points need to be included in the shared schema for events and screen views?\n- How will user privacy be addressed, especially regarding data collection and storage?\n- What error handling and retry mechanisms will be implemented in the abstraction layer? \n","GenTime":"2024-08-19 15:18:18","text":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n\n\n##Decision\n```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"Prediction":"## Decision\n\nWe will maintain an internal, fully duplicated keyset within the library. This addresses the issues of unexpected key returns and update complexities with `kdbGet()` and `kdbSet()`.\n\nHere's how this decision will be implemented:\n\n1. **Internal Keyset:**  A complete, deep copy of the key database will be maintained internally. This copy will be updated upon each successful `kdbSet()`.\n\n2. **`kdbGet()` Behavior:**\n    *  `kdbGet()` will operate solely on this internal keyset. \n    *  Only the keys specifically requested by the provided keyset will be duplicated and returned to the user. This ensures users receive only what they requested, even if the internal keyset contains additional keys.\n    *  If the requested keys are already up-to-date in the provided keyset, `kdbGet()` will still return those keys (as duplicates from the internal set) to provide consistent behavior.\n\n3. **`kdbSet()` Behavior:**\n    * `kdbSet()` will continue to perform deep duplication of the provided keyset. This ensures data integrity within the library.\n    * After a successful `kdbSet()`, the internal keyset will be updated accordingly.\n\nThis approach offers several advantages:\n\n* **Predictability:** Users can rely on `kdbGet()` to return only the requested keys, regardless of the internal keyset's content.\n* **Efficiency:**  Deep duplication is limited to `kdbSet()` operations, optimizing performance for `kdbGet()`.\n* **Data Integrity:** The internal keyset ensures data consistency and avoids potential modification issues with shared keysets.\n\nHowever, this decision introduces the following trade-off:\n\n* **Memory Usage:** Maintaining a duplicate keyset will increase memory consumption. The impact of this should be carefully considered, especially for applications handling large key databases. \n","GenTime":"2024-08-19 15:18:27","text":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n\n\n##Decision\nNot yet decided.\\n- [Global Validation](global_validation.md)\\n"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"Prediction":"## Decision\n\nTo prevent future build failures due to git version mismatches, we will enforce the required git version within our CircleCI environment. This will be achieved by:\n\n1. **Specifying the required git version in the CircleCI configuration file (`config.yml`)**: We will utilize a Docker image that comes pre-installed with the specific git version needed. Alternatively, we can explicitly install the desired git version within the build process defined in the `config.yml` file. \n2. **Documenting the chosen git version**: The chosen git version and the rationale behind its selection will be documented in our project's README or a dedicated documentation page. This ensures all team members are aware of the required version and can set up their local environments accordingly.\n\nThis approach guarantees consistency between the development and CI environments, preventing unexpected build failures due to git version discrepancies. \n","GenTime":"2024-08-19 15:18:31","text":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n\n\n##Decision\nUse `GitProcess.exec` even in tests that need to lean on the git process.\\n"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file in a dedicated \"adr\" directory at the root level of the project repository.  We will follow the MADR template [https:\/\/adr.github.io\/madr\/](https:\/\/adr.github.io\/madr\/) for structuring the ADRs.\n","GenTime":"2024-08-19 15:18:34","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"Prediction":"## Decision\n\nWe will use Auth0's **Classic** views for user authentication. \n\n**Reasons:**\n\n* **Speed of implementation:** Using Auth0's pre-built views allows us to deliver authentication functionality quickly.\n* **Migration path:** Auth0's use of OAuth2 provides flexibility for future migration to a different authentication provider.\n* **Usability:** While not ideal, the \"Classic\" views offer a better user experience compared to the \"New\" views which have known performance issues.\n\n**Consequences:**\n\n* **Technical debt:** Using the \"Classic\" views introduces potential technical debt due to their reliance on JavaScript and lack of progressive enhancement. This may impact accessibility and maintainability in the long run.\n* **User experience compromise:** While the chosen option offers a better experience than the \"New\" views, it is not ideal. We will need to investigate mitigation strategies and prioritize migrating away from the \"Classic\" views in the future.\n\n**Action Items:**\n\n* **Implement Auth0 authentication using the \"Classic\" views.**\n* **Investigate mitigation strategies for the \"Classic\" view limitations (e.g., accessibility improvements).**\n* **Develop a plan for migrating away from the \"Classic\" views in the future, potentially coinciding with a move away from Auth0 if required.**\n","GenTime":"2024-08-19 15:18:39","text":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n\n##Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"Prediction":"## Decision\n\nGiven the current lack of a definitive single sign-on provider within Hackney and the time constraint of two weeks, we will be implementing **Auth0 as a temporary solution** for authentication and authorization in the Report a Defect application.\n\nThis decision is based on the following factors:\n\n* **Urgency:** The immediate need to provide agent names within the service and enhance security beyond basic authentication necessitates a quick solution.\n* **Flexibility:**  Auth0 offers a readily available and configurable solution that can be implemented within the remaining timeframe. \n* **Future Proofing:** While not a long-term solution, Auth0 provides a robust authentication platform that can be easily integrated with a future Hackney-approved single sign-on provider.\n\nThis approach allows us to:\n\n1. **Meet immediate user needs:** by providing agent identification and a more secure login process.\n2. **Maintain alignment with best practices:** by leveraging a dedicated authentication service rather than relying on basic authentication.\n3. **Minimize future rework:** by selecting a solution that can be integrated with a future single sign-on provider, reducing potential disruption and development effort.\n\nWe will re-evaluate this decision and plan for integration with Hackney's chosen single sign-on provider before the end of 2019. \n","GenTime":"2024-08-19 15:18:45","text":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n\n##Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"Prediction":"## Decision\n\nWe will not use the CLR type name as the event type in our event store. Instead, we will adopt a more robust and future-proof approach:\n\n1. **Define a dedicated type identifier (string) for each event type.** This identifier will be decoupled from the CLR type name and namespace, providing flexibility for future code refactoring and migration.\n2. **Use a consistent naming convention for event type identifiers.** This will improve readability and maintainability. A suitable convention could be `EventName` or `DomainName.EventName`.\n3. **Store the event type identifier as metadata alongside the event data.** This will allow consumers to identify the event type regardless of potential changes in the CLR type.\n4. **Consider using a dedicated NuGet package or shared library to manage event type definitions.** This will centralize event type management and promote consistency across different parts of the system.\n\n## Consequences\n\n* **Increased complexity:** This approach introduces the need for managing event type identifiers separately from CLR types.\n* **Potential for identifier collisions:** Using a centralized management strategy (e.g., NuGet package) will help mitigate this risk.\n* **Improved maintainability:** Decoupling event types from CLR types enhances flexibility and facilitates code evolution without breaking event processing.\n* **Enhanced interoperability:** Using string-based identifiers makes it easier to integrate with systems or components written in different languages.\n\nThis approach prioritizes long-term maintainability and flexibility over the simplicity of using CLR type names. We believe that the benefits of this decision outweigh the added complexity, particularly in a system where event sourcing plays a central role.\n","GenTime":"2024-08-19 15:18:53","text":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n\n\n##Decision\nUse a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"Prediction":"## Decision\n\nWe will use Jest as our test framework. \n","GenTime":"2024-08-19 15:18:54","text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"Prediction":"## Decision \n\n**End-to-end encryption will be achieved using a layered approach:**\n\n1. **User to ELB:** Utilize TLS encryption with a certificate issued by a trusted Certificate Authority. The ELB will be configured with the certificate, and access to the private key will be strictly controlled. \n2. **ELB to Router:** Implement TLS encryption between the ELB and the router VM. The router VM will be configured to serve TLS connections.\n3. **Router to Cells:** Due to Cloud Foundry limitations in supporting TLS between the router and application instances, IPsec will be employed on both the router and cell levels. This will ensure encrypted traffic between these components transparently, even if the application instances themselves do not support TLS. \n","GenTime":"2024-08-19 15:18:57","text":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n\n\n##Decision\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"Prediction":"## Decision \n\nWe will use **SSH key pair authentication** for accessing servers in the cluster.\n\n## Reasons\n\n* **Security:** SSH keys provide stronger security compared to passwords. They are significantly harder to brute-force and eliminate the risk of password-related vulnerabilities like weak passwords or credential stuffing.\n* **Automation:** SSH keys simplify automated deployments and management tasks. Scripts and tools can be configured to use keys, avoiding the need for manual password input.\n* **Granular Control:** SSH keys allow for fine-grained access control. Different keys can be issued for different users and actions, limiting privileges and improving security.\n\n## Consequences\n\n* **Key Management:** Implementing SSH key authentication requires a robust key management strategy. This includes secure key generation, distribution, storage, and revocation processes.\n* **Initial Setup:** Setting up SSH key authentication involves generating keys, distributing public keys to servers, and configuring SSH clients. While not overly complex, this represents a slightly higher initial effort compared to password authentication.\n\n## Status\n\nAccepted\n\n## Notes\n\n* We will implement a centralized SSH key management solution to streamline key lifecycle management.\n* Clear documentation will be provided to users on generating, using, and managing their SSH keys. \n","GenTime":"2024-08-19 15:19:03","text":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n\n\n##Decision\nWe have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"Prediction":"## Decision\n\n**We will adopt a devcontainer-based workflow for local development of the Teaching Vacancies app.** This decision is driven by the desire to simplify the onboarding process, improve developer experience, and facilitate future adoption of cloud-based development environments. \n\nThis approach offers numerous benefits:\n\n* **Simplified Setup:** Provides a single, consistent setup process for all contributors, regardless of their technical expertise.\n* **Reproducibility:** Ensures a consistent development environment across different machines and operating systems, minimizing \"works on my machine\" issues.\n* **Reduced Onboarding Friction:**  New developers can become productive quickly without needing to install and configure numerous dependencies.\n* **Improved Developer Experience:** Frees developers from tedious setup tasks, allowing them to focus on building and improving the application.\n* **Future-Proofing:** Enables potential future use of cloud-based development environments, further enhancing accessibility and collaboration.\n\nThis decision aligns with industry best practices and leverages the increasing popularity of devcontainers within the development community. \n","GenTime":"2024-08-19 15:19:08","text":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n\n\n##Decision\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n"}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"Prediction":"## Decision\n\n**Introduce shared ownership semantics for KeySets and Keys using shared pointers.**\n\nThis decision addresses both identified problems:\n\n1. **Locking Issue:** Shared pointers inherently manage reference counting and will release locks automatically when the last reference is gone. This eliminates the issue of locks not being reset.\n\n2. **API Consistency:** Employing shared pointers for both KeySets and Keys aligns their ownership semantics and provides a consistent API experience. This approach eliminates the unexpected differences between the C++ API for KeySet and Key. \n","GenTime":"2024-08-19 15:19:10","text":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n\n\n##Decision\n- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"Prediction":"## Decision\n\nWe will adopt a two-pronged approach for network monitoring and testing within the Modernisation Platform:\n\n**1. Continuous Monitoring with [Monitoring Tool]:**\n\n* **Tool:** We will leverage [Monitoring Tool] (e.g., Datadog, Prometheus, AWS CloudWatch) for real-time visibility into network health and performance. \n* **Features:** We will configure [Monitoring Tool] to monitor critical metrics such as latency, bandwidth usage, connection drops, firewall activity, and resource utilization.  \n* **Alerts:** We will define alert thresholds and notifications to promptly address performance bottlenecks, security anomalies, or deviations from desired state configurations.\n\n**2. Automated Infrastructure Testing with [Testing Framework]:**\n\n* **Framework:** We will employ [Testing Framework] (e.g.,  AWS Cloud Development Kit (CDK), Terraform, Ansible) in conjunction with a testing library like [Testing Library] (e.g.,  pytest, Serverspec, Inspec) for automated testing of network infrastructure.\n* **Tests:**  Tests will be written to validate:\n    * **Security Group Configurations:** Ensure adherence to security policies by verifying allowed ports, protocols, and IP ranges. \n    * **Network Segmentation:** Confirm isolation between different environments (development, testing, production).\n    * **Routing and Connectivity:** Validate proper routing rules and connectivity between various components.\n    * **Firewall Rules:** Ensure accuracy and effectiveness of firewall configurations.\n    * **Compliance:**  Incorporate compliance checks (e.g., PCI DSS, HIPAA) into automated tests.\n\n**Justification:**\n\nThis combined approach of continuous monitoring and automated infrastructure testing provides a comprehensive solution:\n\n* **Proactive Issue Identification:** Continuous monitoring allows us to detect and address performance issues and potential security threats in real-time.\n* **Infrastructure-as-Code Validation:** Automated tests ensure that our infrastructure is always provisioned and configured as intended.\n* **Improved Security Posture:**  By continuously monitoring and testing security-related configurations, we can maintain a strong security posture and reduce the risk of vulnerabilities.\n* **Simplified Compliance Audits:** Automated compliance checks simplify the process of auditing and demonstrating adherence to regulatory requirements. \n\n**Alternatives Considered:**\n\n* **Manual Testing:** While feasible for small-scale deployments, manual testing is not scalable or reliable in the long term for a platform undergoing modernization.\n* **Single-Purpose Network Monitoring Tools:**  These tools may lack the extensibility and integration capabilities of a more comprehensive solution. \n\n**Consequences:**\n\n* **Increased Operational Overhead:** Implementing and maintaining monitoring and testing infrastructure requires dedicated resources.\n* **Potential for False Positives:**  Careful configuration of alerts and test cases is crucial to minimize alert fatigue and unnecessary investigation. \n","GenTime":"2024-08-19 15:19:22","text":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n\n\n##Decision\n[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"Prediction":"## Decision\n\nWe will host the web application on a Platform as a Service (PaaS) offering. \n\n## Considered Options\n\n* **Platform as a Service (PaaS):** Cloud providers manage the underlying infrastructure (servers, networking, operating systems), while we manage the application and its configuration.\n* **Infrastructure as a Service (IaaS):**  We manage the entire infrastructure, including servers, networking, operating systems, and the application.\n* **On-premise:** We host the application on servers we own and manage.\n\n## Decision Outcome\n\nPaaS aligns best with the decision drivers:\n\n* **Easy to update:** PaaS providers typically offer tools and automation for easy application deployment and updates.\n* **High availability:** PaaS providers offer built-in redundancy and scaling features.\n* **Easy to configure HTTPS:** PaaS platforms often streamline the process of obtaining and configuring SSL\/TLS certificates.\n* **Observability:** PaaS solutions commonly integrate logging, monitoring, and alerting tools. \n\n## Pros and Cons of the Chosen Option\n\n**Pros:**\n* Reduced operational overhead.\n* Faster time to market.\n* Scalability and reliability.\n* Cost-effective compared to managing our own infrastructure.\n\n**Cons:**\n* Vendor lock-in.\n* Less control over the underlying infrastructure.\n* Potential for higher costs at scale compared to well-optimized IaaS solutions.\n\n##  Next Steps\n\n* Evaluate different PaaS providers (e.g., AWS Elastic Beanstalk, Google App Engine, Azure App Service) based on specific needs and pricing.\n* Determine the resources needed for the application and select an appropriate pricing plan.\n* Configure the chosen PaaS environment, including deployment pipeline, HTTPS, and monitoring. \n","GenTime":"2024-08-19 15:19:30","text":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\n\n##Decision\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"Prediction":"## Decision\n\nWe will **require kwargs to be passed in every time the `start` function is called**, rather than only once during package addition.  \n\nThis decision addresses the limitations of relying solely on kwargs provided during package addition. While less convenient, this approach offers greater flexibility and avoids potential issues stemming from state management and argument immutability.\n","GenTime":"2024-08-19 15:19:32","text":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n\n\n##Decision\nAdd kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"Prediction":"## Decision\n\nWe will rename the PagesViewer component to MultiFrameViewer to better reflect its broader purpose beyond just displaying pages. This change aligns with our roadmap of supporting diverse project types and features in the future.\n\nThe initial iteration of MultiFrameViewer will prioritize features critical for transcription projects, as outlined in Issue #1142.  We acknowledge the need for future enhancements to support other project types, such as ecology projects (as discussed in Decisions 6 and 7).  These additions will be prioritized and implemented in subsequent iterations.\n","GenTime":"2024-08-19 15:19:35","text":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n\n\n##Decision\n1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"Prediction":"## Decision\n\nTo mitigate the risk of breaking changes during database upgrades, and to provide a smoother upgrade path for users, we will implement a versioning strategy for GOV.UK Docker database volumes. This will involve:\n\n1. **Versioned Volume Names:**  Instead of using generic names like `mongodb_data`, volumes will be named with the database engine and version, e.g., `mongodb_data_3.6`. This clearly indicates the data format within the volume.\n\n2. **Upgrade Documentation:**  Comprehensive upgrade instructions will be included in the GOV.UK Docker documentation. These instructions will detail:\n    * The new volume naming convention.\n    * Steps for migrating data from older versions to newer versions, if applicable.\n    * Clear communication about potential data incompatibility issues.\n\n3. **Automated Migration (where feasible):**  Where possible, we will investigate and implement automated data migration scripts that can be run by users to upgrade their data from older versions to newer versions. This will simplify the upgrade process for users and minimize the risk of data loss or corruption.\n\n4. **Deprecation Policy:** A clear deprecation policy for older database versions will be established and communicated to users. This policy will outline the timeline for support and any recommended actions for users still on older versions. \n","GenTime":"2024-08-19 15:19:41","text":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n\n\n##Decision\nWe decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"Prediction":"## Decision\n\nWe will implement a centralized Task Registry service. This service will be responsible for:\n\n* **Registration:** Each Task Manager, upon starting a new Task, will register the Task with the Task Registry, providing relevant metadata (e.g., Task ID, Task type, start time, assigned node).\n* **Heartbeat Mechanism:** Task Managers will periodically send heartbeats to the Task Registry, confirming the status of their running Tasks. \n* **Listing:** The `list` functionality will query the Task Registry, which will return information on all registered and active Tasks across the cluster.\n\n## Consequences\n\n* **Centralized Point of Failure:** The Task Registry becomes a critical component. Failure of the Task Registry will impact the ability to list and potentially manage Tasks.\n* **Increased Complexity:** Introducing a new service increases the overall system complexity. We need to handle service discovery, communication protocols, and fault tolerance for the Task Registry.\n* **Scalability Considerations:**  The Task Registry should be designed to handle a growing number of Task Managers and Tasks without impacting performance.\n\n## Alternatives Considered\n\n* **Decentralized Gossip Protocol:** Task Managers could share Task information amongst themselves using a gossip protocol. This eliminates the single point of failure but adds complexity to data consistency and querying.\n* **Polling Task Managers:** The `list` function could directly poll each Task Manager for its running tasks. This approach is simple but can be inefficient and resource-intensive with a large number of Task Managers. \n\n## Status\n\nAccepted\n\n## Notes\n\nWe will investigate and implement appropriate redundancy and failover mechanisms for the Task Registry to minimize the impact of potential failures. Performance testing will be conducted to ensure the chosen solution scales efficiently with the anticipated cluster size and workload. \n","GenTime":"2024-08-19 15:19:49","text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n\n\n##Decision\n* Read a Cassandra projection to get all `Task`s and their `Status`\\n"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"Prediction":"## Decision\n\nWe will implement a generic NFT module within the Cosmos SDK, drawing inspiration from the existing work done by the IRISnet team (`irismod\/nft`) and the previous implementation in the Cosmos repository (`modules\/incubator\/nft`). \n\nThis decision is based on the following:\n\n* **Flexibility and Composability:** A generic module provides the foundation for NFT functionality without being limited to specific use cases. This allows for greater flexibility and composability, enabling developers to build application-specific functionality on top of the base NFT module, either within Cosmos Hub or other zones. \n* **Interchain Compatibility:**  A generic NFT module, coupled with IBC and Gravity Bridge integration, facilitates interoperability and enables seamless transfer of NFTs across the Cosmos ecosystem.\n* **Avoid Feature Bloat:**  Attempting to implement every possible NFT use case within a single module would lead to unnecessary complexity and bloat. A generic approach allows the SDK to remain lean and focused. \n\nThis generic NFT module will handle core functionalities such as:\n\n* **NFT Minting and Issuance**\n* **NFT Ownership Representation and Transfers**\n* **Querying NFT Metadata**\n\nFurther application-specific features and logic will be delegated to separate modules that can utilize and extend the base NFT module's capabilities.\n","GenTime":"2024-08-19 15:19:54","text":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n\n\n##Decision\nWe create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"Prediction":"## Decision\n\nWe will use a finite state machine implemented as an enumerated type in code to manage the lifecycle of a submission within the Data Submission Service. \n\nThis approach offers several benefits:\n\n* **Clarity and Maintainability:** Explicitly defining states and transitions enhances code readability and maintainability. \n* **Validation and Control:** The state machine enforces valid transitions between states, preventing invalid submission lifecycle progressions.\n* **Extensibility:** New states can be easily added to the enumerated type as requirements evolve, accommodating future approval processes and error correction mechanisms.\n\nWe will document the state machine diagram and state transition rules alongside the code for easy reference.\n","GenTime":"2024-08-19 15:19:58","text":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n\n\n##Decision\nThe system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"Prediction":"## Decision\n\n**Adopt an open source licensing model for the software.** \n\nThis decision aligns with the principles outlined in the context and will:\n\n* **Promote transparency:**  By making the source code accessible, we ensure everyone understands how the software works and can contribute to its improvement.\n* **Foster collaboration:**  Open sourcing allows a wider community to participate in development, leading to faster innovation and problem-solving.\n* **Encourage early and frequent releases:**  Open source development encourages iterative progress and community feedback, enabling us to identify and address issues quickly.\n* **Cultivate an inclusive meritocracy:**  Contributions are judged on merit, fostering a welcoming environment for diverse perspectives and ideas. \n* **Build a strong community:**  Open sourcing naturally attracts individuals passionate about the software's purpose, creating a vibrant community for support and collaboration. \n","GenTime":"2024-08-19 15:20:02","text":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n\n\n##Decision\nWe make our code available as open source.\\n"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"Prediction":"## Decision\n\nWe will consolidate the two staging environments into a single staging environment deployed to the `tanf-staging` Space. We will also create a new, persistent,  `tanf-development` environment in the `tanf-dev` Space. \n\nThis addresses the following:\n\n* **Simplified Deployment:** Aligns the deployment strategy with the simplified Git workflow by eliminating the need for two staging environments. \n* **Dedicated Testing Environment:** Provides the vendor team with a persistent `tanf-development` environment for concurrent feature development and testing, alleviating the \"crowding\" issue.\n* **Release-Specific Environments:**  Allows for the creation of multiple, ephemeral environments in `tanf-staging` to support release-specific testing as outlined in ADR 018.\n\nThis decision promotes efficiency, clarity, and better management of different stages of the software development lifecycle. \n","GenTime":"2024-08-19 15:20:07","text":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n\n\n##Decision\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"Prediction":"## Decision\n\nWe will leverage Amazon Elastic Kubernetes Service (EKS) to deploy and manage both the Jenkins master and agent nodes. \n\n**Rationale:**\n\n* **Scalability and Flexibility:** EKS provides a scalable and highly available Kubernetes cluster, allowing us to easily scale the number of Jenkins agents based on workload demands.\n* **Simplified Management:**  Using EKS eliminates the need to manage the underlying infrastructure for the master and agents. Kubernetes handles provisioning, scaling, and health checks, reducing operational overhead.\n* **Dynamic Agent Provisioning:** Kubernetes, combined with the Jenkins Kubernetes plugin, allows for dynamic provisioning of agents based on job requirements. This ensures optimal resource utilization and reduces costs by spinning up agents only when needed.\n* **Configuration Management:** We can leverage Kubernetes configuration files to define and manage the deployment of the Jenkins master, agents, and their dependencies in a consistent and reproducible manner.\n* **Integration with AWS Ecosystem:** EKS integrates seamlessly with other AWS services, such as IAM for authentication and authorization, Elastic Load Balancing for distributing traffic, and CloudWatch for monitoring and logging.\n\n**Consequences:**\n\n* **Learning Curve:**  Teams will need to acquire knowledge of Kubernetes concepts and configuration to effectively manage the Jenkins deployment on EKS.\n* **Cost Considerations:** While EKS offers cost savings through dynamic scaling and managed infrastructure, there are costs associated with running the EKS cluster itself. Careful capacity planning is necessary to optimize costs. \n","GenTime":"2024-08-19 15:20:13","text":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n\n\n##Decision\nWe will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"Prediction":"## Decision\n\nBased on the results of the JMH benchmark comparing the performance of `java.lang.reflect.Array.getLength(array)` against directly accessing the `length` property of an array:\n\n* **If the performance difference is negligible**: We will utilize  `java.lang.reflect.Array.getLength(array)` to determine array lengths within the array constraint. This approach promotes code reusability and avoids code duplication, aligning with Decision Driver 2. \n* **If the performance difference is significant**: We will prioritize performance (Decision Driver 1) and implement separate constraint logic for each array type, directly accessing the `length` property. This will involve a trade-off with Decision Driver 2, potentially increasing code complexity.\n\nThis decision will be revisited if future requirements demand support for a large number of array types, potentially shifting the balance between performance and code maintainability. \n","GenTime":"2024-08-19 15:20:17","text":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n\n\n##Decision\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"Prediction":"## Decision\n\nThe application will be designed with a mobile-first, offline-first architecture. \n\n* **Data will be primarily stored locally on the device using a robust and efficient local database (e.g., SQLite).** \n* **All application features will be designed to function offline first, allowing users to enter and access data even without network connectivity.**\n* **A comprehensive sync mechanism will be implemented to handle data synchronization between the device and the central server.** \n    * This mechanism will prioritize reliability and data integrity, ensuring data is seamlessly merged even with intermittent connectivity.\n    * It will handle complex synchronization scenarios, allowing for data changes across multiple devices and facilities within a short timeframe.\n* **Conflict resolution mechanisms will be implemented to manage potential data conflicts arising from offline edits and concurrent updates.**  This will ensure data accuracy and consistency across all devices and the central server.\n* **The user interface will clearly indicate online\/offline status and provide feedback during the synchronization process.**  This will keep users informed about the data status and any potential delays in synchronization.\n\nThis approach ensures the application remains functional and valuable in areas with poor connectivity while enabling efficient data sharing and collaboration across devices and facilities. \n","GenTime":"2024-08-19 15:20:23","text":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n\n\n##Decision\n### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"Prediction":"## Decision\n\nWe will use a cloud-based object storage service with integrated image processing and indexing capabilities, such as Amazon S3 with AWS Lambda for image processing and Amazon Elasticsearch Service for search. \n\n## Reasons\n\n* **Scalability and Reliability:** Cloud-based solutions offer high scalability and reliability out of the box, allowing the application to handle a growing number of uploads and users without concerns about infrastructure management.\n* **Cost-effectiveness:**  Leveraging cloud services allows us to adopt a pay-as-you-go model, only paying for the storage and processing power we consume. \n* **Simplified Development:**  Utilizing integrated image processing services like AWS Lambda eliminates the need to build and maintain our own image processing pipeline, simplifying development and reducing complexity.\n* **Efficient Search:** Integrating a search service like Amazon Elasticsearch Service allows for quick and efficient searching of images by title and tags. \n* **Future-proofing:** This architecture provides flexibility for future enhancements, such as implementing more advanced search features, integrating with a content delivery network (CDN) for faster image delivery, or implementing machine learning-based image analysis.\n\n## Considered Alternatives\n\n* **On-premise storage solution with a dedicated image processing library:** This offers more control over the system but requires significant upfront investment in hardware and software, as well as ongoing maintenance efforts. It may not be cost-effective for our current stage.\n* **Cloud-based object storage with self-managed image processing and search:** This offers more control over the technology stack but increases the complexity of development and deployment. \n\n## Consequences\n\n* **Vendor lock-in:** Choosing a specific cloud provider leads to some level of vendor lock-in. However, the benefits of scalability, cost-effectiveness, and reduced development overhead outweigh this concern.\n* **Security considerations:**  Storing user data in the cloud requires implementing robust security measures to protect user privacy and comply with data protection regulations. \n\nThis architecture will allow us to efficiently store and manage user-uploaded images, providing a scalable and cost-effective solution for our application. We will need to carefully consider security implications and implement appropriate measures to ensure data protection. \n","GenTime":"2024-08-19 15:20:32","text":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n\n\n##Decision\nAfter a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"Prediction":"## Decision\n\nWe will implement a **Strategy Pattern** to encapsulate the different sorting methods and their corresponding comparison logic. \n\nThis pattern will allow us to:\n\n* **Define a family of sorting algorithms (e.g., by size, by date, by species) and encapsulate each one into a separate strategy class.**  \n* **Make the sorting algorithms interchangeable within the Whale Observation class.** This avoids tying the observation class to a specific sorting algorithm and allows for easy addition of new sorting methods in the future. \n* **Improve code readability and maintainability** by separating the sorting logic from the Whale Observation class itself. \n\nThis approach promotes flexibility and extensibility, essential for accommodating evolving research needs and potentially new sorting criteria in the future. \n","GenTime":"2024-08-19 15:20:36","text":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n\n\n##Decision\nImplement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"Prediction":"## Decision\n\n**(Replace this with a concise description of the decision that was made.  Be specific about what you decided to do.)**\n","GenTime":"2024-08-19 15:20:37","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate Markdown file stored in the `\/docs\/adr` directory of our repository, following a standardized template. \n","GenTime":"2024-08-19 15:20:39","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight method to document significant architectural decisions within the project. Each ADR will be a separate file within a dedicated \"adr\" directory at the root level of the project repository. \n\nThe following template will be used for each ADR:\n\n```markdown\n# [ADR Number]: [Short, descriptive title]\n\n## Status: [Proposed | Accepted | Rejected | Deprecated | Superseded]\n\n## Context\n\n[Describe the forces at play, including technological, political, social, and project local factors. These forces should create tension or a dilemma that needs to be resolved with a decision.]\n\n## Decision\n\n[Describe the decision that was made. This should be a clear and concise statement of the chosen option.]\n\n## Consequences\n\n[Describe the resulting context, after applying the decision. This should include the benefits and drawbacks, as well as any other consequences.]\n\n## Links\n\n[Optional - Link to related ADRs or external documentation]\n```\n\nWe will follow these guidelines:\n\n* **Atomic Decisions:** Each ADR should document a single, specific architectural decision. \n* **Concise and Focused:** ADRs should be concise and focus on the decision itself, avoiding unnecessary details.\n* **Clear Rationale:** The rationale behind the decision should be clearly explained in the \"Context\" and \"Consequences\" sections.\n* **Version Control:** ADRs will be version-controlled alongside the codebase, allowing us to track their evolution over time. \n\nThis approach will ensure that:\n\n* Architectural decisions are documented consistently and transparently.\n* The rationale behind decisions is preserved for future reference.\n* The impact of decisions on the system architecture is understood.\n* Decisions can be easily reviewed, discussed, and revisited as needed.\n","GenTime":"2024-08-19 15:20:46","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"Prediction":"## Decision\n\nWe will adopt Stylelint for linting our CSS and SASS code. This decision is driven by the following factors:\n\n* **Code Consistency:** Stylelint helps enforce a consistent coding style across our codebase, improving readability and maintainability.\n* **Early Error Detection:** Stylelint can detect potential errors and style violations during development, reducing the likelihood of bugs in production.\n* **SASS Support:** Stylelint offers dedicated support for SASS syntax, ensuring compatibility with our chosen CSS preprocessor.\n* **Configurability:** Stylelint is highly configurable, allowing us to tailor the linting rules to our specific project requirements and coding conventions.\n* **Popularity and Community:** As a popular linting tool, Stylelint benefits from a large and active community, providing ample resources, plugins, and support.\n \nWe will integrate Stylelint into our development workflow, ideally as part of our automated build process and\/or pre-commit hooks, to ensure consistent enforcement. \n","GenTime":"2024-08-19 15:20:51","text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n\n##Decision\nWe will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n"}
{"File Name":"log4brains\/20210113-distribute-log4brains-as-a-global-npm-package.md","Context":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n","Decision":"- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","tokens":224,"id":1065,"Prediction":"## Decision\n\nWe will refactor the Log4brains installation procedure to be language-agnostic and simplify its usage for non-JS projects. This will involve the following changes:\n\n1. **Decoupling the CLI from the Web Interface:** The `@log4brains\/cli` package will be separated from `@log4brains\/web`. This allows users to install and utilize the CLI functionalities independently, regardless of their project's programming language. \n\n2. **Simplified Installation:** The `npx init-log4brains` command will be deprecated in favor of a simpler, more general command like `npm install -g @log4brains\/cli` to install the CLI globally.  \n\n3. **Language-Agnostic Initialization:** The CLI will be enhanced to initialize Log4brains in any project directory, regardless of the technology stack. It will prompt the user for necessary information (like preferred storage format, knowledge base location, etc.) to create a basic configuration file.\n\n4. **Optional Web Interface Installation:** Users can then optionally install the `@log4brains\/web` package if they desire the web interface for visualization and interaction with their knowledge base. The CLI will provide clear instructions on how to do so.\n\n5. **Improved Documentation:**  The documentation will be updated to reflect these changes and provide clear, step-by-step installation instructions for both JS and non-JS projects.  \n\nThis approach addresses the identified decision drivers:\n\n- **Simplicity:** A single command installs the CLI, and the initialization process is straightforward.\n- **Language Agnostic:**  The CLI functions independently of any specific programming language.\n- **Universal Initialization:** The CLI can initialize Log4brains in any project directory. \n- **Faster \"Getting Started\":**  Users can quickly install the CLI and begin using Log4brains core features without additional setup. \n","GenTime":"2024-08-19 15:20:59","text":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n\n\n##Decision\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n"}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"Prediction":"## Decision\n\n**Split `markmap-lib` into two packages: `@markmap\/parser` and `@markmap\/renderer`.**\n\n**Rationale:**\n\n- This addresses the issue of redundant dependencies by separating the transformation and rendering functionalities into distinct packages.\n- Users can then choose to install only the necessary package based on their use case (transforming or rendering).\n- This promotes a smaller footprint for projects utilizing only one of the functionalities. \n\n**Implementation:**\n\n1. **Create `@markmap\/parser`:**\n    - This package will handle the transformation of markdown content into the internal markmap data structure.\n    - It will include dependencies like `remarkable` and `katex`, but exclude `d3` and rendering-specific libraries.\n2. **Create `@markmap\/renderer`:**\n    - This package will handle the rendering of the markmap data structure using `d3`. \n    - It will depend on `@markmap\/parser` for the data structure definition.\n3. **Update `coc-markmap` and `markmap-cli`:**\n    - Modify these packages to utilize the new `@markmap\/parser` and `@markmap\/renderer` packages as needed.\n4. **Documentation:**\n    - Update the documentation to reflect the changes in package structure and guide users on installing the appropriate packages based on their needs.\n\n**Addressing Extraneous Bundles:**\n\n- During the creation of `@markmap\/parser` and `@markmap\/renderer`, prioritize building a single, minimized bundle for each package. \n- Explore using bundlers that effectively eliminate unused code and optimize for minimal size.\n\nThis approach tackles both the redundancy and bundle size concerns, leading to a leaner and more efficient `markmap` ecosystem. \n","GenTime":"2024-08-19 15:21:07","text":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n\n\n##Decision\n- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n"}
{"File Name":"james\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2145,"Prediction":"## Decision\n\nTo address the performance issues caused by varying blob sizes and access patterns in James's BlobStore API, we will introduce a two-tiered storage system:\n\n**Tier 1: High-Performance Storage (e.g., Cassandra)**\n\n* Used for storing small, frequently accessed blobs like mailbox message headers.\n* Leverages Cassandra's low latency characteristics for optimal read performance.\n* A size threshold will be defined to determine which blobs qualify for this tier.\n\n**Tier 2: Object Storage (Existing Solution)**\n\n* Remains the default storage for larger blobs and infrequently accessed data like message bodies and DeletedMessageVault headers.\n* Suitable for cost-effective storage of large data volumes. \n\n**Implementation Details:**\n\n* The BlobStore API will remain unchanged, abstracting the underlying storage tiers.\n* A metadata layer will be introduced to track the storage location (Tier 1 or Tier 2) for each blob.\n* When a blob is stored, its size and expected access pattern will be evaluated to determine the appropriate storage tier.\n* Read operations will query the metadata layer to identify the blob's location and retrieve it from the corresponding storage tier.\n\n**Benefits:**\n\n* Improved performance for operations involving small, frequently accessed blobs.\n* Maintains cost-effectiveness for storing large data volumes.\n* Provides a flexible solution that can adapt to evolving data access patterns.\n\n**Tradeoffs:**\n\n* Increased complexity due to the introduction of a two-tiered storage system.\n* Requires defining clear criteria for determining storage tiers based on blob size and access patterns.\n* Potential data duplication if the same blob exists in both tiers.\n\n**Monitoring:**\n\n* We will monitor the performance of both storage tiers and the metadata layer to ensure optimal performance and identify potential bottlenecks.\n* Metrics will include read\/write latency, storage utilization, and cache hit rate (if applicable).\n\nThis decision aims to leverage the strengths of both Cassandra and Object Storage, providing a performant and scalable solution for James's BlobStore API while optimizing cost efficiency. \n","GenTime":"2024-08-19 15:21:16","text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n"}
{"File Name":"terraform\/Automation.md","Context":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"Prediction":"## Decision\n\nWe will adopt a **phased approach** to addressing increased maintenance needs for migrating and recently migrated services. This approach balances the need for rapid migration velocity with the long-term benefits of automation and reduced maintenance overhead. \n\n**Phase 1: Tactical Fixes (First 3 months):**\n\n* Prioritize rapid migration completion.\n* Address maintenance issues on a case-by-case basis.\n* Document common pain points and identify potential automation opportunities.\n\n**Phase 2: Targeted Automation (Next 3-6 months):**\n\n* Based on the documentation from Phase 1, invest in automating the most impactful maintenance tasks.\n* Prioritize automation that yields significant time savings or reduces the risk of errors. \n* Continuously evaluate the effectiveness of automation and iterate as needed.\n\n**Phase 3: Continuous Improvement (Ongoing):**\n\n* Establish a culture of continuous improvement for service maintenance.\n* Regularly review and update automation scripts and processes.\n*  Investigate and adopt new tools and technologies to further streamline maintenance tasks.\n\n**Justification:**\n\n* **Balances speed and sustainability:**  Allows for rapid migration progress initially while establishing a foundation for long-term maintainability.\n* **Data-driven decision making:**  Uses empirical data collected during the initial phases to guide automation efforts.\n* **Flexibility and adaptability:**  Enables adjustments to the approach based on evolving needs and priorities.\n\n\nThis phased approach ensures we maintain momentum in our migration efforts while proactively addressing the need for efficient and sustainable service maintenance. \n","GenTime":"2024-08-19 15:21:23","text":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n\n\n##Decision\nWe will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n"}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"Prediction":"## Decision\n\nWe will organize the platform's codebase into multiple projects, with each project representing a coherent set of functionalities within a single domain. Each project will exist in its own repository with appropriate access control. This approach aligns with the principle of separation of concerns and promotes a modular, loosely coupled design.\n\nThis decision is driven by the following factors:\n\n* **Security:** Separating code into projects allows us to enforce stricter access control. Developers and teams will only have access to the projects relevant to their domain and responsibilities. \n* **Modularity:** This structure promotes the development of self-contained, reusable components with well-defined interfaces. \n* **Maintainability:** Smaller, focused codebases are easier to understand, maintain, and evolve independently. \n* **Scalability:** This approach allows teams to work independently and scale their work more efficiently without impacting other parts of the platform.\n\nWhile a domain can consist of multiple projects, each project should maintain a focused scope within that domain. The relationship and dependencies between projects within and across domains will be managed through well-defined APIs and interfaces.\n\nThis decision will be further detailed in subsequent ADRs, focusing on:\n\n* Defining the criteria for creating new projects.\n* Establishing a naming convention for projects.\n* Defining the process for managing dependencies between projects.\n* Detailing the access control strategy at the project level.\n","GenTime":"2024-08-19 15:21:29","text":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n\n\n##Decision\nThe set of functions implemented in one GCP project belongs to a single domain.\\n"}
{"File Name":"libelektra\/boolean.md","Context":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n","Decision":"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","tokens":18,"id":1293,"Prediction":"## Decision\n\nAdopt a consistent approach to using booleans across Elektra, focusing on clarity, readability, and alignment with C conventions. \n\nSpecifically:\n\n* **Use integers (`int`) for boolean values:** This aligns with C's historical lack of a built-in boolean type and avoids potential issues with different representations of `bool` across platforms.\n* **Define clear constants for true and false values:** Instead of using literal `1` and `0`, define constants like `ELEKTRA_TRUE` and `ELEKTRA_FALSE` within a central header file (e.g., `elektra.h`). This improves code readability and consistency.\n* **Avoid using booleans as error indicators:**  Utilize Elektra's existing error handling mechanism (e.g., returning error codes or setting error flags) instead of relying solely on boolean return values. This allows for more informative error reporting and handling.\n* **Refactor existing code to comply with these guidelines:**  Gradually update existing parts of Elektra's codebase to adhere to these decisions, ensuring backward compatibility where necessary.\n\nThis approach prioritizes consistency and clarity over relying on newer language features that might not be universally supported.\n","GenTime":"2024-08-19 15:21:35","text":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n\n\n##Decision\nOnly the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n"}
{"File Name":"ftd-scratch3-offline\/0004-use-multiple-projects.md","Context":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n","Decision":"We will try to modularize the software and will use multiple projects to achieve this goal.\\n","tokens":67,"id":2626,"Prediction":"## Decision\n\nWe will **not** develop the software in one big monolithic Gradle project. \n\nWhile acknowledging the benefits of simplified integration during the initial development phase, we prioritize **long-term code quality, reusability, and maintainability**.  \n\nTherefore, we will adopt a **modular architecture** with well-defined APIs between modules. Each module will represent a distinct functional unit and be developed as a separate Gradle project. This approach yields the following advantages:\n\n* **Improved code organization and maintainability:** Enforces separation of concerns and clearer boundaries, making the codebase easier to understand, navigate, and maintain.\n* **Increased reusability:** Individual modules can be easily reused in other projects or contexts, extending the impact and value of the developed components.\n* **Facilitates team scalability:** Different teams can work independently on specific modules, fostering parallel development and reducing potential conflicts.\n\nWhile this decision might introduce some initial overhead in setting up the modular structure, we believe the long-term benefits outweigh the short-term costs. We will strive to minimize integration complexities by utilizing robust dependency management and automation tools within our chosen build system. \n","GenTime":"2024-08-19 15:21:40","text":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n\n##Decision\nWe will try to modularize the software and will use multiple projects to achieve this goal.\\n"}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"Prediction":"## Decision\n\n**Adopt Test-Driven Development (TDD) and Continuous Integration\/Continuous Delivery (CI\/CD) practices.**\n\n**Reasoning:**\n\n* **TDD** enforces writing tests before code, leading to:\n    * **Improved Code Quality:**  Focus on functionality and testability from the outset reduces defects.\n    * **Ease of Change:**  Tests act as a safety net, ensuring changes don't break existing functionality.\n    * **Simplified Error Correction:** Tests help pinpoint the source of errors quickly.\n\n* **CI\/CD** automates the build, testing, and deployment pipeline, resulting in:\n    * **Early Issue Detection:** Frequent integration and testing uncover conflicts and errors sooner.\n    * **Faster Feedback Loops:**  Developers receive quicker feedback on code changes, facilitating rapid iteration and error correction.\n    * **Increased Deployment Frequency:**  Automated pipelines enable frequent, reliable deployments, promoting agility and faster delivery of value.\n\n**Consequences:**\n\n* **Increased Upfront Effort:** Writing tests before code requires an initial time investment.\n* **Potential for Over-Testing:**  Writing comprehensive tests can be challenging and lead to an overly large test suite.\n* **Infrastructure Investment:** Implementing CI\/CD necessitates investing in appropriate tooling and infrastructure.\n\n**Overall, the benefits of improved code quality, ease of change, and simplified error correction outweigh the potential drawbacks of TDD and CI\/CD. This decision aligns with the goal of ensuring a maintainable and robust codebase.** \n","GenTime":"2024-08-19 15:21:47","text":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n\n##Decision\nIt will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n"}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Ingestion.md","Context":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n","Decision":"The recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":135,"id":1095,"Prediction":"## :dart: Decision\n\nWe will use **Azure Functions** to host the ingestion sub-system. \n\n## :heavy_check_mark: Justification\n\nAzure Functions best meets the requirements for the following reasons:\n\n* **Scheduling:** Azure Functions have built-in support for cron expressions, allowing for easy daily scheduling.\n* **Background Execution:** Functions are inherently designed for short-lived, background tasks.\n* **Cosmos DB Connectivity:** Functions seamlessly integrate with other Azure services, including Cosmos DB, through bindings.\n* **.NET Core Support:** Functions fully support .NET Core development.\n* **Managed Service:** As a serverless offering, Functions minimizes operational overhead.\n* **Cost-Effectiveness:** The \"consumption plan\" for Functions will be economical given the service runs only once a day.\n\n## :x: Alternatives Considered\n\n* **App Services:** While offering more flexibility, App Services introduce more management overhead and potential cost compared to Functions for this use case.\n* **Container Instances:** Suitable for containerized applications, but introduces unnecessary complexity for a single, scheduled .NET Core function.\n* **Service Fabric:**  Overly complex and resource-intensive for a single, daily task.\n* **AKS:** Provides a fully managed Kubernetes cluster, which is overkill for the ingestion sub-system's requirements.\n\n## :information_source:  Consequences\n\n* **Simplified Development:** Azure Functions streamline the development process with a focus on code and minimal infrastructure management. \n* **Cost Optimization:**  Consumption-based pricing aligns costs with actual usage, making it cost-effective for infrequent executions.\n* **Potential Vendor Lock-in:** Choosing a serverless platform like Azure Functions may introduce some degree of vendor lock-in. However, this is mitigated by .NET Core's portability should a future migration be necessary. \n","GenTime":"2024-08-19 15:21:55","text":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n\n\n##Decision\nThe recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n"}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"Prediction":"## Decision\n\nWe will use a subset of the Abstract Syntax Tree (AST) generated by the **Lucene Query Parser** syntax as the basis for our abstract query language. This choice offers several advantages:\n\n* **Familiarity:** Lucene syntax is widely used and understood, lowering the learning curve for users already familiar with its structure. \n* **Expressiveness:** Lucene supports a wide array of operators and features, allowing us to handle complex search scenarios and potentially expanding functionality in the future.\n* **Existing Tooling:**  Leveraging an established syntax grants us access to existing parsing libraries and tools, simplifying implementation and reducing development time.\n\n**Supported Features:**\n\nInitially, we will focus on supporting the following core features of the Lucene syntax:\n\n* **Basic keyword searches:**  E.g., `keyword1 keyword2`\n* **Field-specific searches:** E.g., `title:keyword`\n* **Boolean operators:** `AND`, `OR`, `NOT` to combine search terms\n* **Phrase searches:**  E.g., `\"exact phrase\"`\n* **Wildcard searches:**  E.g., `te?t`, `test*`\n* **Range searches:** E.g., `year:[2000 TO 2010]`\n\n**Future Considerations:**\n\nAs the system evolves, we can consider incorporating more advanced features from the Lucene syntax, such as:\n\n* **Proximity searches:** Searching for words near each other.\n* **Fuzzy searches:** Finding words similar to the search term.\n* **Boosting:** Giving more weight to specific terms or fields.\n\n**Justification:**\n\nThis approach strikes a balance between ease of use, flexibility, and development effort. By leveraging an existing syntax and its ecosystem, we benefit from a well-defined structure and readily available tooling while retaining the ability to tailor the supported feature set to our specific needs. \n","GenTime":"2024-08-19 15:22:03","text":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n\n##Decision\nChosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n"}
{"File Name":"dos-server\/adr-11-exception-monitoring.md","Context":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n","Decision":"We will use Sentry for application exception monitoring.\\n","tokens":30,"id":3370,"Prediction":"## Decision\n\n**Use Sentry for real-time production error discovery and reporting.**\n\n**Reasoning:**\n\n* **Existing Infrastructure:** DLS already utilizes Sentry, eliminating the need to onboard and learn a new tool, saving time and resources. \n* **Familiarity:** Existing teams have experience with Sentry, streamlining implementation and ongoing maintenance.\n* **Proven Effectiveness:** Sentry's track record within DLS demonstrates its capability to effectively handle error discovery and reporting.\n* **Feature Rich:**  Sentry offers robust features for error tracking, reporting, alerting, and debugging, potentially exceeding the new application's needs. \n\n**Alternatives Considered:**\n\n* **Building a custom solution:**  While this offers maximum control, it demands significant development time, resources, and ongoing maintenance. \n* **Other error tracking tools:**  Exploring other tools (e.g., Rollbar, Bugsnag) might uncover a better fit, but the benefits might not outweigh the cost of onboarding and integration.\n\n**Consequences:**\n\n* **Reliance on Sentry:**  Choosing Sentry creates a dependency on the platform's stability and continued support.\n* **Potential Cost:** While DLS already uses Sentry, increased usage might lead to higher subscription costs.  \n\n**Status:** Accepted\n\n**Date:** 2023-10-27 \n","GenTime":"2024-08-19 15:22:09","text":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n\n\n##Decision\nWe will use Sentry for application exception monitoring.\\n"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"Prediction":"## Decision\n\nWe will use the **img2pdf** external command to convert image files to PDF. \n\n## Reasons\n\n* **Simplicity and Focus:** `img2pdf` is specifically designed for converting images to PDF, making it a straightforward and focused solution. It avoids the complexity and overhead of libraries like `pdfbox` or `ImageMagick` when our primary goal is simple image to PDF conversion.\n* **Preservation of Image Data:**  `img2pdf` directly embeds the image data into the PDF without re-encoding, ensuring no loss of quality. This is crucial for document images where fidelity is paramount.\n* **Lightweight:** As an external command, `img2pdf` has minimal dependencies and doesn't require us to include a large library in our project.\n* **Performance:** While not extensively benchmarked in this ADR, `img2pdf` is known for its speed and efficiency in handling image to PDF conversions.\n\n## Considered Trade-offs\n\n* **External Dependency:** Relying on an external command introduces a dependency outside our codebase. However, `img2pdf` is a well-maintained and widely used tool, mitigating this risk.\n* **Limited Functionality:** `img2pdf` is purpose-built for image to PDF conversion. If we anticipate needing extensive PDF manipulation capabilities beyond basic conversion in the future, a more feature-rich library like `pdfbox` might be a better long-term choice.\n\n## Rejected Options\n\n* **pdfbox:** While powerful, `pdfbox` introduces a significant dependency and requires more code to achieve simple image to PDF conversion. It's overkill for our current needs.\n* **ImageMagick:** Similar to `pdfbox`, `ImageMagick` is a powerful image manipulation library, but it's unnecessarily complex for our use case.\n* **tesseract:** `tesseract` is an OCR (Optical Character Recognition) engine, not suited for our primary goal of image to PDF conversion.  We are focused on preserving the image data, not extracting text.\n","GenTime":"2024-08-19 15:22:18","text":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n\n\n##Decision\nTesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n"}
{"File Name":"html-diff-service\/0002-implement-using-spring-boot.md","Context":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n","Decision":"Spring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","tokens":60,"id":3249,"Prediction":"## Decision\n\nWe will implement the HTML Diff Service as an independent microservice with a RESTful API.  This service will be responsible for receiving two HTML documents, performing the diff operation, and returning the results in a structured format (e.g., JSON). \n\nThis decision addresses the following:\n\n* **Performance:**  Offloading the diff operation to a dedicated service eliminates the overhead of command line execution within the web application, improving performance.\n* **Dependency Removal:** Building the service independently from the Rails application removes the Java dependency and its associated security concerns.\n* **Scalability:** Microservice architecture allows for independent scaling of the HTML Diff Service based on demand. \n* **Maintainability:**  A separate service promotes code organization, simplifies testing, and facilitates independent deployments and updates. \n","GenTime":"2024-08-19 15:22:22","text":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n\n\n##Decision\nSpring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n"}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"Prediction":"## Decision\n\nThe Modernisation Platform will adopt a hybrid approach to Terraform modules, leveraging both **publicly available modules** and developing **custom internal modules**.\n\n**1. Publicly Available Modules:**\n\n* We will prioritize the use of well-maintained and widely adopted public modules from reputable sources like the Terraform Registry for common infrastructure components (e.g., VPCs, databases, load balancers).\n* This approach minimizes development effort, leverages community expertise, and benefits from ongoing maintenance and updates.\n* Strict vetting criteria will be established to evaluate public modules for security, reliability, and licensing compatibility.\n\n**2. Custom Internal Modules:**\n\n* We will develop custom modules for platform-specific infrastructure patterns or functionalities that are not effectively addressed by existing public modules.\n* These modules will encapsulate reusable logic, configurations, and best practices tailored to the Modernisation Platform's specific requirements.\n* Internal modules will adhere to strict coding standards, documentation practices, and testing methodologies to ensure maintainability, reusability, and quality.\n\n**Module Structure and Organization:**\n\n* Modules will be organized hierarchically within the platform's codebase to promote clarity and maintainability.\n* A clear naming convention will be established for modules, reflecting their purpose and scope.\n* Comprehensive documentation will be provided for each module, outlining its purpose, inputs, outputs, and any dependencies.\n\n**Versioning and Release Management:**\n\n* A consistent versioning strategy will be implemented for both internal and public modules to ensure compatibility and facilitate upgrades.\n* Internal modules will undergo thorough testing and review before release.\n\n**Rationale:**\n\nThis hybrid approach balances the benefits of leveraging existing solutions with the need for customization and flexibility in a complex platform. By strategically utilizing both public and custom modules, we aim to:\n\n* **Accelerate infrastructure deployment:** Reduce development time and effort by reusing existing solutions.\n* **Improve code quality and maintainability:** Enforce consistency and best practices through modularization.\n* **Enhance collaboration and knowledge sharing:** Provide a structured approach for teams to share and reuse infrastructure code.\n* **Reduce the risk of configuration drift and errors:** Promote standardized configurations and automated deployments. \n","GenTime":"2024-08-19 15:22:31","text":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n\n\n##Decision\nModules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\\nWe need to choose webserver.\\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"Prediction":"## Decision\n\nWe will use **Nginx** as our web server. \n\nThis decision is based on the following factors:\n\n* **Performance:** Nginx is known for its high performance and low resource consumption, making it suitable for handling high traffic loads. \n* **Scalability:** Nginx's architecture allows for easy horizontal scaling, which is crucial for our application's future growth.\n* **Reverse Proxy Capabilities:** Nginx can act as a reverse proxy, efficiently routing traffic to different backend servers and improving security.\n* **Cost:** Nginx is open-source and free to use, which aligns with our budget considerations. \n\nWhile other options like Apache are available, Nginx's performance, scalability, and reverse proxy capabilities make it a more suitable choice for our specific needs.\n","GenTime":"2024-08-19 15:22:35","text":"## Context\\nWe need to choose webserver.\\n\n\n##Decision\nWe will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n"}
{"File Name":"CCS-Architecture-Decision-Records\/0012-use-govuk-notify-for-bulk-communication-with-users.md","Context":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n","Decision":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","tokens":261,"id":996,"Prediction":"## Decision\n\nThe team will adopt GOV.UK Notify as the standard service for sending email and SMS notifications for the following reasons:\n\n* **Centralized and Secure:** Utilizing a government-backed service like GOV.UK Notify eliminates the need to build and maintain our own notification infrastructure, reducing development time and potential security risks. \n* **Reduced Data Breach Risk:** GOV.UK Notify's secure platform minimizes the risk of accidental data breaches compared to less secure methods like using BCC in emails. \n* **Feature Rich:** The service offers features such as delivery status feedback, allowing us to monitor and improve the effectiveness of our notifications. \n* **Cost Effective:** Sending emails via GOV.UK Notify is free, making it a cost-effective solution for our notification needs.\n* **Proven Track Record:** GOV.UK Notify is already used successfully within CCS by services like MISO, Report MI, and DigiTS, demonstrating its reliability and suitability for our needs.\n\nWhile email will be the primary notification channel, we will leverage GOV.UK Notify's SMS capabilities when a more immediate or direct communication channel is required. \n","GenTime":"2024-08-19 15:22:40","text":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n\n\n##Decision\nWe will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n"}
{"File Name":"FindMeFoodTrucks\/Hosting Model.md","Context":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n","Decision":"The application will be hosted on Microsoft Azure Cloud datacenter.\\n","tokens":43,"id":1089,"Prediction":"## :dart: Decision\n\nWe will host the application on a **managed cloud datacenter**. \n\nThis decision is driven by the following factors:\n\n* **Scalability:** Cloud providers offer auto-scaling and on-demand resource provisioning, enabling the application to easily adapt to fluctuating workloads. This is more efficient and cost-effective than managing scalability in a private datacenter.\n* **Cost:** While private datacenters offer more control over hardware and software, they come with significant upfront investment and ongoing maintenance costs. Cloud providers offer a pay-as-you-go model, allowing for better cost management and flexibility. \n* **Technology Choices:** Cloud providers offer a wide range of managed services for databases, messaging queues, monitoring, and more. This reduces the complexity of our technology stack and allows us to focus on application development.\n* **Time to Market:** Deploying to a managed cloud platform is generally faster and simpler than setting up and configuring a private datacenter.\n\nHowever, we acknowledge that hosting in a managed cloud environment might come with:\n\n* **Security Concerns:** We will mitigate this by carefully selecting a reputable cloud provider with robust security features and compliance certifications.\n* **Vendor Lock-in:** To mitigate vendor lock-in, we will adopt a cloud-agnostic architecture and use open-source technologies where possible. \n\nWe believe that the advantages of using a managed cloud datacenter outweigh the potential drawbacks for this application. This decision aligns with our goals of achieving high scalability, cost-effectiveness, and faster time-to-market. \n","GenTime":"2024-08-19 15:22:47","text":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n\n\n##Decision\nThe application will be hosted on Microsoft Azure Cloud datacenter.\\n"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"Prediction":"## Decision\n\n**Adopt an event-driven architecture for ingesting and processing data from various sources like LA and IFA CMS and SSDA903.**\n\nThis decision is based on the following rationale:\n\n* **Alignment with data structure:** The inherent event stream nature of SSDA903 and the focus on individual pathways as a series of events make an event-driven architecture a natural fit. \n* **Scalability and flexibility:** Event-driven architectures are inherently scalable and flexible, allowing for easy integration of new data sources (e.g., future data collections or CMS systems) and adaptation to evolving data requirements.\n* **Real-time insights:**  Processing events as they occur enables near real-time insights into individual pathways and system-wide trends, facilitating timely interventions and improved decision-making.\n\nThis decision will influence the technology choices and system design, requiring:\n\n* **Selection of an event streaming platform:**  (e.g., Kafka, Pulsar) for handling the ingestion, storage, and distribution of events from various sources.\n* **Development of event producers:**  Adapting existing systems (e.g., LA and IFA CMS) or building new ones to publish relevant events to the event stream.\n* **Implementation of event consumers:**  Developing microservices or components that subscribe to specific event types and perform relevant processing or trigger actions based on those events. \n\nThis architecture will provide a robust and scalable foundation for the Children's Social Care Alpha Service to ingest, process, and analyze data from diverse sources, enabling data-driven insights and supporting better outcomes for children in care. \n","GenTime":"2024-08-19 15:22:54","text":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n\n\n##Decision\n* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n"}
{"File Name":"dlp-lux\/0002-Blacklight-Version.md","Context":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n","Decision":"We will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","tokens":31,"id":3215,"Prediction":"## Decision\n\nWe will use **Blacklight version 7.4.1** for the `dlp-lux` project.  \n\nThis decision is based on the following factors:\n\n* **Compatibility:** Version 7.4.1 is the most recent version of Blacklight that is confirmed to be compatible with the other dependencies and technologies we are using in `dlp-lux`. \n* **Stability:** Blacklight 7.4.1 is a stable release with known bug fixes and security patches.\n* **Community Support:**  This version has a large active community and extensive documentation, which will be helpful for troubleshooting and future development. \n\nWhile newer versions of Blacklight are available, they may introduce compatibility issues or require significant code changes. Upgrading to a newer version can be revisited once `dlp-lux` is further along in development and the benefits of upgrading are clearer. \n","GenTime":"2024-08-19 15:22:58","text":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n\n\n##Decision\nWe will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n"}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"Prediction":"## Decision\n\nWe will adopt a hybrid approach to using wrapper scripts:\n\n**1. Strategic Use of Wrapper Scripts:**\n\n* We will primarily use wrapper scripts for **complex, critical, or frequently repeated tasks** where the benefits of automation, consistency, and reduced error rates outweigh the potential drawbacks. \n* Examples include:\n    * Provisioning and configuring new infrastructure components.\n    * Deploying and updating applications following specific workflows.\n    * Performing security audits and remediation tasks.\n    * Running complex data pipelines or batch jobs.\n\n**2. Transparency and Documentation:**\n\n* All wrapper scripts will be thoroughly documented, explaining their purpose, usage, and any underlying assumptions or dependencies.\n* Scripts will be designed with transparency in mind, making it easy to understand how they interact with underlying tools. \n* Where possible, scripts will provide options for increased verbosity or debugging to aid in understanding their execution flow.\n\n**3. Modularity and Reusability:**\n\n* We will strive to make scripts modular, breaking down complex functionality into smaller, reusable components.\n* This will improve maintainability, reduce code duplication, and make it easier to adapt scripts for new use cases.\n\n**4. Gradual Introduction and Training:**\n\n* New wrapper scripts will be introduced gradually, accompanied by training and documentation to facilitate adoption.\n* We will provide clear guidance on when to use wrapper scripts and when to interact directly with the underlying tools.\n\n**5. Continuous Evaluation:**\n\n*  We will regularly evaluate the use of wrapper scripts, assessing their effectiveness and identifying any areas for improvement or potential drawbacks. \n* This will involve gathering feedback from users, monitoring script usage patterns, and evaluating the impact on operational efficiency and security. \n\n**This approach seeks to harness the benefits of wrapper scripts while mitigating their potential disadvantages.  By focusing on strategic use, transparency, modularity, and continuous evaluation, we aim to achieve a balance between automation, flexibility, and ease of use.** \n","GenTime":"2024-08-19 15:23:07","text":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n\n\n##Decision\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n"}
{"File Name":"ReportMI-service-manual\/0019-store-data-validation-rules-in-json-files.md","Context":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n","Decision":"We will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","tokens":391,"id":2045,"Prediction":"## Decision\n\nWe will implement an interim solution based on JSON files stored in a GitHub repository to define and manage data submission rules for each framework. This approach offers a balance between simplicity and functionality for our immediate needs:\n\n**Rationale:**\n\n* **Simplicity and Speed:** JSON is a lightweight, human-readable format that is easily parsed and generated, enabling us to quickly define and update data submission rules. \n* **Version Control:** GitHub provides essential version control for the JSON files, allowing us to track changes, revert to previous versions, and manage different rule sets for different framework versions.\n* **Centralized Repository:** Storing all framework rules in a single GitHub repository ensures easy access, management, and sharing among developers and services.\n\n**This approach will be used to:**\n\n1. **Generate submission templates:**  Dynamically generate templates in various formats (Excel, ODS, CSV) based on the rules defined in the JSON files.\n2. **Validate submitted data:** Validate uploaded data against the rules specified in the corresponding framework's JSON file to ensure data integrity.\n\n**Future State:**\n\nThis interim solution will be replaced by a centralized API-driven approach once we have a better understanding of the complexity and variety of data submission rules across different frameworks. This API will provide a more robust, scalable, and maintainable solution for managing and accessing data submission rules. \n","GenTime":"2024-08-19 15:23:13","text":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n\n\n##Decision\nWe will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n"}
{"File Name":"tul_cob\/0001-use-feature-flags-to-toggle-features.md","Context":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n","Decision":"We've decided to implement very simple feature flags that can be toggled with environment variables.\\n","tokens":49,"id":3171,"Prediction":"## Decision\n\nWe will utilize a feature flagging system to control the visibility and activation of features within our application. This system will allow us to:\n\n* **Develop features in the main branch:** Instead of creating long-lived feature branches, we can build new features directly in the main branch, keeping them hidden behind feature flags.\n* **Decouple deployment from release:** Features can be deployed to production in an inactive state, allowing for staged rollouts and minimizing release risk.\n* **Conduct A\/B testing:**  We can leverage feature flags to enable features for specific user segments, allowing for controlled experimentation and data-driven decision-making.\n\nWe will investigate and select a feature flagging solution that meets our needs based on criteria like:\n\n* **Integration with our tech stack:** Compatibility with our existing programming languages, frameworks, and deployment infrastructure.\n* **Feature set:** Support for different flag types (boolean, operational), user segmentation, targeting rules, and remote configuration.\n* **Scalability and performance:** Ability to handle our user base and traffic demands without impacting application performance.\n* **Monitoring and analytics:** Tools for tracking flag usage, analyzing experiment results, and gaining insights into feature adoption.\n* **Cost:** Evaluation of pricing models and ensuring alignment with our budget. \n\nThis approach will streamline our development process, enhance our release management capabilities, and enable data-driven decision making regarding feature releases. \n","GenTime":"2024-08-19 15:23:19","text":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n\n\n##Decision\nWe've decided to implement very simple feature flags that can be toggled with environment variables.\\n"}
{"File Name":"community\/dr-017-Application-Integration-without-Wormhole-Connector.md","Context":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n","Decision":"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","tokens":42,"id":3448,"Prediction":"## Decision\n\n**We will implement the Wormhole Connector as a lightweight application deployed in the customer's infrastructure.**\n\n**Reasoning:**\n\n* **Security:**  Deploying the connector within the customer's environment ensures that sensitive data never leaves their network perimeter. Only the connector itself needs outbound access, reducing the attack surface for the Kyma platform. \n* **Flexibility:** This approach allows for greater flexibility in terms of integration with diverse customer environments and network configurations.\n* **Control:** Customers retain complete control over the connector's deployment, configuration, and security policies within their own infrastructure.\n\n**Consequences:**\n\n* **Maintenance:** Customers will be responsible for the deployment, maintenance, and security of the Wormhole Connector within their own infrastructure.\n* **Onboarding:** The onboarding process for new customers will involve installing and configuring the Wormhole Connector within their environments. \n\n**Alternatives Considered:**\n\n* **VPN-based connection:** While a VPN could provide a secure tunnel, it often requires complex network configurations and may not be suitable for all customer environments. \n* **Reverse proxy:** A reverse proxy located in the Kyma platform could expose specific services, but this approach would require exposing endpoints to the public internet, potentially increasing security risks.\n\n**Status:**  Accepted\n\n**Open Questions:**\n\n* What specific resources and support will be provided to customers for the deployment and maintenance of the Wormhole Connector?\n* How will the onboarding process be streamlined to simplify the installation and configuration of the Wormhole Connector for new customers? \n","GenTime":"2024-08-19 15:23:26","text":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n\n\n##Decision\nThe decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"Prediction":"## Context and Problem Statement \n\nOur application needs to persist user data. We need to decide between using a relational database (SQL) or a NoSQL database solution. What type of database will best suit our application's data structure and access patterns?\n\n## Decision Drivers \n\n* **Data Structure:**  The complexity and relationships within our user data model.\n* **Scalability Requirements:** Anticipated growth in users and data volume.\n* **Query Flexibility:** The types of queries and data retrieval operations the application will perform. \n* **Team Expertise:**  Existing skills and experience with different database technologies.\n* **Cost:**  Budget constraints and the cost of implementing and maintaining different database solutions. \n","GenTime":"2024-08-19 15:23:30","text":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file living within the `\/docs\/adr` directory of our repository, following a standardized template for consistency. \n","GenTime":"2024-08-19 15:23:32","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"thing\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2583,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a mechanism for capturing significant architectural decisions along with their context and consequences. \n\nWe will utilize the Markdown format for ADRs due to its simplicity, readability, and compatibility with version control systems like Git. \n\nA template will be provided for consistency and completeness of information in each ADR. This template will include sections for:\n\n* **Title:** A unique and descriptive title for the decision.\n* **Status:**  The current state of the decision (e.g., Proposed, Accepted, Rejected, Superseded).\n* **Context:** The background and forces that led to this decision.\n* **Decision:** The architectural decision being made.\n* **Consequences:** The known pros, cons, and implications of making this decision.\n* **Alternatives Considered:** A brief description of other options considered and why they were rejected.\n\nADRs will be stored within the project repository in a dedicated \"docs\/adr\" directory to ensure visibility and accessibility for all team members. We will follow a consistent naming convention for ADR files (e.g., ADR-001-decision-title.md).\n","GenTime":"2024-08-19 15:23:37","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"rails-template\/0006-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":4179,"Prediction":"## Decision\n\nWe will implement a Continuous Integration and Continuous Delivery (CI\/CD) pipeline that includes automated testing as a core stage. This pipeline will be triggered by every code commit, ensuring that all new code changes are automatically tested for functionality and code coverage. We will strive for a high percentage of code coverage (target to be defined based on project specifics) and investigate tools that can assist with this goal, such as:\n\n* **Code Coverage Tools:** Integrate tools like SonarQube, JaCoCo, or Coveralls to track code coverage metrics and identify areas with insufficient testing.\n* **Automated Test Suite:** Develop a comprehensive suite of unit, integration, and end-to-end tests to cover various aspects of the application's functionality and edge cases.\n* **Test Coverage Gate:** Implement a quality gate within the CI\/CD pipeline that prevents merging code with insufficient test coverage into the main branch.\n\nThis approach will help us achieve the following:\n\n* **Maintain High Test Coverage:** Automated testing ensures consistent and frequent testing, leading to higher test coverage.\n* **Reduce Manual Effort:** Automation eliminates the need for manual checks, saving time and effort.\n* **Early Bug Detection:** Running tests on every code change enables early identification and resolution of bugs, leading to higher software quality.\n* **Faster Development Cycles:** Automated testing and CI\/CD streamline the development process, allowing for faster iteration and deployment of new features. \n","GenTime":"2024-08-19 15:23:43","text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n"}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"Prediction":"## Decision\n\nWe will implement a custom distance calculation solution within our application code. This decision is based on the following factors:\n\n* **Missing Feature in Azure Search:** Azure Search currently lacks the capability to return calculated distances within the search results. \n* **Urgency:** Waiting for the feature request to be implemented by Microsoft is not a viable option due to project timelines and the importance of distance-based results.\n* **Control and Flexibility:**  A custom solution offers greater control over the distance calculation logic, allowing us to tailor it to our specific needs and potentially integrate with preferred mapping APIs or libraries. \n\nThis approach avoids relying on external dependencies and gives us complete control over the feature. We will evaluate the performance impact of this solution and optimize the implementation to minimize any overhead. Should Azure Search introduce native support for distance calculation in the future, we will re-evaluate this decision and consider migrating to the platform's built-in functionality. \n","GenTime":"2024-08-19 15:23:48","text":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n\n\n##Decision\nThe decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n"}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture architectural decisions along with their context and consequences. Each significant architectural decision will be documented in a separate ADR file following a standardized template. \n","GenTime":"2024-08-19 15:23:49","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture-Message-Types.md","Context":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n","Decision":"**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","tokens":384,"id":2316,"Prediction":"## **Decision**\n\nWe will adopt a **hybrid approach** to event structure. This approach balances the benefits of both thin and fat events while mitigating their respective drawbacks. \n\n**Rationale:**\n\n* **Flexibility:** The hybrid approach allows us to use thin events as the default, promoting smaller payloads, data consistency, and easier versioning.  We can selectively incorporate larger payloads within the optional message body only when absolutely necessary, such as for audit logs or situations where retrieving data via API calls is impractical or undesirable.\n\n* **Performance and Scalability:**  Prioritizing thin events reduces network bandwidth consumption and storage requirements, contributing to better system performance and scalability.\n\n* **Maintainability:** Using a standardized structure with an optional payload simplifies event handling and reduces the complexity of managing multiple event versions. \n\n**Implementation Details:**\n\n* All events will follow a standardized structure, including a mandatory header containing essential metadata and an optional body for larger payloads.\n* The decision to include a payload in the body will be made on a case-by-case basis, carefully considering the trade-offs and prioritizing thin events whenever possible.\n* Clear guidelines and documentation will be provided to developers regarding when to use thin events versus when to include a payload in the message body.\n\nThis hybrid approach provides a flexible, scalable, and maintainable solution for our event-driven architecture. \n","GenTime":"2024-08-19 15:23:56","text":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n\n\n##Decision\n**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n"}
{"File Name":"news\/0006-a-state-requirement.md","Context":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"Prediction":"## Decision\n\nWe will implement a client-side caching mechanism using a Javascript `Map` to store the necessary bookmark information temporarily. \n\n1. **Cache Implementation:** A `Map` will be used to store the bookmark data keyed by the news item ID. This will allow for fast retrieval of the information needed to create a `Bookmark` object.\n2. **Data Population:** When the page loads, we'll make an API call to fetch the titles and URLs for all visible news items. This data will be used to populate the cache. \n3. **Bookmark Creation:** When the user clicks the \"bookmark\" link:\n    * The cache will be checked for the corresponding news item ID. \n    * If found, the title and URL will be retrieved from the cache, and a `Bookmark` object will be created and passed to `application.bookmarks.add()`.\n    * If not found, we will either:\n        * Make a synchronous API call to retrieve the information. This will introduce a slight delay in bookmarking but ensures data consistency.\n        * Display an error message to the user, indicating that the bookmark cannot be created at this time.\n4. **Cache Invalidation:** The cache will be invalidated in the following scenarios:\n    * The user navigates away from the page.\n    * The user refreshes the page. \n    * A set time interval elapses (e.g., 5 minutes), ensuring the cache doesn't grow indefinitely.\n\n## Consequences\n\n**Pros:**\n\n* Avoids storing all article data for bookmarking.\n* Eliminates the need for additional lookups when creating bookmarks.\n* Keeps the UI code clean and focused.\n* Improves the user experience by providing faster bookmark creation.\n\n**Cons:**\n\n* Introduces a client-side caching mechanism that needs to be managed.\n* May require a synchronous API call in some cases if the cached data is unavailable. \n* Increases the complexity of the front-end code. \n\n**Alternatives Considered:**\n\n* Passing all information as arguments to `application.bookmarks.add()`: This clutters the UI code and makes it harder to maintain.\n* Baking all information into the link: This breaks the separation of concerns and makes the code less readable.\n\nThis solution strikes a balance between performance, maintainability, and data consistency. By leveraging client-side caching, we can provide a smooth and efficient bookmarking experience without sacrificing code quality or introducing unnecessary server-side storage requirements. \n","GenTime":"2024-08-19 15:24:07","text":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n\n\n##Decision\nIntroduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n"}
{"File Name":"meadow\/0029-npm.md","Context":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n","Decision":"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","tokens":37,"id":3841,"Prediction":"## Decision\n\nWe will revert to the previous stable version of Yarn and investigate alternative package managers for future projects. This decision prioritizes stability and minimizes disruption to our current development workflow.\n","GenTime":"2024-08-19 15:24:09","text":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n\n\n##Decision\nSwitch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n"}
{"File Name":"libelektra\/error_semantics.md","Context":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n","Decision":"Provide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","tokens":28,"id":1302,"Prediction":"## Decision\n\nWe will establish clear guidelines and criteria for plugin developers to determine when to emit errors and warnings. This will involve:\n\n1. **Defining severity levels:** Expanding upon our existing classification of errors and warnings to include detailed descriptions and examples for each level. For example, we can introduce levels like \"critical,\" \"error,\" \"warning,\" \"info,\" and \"debug.\"\n2. **Mapping error\/warning conditions:** Creating a comprehensive list of common scenarios within the plugin ecosystem and assigning appropriate severity levels to them. This mapping will serve as a reference for plugin developers.\n3. **Providing code-level guidance:** Offering concrete examples and best practices within the plugin development documentation on how to emit errors and warnings consistently. This will include utilizing specific error codes, formatting messages effectively, and leveraging logging frameworks.\n4. **Developing linting rules:** Introducing automated linting rules to the plugin development workflow to enforce the defined guidelines and ensure consistency across plugins.\n\nThis approach will bring clarity and consistency to error and warning reporting across plugins, ultimately improving the developer experience and maintainability of the software. \n","GenTime":"2024-08-19 15:24:14","text":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n\n\n##Decision\nProvide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"Prediction":"## Decision\n\nWe will use **Constructor Injection** to resolve dependencies for objects within each module's Command and Query handlers. \n\n## Justification\n\n* **Simplicity and Testability**: Constructor Injection promotes loose coupling by explicitly declaring dependencies through constructor parameters. This makes it easier to understand object relationships and facilitates unit testing by allowing for straightforward mocking of dependencies. \n* **Immutability**: By injecting dependencies through the constructor, we can enforce the immutability of object dependencies after object creation, leading to more predictable and maintainable code.\n* **Framework Agnosticism**: Constructor Injection is a widely adopted pattern that is not tied to any specific framework, ensuring portability and flexibility in the long run. \n\n## Alternatives Considered\n\n* **Service Locator**: While this pattern provides a central registry for dependencies, it can lead to hidden dependencies and make it harder to reason about object construction.\n* **Property Injection**: Injecting dependencies through mutable properties can introduce temporal coupling and make it harder to guarantee that an object is in a valid state.\n\n## Consequences\n\n* **Increased Boilerplate**: Constructor Injection might require slightly more code compared to other approaches, especially for objects with many dependencies.\n* **Dependency Management**:  We might need to introduce a Dependency Injection framework (e.g., Spring, Guice) to manage object creation and lifecycle if the dependency graph becomes complex. \n\n## Notes\n\n* We will explore specific Dependency Injection frameworks during the implementation phase to evaluate their suitability for our needs.\n* We will strive to keep our dependency graphs as simple as possible to minimize the complexity overhead associated with Constructor Injection. \n","GenTime":"2024-08-19 15:24:21","text":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n\n\n##Decision\nSolution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n"}
{"File Name":"govuk-aws\/0002-hosting-platforms.md","Context":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n","Decision":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","tokens":86,"id":4041,"Prediction":"## Decision\n\nWe will utilize a two-pronged approach for hosting the future GOV.UK infrastructure:\n\n**Short-term:** \n\n* **Modernize and migrate existing infrastructure to AWS.**  This aligns with GDS policy for non-PaaS services and provides a robust, scalable, and secure platform for the interim. \n* **Begin migrating applicable services to GOV.UK PaaS.** This initiates the convergence with the long-term vision and allows for iterative learning and adaptation to the platform.\n\n**Long-term:**\n\n* **GOV.UK PaaS will be the primary hosting platform.** This adheres to GDS policy and offers the benefits of a managed platform, including improved developer experience, operational efficiency, and cost optimization.\n* **AWS will remain a secondary option for services incompatible with PaaS.** This ensures flexibility and accommodates potential edge cases. \n\nThis approach allows us to benefit from modern cloud infrastructure in the short term while strategically aligning with the long-term vision of utilizing GOV.UK PaaS. It also provides an opportunity for phased migration, minimizing disruption and allowing for continuous improvement. \n","GenTime":"2024-08-19 15:24:26","text":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n\n\n##Decision\nWe are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n"}
{"File Name":"libelektra\/cmake_spec.md","Context":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n","Decision":"Rejected: keep default plugins as-is\\n","tokens":37,"id":1279,"Prediction":"## Decision\n\nWe will implement a plugin definition format that allows specifying compilation variants and dependencies within a single plugin definition. This will allow us to:\n\n1. **Reduce the number of plugin artifacts:** Instead of having separate artifacts for each compilation variant, a single artifact can contain all variants.\n2. **Simplify dependency management:** Dependencies for specific variants and the default storage\/resolver can be declared within the plugin definition, making it easier to manage and reason about.\n3. **Improve user experience:** Users will only need to interact with a single plugin definition, regardless of the desired compilation variant.\n\nThis approach avoids the combinatorial explosion of plugin artifacts and provides a more streamlined and maintainable plugin management system. \n","GenTime":"2024-08-19 15:24:30","text":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n\n\n##Decision\nRejected: keep default plugins as-is\\n"}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"Prediction":"## Decision\n\nWe will use the standard HTTP status codes to signal the general nature of the problem with a request. In the response body, we will include a JSON object with the following structure:\n\n```json\n{\n  \"status\": \"error\",\n  \"error_code\": \"SHORT_UNIQUE_CODE\", \n  \"message\": \"Human-readable description of the error\", \n  \"details\": {\n    \"field1\": \"Specific information about the error, potentially including allowed values\",\n    \"field2\": \"Further information about the error\",\n    ...\n  }\n}\n```\n\n**Fields:**\n\n- **status**: Always set to \"error\" for client-side errors.\n- **error_code**: A short, unique, and machine-readable code that identifies the specific error type. This facilitates programmatic error handling on the client-side.\n- **message**: A human-readable, concise explanation of the error that can be displayed to the end-user. This message should avoid technical jargon and be tailored for a non-technical audience. \n- **details**: An optional field containing additional information about the error, such as:\n    - Specific fields that caused the error.\n    - Allowed values for invalid fields.\n    - Links to relevant documentation.\n\n**Example:**\n\n```json\n{\n  \"status\": \"error\",\n  \"error_code\": \"INVALID_EMAIL_FORMAT\",\n  \"message\": \"The email address you entered is invalid.\",\n  \"details\": {\n    \"field\": \"email\",\n    \"message\": \"Please enter a valid email address.\"\n  }\n}\n```\n\n**Rationale:**\n\n- **Standard and Widely Adopted:** Using HTTP status codes provides a standard and widely understood way of communicating errors.\n- **Machine-Readable Error Codes:** Unique error codes facilitate programmatic error handling and allow clients to implement specific logic based on the error type.\n- **Informative Error Messages:** Providing both human-readable messages and detailed technical information helps developers debug issues effectively while offering end-users a clear understanding of what went wrong.\n- **Flexible and Extensible:** The \"details\" field allows us to provide additional context-specific information as needed without cluttering the response structure. \n\nThis approach balances clarity and information density, ensuring that our API communicates errors effectively to both developers and end-users. \n","GenTime":"2024-08-19 15:24:40","text":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n\n\n##Decision\nWe will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n"}
{"File Name":"unit-e-project\/ADR-0007.md","Context":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n","Decision":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","tokens":50,"id":3764,"Prediction":"## Decision\n\nWe will implement a new mechanism for initial coin distribution that bypasses the limitations of spending from the genesis block coinbase. This decision is driven by the following:\n\n1. **Security Concerns:** Directly spending from the genesis block coinbase can introduce potential security risks and complicate future upgrades or chain reorganizations.\n2. **TxDB Limitation:**  The current inability to index genesis block transactions in the txdb necessitates a different approach for initial supply distribution.\n3. **Flexibility:**  A dedicated mechanism allows for greater control and flexibility in defining the initial distribution of coins.\n\nSeveral alternatives will be considered, including:\n\n* **Pre-allocated Genesis State:** Define the initial coin distribution directly within the genesis state of the blockchain. This approach offers simplicity and clarity.\n* **System Transaction:** Introduce a special system transaction in the first block after the genesis block, specifically designed to distribute the initial coin supply. This provides more flexibility compared to modifying the genesis state.\n* **Founders Reward:** Implement a system where a portion of block rewards in the early stages of the blockchain is allocated to a designated address controlled by the project or founders. This approach offers gradual distribution over time.\n\nThe optimal solution will be selected based on a thorough evaluation of the following criteria:\n\n* **Security Implications:** Prioritize solutions that minimize potential attack vectors and ensure the integrity of the coin distribution.\n* **Implementation Complexity:**  Choose an approach that can be implemented efficiently and effectively within the existing codebase.\n* **Flexibility and Control:**  Consider the level of control and flexibility required for the initial coin distribution.\n* **Community Acceptance:** Evaluate potential community perceptions and preferences regarding different distribution mechanisms. \n\nA detailed analysis of these alternatives will be documented in a separate ADR to ensure a transparent and well-informed decision-making process. \n","GenTime":"2024-08-19 15:24:48","text":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n\n\n##Decision\nWe want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"Prediction":"## Decision\n\nWe will adopt a unified language for translating domain words across the entire application. This means that both MobX-State-Tree model properties and React component props will use the **same language**, which will be **[Choose one: English \/ Finnish]**. \n\n## Reasoning\n\n* **Consistency and Maintainability:** Having a single language for domain words improves code readability and consistency across the codebase, making it easier to understand and maintain for both current and future developers.\n* **Reduced Cognitive Load:** Developers won't have to switch between languages when working with different parts of the application, leading to less confusion and improved productivity.\n* **Simplified Translation:** Using one language simplifies the translation process, as only one set of translations needs to be managed.\n\n## Consequences\n\n* **Refactoring Effort:**  Existing code will need to be refactored to adhere to the chosen language. This will require development time and effort.\n* **Potential for Errors:** During the refactoring process, there's a risk of introducing errors due to translation inconsistencies.\n* **Impact on Existing Translations:** If translations already exist for both languages, a decision needs to be made about which set to keep and which to discard. \n\n## Action Items\n\n* **Choose the Preferred Language:** Decide whether to use English or Finnish for all domain word translations. \n* **Define a Clear Translation Strategy:** Document the chosen language and the process for translating domain words.\n* **Refactor Existing Code:** Allocate time and resources to refactor the codebase to use the unified language.\n* **Update Tests:** Ensure all tests are updated to reflect the changes made during refactoring. \n* **Communicate the Decision:** Inform all stakeholders, including developers, testers, and translators, about the decision and the updated conventions.\n","GenTime":"2024-08-19 15:24:56","text":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n\n\n##Decision\nWe will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n"}
{"File Name":"ng-kaart\/0005-links-in-tabel-tonen.md","Context":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n","Decision":"Momenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","tokens":172,"id":2074,"Prediction":"## Decision\n\nOm URL's dynamisch en gebruiksvriendelijk in de tabel weer te geven, implementeren we de volgende strategie:\n\n1. **Veldtype:** Alle URL's, ongeacht hun huidige type (URL of String), worden consistent opgeslagen als veldtype \"URL\" in de lagen.json. Dit zorgt voor uniformiteit en vergemakkelijkt toekomstige verwerking.\n\n2. **Dynamische Weergave:** We introduceren een nieuw attribuut \"displayLabel\" in de lagen.json. Dit attribuut definieert hoe de URL in de tabel getoond moet worden. De volgende opties zijn beschikbaar:\n\n    * **fieldReference:** Verwijst naar een ander veld in dezelfde rij, waarvan de waarde als label gebruikt wordt (bijv. \"dossiernummer\" voor AV).\n    * **staticString:** Specificeert een vaste string die als label getoond wordt.\n    * **combinedFields:** Combineert de waardes van twee velden (gespecificeerd in de configuratie) tot \u00e9\u00e9n label.\n\n3. **Implementatie:** De tabelcomponent wordt aangepast om het \"displayLabel\" attribuut te interpreteren en de URL's correct weer te geven. Bij een klik op het label wordt de gebruiker naar de betreffende URL geleid.\n\n## Argumenten\n\n* **Gebruiksvriendelijkheid:** Door betekenisvolle labels te tonen in plaats van de ruwe URL's, wordt de tabel overzichtelijker en gebruiksvriendelijker.\n* **Flexibiliteit:** De configuratie via \"displayLabel\" biedt flexibiliteit om per laag de gewenste weergave te bepalen.\n* **Consistentie:** Het gebruik van \u00e9\u00e9n veldtype (\"URL\") voor alle URL's bevordert consistentie en vereenvoudigt toekomstige aanpassingen.\n\n## Gevolgen\n\n* **Aanpassingen lagen.json:** De lagen.json moet worden bijgewerkt om de nieuwe veldtypes en het \"displayLabel\" attribuut te bevatten.\n* **Ontwikkeling:** De tabelcomponent moet worden aangepast om de nieuwe logica te implementeren.\n\n## Openstaande Vragen\n\n* Welke specifieke combinaties van velden zijn nodig voor \"combinedFields\"?\n* Zijn er performance-optimalisaties nodig bij het ophalen en weergeven van de dynamische labels? \n","GenTime":"2024-08-19 15:25:07","text":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n\n\n##Decision\nMomenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n"}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"Prediction":"## Decision\n\nWe will implement **a new microservice that sits between Solr and the transformation service (option 2)**. This approach provides several key advantages:\n\n* **Decoupling:** It effectively decouples Solr indexing from the transformation process, improving scalability and fault tolerance. \n* **Asynchronous Processing:** Offloading transformations asynchronously allows Solr indexing to proceed without waiting for potentially time-consuming transformations.\n* **Synchronous API:** Maintaining a synchronous API for Search Services ensures backward compatibility and minimizes disruption to existing functionalities.\n* **Leverages Existing Infrastructure:** This option leverages the already existing Transformation Service, minimizing development effort and complexity compared to building a new content tracker (option 4).\n\n**Reasons for not choosing other options:**\n\n* **Refactoring V0 API (option 1):** While seemingly straightforward, refactoring the existing API to directly use RenditionService2 introduces tight coupling between Search Services and the transformation process, limiting flexibility and scalability.\n* **Search Service using V1 Public API (option 3):**  Directly using the public API within Search Services exposes internal complexities and creates a dependency on the API's structure, hindering future evolution and potentially impacting performance due to API call overhead.\n* **New content tracker (option 4):** This approach introduces significant complexity by requiring the development and maintenance of a completely new component. It might be beneficial for long-term architectural goals, but is deemed overkill for the current need.\n\n\nThis decision prioritizes a balanced approach that addresses the immediate need for scalable content extraction while keeping future flexibility in mind. We can revisit the content tracker option (option 4) as part of a larger architectural evolution if necessary. \n","GenTime":"2024-08-19 15:25:15","text":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n\n\n##Decision\nBased on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n"}
{"File Name":"react-transcript-editor\/2018-10-05-components-comunication.md","Context":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n","Decision":"* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","tokens":71,"id":3187,"Prediction":"## Decision\n\nWe will use a **centralized event bus** for communication between internal components of the Transcript Editor. \n\n## Reasoning\n\nAn event bus provides a flexible and decoupled approach that addresses the key drivers:\n\n* **Simplicity:** Components publish events to the bus and subscribe to specific events, without needing direct knowledge of each other. This promotes modularity and simplifies reasoning about data flow. \n* **Extensibility:**  The event-driven nature allows for easy extension. New components can be introduced without modifying existing code, simply by subscribing to relevant events. This allows for future use cases and exposing internal information as needed.\n\n## Consequences\n\n* **Potential for Event Spaghetti:**  Overuse or poorly structured events can lead to complexity in understanding the overall system behavior. Careful planning and event naming conventions are crucial to mitigate this risk.\n* **Slight Performance Overhead:**  The event bus introduces a small performance overhead compared to direct component-to-component communication. However, this is usually negligible in most applications and outweighed by the benefits of flexibility and maintainability. \n\n## Status\n\nAccepted.\n\n## Notes\n\nWe will use a robust event bus library with features like:\n\n* **Typed Events:**  To ensure type safety and clarity in event handling.\n* **Event Filtering:**  To allow components to selectively subscribe to specific events.\n* **Asynchronous Event Handling:** To prevent blocking operations and ensure responsiveness.\n\nFurthermore, we will establish clear guidelines for event naming, documentation, and overall event bus usage to prevent potential pitfalls and maintain a maintainable architecture. \n","GenTime":"2024-08-19 15:25:22","text":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n\n\n##Decision\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n"}
{"File Name":"tendermint\/adr-015-crypto-encoding.md","Context":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n","Decision":"### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","tokens":100,"id":1962,"Prediction":"## Decision\n\nWe will transition from Amino encoding to Protobuf encoding for public keys and signatures on-chain. \n\n## Reasons\n\n* **Improved Performance:** Protobuf generally offers better performance in terms of encoding\/decoding speed and message size compared to Amino.\n* **Wider Language Support:** While Amino prioritizes ease of parsing, Protobuf boasts wider language support with mature libraries available. \n* **Future-Proofing:** Protobuf is a more widely adopted and actively maintained serialization format, making it a safer choice for long-term compatibility and integration with other systems.\n* **Simplicity:** Given the limited requirement of upgradability for cryptosystems in this context, Protobuf's simpler design is sufficient and preferable.\n\n## Consequences\n\n* **Migration Effort:**  Switching from Amino to Protobuf will require code changes and testing to ensure a smooth transition.\n* **Potential Breaking Change:** Depending on the existing infrastructure and integrations, this change might require coordinated updates from consumers of the encoded data.\n\n## Status\n\nAccepted\n\n## Action Items\n\n* Define a migration plan outlining the steps to transition from Amino to Protobuf encoding.\n* Evaluate existing Protobuf libraries for supported languages and select the most suitable options.\n* Update documentation to reflect the change in encoding format for public keys and signatures. \n","GenTime":"2024-08-19 15:25:28","text":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n\n\n##Decision\n### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n"}
{"File Name":"gsp\/ADR032-sre-permissions.md","Context":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n","Decision":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","tokens":166,"id":3915,"Prediction":"## Decision\n\nGrant developers a specific set of elevated permissions to manage deployments in the Kubernetes cluster without requiring full cluster admin privileges. \n\nThis decision aims to strike a balance between security and operational efficiency. \n","GenTime":"2024-08-19 15:25:30","text":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n\n\n##Decision\nWe will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n"}
{"File Name":"texas-holdem-code-challenge\/0002-use-node-js.md","Context":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n","Decision":"Use Node.js to complete this problem.\\n","tokens":67,"id":2294,"Prediction":"## Decision\n\nUse node.js to implement this software. \n","GenTime":"2024-08-19 15:25:31","text":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n\n\n##Decision\nUse Node.js to complete this problem.\\n"}
{"File Name":"portfolio\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","tokens":135,"id":2091,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) to document significant architectural decisions for this project. Each ADR will be a single Markdown file stored within a dedicated \"adr\" directory at the root level of the project. We will follow a lightweight ADR template inspired by  Michael Nygard's template, including sections for:\n\n- **Title**: A concise and descriptive title for the decision.\n- **Status**: Proposed, Accepted, Rejected, Superseded, Amended, Deprecated, etc.\n- **Context**: The background and motivation for the decision.\n- **Decision**: The actual decision being made.\n- **Consequences**: The expected positive and negative impacts of the decision.\n\nThis lightweight approach balances the need for documentation with the size and scope of the project. As a single-engineer side project, we prioritize agility and avoid excessive overhead.\n","GenTime":"2024-08-19 15:25:35","text":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n"}
{"File Name":"dogma\/0018-projection-compaction.md","Context":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n","Decision":"We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","tokens":210,"id":1606,"Prediction":"## Decision\n\nWe will implement a generic time-to-live (TTL) mechanism for projections. This mechanism will allow application developers to specify a retention period for projection data based on a timestamp column. \n\n**Technical Implementation:**\n\n* A new `ttl` configuration option will be added to the projection definition. This option will accept:\n    * A duration string (e.g., \"1 hour\", \"3 days\", \"2 weeks\").\n    * The name of a column in the projection table containing a timestamp. \n* A background process will be introduced to periodically scan projections configured with the `ttl` option. \n* For each projection, the process will delete rows where the timestamp column value plus the configured `ttl` is older than the current time.\n\n**Specific to the \"cool down\" scenario:**\n\n* The SQL projection for the \"cool down\" functionality will include a timestamp column recording the action execution time.\n* The `ttl` option for this projection will be set to the desired \"cool down\" duration.\n\n**Benefits:**\n\n* Provides a generic and reusable solution for managing data retention in projections.\n* Avoids the need for application-specific logic to remove outdated data.\n* Improves performance and reduces storage costs by regularly purging unnecessary data.\n\n**Considerations:**\n\n* The background process needs to be configured carefully to balance performance impact with data retention requirements.\n* Developers need to be aware of the `ttl` option and utilize it appropriately when designing projections.\n* Data deletion by the background process might lead to slight inconsistencies for a short period, which should be acceptable considering the nature of \"cool down\" periods. \n","GenTime":"2024-08-19 15:25:43","text":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n\n\n##Decision\nWe have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n"}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"Prediction":"## Decision\n\nWe will implement a Continuous Integration and Continuous Delivery (CI\/CD) pipeline using [**Choose a CI\/CD tool** e.g., Jenkins, GitLab CI, CircleCI, etc.]. This pipeline will be triggered on every code commit and will perform the following:\n\n1. **Build:** Compile and build the codebase.\n2. **Test:** Run automated unit, integration, and end-to-end tests.\n3. **Deploy (Optional):** Automatically deploy the application to a staging environment for further testing or to production if all tests pass and predefined conditions are met. \n\nThis approach offers several benefits:\n\n* **Faster Integration:**  Developers receive immediate feedback on code changes, facilitating quicker identification and resolution of integration issues.\n* **Increased Reliability:** Automated testing at each stage ensures a higher level of code stability and reduces the risk of introducing bugs.\n* **Faster Delivery:**  Automated deployment streamlines the release process, enabling faster and more frequent delivery of new features and bug fixes.\n* **Improved Collaboration:**  The CI\/CD pipeline fosters a collaborative development environment by providing a transparent and consistent process for integrating and testing code changes.\n\n\nWe will select a CI\/CD tool based on our specific requirements, team familiarity, and budget constraints.\n","GenTime":"2024-08-19 15:25:48","text":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n\n\n##Decision\nWe choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n"}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"Prediction":"## Decision\n\nWe will implement a two-tiered permission system that evaluates both resource-specific authorizations and ACLs to determine access permissions for a given user on a work or collection. \n\n**Tier 1: Resource-Specific Permissions**\n\n- Each work and collection will maintain a record of permissions granted directly by the author (depositor or proxy depositor). \n- These permissions will be specific to individual users or groups. \n- This tier will take precedence over ACLs, allowing authors to maintain fine-grained control over their resources.\n\n**Tier 2: Access Control Lists (ACLs)**\n\n- ACLs will be defined at different levels (e.g., system, collection, work) to manage access based on user roles and groups. \n- The system will evaluate applicable ACLs based on the user's identity and group memberships. \n- ACLs will provide a flexible mechanism for managing broader access patterns and administrative privileges.\n\n**Combined Evaluation:**\n\n- When a user requests access to a resource, the system will first check for resource-specific permissions granted by the author. \n- If no specific permission is found or the request falls outside its scope, the system will evaluate applicable ACLs. \n- Access will be granted only if both tiers permit the requested action.\n\nThis two-tiered approach provides a balance between granular author control and scalable access management through roles and groups. \n","GenTime":"2024-08-19 15:25:55","text":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n\n##Decision\nAccess controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n"}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"Prediction":"## Decision\n\nTo fulfill the requirements of user and permission management in Armadillo, we will implement a Role-Based Access Control (RBAC) system with the following characteristics:\n\n**Cohort Management:**\n\n* **Cohort Creation:** Representatives of the consortium will create cohorts as OpenID applications within Armadillo. They will define the application scope and configure the necessary permissions for data access within the shared folder `<study>`.\n* **Researcher Admission:** Representatives of each cohort will manage the admission of researchers to their respective cohorts. They will be responsible for assigning the `ROLE_<STUDY>_RESEARCHER` role to authorized users within their cohort's OpenID application. \n\n**User and Permission Management:**\n\n* **Federated Identity:** Armadillo will utilize a federated identity system, allowing users to authenticate with their local institution credentials. This ensures centralized identity management and simplifies user access.\n* **Role Assignment:** User roles will be assigned at the cohort level. Representatives of the consortium will have visibility into all roles assigned across all cohorts through a centralized dashboard. This dashboard will allow them to view which users have which roles on which cohort for auditing and management purposes.\n* **Data Access Control:** Access to shared folders will be enforced through the assigned roles. Only users with the `ROLE_<STUDY>_RESEARCHER` role on a specific cohort will be granted read access to the corresponding `<study>` shared folder.\n\n**Rationale:**\n\nThis approach leverages the existing OpenID infrastructure within Armadillo while providing a clear separation of concerns regarding user and permission management. By delegating researcher admission to cohort representatives and providing consortium representatives with oversight through a centralized dashboard, we ensure a secure and manageable access control model that adheres to the specified requirements.\n","GenTime":"2024-08-19 15:26:02","text":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n\n\n##Decision\nImplement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n"}
{"File Name":"origin\/0010-more-complete-database-type-and-adjust-migrations.md","Context":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n","Decision":"A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","tokens":64,"id":1561,"Prediction":"## Decision\n\nMigrate from SQLite to a more robust and scalable database solution, such as PostgreSQL. This decision is driven by the need to support production-level workloads and ease the transition for developers using the Origin SDK. \n\n**Justification:**\n\n- **Scalability and Performance:** SQLite, while easy to use, is not designed for high-volume, concurrent environments expected in a production setting. PostgreSQL offers better scalability, performance, and concurrency handling.\n- **Production Readiness:** Transitioning to PostgreSQL aligns Origin with industry best practices for production-ready applications, bolstering its reliability and appeal to developers.\n- **Ecosystem and Tooling:** PostgreSQL benefits from a mature ecosystem of tools, libraries, and community support, facilitating easier development, monitoring, and maintenance.\n- **Future Growth:**  Adopting PostgreSQL provides a scalable foundation for Origin's anticipated growth in users, data volume, and feature complexity.\n\n**Consequences:**\n\n- **Migration Effort:**  A well-planned migration process will be required to transition data and adapt the application codebase to interact with PostgreSQL.\n- **Operational Overhead:** Managing a PostgreSQL instance may require additional infrastructure and expertise compared to SQLite.\n\n**Alternatives Considered:**\n\n- **Staying with SQLite:**  This option was dismissed due to scalability and production readiness limitations.\n- **Other Database Solutions (MySQL, MongoDB, etc.):**  While other options were explored, PostgreSQL emerged as the preferred choice due to its balance of performance, scalability, and a strong reputation within the developer community.\n\n**Status:** Accepted\n\n**Date:** 2023-10-26 \n","GenTime":"2024-08-19 15:26:10","text":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n\n\n##Decision\nA decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n"}
{"File Name":"libelektra\/error_codes.md","Context":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n","Decision":"All \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","tokens":312,"id":1319,"Prediction":"## Decision\n\nTo address the identified problems with the current error concept, we will introduce a new hierarchical error model for the software. This model will focus on:\n\n1. **Error Categorization and Consolidation:** Similar errors will be grouped into categories and subcategories, significantly reducing redundancy and promoting consistency.  This will involve analyzing existing error codes, identifying commonalities, and mapping them to a new hierarchical structure. \n2. **Standardized Specification and Management:** We will define a clear and concise specification for defining and documenting errors within the new hierarchical structure. This specification will be enforced through tooling and automation (e.g., code generation, linting) to ensure consistency and minimize merge conflicts.\n3. **Hierarchical Error Handling for Applications:** The new error model will enable developers to handle errors at different levels of granularity. They can choose to handle specific error codes or utilize broader categories for more general error handling logic. This will provide greater flexibility and simplify error handling for plugin and application developers.\n\nThis decision prioritizes maintainability, consistency, and ease of use for both developers contributing to the core software and those building upon it. The implementation details of the new error model, including the specific hierarchy and specification format, will be further elaborated in a separate design document. \n","GenTime":"2024-08-19 15:26:15","text":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n\n\n##Decision\nAll \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n"}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"Prediction":"## Decision\n\n**Option:** Silently create a package lock file as part of the pipeline \n\n**Reason:** While committing the `package-lock.json` is the recommended practice, enforcing it by refusing to build the project can be too disruptive, especially for legacy projects. Silently creating the file allows the pipeline to proceed without developer intervention while still enabling crucial commands like `npm ci` and `npm audit`.\n\n**Implications:**\n\n* **Reproducibility:**  This approach ensures consistent builds across environments as `npm ci` will install the exact dependency tree specified in the automatically generated lock file.\n* **Security:** Enables running `npm audit` in the pipeline, which is crucial for identifying and mitigating security vulnerabilities in dependencies. \n* **Maintainability:** Avoids disrupting the pipeline and causing unnecessary friction for developers, particularly on projects where committing the lock file wasn't standard practice.\n* **Potential for Discrepancies:** If developers are used to working without a lock file locally, there's a chance their local environment might differ from the CI environment. \n\n**Mitigation for Potential Discrepancies:**\n\n* **Documentation:** Clearly document the pipeline's behavior of automatically generating the lock file and emphasize the importance of running `npm install` locally after a fresh checkout or when dependencies change. \n* **Post-build Action:** Consider adding a step to the pipeline that compares the automatically generated `package-lock.json` with the repository's root. If a discrepancy is found, surface a warning message to the developers, prompting them to update their local lock file by running `npm install`.\n\nThis decision prioritizes the stability and security of the pipeline while acknowledging the potential for discrepancies with local development environments. By mitigating these discrepancies through documentation and potential pipeline enhancements, we can ensure a smoother experience for developers while reaping the benefits of a consistent and secure build process. \n","GenTime":"2024-08-19 15:26:23","text":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n\n\n##Decision\nIf a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n"}
{"File Name":"court-case-service\/0005-guiding-principles.md","Context":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n","Decision":"1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","tokens":129,"id":3579,"Prediction":"## Decision\n\nWe will implement `court-case-service` as a **general-purpose API** that can be consumed by multiple clients, including but not limited to `prepare-a-case`. This decision prioritizes **reusability**, **maintainability**, and **loose coupling** over the potential performance benefits of a dedicated gateway.\n\n## Reasons\n\n* **Reusability:** Designing `court-case-service` as a general-purpose API allows for future reuse by other applications or services that require access to court case data. This avoids code duplication and promotes consistency in data access patterns.\n* **Maintainability:** A general-purpose API with a well-defined contract simplifies maintenance and evolution. Changes to the API can be made independently of specific clients, as long as the contract is respected.\n* **Loose Coupling:** Decoupling `court-case-service` from `prepare-a-case` reduces the impact of changes in one service on the other. This promotes independent development and deployment cycles.\n* **Flexibility:**  A general API provides flexibility in how clients consume and process the data. `prepare-a-case` can implement its own error handling and data transformation logic based on its specific needs.\n\n## Implications\n\n* **Error Handling:** `prepare-a-case` will be responsible for implementing robust error handling and graceful degradation mechanisms to handle partial failures from `court-case-service`.\n* **API Design:** The API design for `court-case-service` should be carefully considered to cater to a wider range of potential consumers while maintaining security and performance.\n* **Documentation:** Clear and comprehensive documentation is crucial for a general-purpose API to facilitate adoption and proper usage by different clients.\n\n## Alternatives Considered\n\n* **Dedicated Gateway:** Implementing `court-case-service` as a dedicated gateway for `prepare-a-case` could offer performance benefits due to tighter integration and specialized optimizations. However, this approach would sacrifice reusability, maintainability, and loose coupling.\n\n## Decision Outcome\n\nThis decision promotes a microservices architecture where services are loosely coupled, reusable, and independently deployable. While it requires `prepare-a-case` to handle error scenarios diligently, it paves the way for a more scalable and maintainable system in the long run. \n","GenTime":"2024-08-19 15:26:33","text":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n\n\n##Decision\n1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n"}
{"File Name":"skypy\/adr-01.md","Context":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"Prediction":"## Decision\n\nWe will implement the `Model` functionality as a decorator. This decision prioritizes ease of use and clarity of implementation. \n\n**Rationale:**\n\n* **Ease of use:** Decorators offer a clean and Pythonic way to add functionality to functions without modifying their core logic. The `redshift_gamma` example demonstrates how a decorator can seamlessly integrate with existing code, enhancing readability and reducing boilerplate. \n* **Clarity of implementation:** Decorators, when well-structured, provide a transparent way to understand how the `Model` functionality is applied. The core logic of the decorated function remains separate, while the decorator handles the table manipulation and DAG construction behind the scenes.\n\n**Example:**\n\n```python\nfrom skypy.core import Model\n\n@Model(inputs=['shape', 'scale', 'size'], outputs=['redshift'])\ndef redshift_gamma(shape, scale, size):\n  \"\"\"Gamma-distributed redshifts (Smail et al. 1994).\n\n  Sample `size` redshifts from a gamma distribution with the\n  given `shape` and `scale` parameters. See `numpy.random.gamma`.\n  \"\"\"\n  # redshift distribution\n  redshift = np.random.gamma(shape=shape, scale=scale, size=size)\n  return redshift\n```\n\nIn this example, the `@Model` decorator clearly defines the inputs and outputs of the `redshift_gamma` function, allowing the framework to automatically handle table creation\/modification and DAG construction. The core logic of the function remains untouched, promoting maintainability and readability.\n\n**Consequences:**\n\n* Requires careful design of the decorator to ensure flexibility and avoid excessive complexity.\n* Might introduce a slight learning curve for developers unfamiliar with Python decorators.\n\nThis decision strikes a balance between ease of use, clarity, and flexibility, making the `Model` concept intuitive and powerful for building simulations in SkyPy. \n","GenTime":"2024-08-19 15:26:41","text":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n\n\n##Decision\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n"}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"Prediction":"## Decision\n\n**Implement an Idempotent Order Processing Pipeline with Guaranteed Delivery and Deduplication**\n\n**Details:**\n\n1. **Guaranteed Delivery:** Use a message queue (e.g., Kafka, RabbitMQ) to ensure \"at least once delivery\" of \"Ready to pay\" orders from the order store to the payment processing service. The queue will persist messages and retry delivery in case of failures, guaranteeing that the payment processor receives the order information.\n\n2. **Idempotency and Deduplication:** Implement idempotent payment processing. This means ensuring that processing the same order multiple times has the same effect as processing it once.  To achieve this:\n\n    * **Unique Order IDs:** Each order will have a unique and immutable order ID generated by the order store.\n    * **Idempotency Key:** The payment processor will use the unique order ID as an idempotency key.\n    * **Deduplication Logic:** Before processing a payment, the payment processor will check if an order with the same idempotency key has already been successfully processed. If so, it will return a success response without recharging the user.\n\n3. **Transactionality:** Utilize database transactions to ensure atomicity. When an order is marked as \"Ready to pay,\" the operation of adding it to the message queue and updating its status in the order store will be performed within a single transaction. This prevents inconsistencies in case of failures.\n\n**Consequences:**\n\n* **Improved Reliability:** Guaranteed delivery of orders to the payment processor reduces the risk of missed payments.\n* **Prevented Double Payments:**  Idempotent processing and deduplication logic eliminate the risk of charging users multiple times for the same order.\n* **Increased Complexity:** Implementing a message queue, idempotency, and transactional operations adds complexity to the system.\n\n**Alternatives Considered:**\n\n* **Relying on Database Constraints:** While database constraints can prevent duplicate entries, they don't guarantee message delivery and might not be sufficient for distributed systems.\n* **Optimistic Locking:** This approach can handle concurrency but might lead to retries and reduced performance under high load. \n\n**Rationale:**\n\nThis solution prioritizes reliability and data integrity, which are crucial for the business-critical scenario of order payment processing. The added complexity is justified by the need to guarantee both delivery and the prevention of double payments.\n","GenTime":"2024-08-19 15:26:51","text":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n\n\n##Decision\nDelivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n"}
{"File Name":"android-guidelines\/0009-gradle-rules.md","Context":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n","Decision":"* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","tokens":37,"id":3860,"Prediction":"## Decision\n\nTo address the issue of increasing build times due to the growing number of modules, unused plugins, and the use of **gradle.kts** files, we will implement the following changes:\n\n1. **Modularization Analysis and Optimization:**\n    * Conduct a thorough analysis of the project's module structure to identify any unnecessary dependencies or modules that can be consolidated or removed.\n    * Investigate the possibility of implementing dynamic feature modules for features that are not critical for the app's core functionality. This allows for on-demand downloading and installation, reducing the initial build and installation time. \n\n2. **Plugin Optimization:**\n    * Evaluate all plugins used in the project and disable any plugins that are not essential for the build process.\n    * Research and utilize plugin alternatives that are known for faster performance. \n\n3. **Gradle Script Optimization:**\n    * Profile the build process to identify performance bottlenecks within the **gradle.kts** files. \n    * Optimize Gradle scripts by using static imports, avoiding unnecessary computations during configuration, and leveraging Gradle's configuration caching feature where applicable.\n    * Evaluate the trade-offs of switching from **gradle.kts** to **build.gradle** (Groovy) for build scripts. While Kotlin offers type safety and IDE support, Groovy might provide faster compilation times in some cases.\n\n4. **Build Infrastructure Improvements:**\n    * Explore the option of upgrading the build server hardware (CPU, RAM, storage) to improve build speeds. \n    * Consider implementing a remote build cache to share build outputs across developers and CI\/CD pipelines.\n\n5. **Continuous Monitoring and Benchmarking:**\n    * Implement build time monitoring and reporting to track build performance over time.\n    * Regularly benchmark build times against different optimization strategies to measure their effectiveness.\n\nThis multifaceted approach aims to significantly reduce build times while ensuring the project remains maintainable and scalable. \n","GenTime":"2024-08-19 15:27:00","text":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n\n##Decision\n* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n"}
{"File Name":"verify-frontend\/0004-journey-hint-tracking-cookie.md","Context":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n","Decision":"While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","tokens":273,"id":1449,"Prediction":"## Decision\n\nWe will refactor the `verify-front-journey-hint` cookie to separate its original purpose of non-repudiation journey support from the newly added sign-in hint functionality. This separation will improve code maintainability, reduce complexity, and allow for independent evolution of both features.\n\nHere's how we will achieve this:\n\n1. **Create a new cookie dedicated to sign-in hints.** This new cookie will store information related to successful IDP logins, including the IDP entity ID and potentially a timestamp for expiry management.\n2. **Refactor the existing `verify-front-journey-hint` cookie.** This cookie will be reverted to its original purpose, storing only the `entity_id` and maintaining its original, shorter expiry time.\n3. **Update the relevant code sections.** This includes updating code responsible for:\n    * Writing and reading from both cookies.\n    * Displaying the sign-in hint based on the new cookie.\n    * Handling the non-repudiation journey using the original cookie.\n\nThis separation will bring several benefits:\n\n* **Improved code organization and readability.**  By having dedicated cookies, the code related to each feature will be easier to understand and maintain.\n* **Reduced coupling and complexity.** Separating the functionalities will decrease the interdependence between different parts of the codebase, making it easier to modify or extend each feature independently.\n* **Enhanced security.**  By reducing the amount of information stored in the original cookie and shortening its lifespan, we can mitigate potential security risks associated with storing sensitive data for extended periods.\n\nThis refactoring will ensure the long-term maintainability and scalability of the codebase while improving its security posture. \n","GenTime":"2024-08-19 15:27:07","text":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n\n\n##Decision\nWhile implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n"}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) to document significant architectural decisions for this project. Each ADR will be a lightweight text file following a standardized template (e.g., MADR, YADAS) to ensure consistency and easy searchability. These ADRs will be version-controlled alongside the codebase in a dedicated directory (\/docs\/adr). \n","GenTime":"2024-08-19 15:27:09","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"Prediction":"## Decision\n\nWe will use an event-driven architecture with a lightweight message broker (e.g., Redis Pub\/Sub, NATS) for communication between services. This promotes loose coupling and extensibility as services interact indirectly through events, allowing for easy addition or modification of components without impacting others.\n\nTo ensure a quick and easy setup, we will provide:\n\n* **Docker Compose configuration:** This will allow developers to spin up the entire system locally with a single command.\n* **Example code and documentation:** Clear, concise documentation and practical examples will be provided to demonstrate the core concepts of the architecture and facilitate rapid onboarding.\n* **Simplified initial services:**  Initially, we'll focus on implementing basic services with minimal dependencies to showcase the core principles of the architecture without overwhelming newcomers.\n\n**Trade-offs:**\n\n**Advantages:**\n\n* **Flexibility and Extensibility:** Easier to adapt to changing requirements and integrate new features.\n* **Loose Coupling:** Reduced dependencies between services, making development and deployment smoother.\n* **Scalability:**  Event-driven systems can scale horizontally with relative ease.\n\n**Disadvantages:**\n\n* **Increased Complexity:** Debugging and tracing events across multiple services can be challenging.\n* **Eventual Consistency:** Data consistency might not be immediate, requiring careful handling in certain scenarios.\n* **Learning Curve:** Developers familiar with traditional architectures might need time to adjust to the event-driven paradigm.\n\n**Justification:**\n\nWhile there's a learning curve, the advantages of flexibility, loose coupling, and scalability outweigh the initial complexities.  Providing a streamlined developer experience with easy setup and clear examples will help mitigate the challenges of adopting this architecture.\n","GenTime":"2024-08-19 15:27:17","text":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n\n\n##Decision\nWe will use a docker-compose.yml (v.2) as the primary entry point of the project\\n"}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"Prediction":"## Decision\n\nWe will use Auth0, a third-party authentication and authorization platform, to manage Link Platform Administrator authentication and authorization.\n\n## Reasons\n\n* **Reduced Development Time:**  Auth0 provides a pre-built, secure, and scalable solution, significantly reducing the development time and effort required to implement and maintain an in-house authentication system.\n* **Enhanced Security:** Auth0 offers industry-standard security features such as multi-factor authentication, password management, and breach detection, bolstering the security of Link Platform and its data.\n* **Simplified User Management:**  Auth0's centralized dashboard simplifies user management tasks like adding, removing, and managing administrator accounts, roles, and permissions. \n* **Flexibility & Integration:** Auth0 integrates seamlessly with various identity providers and technologies, providing flexibility for future integrations and growth.\n\n## Consequences\n\n* **Third-Party Dependency:** The platform will rely on Auth0 for a critical function, introducing a potential single point of failure and dependency on the provider's uptime and security.\n* **Cost Implications:** Utilizing Auth0's services will incur subscription costs, potentially impacting the overall budget of the Link Platform.\n\n## Status\n\nAccepted\n\n## Action Items\n\n* Evaluate Auth0's pricing plans and select the most suitable option for the Link Platform.\n* Create an Auth0 tenant and configure it for the Link Platform.\n* Implement Auth0 integration within the Link Platform's codebase.\n* Define roles and permissions for Link Platform administrators within Auth0.\n* Develop documentation and training materials for administrators on accessing and managing the Link Platform using Auth0. \n","GenTime":"2024-08-19 15:27:24","text":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n\n##Decision\n[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n"}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each record will adhere to a consistent template (e.g., MADR, Y-ADRs) to ensure clarity and searchability.  We will store these ADRs in a dedicated directory within the project's version control system (e.g., \/docs\/adr) to maintain a clear history of decisions and facilitate collaboration. \n","GenTime":"2024-08-19 15:27:26","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"cf-k8s-networking\/0016-job-for-upgrading-istio-sidecars-on-workloads.md","Context":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n","Decision":"We will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","tokens":224,"id":2240,"Prediction":"## Decision\n\nWe will not be using Istio's sidecar injection model for cf-for-k8s. \n\nWhile Istio offers compelling features, the requirement to roll all pods after an Istio upgrade is untenable for cf-for-k8s. This approach would result in significant disruption for users and contradict the operational expectations of cf-for-k8s operators who rely on a single `kapp deploy` for upgrades. \n\nInstead of relying on Istio sidecars, we will investigate alternative solutions for achieving the desired functionality. This may involve:\n\n* **Exploring alternative service mesh solutions:**  Evaluate other service meshes that offer similar capabilities to Istio but with more flexible upgrade paths that align with cf-for-k8s operational requirements. \n* **Implementing specific functionalities independently:** Determine which Istio features are essential for cf-for-k8s and explore implementing those functionalities using alternative tools and approaches. This might involve using standalone proxies, service discovery mechanisms, or other Kubernetes-native features. \n* **Collaborating with the Istio community:** Engage with the Istio community to explore potential solutions or roadmap alignments that could address the upgrade challenges within the context of cf-for-k8s.\n\nThis decision prioritizes the operational stability and upgrade simplicity of cf-for-k8s. We will continue to monitor the Istio project and re-evaluate its suitability as the project evolves and potential solutions emerge. \n","GenTime":"2024-08-19 15:27:33","text":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n\n\n##Decision\nWe will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n"}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"Prediction":"## Decision\n\nWe will use **react-testing-library** for integration tests to focus on testing user interactions and avoid testing implementation details. While this library allows for DOM assertions, we will prioritize asserting changes to the state\/model over verifying the rendered DOM directly. We trust that, in most cases, the view will render the model correctly. Our primary use of react-testing-library will be to simulate user interactions like clicking UI elements and then asserting that the expected state changes occur as a result. This approach will lead to more robust and maintainable tests by preventing unnecessary coupling to specific DOM structures. \n","GenTime":"2024-08-19 15:27:36","text":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n\n\n##Decision\nWe will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n"}
{"File Name":"blueprint\/0003-feature-selection-policy.md","Context":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n","Decision":"* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":90,"id":3551,"Prediction":"## Decision\n\nWe will maintain alignment with the Open Data Hub v0.8.0 release for the majority of components. However, to address the immediate user needs for specific Elyra features essential for Thoth Station experiments, we will selectively update the relevant components to the HEAD of the main branch of the ODH upstream project. \n\nThis approach represents a compromise between stability and functionality:\n\n* **Stability:** By sticking with the ODH v0.8.0 release as much as possible, we minimize operational complexity and potential for unforeseen issues. \n* **Functionality:**  The targeted updates to specific ODH components ensure we can deliver the necessary Elyra features for Thoth Station experiments. \n\nThis decision will be reviewed regularly, taking into account new ODH releases, evolving user needs, and the operational overhead of maintaining the selectively updated components. \n","GenTime":"2024-08-19 15:27:40","text":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n\n\n##Decision\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n"}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"Prediction":"## Decision\n\nStreams **will be readable within the same transaction** as the `loadStream()` call. This approach effectively addresses the concerns of resource leaks under transaction retries and ensures data consistency by coupling stream reads with potential subsequent write operations within a single transactional boundary. \n\nThis decision prioritizes:\n\n* **Resource Safety:** Guaranteeing streams are closed even under transaction retries, preventing potential resource leaks.\n* **Data Consistency:** Enabling reads and writes based on the stream data to reside within the same transaction, maintaining data integrity.\n* **Simplified Error Handling:**  Failed reads within the transaction contribute to the overall transaction failure, offering a unified error handling mechanism. \n","GenTime":"2024-08-19 15:27:44","text":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n\n\n##Decision\nReading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n"}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"Prediction":"## Decision\n\nWe will use **TypeScript** to add static types to the dotcom-rendering project.\n\n## Reasoning\n\nWhile Flow offers a seemingly easier entry point for legacy codebases, TypeScript provides a more robust and developer-friendly experience in the long run. \n\nThe following factors contributed to this decision:\n\n* **Clarity and Maintainability**: TypeScript's error messages are clearer and easier to understand, leading to faster debugging and reduced cognitive load. Its fine-grained configuration allows for better control over type checking and minimizes the need for workarounds like opt-outs and `any` types, contributing to cleaner and more maintainable code.\n* **Ecosystem and Tooling**: The mature TypeScript ecosystem offers better tooling support, pre-built type definitions for a wider range of third-party libraries, and an active community, which translates to higher productivity and fewer integration headaches.\n* **Language Features and Type Safety**: TypeScript's nature as a language allows it to introduce improvements to JavaScript, making it more type-friendly. Its default exact type system provides a higher level of type safety compared to Flow.\n\nWhile the initial migration effort might be slightly higher with TypeScript, the long-term benefits in terms of code quality, maintainability, and developer productivity outweigh the initial cost. \n","GenTime":"2024-08-19 15:27:49","text":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n\n##Decision\nUse TypeScript for static typing.\\n"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"Prediction":"## Decision\n\nCloud Security Watch will be tightly coupled to the AWS ecosystem and will not be designed with cloud agnosticism in mind. This means the tool will leverage AWS-specific services, APIs, and configurations for monitoring and identifying misconfigurations. \n","GenTime":"2024-08-19 15:27:51","text":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n\n\n##Decision\nWhilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"Prediction":"## Decision\n\nWe will use **Jest** as our primary testing framework due to its strong support for TypeScript, excellent performance, and robust mocking capabilities. This choice aligns well with our existing Node.js environment and provides a rich ecosystem of resources and tooling. \n","GenTime":"2024-08-19 15:27:53","text":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n"}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"Prediction":"## Decision\n\n**Introduce a versioned history model for documents and images.**\n\nThis model should allow us to store:\n\n* **Multiple versions of a document and its associated images:** This addresses the issue of overwriting data when a new draft is created and allows us to maintain a complete history of the document.\n* **Metadata about each version:** This includes information like the creation date, author, and publication status. \n* **Specific changes made between versions (diffs):** This enables features like comparing editions and highlighting specific user changes.\n\n**Migrate relevant information from TimelineEntry to the versioned history.**\n\nInformation about actions like document publication, withdrawal, or removal should be migrated to the versioned history model. This ensures that the TimelineEntry model is focused solely on presenting a chronological timeline of events and avoids data duplication.\n\n**Update Content Publisher features to leverage the versioned history.**\n\nExisting features, such as displaying document status and links to live editions, should be updated to use the versioned history. This ensures data accuracy and consistency across the application.\n\n**Prioritize the development of features enabled by the versioned history.**\n\nFeatures like comparing document editions, republishing live content, and showing user-specific changes should be prioritized as they directly address the identified pain points and user needs.\n\nThis approach provides a more robust and flexible solution for managing content history in Content Publisher. It enables the development of desired features, addresses existing pain points, and provides a solid foundation for future enhancements. \n","GenTime":"2024-08-19 15:28:00","text":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n\n\n##Decision\nThis ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"Prediction":"## Decision\n\nA VPN connection will be established between the user's environment and the JASMIN tenancy. This will allow access to the Datalabs network servers without exposing any services outside of the JASMIN environment. \n\nThis approach provides a good balance between security and accessibility. \n","GenTime":"2024-08-19 15:28:01","text":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n\n\n##Decision\nWe have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n"}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"Prediction":"## Decision\n\nWe will implement an ephemeral environment strategy for pull requests (PRs) within the Development AWS account. This strategy will provision a dedicated environment for each PR, allowing for isolated automated and user acceptance testing. \n\n**Here's a breakdown of the decision:**\n\n* **Ephemeral Environments:** Every PR will trigger the creation of a temporary, isolated environment mirroring production as closely as possible.\n* **Automated Provisioning:** We will utilize Infrastructure as Code (IaC) tools like AWS CloudFormation or Terraform to automate the environment provisioning and teardown process. \n* **Environment Isolation:** Each PR environment will have its own dedicated resources (e.g., EC2 instances, databases, S3 buckets) with unique namespaces to avoid conflicts.\n* **Automated Testing:** Continuous Integration\/Continuous Delivery (CI\/CD) pipelines will automatically deploy code changes to the PR environment and execute pre-defined automated test suites.\n* **User Acceptance Testing (UAT):** Testers and stakeholders will receive temporary access to the PR environment for manual testing and validation. \n* **Artifact and Log Retention:**  Before tearing down the environment, crucial logs, test results, and artifacts will be persisted to a central location (e.g., S3 bucket, logging service) for analysis and debugging purposes.\n* **Environment Cleanup:** Once the PR is merged or closed, the corresponding environment will be automatically de-provisioned to optimize resource utilization and costs.\n\n**This strategy offers several benefits:**\n\n* **Improved Developer Productivity:** Provides immediate feedback on code changes and facilitates rapid iteration.\n* **Enhanced Collaboration:** Enables testers and stakeholders to evaluate features and provide feedback early in the development cycle.\n* **Higher Quality Code:** Automated and manual testing in dedicated environments increases the likelihood of catching bugs before they reach production. \n* **Cost Optimization:** Ephemeral environments ensure that resources are only consumed when needed. \n\n**However, there are also considerations:**\n\n* **Implementation Complexity:** Setting up the automation for provisioning, testing, and teardown requires careful planning and configuration.\n* **Resource Limits:** The number of concurrent PR environments might be limited by the available resources in the Development AWS account.\n* **Cost Management:** While ephemeral environments optimize costs, it's crucial to monitor resource usage and implement cost control mechanisms.\n\nThis decision lays the groundwork for a robust and efficient development workflow. By embracing ephemeral environments, we can enhance collaboration, improve code quality, and accelerate our delivery cycles. \n","GenTime":"2024-08-19 15:28:12","text":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n\n\n##Decision\n### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n"}
{"File Name":"james\/0002-make-taskmanager-distributed.md","Context":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2127,"Prediction":"## Decision\n\nWe will introduce a **distributed task management system** to enable cluster-wide task orchestration and visibility in James. This system will consist of the following components:\n\n1. **Centralized Task Queue:** A shared, persistent queue will be introduced to store all `Task`s submitted to the James cluster. This queue will be accessible by all nodes, providing a single source of truth for task management. \n\n2. **Task Scheduler:** A dedicated task scheduler component will be responsible for dequeuing tasks from the central queue and distributing them to appropriate nodes for execution. The scheduler can employ various strategies for task assignment, such as load balancing or affinity-based routing.\n\n3. **Distributed Task Execution:**  Each James node will be capable of executing tasks assigned by the scheduler. Nodes will communicate with the central queue to update task status and report completion.\n\n4. **Task Metadata Storage:** A persistent storage mechanism (e.g., database) will be introduced to store metadata about each `Task`, including its status, execution history, and associated data. This information will be accessible through an API, enabling cluster-wide task monitoring and management.\n\nThis approach provides several benefits:\n\n* **Cluster-wide Task Visibility:**  All tasks are centrally managed and their status is accessible, enabling monitoring of the entire cluster's workload.\n* **Scalability and Load Balancing:** The scheduler can distribute tasks efficiently across the cluster, ensuring optimal resource utilization and scalability.\n* **Fault Tolerance:** If a node fails, tasks assigned to it can be re-queued and executed by another node, ensuring high availability.\n\nThis ADR will be further detailed with specific implementation choices and trade-off analysis in subsequent documentation. \n","GenTime":"2024-08-19 15:28:19","text":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n\n##Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n"}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"Prediction":"## Decision\n\nWe will implement a deployment strategy for hosted 3Bots that utilizes pre-built flists from the CI pipeline. This approach aims to significantly reduce deployment time by eliminating the need for on-the-fly cloning and installation. \n\nInstead of building the 3Bot from scratch on each deployment, the deployment process will pull the latest stable flist artifact generated by the CI pipeline. This flist will contain the pre-built 3Bot environment, including all necessary dependencies and configurations.\n\nThis strategy offers several benefits:\n\n* **Faster deployments:**  Eliminating the cloning and installation steps will significantly reduce the overall deployment time.\n* **Improved consistency:** Utilizing pre-built flists ensures that each deployment uses a consistent and validated build of the 3Bot.\n* **Reduced deployment failures:** By relying on pre-tested artifacts from the CI pipeline, we minimize the risk of deployment failures due to inconsistent environments or build issues.\n\n## Consequences\n\n* **CI\/CD pipeline modifications:** The CI\/CD pipeline needs adjustments to build and publish the 3Bot flist artifact to a readily accessible location for the deployment process.\n* **Deployment script changes:**  The deployment scripts must be modified to pull and utilize the pre-built flist instead of performing a fresh installation.\n* **Flist management:** A strategy for versioning, storing, and managing the flist artifacts needs to be established. This might involve utilizing artifact repositories or dedicated storage solutions. \n* **Rollbacks:** The rollback strategy might need to be adapted to leverage previous flist versions in case of issues.\n\nThis decision prioritizes deployment speed and consistency. However, it introduces a dependency on the CI pipeline for successful deployments. \n","GenTime":"2024-08-19 15:28:27","text":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n\n\n##Decision\nUpdate the flist with the up to date one\\n"}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"Prediction":"## Decision\n\nImplement a Time-To-Live (TTL) mechanism for both **enqueuedMailsV3** and **deletedMailsV2** topics to address privacy and storage cost concerns. \n\nThis decision entails the following:\n\n1. **Configure TTL for each topic:** Define a suitable TTL duration for both **enqueuedMailsV3** and **deletedMailsV2**. This duration should be determined based on legal requirements for data retention, business needs, and the acceptable latency for email processing.\n2. **Leverage Kafka's built-in TTL feature:** Implement the TTL mechanism using Kafka's built-in message timestamp-based retention policy. This will automatically delete messages exceeding the configured TTL, effectively cleaning up the topics and associated blobs.\n3. **Monitor and adjust:** Continuously monitor the storage consumption and TTL effectiveness for both topics. Adjust the TTL durations if necessary to achieve optimal balance between data retention needs and resource utilization. \n\nThis approach provides an efficient and automated solution for managing data lifecycle in the distributed mail queue, ensuring privacy compliance and controlling storage costs without requiring complex custom logic. \n","GenTime":"2024-08-19 15:28:32","text":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n\n\n##Decision\nAdd a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"Prediction":"## Decision\n\nWe will implement a process to ensure that all namespaces are using the same version of our terraform modules. This will involve:\n\n* **Centralized Module Registry:** Establishing a centralized registry for all terraform modules, enabling versioning and controlled access. \n* **Standardized Release Process:** Implementing a clear and consistent release process for new module versions, including documentation of changes and upgrade instructions.\n* **Mandatory Upgrade Policy:**  Defining and enforcing a mandatory upgrade policy for all namespaces, ensuring they remain up-to-date with the latest module version.\n* **Automated Tooling:** Investigating and implementing tooling to automate the process of upgrading modules across namespaces, minimizing manual effort and risk.\n* **Communication and Support:** Establishing clear communication channels and providing support to teams during the upgrade process.\n\nThis approach prioritizes consistency and maintainability, reducing the risk and complexity associated with managing multiple versions of terraform modules across different namespaces. \n","GenTime":"2024-08-19 15:28:36","text":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n\n\n##Decision\nWe decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n"}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"Prediction":"## Decision\n\n**Adopt the MVVM (Model-View-ViewModel) architectural pattern as the core structure for organizing code within the application.** This entails:\n\n* **Creating dedicated folders for Models, Views, and ViewModels.** \n* **Placing code artifacts related to data structures and business logic within the Models folder.**\n* **Placing UI elements and components within the Views folder, adhering to WPF's XAML-based approach.**\n* **Implementing ViewModels to encapsulate presentation logic, bind data from Models to Views, and handle user interactions.**\n\n**Further organization regarding supporting elements like libraries, images, and fonts will be determined in a separate decision record.** This ensures focused discussion and clarity on each aspect of the project structure. \n","GenTime":"2024-08-19 15:28:40","text":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n\n\n##Decision\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"Prediction":"## Decision\n\nWe will **merge the database API layer into the frontend application layer**. This means that the frontend services, including the auth-server and graphql-api-server, will have direct access to the database via knex. \n\n## Reasoning\n\nThe original separation of the database API layer was likely driven by security or performance concerns. However, with the current architecture and the implementation of direct database access from services like graphql-api-server, this separation adds complexity without providing clear benefits. \n\nThis decision is driven by the following factors:\n\n* **Reduced complexity:** Merging the layers simplifies the architecture, deployment, and maintenance.\n* **Improved performance:**  Direct database access eliminates the overhead of inter-process communication between the layers.\n* **Code sharing:**  This change allows us to reuse the existing database access code (knex) across different services.\n\n## Consequences\n\n* **Security:** Direct database access requires careful attention to security best practices, including input validation, parameterized queries, and least privilege principles.\n* **Data integrity:**  Shared database access requires robust data validation and consistency mechanisms to prevent data corruption.\n\n## Action Items\n\n* Migrate the existing database API code into the fxa-shared library.\n* Update the auth-server to use knex for database access.\n* Implement robust security measures and data validation within the frontend services.\n* Review and refactor the database access code for consistency and efficiency.\n* Update documentation and diagrams to reflect the new architecture.\n","GenTime":"2024-08-19 15:28:46","text":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n\n\n##Decision\n- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n"}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"Prediction":"## Decision\n\nWe will use an Object-Relational Mapper (ORM) for future database interactions in the Python\/React codebase.\n\n## Reasons\n\n* **Improved Developer Experience:** ORMs offer a more Pythonic and JavaScript-friendly way to interact with the database compared to writing raw SQL queries. This will improve developer productivity and reduce the learning curve for new developers. \n* **Maintainability:** Using an ORM generally leads to cleaner, more organized code, improving maintainability in the long run. \n* **Abstraction:** An ORM provides a layer of abstraction over the database, making it easier to switch databases or database versions in the future with minimal code changes.\n* **Data Validation:** Many ORMs offer built-in data validation features, reducing the need to write separate validation logic.\n\n## Consequences\n\n* **Performance Overhead:** ORMs can introduce a slight performance overhead compared to highly optimized raw SQL queries. However, this is often negligible for most applications and can be mitigated with proper optimization techniques.\n* **Learning Curve:** While ORMs generally simplify database interactions, there is still a learning curve associated with understanding the specific ORM's API and best practices.\n* **Potential Vendor Lock-in:** Choosing an ORM can create a level of vendor lock-in, making it harder to switch to a different ORM in the future. However, this is often less of a concern than database lock-in, which ORMs help mitigate.\n\n## Considered Options\n\n* **Sticking with Phinx:**  While familiar, using Phinx would require maintaining the PHP dependency and wouldn't align with the goal of migrating to a Python\/React stack.\n* **Raw SQL Queries:**  Provides the most control and potential for optimization but requires more boilerplate code, is prone to errors, and has a steeper learning curve. \n\n## Decision Outcome\n\nThis decision will be revisited if performance becomes a critical issue or the chosen ORM proves unsuitable for the project's needs.\n","GenTime":"2024-08-19 15:28:55","text":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n\n\n##Decision\n1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n"}
{"File Name":"where-away\/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md","Context":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n","Decision":"Model the links as `<button>` elements to make the interaction more normal.\\n","tokens":100,"id":2280,"Prediction":"## Decision\n\nWe will use `<button>` elements styled as links to represent bookmarks, rather than traditional `<a>` anchor tags. \n\nThis decision prioritizes accessibility and user experience by ensuring that keyboard navigation (using Tab and Enter keys) functions consistently throughout the application. While `<a>` tags are semantically accurate for linking, their limitations regarding focus and keyboard accessibility outweigh this benefit in the context of interactive bookmarks. \n","GenTime":"2024-08-19 15:28:57","text":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n\n\n##Decision\nModel the links as `<button>` elements to make the interaction more normal.\\n"}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"Prediction":"## Decision\n\nWe will prioritize anonymizing entities that meet at least two of the following criteria:\n\n1. **Directly or indirectly identifying:** This includes entities like names, phone numbers, addresses, and other information that could easily be used to identify an individual.\n2. **Special category data:**  This encompasses sensitive information such as medical history, genetic data, religious beliefs, and other categories often subject to stricter privacy regulations.\n3. **Significant presence:**  We will focus on entities that appear frequently within the dataset. This ensures our efforts are concentrated on data points with a meaningful impact on anonymization effectiveness.\n\nThis decision balances the need for robust anonymization with the constraints of model performance and project timelines. By focusing on entities meeting at least two criteria, we target the most critical data points for anonymization while maintaining feasibility.\n","GenTime":"2024-08-19 15:29:01","text":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n\n\n##Decision\nWe have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n"}
{"File Name":"ftd-scratch3-offline\/0011-implement-complex-scratch-functions-in-a-helper-function.md","Context":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n","Decision":"Complex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","tokens":99,"id":2629,"Prediction":"## Decision\n\nWe will implement a helper function approach for complex scratch\/ftduino functions. This means:\n\n1. **Code Generation:** Instead of directly generating code for complex functions in the Java files, we will generate a single line that calls a corresponding helper function. \n2. **Helper Functions:**  These functions will be defined separately and contain the complete logic for argument verification and function implementation.\n3. **Centralized Logic:** All complex function logic will reside in these helper functions, promoting code reusability and maintainability.\n\n## Reasoning\n\nWhile direct code generation might seem simpler initially, the helper function approach offers significant advantages in the long run:\n\n* **Improved Code Readability:** Java files for specific functions remain concise and focused on function invocation rather than complex logic.\n* **Enhanced Maintainability:**  Changes to complex logic only need to be made in the helper function, simplifying maintenance and reducing potential errors. \n* **Increased Code Reuse:** The same helper function can be reused by different generated functions, reducing redundancy and promoting consistency.\n* **Simplified Testing:**  Helper functions can be easily unit tested in isolation, leading to more robust and reliable code.\n\nThis approach aligns with the principles of clean code and maintainability, ultimately contributing to a more robust and scalable software architecture. \n","GenTime":"2024-08-19 15:29:07","text":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n\n\n##Decision\nComplex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n"}
{"File Name":"iampeterbanjo.com\/0007-module.exports-vs-export-default.md","Context":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n","Decision":"Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","tokens":202,"id":1596,"Prediction":"## Decision\n\nWe will use `module.exports` for exporting HapiJS plugins to maintain compatibility with HapiJS's CommonJS module system. This ensures consistent behavior and avoids confusion when using or publishing the plugins.\n","GenTime":"2024-08-19 15:29:08","text":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n\n\n##Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n"}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"Prediction":"## Decision\n\nWe will implement a schema registry as part of our messaging infrastructure to manage and enforce schemas for all topics. Each topic will have a corresponding schema defined in the registry. Producers will be responsible for validating messages against the relevant schema before publishing to a topic. Consumers will be able to retrieve the schema for a topic from the registry to deserialize messages and ensure compatibility. \n","GenTime":"2024-08-19 15:29:10","text":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n\n\n##Decision\nSince every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n"}
{"File Name":"iampeterbanjo.com\/0009-place-tests-next-to-source-files.md","Context":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n","Decision":"Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","tokens":60,"id":1588,"Prediction":"## Decision\n\nWe will adopt a co-located testing strategy for our TypeScript codebase. This means test files will reside alongside the source files they test, within the same directory. \n\n## Reasoning\n\n* **Improved Developer Experience:** Co-locating tests significantly reduces the overhead of navigating between source and test files, making the development process, particularly during refactoring and bug fixing, much more efficient. \n* **Enhanced Discoverability:** Finding related test files becomes trivial, fostering a test-driven development approach and improving code comprehension for new developers joining the project. \n* **Simplified Organization:** Keeping tests close to their corresponding source code provides a clear and intuitive structure, minimizing confusion and easing maintenance.\n\n## Consequences\n\n* **Potential for Cluttered Directories:** With a large number of test files, directories could become harder to navigate. This can be mitigated by adopting clear naming conventions and potentially using subdirectories for complex components. \n* **Tooling and Build Adjustments:** Our build process and IDE configurations will need adjustments to accommodate the co-located testing structure. \n\n## Status\n\nAccepted\n\n## Notes\n\nWe will re-evaluate this decision if the directory structure becomes unmanageable or if the co-located approach proves detrimental to the developer experience in the future.\n","GenTime":"2024-08-19 15:29:16","text":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n\n\n##Decision\nMoving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n"}
{"File Name":"insight\/0001-python.md","Context":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"Prediction":"## Decision\n\nWe will be using **Python** as the primary programming language for this project. \n\n## Reasons\n\n* **Readability and Ease of Use:** Python is known for its clear syntax and readability, making it easier for team members to understand and maintain code. This is especially important for a new project where rapid development and iteration are crucial.\n* **Extensive Libraries and Frameworks:** Python boasts a rich ecosystem of libraries and frameworks for various tasks, including data science, machine learning, web development, and scripting. This reduces development time and effort as we can leverage existing solutions. \n* **Large and Active Community:** Python has a large and active community of developers, which translates to ample resources, tutorials, and support available online.  This will prove beneficial when encountering challenges or seeking best practices.\n* **Versatility:** While we will primarily use Python, its versatility allows for integration with other languages if specific needs arise. This ensures flexibility for future expansion or specialized tasks.\n\n## Consequences\n\n* **Performance Limitations:**  While generally sufficient, Python's interpreted nature may lead to performance bottlenecks for computationally intensive tasks.  Should such situations arise, we will explore alternative solutions, potentially involving other languages,  for those specific components. \n* **Potential Learning Curve:** While Python is considered beginner-friendly, developers unfamiliar with the language may require some initial ramp-up time. We will address this through knowledge sharing and potentially dedicated training if needed. \n\nThis decision establishes Python as our default language, promoting code consistency and simplifying development. However, we recognize that specific circumstances might necessitate alternative solutions, which we will evaluate on a case-by-case basis with justifications documented.\n","GenTime":"2024-08-19 15:29:24","text":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n\n##Decision\nWe choose Python as our base programming language.\\n"}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"Prediction":"## Decision\n\nWe will use the `adr-tools` utility to manage our Architectural Decision Records (ADRs). This decision is based on the following factors:\n\n* **Standardization and Automation:** `adr-tools` provides a standardized structure and commands for creating, managing, and linking ADRs, streamlining the decision-making process and reducing overhead. \n* **Simplicity and Ease of Use:** The tool is lightweight, easy to install (via brew in our case), and offers a simple command-line interface that integrates well with our existing development workflow.\n* **ThoughtWorks Technology Radar:** The fact that `adr-tools` is referenced in the ThoughtWorks Technology Radar provides confidence in its relevance and potential value to our project. \n\nThis decision promotes consistent documentation of architectural decisions, improves transparency and traceability within the project, and ultimately aims to enhance maintainability and collaboration among team members. \n","GenTime":"2024-08-19 15:29:28","text":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n\n\n##Decision\nWe will use the system adr-tools to manage the ADR documents\\n"}
{"File Name":"bookit-api\/0010-jpa-manages-schema.md","Context":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":3221,"Prediction":"## Decision\n\nWe will transition from using Spring Boot's automatic database initialization to a Flyway-based solution. \n\nThis decision is driven by the need for a more robust and controlled approach to database schema management as our application matures beyond initial development. \n\nHere's a breakdown of the rationale:\n\n* **Version Control:** Flyway provides versioned database migrations, ensuring that schema changes are tracked and applied consistently across environments. \n* **Repeatable Deployments:** Migrations will run once on each deployment, eliminating the risk of data loss from repeated database drops and recreations.\n* **Incremental Updates:**  Future schema modifications can be deployed incrementally through new migration scripts instead of wholesale database replacements.\n* **Rollback Capability:** Flyway's rollback functionality allows for easier recovery in case of erroneous migrations.\n* **Integration with Spring Boot:** Flyway integrates seamlessly with Spring Boot, allowing for easy configuration and automation within our existing setup.\n\nThis approach offers a more sustainable and scalable solution for managing database changes throughout the application's lifecycle.\n","GenTime":"2024-08-19 15:29:33","text":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n\n\n##Decision\n* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n"}
{"File Name":"launcher\/2018-06-15_request_enrollment_details.md","Context":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n","Decision":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","tokens":116,"id":2546,"Prediction":"## Decision\n\nInstead of relying solely on ad-hoc queries after enrollment, we will implement a mechanism to include a set of predefined host details during the osquery enrollment process. This will ensure that crucial information about the osquery node is immediately available in the management server upon successful enrollment, even if the node experiences subsequent issues. \n\n**Details of the mechanism:**\n\n* **Method:** Osquery agents will gather the predefined set of host details during the enrollment process itself.\n* **Data Transmission:**  The gathered information will be included in the enrollment request sent to the management server (e.g., Fleet). \n* **Data Format:** The data will be sent in a structured format (e.g., JSON) for easy parsing and storage by the server. \n\n**Predefined Host Details:**\n\nThe initial set of host details will include but is not limited to:\n\n* Hostname\n* Operating System (name, version)\n* CPU Architecture\n* Total RAM\n* Disk Space\n* Network Interfaces (name, IP addresses)\n* Osquery Version\n* Enrollment Timestamp\n\n**Benefits:**\n\n* **Immediate Context:** Provides critical host information upon enrollment, aiding in troubleshooting potential issues.\n* **Reduced Latency:** Eliminates the need for separate ad-hoc queries after enrollment, speeding up the process.\n* **Improved Reliability:**  Ensures host details are captured even if the node encounters problems immediately after enrollment.\n\n**Potential Drawbacks:**\n\n* **Increased Enrollment Payload Size:** Sending additional data during enrollment will slightly increase the request size. \n* **Configuration Complexity:** Defining and managing the predefined set of host details might add some complexity. \n\n**Mitigation:**\n\n* The increase in payload size is expected to be negligible and outweighed by the benefits.\n* Configuration complexity can be mitigated by providing a well-documented and user-friendly interface for managing the predefined host details. \n","GenTime":"2024-08-19 15:29:41","text":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n\n\n##Decision\nAdd a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n"}
{"File Name":"meadow\/0007-code-analysis.md","Context":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n","Decision":"Use a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","tokens":20,"id":3853,"Prediction":"## Decision\n\nWe will implement a mandatory CI\/CD pipeline that enforces our coding standards and best practices through the following stages:\n\n**1. Code Style Check:** Utilize a linter configured with our specific style guide (e.g., ESLint, pylint, Rubocop) to check code style consistency before any code is merged.\n\n**2. Static Analysis:** Integrate static analysis tools (e.g., SonarQube, Codacy) to identify potential bugs, vulnerabilities, code smells, and complexity issues early in the development process.\n\n**3. Unit Testing:** Enforce a high level of unit test coverage (e.g., 80% or higher) as a gate for merging code. This ensures code functionality and acts as a safety net for future changes.\n\n**4. Integration Testing:**  Include automated integration tests in the pipeline to verify the interoperability of different components and catch integration errors before deployment.\n\n**5. Automated Deployment:** Implement automated deployment to designated environments (e.g., staging, production) upon successful completion of all previous stages. This ensures consistent and repeatable deployments while minimizing human error.\n\n**Failure in any of these stages will halt the pipeline and require remediation before proceeding.**\n\nThis approach ensures continuous adherence to our coding standards, improves code quality, reduces bugs, and promotes maintainability in the long run.\n","GenTime":"2024-08-19 15:29:47","text":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n\n##Decision\nUse a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n"}
{"File Name":"gsp\/ADR043-k8s-resource-access.md","Context":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n","Decision":"We will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","tokens":40,"id":3890,"Prediction":"## Decision\n\nWe will utilize Kubernetes' built-in Role-Based Access Control (RBAC) to manage access levels within the cluster. Different user groups and service accounts will be assigned specific Roles and RoleBindings to grant them the appropriate permissions. This approach leverages Kubernetes' native security features and provides a flexible and granular way to control access to cluster resources. \n","GenTime":"2024-08-19 15:29:49","text":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n\n\n##Decision\nWe will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n"}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"Prediction":"## Decision\n\nThe study should remain a pure Data Transfer Object (DTO) and not contain direct object instances of complex queries or fetchers. \n\n## Reasons\n\n* **Separation of Concerns:** Keeping the study as a DTO maintains a clear separation between data representation and business logic. The study should focus solely on holding data, while the logic for handling queries and fetching data should reside in dedicated classes.\n* **Testability:**  Pure DTOs are inherently easy to test as they contain minimal logic. Introducing complex object instances within the study would complicate unit testing.\n* **Serializability:** DTOs are easily serializable, making them suitable for data transfer across different layers of the application or over the network. Complex objects might pose challenges in serialization.\n* **Maintainability:**  Separating data and logic makes the codebase more understandable and maintainable. Changes in query or fetching mechanisms wouldn't require modifications in the study's structure.\n\n## Consequences\n\n* **Increased Complexity:** Handling complex queries and fetching logic outside the study might introduce some level of complexity in managing these objects separately.\n* **Potential Performance Overhead:** Creating instances of complex objects for each study object might introduce a slight performance overhead compared to directly embedding them. However, this overhead is usually negligible and outweighed by the benefits of separation of concerns.\n\n## Status\n\nAccepted.\n\n## Notes\n\nThis decision might be revisited if future requirements demonstrate a significant performance bottleneck due to the separation of data and logic. However, it's crucial to carefully weigh the trade-offs before making any changes to this fundamental architectural decision. \n","GenTime":"2024-08-19 15:29:56","text":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n\n\n##Decision\nChosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n"}
{"File Name":"princeton_ansible\/0002-group-vars.md","Context":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n","Decision":"Unique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","tokens":82,"id":2255,"Prediction":"## Decision\n\nWe will be using Ansible group variables to manage variables specific to different groups. This means:\n\n1. **Structure:** We will create a separate YAML file for each group within a dedicated \"group_vars\" directory. For example, variables for the \"webservers\" group will reside in \"group_vars\/webservers.yml\".\n2. **Vault Usage:** Each group-specific YAML file will be encrypted using Ansible Vault. This ensures secrets are stored securely while allowing for smaller, manageable vault files. \n3. **Variable Scoping:** We will leverage Ansible's variable precedence rules to ensure clarity. Group variables will take precedence over global variables defined in other locations, providing predictable behavior. \n4. **Documentation:** We will maintain clear documentation explaining the purpose of group variables and how they should be used. This documentation will be part of our standard development process.\n\nThis approach addresses the following:\n\n* **Consistency:**  Enforces a standardized way of setting group-specific variables across all playbooks.\n* **Reduced Merge Conflicts:** Smaller, group-specific vault files minimize the likelihood of merge conflicts.\n* **Security:** Secrets remain protected within encrypted vault files. \n* **Maintainability:**  A clear structure and documentation improve the maintainability of our Ansible codebase. \n","GenTime":"2024-08-19 15:30:02","text":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n\n\n##Decision\nUnique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n"}
{"File Name":"Wikibase\/0021-federated-properties-source-and-type-dispatching-services.md","Context":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n","Decision":"Make all relevant services source and type dispatching.\\n","tokens":172,"id":1346,"Prediction":"## Decision\n\nWe will introduce a new concept of **entity origin** alongside the existing entity type dispatching mechanism. This will allow services to differentiate between local (database-backed) and remote (API-backed) entities of the same type. \n\n### Details\n\n-  Entity origin will be represented as an enum with two possible values: `local` and `federated`. \n- Dispatching services will be updated to accept both entity type and entity origin as parameters.\n- Services will be responsible for handling both local and federated entities of a given type, if applicable. This allows for shared logic where appropriate, while maintaining separate code paths for origin-specific behavior.\n-  Existing entity type definitions will be extended to support the configuration of services based on both entity type and origin.\n\n### Consequences\n\n**Advantages:**\n- Clear separation of concerns between local and federated entities.\n- Explicit handling of entity origin within the codebase.\n- Flexibility to implement different behaviors for local and federated entities, while leveraging shared logic where possible.\n\n**Disadvantages:**\n- Increased complexity in dispatching logic and service configuration.\n- Potential for code duplication if not carefully managed.\n\n**Migration Path:**\n- Existing services will be treated as handling `local` entities by default. \n- New services will be introduced to handle `federated` entities. \n- As Federated Properties v2 is enabled for existing Wikibase instances, services will need to be updated to handle both entity origins appropriately.\n","GenTime":"2024-08-19 15:30:09","text":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n\n\n##Decision\nMake all relevant services source and type dispatching.\\n"}
{"File Name":"govuk-aws\/0029-combine-api-redis-into-backend-redis.md","Context":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n","Decision":"Update Rummager configuration so it uses backend-redis, and remove api-redis.\\n","tokens":101,"id":4044,"Prediction":"## Decision\n\nWe will consolidate the three separate Redis instances (backend-redis, logs-redis, api-redis) into a single, larger Redis cluster. \n\n### Justification\n\nMaintaining separate Redis instances incurs unnecessary overhead in terms of cost and management complexity.  Since the original reason for separation (different vDCs) is no longer relevant in AWS, consolidation is the most efficient approach.\n\nThis consolidation offers several advantages:\n\n* **Reduced operational overhead:** Managing a single cluster is simpler than managing three separate instances. \n* **Cost optimization:** Combining instances allows us to leverage economies of scale and potentially reduce overall Redis costs.\n* **Improved resource utilization:**  A single cluster allows for better resource allocation and avoids potential underutilization of individual instances.\n\n### Consequences\n\n* **Migration effort:**  Migrating data from the existing instances to the consolidated cluster will require careful planning and execution.\n* **Potential performance impact:**  Combining workloads could impact performance, requiring monitoring and potential optimization. \n* **Single point of failure:** A single cluster introduces a single point of failure. However, this risk can be mitigated through robust redundancy and failover mechanisms within the consolidated cluster. \n\n### Action Items\n\n1. **Capacity Planning:** Analyze the resource utilization of existing instances to determine the appropriate size and configuration for the consolidated cluster.\n2. **Migration Strategy:** Develop a detailed migration plan, including data backup, downtime minimization, and rollback procedures.\n3. **Implementation and Testing:**  Implement the consolidated cluster, migrate data, and conduct thorough testing to ensure functionality and performance. \n4. **Monitoring and Optimization:**  Establish robust monitoring for the consolidated cluster to proactively identify and address any performance issues. \n","GenTime":"2024-08-19 15:30:16","text":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n\n\n##Decision\nUpdate Rummager configuration so it uses backend-redis, and remove api-redis.\\n"}
{"File Name":"drt-v2\/0013-use-lihaoyi-s-autowire.md","Context":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n","Decision":"Use autowire for now.\\n","tokens":147,"id":1907,"Prediction":"## Decision\n\nWe will use lihaoyi's autowire library for communication between our Single Page Application (SPA) and the server. \n\n## Reasons\n\n* **Speed and Simplicity:** Autowire offers a streamlined development experience with minimal boilerplate code thanks to its macro-based approach. This aligns with our need for rapid development and iteration.\n* **Type Safety:** Leveraging Scala's type system, autowire provides type-safe communication between the client and server, reducing runtime errors.\n* **Current Client Needs:** As we currently have only one client (the SPA), the tight coupling introduced by autowire is acceptable and will not impact other potential clients.\n* **Future Flexibility:**  While not RESTful, we can incrementally introduce RESTful routes alongside autowire endpoints if and when we identify the need for greater decoupling or external API consumption.\n\n## Consequences\n\n* **Tight Coupling:** The client and server codebase will be tightly coupled due to autowire's code-generation approach. Changes in API definitions might require updates on both sides.\n* **Limited External API Consumption:** The lack of a standard RESTful interface might pose challenges if we need to expose the API for consumption by external clients in the future. \n\n## Status\n\nAccepted. We will re-evaluate this decision if we encounter significant drawbacks or if the need for a more decoupled and standardized API arises. \n","GenTime":"2024-08-19 15:30:22","text":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n\n\n##Decision\nUse autowire for now.\\n"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"Prediction":"## Decision\n\nWe will migrate Twig from the older riglet flavor to the new, simpler flavor. This migration will encompass the following:\n\n* **Identify key differences:**  Thoroughly analyze the differences between the old and new riglet flavors, focusing on configuration, dependencies, and potential compatibility issues.\n* **Develop a migration plan:**  This plan should outline the steps involved in the migration, including testing procedures, rollback mechanisms, and a timeline for completion. \n* **Incremental migration (if possible):**  Explore the feasibility of migrating Twig incrementally. This could involve moving individual components or services first to minimize disruption and risk.\n* **Thorough testing:** Implement a comprehensive testing strategy that covers all aspects of Twig's functionality within the new riglet environment. \n* **Performance benchmarking:**  Establish a baseline for Twig's performance on the old flavor and compare it to performance on the new flavor. This will help identify and address any performance regressions. \n* **Documentation and knowledge transfer:** Document the migration process, including any challenges encountered and solutions implemented. This documentation will facilitate future migrations and ensure knowledge transfer within the team.\n\nThis decision aims to simplify Twig's infrastructure, take advantage of newer technologies offered by the new riglet flavor, and provide a platform for testing innovative approaches. \n","GenTime":"2024-08-19 15:30:28","text":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"Prediction":"## Decision\n\nWe will provide official Node.js client library bindings that leverage Passport.js for authentication and authorization. This caters to our existing Node.js user base and offers a familiar, frictionless integration path. \n\nFor users of other languages and frameworks, we will offer a well-documented REST API alongside community-driven client libraries. This approach ensures broad compatibility while allowing the community to contribute to language-specific integrations. \n","GenTime":"2024-08-19 15:30:31","text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n"}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"Prediction":"## Decision\n\nWe will use OpenTelemetry as our distributed tracing framework. OpenTelemetry clients will be integrated into the MilMove codebase at appropriate call sites. \n\n## Rationale\n\nOpenTelemetry is a Cloud Native Computing Foundation (CNCF) project, currently in the Incubating stage. It offers several advantages:\n\n* **Wide Language Support:** OpenTelemetry supports all major programming languages used in the MilMove codebase.\n* **Vendor Neutrality:**  OpenTelemetry allows us to defer the decision of choosing a backend tracing platform. We can switch between backends like Jaeger, Zipkin, or cloud-based solutions without modifying the instrumentation. \n* **Community and Industry Adoption:**  As a CNCF project, OpenTelemetry has robust community support and is rapidly becoming the industry standard for distributed tracing, ensuring its longevity and a rich ecosystem of tools.\n\nThis decision fulfills the ADR goals by:\n\n* Selecting a specific set of libraries (OpenTelemetry) for instrumentation.\n* Avoiding commitment to a specific backend, allowing flexibility in the future.\n* Maximizing backend options due to OpenTelemetry's vendor-neutral approach. \n","GenTime":"2024-08-19 15:30:36","text":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n\n\n##Decision\n* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"Prediction":"## Decision\n\nWe will use **LibreOffice** accessed through **Unoconv** (external command) for document conversion to PDF. \n\n**Reasoning:**\n\nWhile a Java-only solution with Apache POI is appealing, our tests showed that LibreOffice through Unoconv provided significantly better visual fidelity to the original documents, particularly for `docx` and `odt` formats. Preserving the visual layout is a priority for us, as stated in the problem statement.\n\n**Justification:**\n\n* **Visual Fidelity:** Unoconv leverages LibreOffice, a full-featured office suite known for its accuracy in rendering documents. Our tests confirmed that it produced PDFs that closely matched the original `docx` and `odt` files in terms of layout, fonts, and image positioning.\n* **Free and Open Source:**  Both LibreOffice and Unoconv are free and open-source, aligning with Docspell's licensing requirements. \n* **Support for Multiple Formats:**  Although our immediate focus is on `docx` and `odt`, Unoconv supports a wide range of document formats, providing flexibility for the future.\n\n**Tradeoffs:**\n\n* **External Dependency:**  Unoconv introduces an external dependency on LibreOffice being installed on the system. \n* **Performance:** External command execution might be slightly slower than a pure Java library, but the difference is expected to be negligible for our use case.\n\n**Mitigation:**\n\n* **Documentation and Installation Scripts:** We will provide clear documentation and potentially installation scripts to simplify the setup of LibreOffice and Unoconv for Docspell users.\n* **Performance Monitoring:**  We will monitor the performance impact of using Unoconv and explore optimization options if necessary. \n","GenTime":"2024-08-19 15:30:43","text":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n\n\n##Decision\nUnoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n"}
{"File Name":"libelektra\/elektra_web.md","Context":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n","Decision":"Use one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","tokens":75,"id":1306,"Prediction":"## Decision\n\nWe will develop a centralized configuration management service. This service will be responsible for storing and managing the connection information of individual Elektra instances and defining the relationships between them (clusters). This approach provides a single source of truth for configuration data and simplifies the management of multiple instances. \n\nWe will consider the following aspects in the design of the service:\n\n* **Scalability**: The service should be able to handle a growing number of instances and clusters.\n* **Security**: Secure storage and access control to the configuration data are paramount.  \n* **User Interface**: A user-friendly interface will be provided to manage instances, clusters, and their configurations.\n* **API**: A well-defined API will allow integration with other tools and automation scripts. \n\nThis approach offers flexibility and scalability for managing Elektra instances in various deployment scenarios. \n","GenTime":"2024-08-19 15:30:47","text":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n\n\n##Decision\nUse one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n"}
{"File Name":"katas\/000-use-adrs.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for capturing important architectural decisions along with their context and consequences.  Each significant architectural decision will be documented in a separate ADR file using a simple, standardized template (e.g., Markdown or AsciiDoc). These ADRs will be version-controlled within the project repository to track their evolution over time. \n","GenTime":"2024-08-19 15:30:50","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"Prediction":"## Decision\n\nWe will build our service using progressive enhancement.  \n\nWhile we acknowledge the existence of other services in the prison and probation space that do not follow this principle, we believe that adhering to the guidelines outlined in the service manual will lead to a more accessible and user-friendly service for all. This approach aligns with our commitment to inclusivity and ensuring a positive experience for every user on their first interaction and beyond, as required by the service standard. \n","GenTime":"2024-08-19 15:30:52","text":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n\n\n##Decision\nWe will use progressive enhancement for all our user-facing applications.\\n"}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"Prediction":"## Decision\n\n**Introduce a dedicated Mail URL generation service and deprecate storefront-specific URL configurations.**\n\n**Rationale:**\n\n- **Centralization and Scalability:** A dedicated service provides a single source of truth for generating mail URLs, regardless of the triggering event or whether the storefront is present. This approach is more scalable and maintainable than relying on scattered configurations.\n- **Flexibility:** The service can encapsulate logic for determining the appropriate URL based on factors like the mail type, available channels, and potentially even user preferences. \n- **Decoupling:** This solution decouples mail URL generation from specific storefront implementations, improving modularity and headless compatibility.\n- **Maintainability:**  Having a single, well-defined API for generating mail URLs simplifies testing, debugging, and future modifications.\n\n**Implementation Details:**\n\n1. **Mail URL Service:** Create a new service responsible for generating absolute URLs for various mail templates.\n2. **Configuration:**  Instead of individual URL configurations, provide a mechanism to define base URLs (e.g., per sales channel) within the service.\n3. **Context Awareness:** The service should accept contextual information (e.g., mail type, entity ID, sales channel) to construct appropriate URLs.\n4. **Business Events Integration:** Utilize Shopware's Business Event system to trigger URL generation within relevant events (e.g., `CustomerRegisteredEvent`, `NewsletterSubscriptionConfirmationEvent`).\n5. **Flow Builder Compatibility:** Ensure the service is accessible within Flow Builder actions to enable custom mail workflows.\n6. **Deprecation:** Gradually deprecate storefront-specific URL configurations and migrate existing code to use the new service.\n\n**Consequences:**\n\n- **Increased Complexity:** Introducing a new service adds some complexity to the system. \n- **Migration Effort:**  Migrating from existing URL configurations will require code changes.\n\n**Overall:**\n\nWhile introducing a new service adds some complexity, the benefits of centralization, flexibility, and improved maintainability outweigh the drawbacks. This decision aligns with Shopware's headless strategy and promotes a more scalable and future-proof architecture. \n","GenTime":"2024-08-19 15:31:01","text":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n\n\n##Decision\nThere shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n"}
{"File Name":"mediawiki-extensions-Popups\/0008-enable-minification-of-bundle-with-uglifyjs.md","Context":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n","Decision":"Webpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","tokens":189,"id":3428,"Prediction":"## Decision\n\nWe will switch from JSMin to UglifyJS for minifying Javascript code. \n\n## Justification\n\nUglifyJS provides a significant reduction in bundle size (40% uncompressed, 25% with gzip) compared to JSMin, leading to faster page load times for users. This aligns with our goal of shipping the smallest possible JS bundle. \n\nThe overhead of using a Node.js based tool like UglifyJS is mitigated by the existing pre-compilation step in Popups.\n\n## Consequences\n\n* **Positive:**\n    * Reduced bundle size, leading to faster page load times.\n    * Improved user experience due to faster loading times.\n    * Potentially lower bandwidth costs.\n* **Negative:**\n    * Increased build time due to UglifyJS potentially being slower than JSMin. This impact should be measured.\n    * Potential compatibility issues with UglifyJS need to be monitored. \n\n## Action Items\n\n* Replace JSMin with UglifyJS in the build process.\n* Measure the impact of UglifyJS on build time.\n* Monitor for potential compatibility issues after deployment. \n* Update documentation to reflect the change in minification tool. \n","GenTime":"2024-08-19 15:31:07","text":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n\n\n##Decision\nWebpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"Prediction":"## Decision\n\nWe will develop a centralised component library to host common styles, components and patterns for NHSUK websites. \n\n**Justification:**\n\n* **Consistency:** A central library promotes consistency across NHSUK sites, improving user experience and brand identity.\n* **Efficiency:** Developers can reuse existing components, saving time and effort compared to starting from scratch for each site.\n* **Maintainability:** Updates and bug fixes can be applied centrally, ensuring all sites benefit from the latest improvements.\n* **Reviewability:** The component library will reside in a version controlled repository, enabling code review and testing before release.\n* **Technology Agnostic:** While SASS can be used for styling, the library's architecture will focus on providing components that can be integrated with different front-end frameworks if needed.\n\n**Implementation Details:**\n\n* **Repository:** A dedicated repository will host the component library, accessible to all NHSUK development teams.\n* **Documentation:** Clear documentation will be provided for each component, including usage instructions, accessibility considerations, and code examples.\n* **Versioning:** Semantic versioning will be used to ensure controlled updates and compatibility across projects.\n* **Tooling:**  The library will provide tooling to facilitate development and testing of components in isolation.  \n* **Integration:**  Guidance and examples will be provided to help developers integrate the component library into their projects.\n\n**Alternatives Considered:**\n\n* **Continuing with the current approach:**  This was deemed unacceptable due to the ongoing inconsistencies, maintenance difficulties, and lack of reviewability.\n* **Using a third-party component library:** This option was considered but ultimately rejected as it might not fully meet the specific needs and design requirements of NHSUK. \n\n**Consequences:**\n\n* **Initial development effort:**  Creating the component library requires an upfront investment.\n* **Adoption and Training:**  Development teams will need to familiarize themselves with the new library and integrate it into existing workflows. \n\nThis decision significantly benefits NHSUK's long-term goals by establishing a scalable and maintainable approach to front-end development, ultimately improving the user experience and brand consistency across all sites.\n","GenTime":"2024-08-19 15:31:16","text":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n\n\n##Decision\nWe will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n"}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"Prediction":"## Decision\n\nWe will use MongoDB as the persistent data store for the Performance Platform. \n","GenTime":"2024-08-19 15:31:17","text":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n\n##Decision\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n"}
{"File Name":"origin\/0005-simplify-off-chain-storage.md","Context":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n","Decision":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","tokens":52,"id":1558,"Prediction":"## Decision\n\nTo mitigate data inconsistency issues arising from the hybrid storage model, we will implement a two-phase commit protocol for all data writes that involve both on-chain and off-chain components. \n\n**Phase 1: Prepare Phase**\n\n1. The application initiates a write operation.\n2. The off-chain data store writes data and returns a success\/failure signal.\n3. If the off-chain write is successful, a transaction is prepared on-chain containing a reference to the off-chain data, but is not yet committed.\n\n**Phase 2: Commit Phase**\n\n1. If the on-chain transaction preparation is successful, the application receives confirmation.\n2. The application then confirms the on-chain transaction, making the data reference permanently available. \n\n**Failure Handling:**\n\n* **Off-chain write failure:** The process is aborted, and no on-chain transaction is prepared.\n* **On-chain preparation failure:** The off-chain write is rolled back to maintain consistency.\n* **Application failure after off-chain write but before on-chain commit:**  A separate cleanup process monitors for incomplete transactions and attempts to finalize or rollback based on the state of the off-chain data.\n\n**Rationale:**\n\nThis approach ensures data consistency by atomically linking on-chain references to successfully written off-chain data. The two-phase commit protocol guarantees that either both the on-chain and off-chain writes are successful, or neither is, preventing data corruption. \n\n**Consequences:**\n\n* **Increased complexity:** Implementing a two-phase commit protocol adds complexity to the write process. \n* **Performance impact:**  The two-phase commit introduces additional steps and potential latency in write operations.\n* **Increased storage costs:**  Temporary storage might be required for managing incomplete transactions.\n\n**Alternative Considered:**\n\n* **On-chain only storage:** This would simplify the architecture but increase costs and potentially limit scalability.\n* **Optimistic updates:** This approach could offer better performance but carries a higher risk of data inconsistency and requires complex rollback mechanisms.\n\nThis decision prioritizes data consistency and robustness over potential performance overhead and increased complexity. We believe that the benefits of a reliable and consistent data store outweigh the potential drawbacks in this particular context. \n","GenTime":"2024-08-19 15:31:27","text":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n\n##Decision\nWe decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n"}
{"File Name":"educational-platform\/0009-architecture-tests.md","Context":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n","Decision":"Architecture tests with using Archunit should be implemented.\\n","tokens":21,"id":2502,"Prediction":"## Decision\n\nWe will implement a shared architectural validation framework. This framework will consist of:\n\n* **A codified set of architectural principles:** These principles will be documented clearly and concisely, specifying the architectural patterns, best practices, and constraints we value in our applications.\n* **Automated validation tools:**  We will develop or utilize existing static analysis tools, linters, or custom scripts integrated into the CI\/CD pipeline. These tools will scan application codebases to ensure adherence to the defined architectural principles.\n* **Centralized reporting and feedback mechanism:** The validation results will be aggregated and presented through a centralized dashboard or reports, highlighting potential violations and providing actionable insights to developers. This mechanism will also offer a platform for discussing and refining architectural decisions. \n\nThis framework will be incorporated as a mandatory step in the CI\/CD process for all applications, ensuring consistent enforcement and promoting early detection of architectural deviations. \n","GenTime":"2024-08-19 15:31:31","text":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n\n##Decision\nArchitecture tests with using Archunit should be implemented.\\n"}
{"File Name":"molgenis-frontend\/0003-have-app-create-settings.md","Context":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n","Decision":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","tokens":90,"id":1514,"Prediction":"## Decision\n\nWe will use a two-pronged approach to address the challenges of linking applications to dynamic settings and managing access control:\n\n**1. Linking Apps with Settings Data:**\n\n* **Unique Identifiers:** Each application will be assigned a unique identifier (e.g., app name, ID) stored within its configuration. This identifier will be used to associate the app with its corresponding settings data in the Molgenis database.\n* **Dedicated Settings Table:** A dedicated \"app_settings\" table will be created in the Molgenis database. This table will store key-value pairs for each application's settings, linked by the application's unique identifier.\n\n**2. Access Control for Settings:**\n\n* **Role-Based Access Control (RBAC):** We will leverage Molgenis's existing RBAC functionality to manage access to app settings. \n* **Granular Permissions:** Different user roles will have varying levels of access to app settings:\n    * **Administrators:** Full access (read, write, update, delete) to all application settings.\n    * **App Owners:** Full access to settings for their specific application(s).\n    * **Other Roles:**  Read-only access to specific settings as defined by administrators or app owners.\n\n**Implementation Details:**\n\n* Settings will be fetched from the database during application initialization.\n* Changes to settings will be persisted to the database in real-time or near real-time, depending on the specific setting and application requirements.\n* The settings UI will be integrated within the Molgenis platform, allowing authorized users to manage application settings securely.\n\nThis approach provides a flexible and secure solution for managing dynamic application settings. By leveraging Molgenis's database and RBAC features, we ensure data integrity, centralize settings management, and enforce fine-grained access control to protect sensitive configuration data.\n","GenTime":"2024-08-19 15:31:39","text":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n\n\n##Decision\nWe leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"Prediction":"## Decision\n\nWe will use the **semver resource** instead of the pool resource for controlling the flow through Concourse pipelines. \n\n**Reasoning:**\n\n* **Security:** The semver resource relies on existing AWS credentials already passed to the pipeline, minimizing the need to introduce additional sensitive credentials like public keys required by the pool resource.\n* **Credential Management:** Reducing the number of credentials simplifies security management and reduces the potential attack surface.\n\n**Consequences:**\n\n* We accept a dependency on AWS for pipeline flow control.\n* We might need to explore alternative solutions if the semver resource proves insufficient for more complex pipeline orchestration needs in the future. \n","GenTime":"2024-08-19 15:31:42","text":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n\n\n##Decision\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n"}
{"File Name":"libelektra\/empty_files.md","Context":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n","Decision":"Remove files on empty KeySet.\\n","tokens":23,"id":1271,"Prediction":"## Decision\n\nWhen an empty `KeySet` is passed to `kdbSet()`, the correct persistent representation is **to remove the corresponding key(s) from the database completely.** \n\nThis decision is based on the principle that an empty `KeySet` represents the absence of configuration for the given key(s). Therefore, persisting an empty `KeySet` should result in the removal of any pre-existing configuration for those keys. \n","GenTime":"2024-08-19 15:31:45","text":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n\n\n##Decision\nRemove files on empty KeySet.\\n"}
{"File Name":"adrflow\/8-Remove_ADR_Util_Sync_Module.md","Context":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n","Decision":"Divide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","tokens":180,"id":3637,"Prediction":"## Decision\n\nWe will refactor the `Util` module and remove its dependency on the global context (ADR files and directory). \n\nThis will be achieved by:\n\n1. **Identifying and extracting distinct functionalities** currently residing within the `Util` module. \n2. **Creating separate, cohesive modules** for each identified functionality. \n3. **Passing required context explicitly** to functions within these new modules instead of relying on the global context. For cases where caching is beneficial, implement it within the respective module.\n4. **Updating existing code** to utilize the new modules and pass the required context explicitly.\n\nThis approach aims to:\n\n* **Improve code organization and readability** by decoupling unrelated functionalities.\n* **Reduce the `Util` module's responsibilities** to promote maintainability and reduce potential for bugs.\n* **Eliminate circular dependencies** caused by the implicit global context.\n* **Increase testability** of individual functionalities due to their increased isolation. \n\nThis decision supersedes ADR 5, which introduced the lazily initialized context, as it eliminates the need for it. It also relates to ADR 6, which centralizes filename definitions, and may benefit from similar refactoring to remove potential dependencies on the global context.\n","GenTime":"2024-08-19 15:31:50","text":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n\n\n##Decision\nDivide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n"}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"Prediction":"## Decision\n\nWe will adopt Rails Internationalization (I18n) to manage all static text within the application. \n\nThis decision is driven by the following factors:\n\n* **Improved maintainability:** Centralizing strings in YAML files will enhance searchability and allow for consistent updates across the application. This approach empowers non-developers to manage and audit text effectively.\n* **Enhanced code quality:** Removing static text from code will improve readability and maintainability, allowing developers to focus on logic rather than string manipulation.\n* **Future-proofing for internationalization:** While not an immediate requirement, adopting I18n from the outset paves the way for seamless internationalization in the future.\n\nWhile there is an upfront cost to migrating existing strings and adapting to the I18n workflow, we believe the long-term benefits outweigh the initial investment. \n","GenTime":"2024-08-19 15:31:54","text":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n\n\n##Decision\nAlthough we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n"}
{"File Name":"nhsuk-prototype-kit-version-one\/0001-use-node-and-express.md","Context":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n","Decision":"We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","tokens":83,"id":3681,"Prediction":"## Decision\n\nWe will implement a lightweight server within the prototype kit application. This server will handle the following responsibilities:\n\n* **Serving static assets:**  This includes HTML, CSS, and JavaScript files for the prototype interface.\n* **Dynamic routing:** Implementing a route matching mechanism to automatically serve the correct HTML template based on the requested URL. This removes the need for manual configuration and simplifies prototyping for users with varying technical skills.\n\n**Justification:**\n\n* **Ease of use:**  Auto-routing significantly reduces the configuration overhead for users, making the prototype kit accessible to a wider audience.\n* **Live service simulation:** A server allows us to more realistically simulate a live service environment, crucial for testing interactions and data flows.\n* **Flexibility:**  A server provides the flexibility to add more advanced features and functionality to the prototype kit in the future, such as mocking API endpoints or simulating user data. \n\n**Alternatives considered:**\n\n* **Static site generators:**  While they offer simple static asset serving, they lack the dynamic routing capabilities required for this use case.\n* **Requiring a separate server environment:** This would introduce unnecessary complexity and dependencies for users, particularly those less familiar with server-side setup.\n\n**Consequences:**\n\n* **Slightly increased application complexity:** Implementing a server adds a small amount of complexity to the prototype kit. However, we will strive for a lightweight and well-documented solution to mitigate this.\n* **Potential performance impact:** Running a server might introduce a minor performance overhead. We will carefully select a suitable framework and optimize for efficiency to minimize this.\n\n**Next steps:**\n\n* **Choose a suitable server framework:**  Prioritize lightweight options with easy routing capabilities (e.g., Express.js for Node.js).\n* **Design and implement the routing mechanism:**  Define clear rules and conventions for route matching and template resolution.\n* **Develop documentation and examples:** Provide clear instructions and examples on how to use the server and routing functionality within the prototype kit. \n","GenTime":"2024-08-19 15:32:03","text":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n\n\n##Decision\nWe will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n"}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"Prediction":"## Decision\n\nWe will adopt a **trunk-based development** workflow with **feature branching** and **mandatory squash merging** to the main branch. This approach balances the need for a clean, semi-linear history with a practical development process for teams.\n\n### Rationale\n\n* **Trunk-based development**: This minimizes merge conflicts and keeps the main branch always in a releasable state. Continuous Integration is inherently supported and encouraged by this approach.\n* **Feature branching**: Allows developers to work on features in isolation without disrupting the main branch. This promotes focused development and easier code reviews.\n* **Squash merging**: Ensures a clean and concise commit history on the main branch.  Each merged feature is represented by a single, meaningful commit, making it easy to understand the evolution of the project. The \"recipe\" style of feature development is preserved in the squashed commit, which can be explored as needed.\n\n### Consequences\n\n* **Pros**:\n    *  Clean and understandable semi-linear history.\n    *  Easier code reviews.\n    *  Reduced merge conflicts.\n    *  Continuous Integration friendly.\n    *  Simplified release management.\n* **Cons**:\n    * Requires developers to be disciplined with squashing commits.\n    * Detailed commit history of a feature is only visible in the feature branch, requiring additional navigation for in-depth analysis.\n\n### Rejected Alternatives\n\n* **Linear history (rebase workflow)**: While providing the cleanest history, the forced rewriting of history introduces significant complexity and potential issues, especially for larger teams. \n* **No enforced workflow**: This would lead to inconsistent and potentially messy history, making it difficult to understand the project evolution and debug issues.\n\n### Implementation Notes\n\n* Define and communicate clear guidelines for commit messages in feature branches and when squashing.\n* Utilize tooling and automation (e.g., protected main branch with required pull request squash merge) to enforce the workflow. \n* Provide clear documentation and training for developers on the chosen workflow. \n","GenTime":"2024-08-19 15:32:12","text":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n"}
{"File Name":"gsp\/ADR019-service-mesh.md","Context":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n","Decision":"We will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","tokens":498,"id":3909,"Prediction":"## Decision\n\n**Option 1: Istio** has been selected for the following reasons:\n\n* **Meets Requirements:** Istio directly addresses our needs for data exfiltration restriction, strict authentication with mutual TLS, and comprehensive service mesh capabilities.\n* **Maturity and Support:** As an emerging standard with a large and active community, Istio benefits from robust documentation, support resources, and continuous development. \n* **Feature Richness:** Istio offers a comprehensive set of features including traffic management, security policies, observability, and integration with existing infrastructure.\n* **GKE Integration:** Being installed by default on Google Kubernetes Engine (GKE) simplifies deployment and management within our chosen environment. \n\n**Reasons for rejecting alternatives:**\n\n* **AWS App Mesh:** While conceptually similar to Istio, App Mesh's unavailability in our required region (London) and lack of key features (automatic sidecar injection) make it unsuitable.\n* **Linkerd:** Although lightweight and Kubernetes-native, Linkerd's limited feature set, especially in version 2.0, does not meet our requirements for security and traffic management. Additionally, the smaller community and ongoing major rewrite introduce potential risks and uncertainties. \n","GenTime":"2024-08-19 15:32:18","text":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n\n\n##Decision\nWe will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n"}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"Prediction":"## Decision\n\nWe will develop a simulated Hub environment for user research purposes. This simulated environment will provide a controlled and consistent experience for users, allowing us to focus on the Service Provider's interface and functionality without dependencies on a live Hub. \n\nThis decision prioritizes:\n\n* **Control and consistency:**  A simulated Hub ensures a predictable user journey, free from external factors that could impact testing. \n* **Focused research:** By isolating the Service Provider, we can gather targeted feedback on its specific features and user interface.\n* **Early testing:** The simulated environment can be developed in parallel with the Service Provider, enabling early user research and iterative design improvements.\n\nThis approach will involve:\n\n1. **Defining key Hub interactions:** Identifying the essential Hub functionalities and data exchanges required for the user journey.\n2. **Developing a simulation:** Creating a lightweight, controlled environment that mimics the identified Hub interactions.\n3. **Integrating with the Service Provider:** Ensuring seamless communication and data flow between the simulated Hub and the Service Provider. \n\nWhile a simulated Hub offers significant benefits for early-stage user research, it's important to acknowledge that:\n\n* **Real-world complexities may be missed:**  A simulation cannot fully replicate the dynamic and potentially unpredictable nature of a live Hub environment.\n* **Future integration testing will be necessary:**  Thorough testing with a live Hub will be crucial before release to ensure robust integration and address any discrepancies. \n","GenTime":"2024-08-19 15:32:25","text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"Prediction":"## Decision\n\nAdopt a CSS architecture that minimizes or eliminates reliance on the `z-index` property for layout. This could involve:\n\n1. **Strict Layering System:** Define a limited set of global stacking contexts with clearly defined purposes (e.g., base layer, modals, overlays, notifications). Components within each layer should not need to directly manage their `z-index`. \n2. **Component-Based Styling:**  Encapsulate styles within components, preventing unintended style bleed and reducing the need for cascading `z-index` values.\n3. **Flexbox\/Grid for Layout:**  Leverage modern layout techniques like Flexbox and CSS Grid to manage element positioning and layering naturally without relying heavily on `z-index`. \n4. **CSS Methodology (e.g., BEM):** Implement a structured CSS methodology like BEM to improve code organization and reduce style conflicts that contribute to `z-index` issues.\n\n**Specific Technology:** \n\nConsider adopting a CSS framework or library that inherently promotes these practices, such as Tailwind CSS or Material UI.\n\n**Justification:**\n\n* **Maintainability:** A structured approach to layering and CSS drastically improves maintainability, as developers won't need to constantly adjust arbitrary `z-index` values with each change.\n* **Scalability:**  This architecture scales better as the application grows, preventing the \"z-index hell\" commonly seen in large, complex projects.\n* **Predictability:** Makes the rendering behavior more predictable and easier to reason about, simplifying debugging and future development. \n","GenTime":"2024-08-19 15:32:31","text":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\n\n##Decision\nWe've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n"}
{"File Name":"james\/0042-james-cli-based-on-webadmin.md","Context":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2133,"Prediction":"## Decision\n\nWe will deprecate the existing JMX-based command-line interface for James servers. A new command-line interface, leveraging the WebAdmin REST API (which utilizes the HTTP protocol), will be developed. \n\nThis decision is driven by the following factors:\n\n* **Security:** The JMX protocol has known security vulnerabilities, including Java deserialization issues (CVE-2017-12628) that expose James servers to remote code execution attacks. Transitioning to the HTTP-based WebAdmin API significantly reduces the attack surface and improves the overall security posture. \n* **Usability:** The current CLI's syntax is not user-friendly. Placing the entity first, followed by possible actions, will create a more intuitive and memorable experience for users.  \n* **Maintainability:**  Aligning the command-line interface with the existing WebAdmin API promotes code reuse and simplifies maintenance efforts. \n\n**Implementation Details:**\n\n1. **New CLI Development:**  The new CLI will be developed using a suitable language and framework, interacting with the James server exclusively through the WebAdmin REST API.\n2. **Entity-First Syntax:**  The command structure will prioritize clarity by placing the entity first, followed by the desired action (e.g., `james-cli users list`, `james-cli mailboxes create`). \n3. **Deprecation Plan:** The existing JMX-based CLI will be marked as deprecated and eventually removed in a future release.  Clear documentation will be provided to guide users through the transition to the new CLI.\n","GenTime":"2024-08-19 15:32:38","text":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n\n\n##Decision\nWe decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n"}
{"File Name":"NorthwindCore\/0001-record-architecture-decision.md","Context":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":33,"id":1792,"Prediction":"## Decision\n\nWe will utilize Architectural Decision Records (ADRs) as a mechanism to document significant architectural decisions made throughout the project lifecycle. Each ADR will capture the context, decision, and consequences, ensuring transparency and traceability for all stakeholders. These records will be version-controlled and readily accessible within the project repository.\n","GenTime":"2024-08-19 15:32:41","text":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n\n\n##Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n"}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"Prediction":"## Decision\n\nWe will use **React Styleguidist** for our component library. \n\n**Reasons:**\n\n* **Strong Alignment with Requirements:** React Styleguidist directly addresses most of our core needs:\n    * **Single Repository:** It's designed for managing components within a single repository.\n    * **Integrated Documentation:**  Styleguidist excels at generating documentation from code comments and supports additional Markdown files for expanded explanations.\n    * **Local Development:**  Provides a hot-reloading development environment for rapid iteration.\n    * **Hostable Web App:**  Easily builds a static site that can be deployed to serve as our style guide.\n    * **Theming:**  Offers customization options to align the style guide's look and feel with our existing documentation.\n\n* **Potential for Designer Collaboration:**  While Styleguidist itself doesn't have dedicated designer-focused editing tools, its Markdown support makes it relatively straightforward for designers to contribute to component documentation.\n\n* **Focus on Documentation and Usage:** Styleguidist's primary strength is showcasing components with clear usage examples, making it well-suited for our goal of guiding developers and designers in using our component library effectively.\n\n**Trade-offs:**\n\n* **Less Interactive Exploration (Compared to Storybook):** While Styleguidist provides a live, editable component playground, it might not offer the same level of interactive exploration and complex scenario testing that Storybook excels in. However, this trade-off is acceptable since our primary focus is on clear documentation and standard component usage. \n\n**Next Steps:**\n\n1. **Project Setup:** Create a new repository or integrate Styleguidist into an existing one.\n2. **Configuration:** Configure Styleguidist to meet our specific requirements (e.g., theming, documentation structure). \n3. **Component Migration:** Start migrating existing components or develop new components directly within the Styleguidist environment.\n4. **Deployment:**  Set up a deployment pipeline to publish our style guide.\n","GenTime":"2024-08-19 15:32:50","text":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n\n\n##Decision\nUse Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n"}
{"File Name":"smjs\/2018121201-stop-using-registers-in-processor.md","Context":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n","Decision":"The interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","tokens":172,"id":3972,"Prediction":"## Decision\n\nWe will **replace the register-based communication between the processor and interpreter with a direct call\/return mechanism**. \n\n**Reasoning:** The current communication flow is straightforward and does not warrant the complexity of an intermediate register component. Using function calls and return values provides a simpler and more efficient alternative. \n\n**Benefits:**\n\n* **Reduced Complexity:** Eliminates the need for registers, simplifying the design and implementation.\n* **Improved Performance:** Direct calls and returns are generally faster than register-based communication, potentially leading to performance gains.\n* **Increased Readability:** Code using function calls and returns is typically easier to understand and maintain.\n\n**Drawbacks:**\n\n* **Tight Coupling:** Direct calls introduce a tighter coupling between the processor and interpreter, potentially impacting modularity. However, given the inherent close relationship between these components, this trade-off is deemed acceptable.\n\n**Consequences:**\n\n* The register component will be removed.\n* The processor will directly call the interpreter with the instruction to execute.\n* The interpreter will return an exit status and updated IP (if applicable) through the return mechanism. \n\n**Future Considerations:**\n\n* If, in the future, the need for asynchronous communication or decoupling arises, we can revisit the decision to explore message queues or other communication patterns. \n","GenTime":"2024-08-19 15:32:56","text":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n\n\n##Decision\nThe interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n"}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"Prediction":"## Decision\n\nWe will store developer scripts in a dedicated `scripts` directory within the `pace-developers` repository. \n\n## Reasons\n\nWhile storing scripts alongside the relevant code in individual project repositories seems convenient, it introduces ambiguity regarding script functionality and version compatibility.  Placing these scripts in the `pace-developers` repository alongside relevant documentation provides several advantages:\n\n* **Clear Purpose and Reduced Confusion:**  This approach clearly separates developer-oriented scripts from production-ready code, managing expectations for users.\n* **Version Control and Reproducibility:**  The `pace-developers` repository allows for versioning of scripts alongside documentation, ensuring reproducibility of experiments and decisions.\n* **Centralized Access and Collaboration:**  A centralized repository makes it easier for developers to discover and share useful scripts, fostering collaboration. \n\n## Consequences\n\n* **Increased Maintenance Overhead:** Maintaining a separate repository for scripts requires additional effort.\n* **Potential for Script Staleness:** Scripts might become outdated if not updated alongside code changes. \n\n## Mitigations\n\n* Implement clear guidelines for script contribution and maintenance within the `pace-developers` repository. \n* Encourage developers to regularly review and update scripts to ensure compatibility with the latest codebase.\n* Consider using a lightweight testing framework to periodically validate script functionality.\n","GenTime":"2024-08-19 15:33:02","text":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n\n##Decision\nDeveloper scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n"}
{"File Name":"inner-source_kotlin_spark_template\/20180617-flyway.md","Context":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n","Decision":"We will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","tokens":17,"id":1462,"Prediction":"## Decision\n\nWe will use **Flyway** as our database migration tool.\n\n## Reasons\n\n* **Simplicity and ease of use:** Flyway offers a straightforward approach to database migration with a minimal learning curve.\n* **Version control integration:** Flyway integrates seamlessly with Git, allowing us to track database changes alongside our codebase.\n* **SQL-based migrations:** We can leverage our existing SQL expertise to write and manage migrations, providing flexibility and control.\n* **Rollback capabilities:** Flyway enables easy rollback to previous database versions in case of issues.\n* **Active community and support:** Flyway benefits from an active community and comprehensive documentation, ensuring we have access to resources and assistance when needed.\n\n## Alternatives considered\n\n* **Liquibase:** While feature-rich, Liquibase can be more complex than Flyway, potentially introducing unnecessary overhead for our current needs.\n* **Custom solution:** Building a custom solution would require significant development time and ongoing maintenance, which is not justified at this stage.\n\n## Consequences\n\n* **Standardization:** Flyway will enforce a standardized approach to database migration across our team.\n* **Improved reliability:** Automated migrations minimize the risk of human error during database updates.\n* **Streamlined development process:** Developers can focus on application logic without manually managing database changes.\n\nThis decision will be revisited if our requirements evolve significantly, such as needing support for NoSQL databases or more complex migration scenarios.\n","GenTime":"2024-08-19 15:33:08","text":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n\n\n##Decision\nWe will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"Prediction":"## Decision\n\nWe will implement a message queueing system with priority levels to ensure messages from Order Management are processed with higher precedence than messages from Rating Manager and Recommendation Manager. This means messages from Order Management will be placed at the front of the queue, ensuring they are handled before messages related to ratings or recommendations. \n","GenTime":"2024-08-19 15:33:10","text":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n\n\n##Decision\nThe decision is to introduce another queue for Order Management\\n"}
{"File Name":"digitalrig-metal-aws\/0006-create-reference-implementation-repository.md","Context":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n","Decision":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","tokens":162,"id":1742,"Prediction":"## Decision\n\nWe will create a central repository that acts as a template or blueprint for the AWS Bare Metal Rig, containing the base infrastructure-as-code (IaC) configuration.  This repository will **not** be directly deployable.  Instead, it will utilize a combination of:\n\n1. **Branching Strategy:** We will use a clear branching strategy (e.g., GitFlow) to manage different versions of the Rig. This will allow for easier discovery of different versions and their corresponding features.\n2. **Parameterization:** We will leverage IaC features (e.g., variables in Terraform, parameters in CloudFormation) to allow for customization of the Rig components. This enables users to choose their desired options (ECS EC2, Aurora PostgreSQL, etc.) without needing separate branches.\n3. **Modular Design:** We will design the IaC in a modular fashion, breaking it down into logical components (compute, database, networking, etc.).  This allows users to easily include or exclude specific components based on their needs.\n4. **Documentation:** Comprehensive documentation will be provided alongside the template, explaining the different components, parameters, and how to instantiate a customized Rig instance.\n\nThis central template repository will serve as the single source of truth for the latest AWS Bare Metal Rig configuration.  Specific instances of the Rig (like Bookit's) will be created by forking this repository and customizing the parameters and modules as needed. This approach promotes reusability, maintainability, and consistency across different Rig instances while providing flexibility for specific needs.\n","GenTime":"2024-08-19 15:33:17","text":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n\n##Decision\nCreate a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n"}
{"File Name":"log430-dashview-architecture\/0002-use-java-to-implement-complete-solution.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3820,"Prediction":"## Decision \n\n(Waiting for your input: Please provide the context, including the issue motivating the decision and any influencing factors, so I can help you formulate a corresponding decision.) \n","GenTime":"2024-08-19 15:33:18","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"opg-data\/0008-hateoas.md","Context":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","Decision":"Our API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","tokens":88,"id":2184,"Prediction":"## Decision\n\nWe will implement HATEOAS using the following approach:\n\n* **Approach:** [Choose one:  Spring HATEOAS Library (Java), HAL (JSON Hypertext Application Language), JSON API, Siren, Collection+JSON, other - specify]\n* **Reasoning:** [Explain the rationale behind the chosen approach, considering factors like: ease of implementation, industry adoption, tooling and library support, alignment with existing technology stack, future extensibility, etc.]\n\nThis decision prioritizes a truly RESTful API design that leverages hypermedia to its full potential, enabling discoverability and loose coupling between client and server. \n","GenTime":"2024-08-19 15:33:22","text":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n\n\n##Decision\nOur API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n"}
{"File Name":"konfetti\/0004-use-lazy-loading.md","Context":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n","Decision":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","tokens":37,"id":3572,"Prediction":"## Decision\n\nWe will adopt a strategy of lazy configuration loading in `konfetti`. This means configuration values will only be loaded when they are first accessed, rather than during the application startup. \n\nThis approach provides the following advantages:\n\n* **Avoids unnecessary side effects:** Configuration loading will not trigger any actions or side effects until a configuration value is explicitly requested by the application code.\n* **Enables partial configuration:** Tests can be run by only configuring the settings strictly required for them, without triggering errors for potentially missing values in other sections of the configuration file. \n* **Improves startup performance:** The application startup time will be reduced, especially with large configuration files, as the processing of the entire configuration file upfront is avoided.\n\nThis approach will require implementing a mechanism to handle missing or invalid configuration values at the point of access, potentially by providing default values or throwing specific exceptions. \n\nWe will use a library or build a custom solution for lazy loading based on the complexity and specific requirements of `konfetti`.\n","GenTime":"2024-08-19 15:33:26","text":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n\n##Decision\nWe will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n"}
{"File Name":"gp-data-etl\/0002-store-local-copy-of-syndication-data.md","Context":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n","Decision":"The syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","tokens":137,"id":2530,"Prediction":"## Decision\n\nWe will create a dedicated service to act as a caching layer between our applications and the NHS Syndicated Content API. This service will:\n\n1. **Fetch data from the Syndication API on a scheduled basis.** This adheres to the API usage guidelines by avoiding direct production access.\n2. **Transform the XML data into a more readily consumable format**, such as JSON.\n3. **Aggregate data from different subpages** (overview, services, facilities) for each GP surgery to provide a unified view. \n4. **Expose a well-defined RESTful API** for consumption by other Connecting to Services applications.\n\nThis approach offers several advantages:\n\n* **Improved performance:** Caching data reduces the load on the NHS Syndication API and minimizes response times for our applications.\n* **Simplified data access:**  Consuming data in a standardized format via a RESTful API is more developer-friendly than directly interacting with the Syndication API.\n* **Data enrichment and consistency:**  The service can perform data cleansing, validation and aggregation, ensuring data consistency and providing a unified view of GP information.\n\nThis decision promotes reusability, maintainability, and scalability while adhering to the NHS Syndication API usage guidelines. \n","GenTime":"2024-08-19 15:33:32","text":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n\n\n##Decision\nThe syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n"}
{"File Name":"Head-Start-TTADP\/0003-configuration-by-environment-variable.md","Context":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n","Decision":"We will use environment variables to configure applications.\\n","tokens":35,"id":1173,"Prediction":"## Decision\n\nWe will use configuration files to manage environment-specific settings for our applications.  These configuration files will be loaded at runtime based on the environment the application is deployed to (e.g., development, testing, production).\n\n**Rationale:**\n\n* **Flexibility:** Configuration files offer a flexible way to manage environment-specific settings without modifying code. This approach allows us to easily switch configurations by simply changing the loaded file. \n* **Maintainability:** Separating configuration from code promotes cleaner code and easier maintenance. Updates to configurations can be made without requiring code changes or recompilation.\n* **Security:** Sensitive data like API keys and database credentials can be stored securely outside the codebase and loaded only in the appropriate environment.\n\n**Implementation Details:**\n\n* We will use a widely adopted configuration file format like YAML, JSON, or .env files. \n* The application will determine the current environment through an environment variable (e.g., `APP_ENV`).\n* A configuration loader module will be responsible for reading the appropriate configuration file based on the detected environment.\n* Default values can be defined within the application code for settings that are not environment-specific.\n\n**Alternatives Considered:**\n\n* **Environment variables:** While environment variables offer a simple solution, they lack structure and can become cumbersome for complex configurations.\n* **Configuration management tools:** Tools like Vault or Consul provide robust solutions for managing secrets and configurations but might introduce additional complexity for smaller projects. \n\n**Consequences:**\n\n* **Positive:** Increased flexibility, maintainability, and security in managing environment-specific configurations.\n* **Negative:**  Slight increase in initial setup complexity compared to using only environment variables. \n","GenTime":"2024-08-19 15:33:39","text":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n\n\n##Decision\nWe will use environment variables to configure applications.\\n"}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"Prediction":"## Decision\n\nWe will consolidate the configuration of Verify Service Provider to use a single YAML file approach. This decision addresses the following:\n\n* **Eliminates configuration duality:**  Having a single mechanism for configuration simplifies the user experience and reduces potential confusion arising from multiple configuration methods.\n* **Flexibility for deployment:**  This approach allows for easier configuration management across different deployment environments.  While the previous \"environment variable only\" approach aligned with 12-factor app principles, it proved too inflexible for our use cases.\n* **Improved developer experience:** Using a YAML file provides a more human-readable and maintainable way to manage configurations compared to environment variables, especially as the configuration complexity grows.\n\n**Implementation Details:**\n\n1. **Configuration File:**  A single YAML file (e.g., `verify-service-provider.yml`) will be used for all configurations.\n2. **Command Line Argument:** The application will accept a command-line argument to specify the path to the configuration file. \n3. **Default Configuration:**  The application will load a default `verify-service-provider.yml` from the classpath if no command-line argument is provided. This allows for sensible defaults while maintaining flexibility.\n4. **Documentation:**  The documentation will be updated to clearly explain the new configuration mechanism and provide examples.\n\n**Addressing Previous Concerns:**\n\n* **Cloud Foundry Limitations:** With the evolution of Cloud Foundry and buildpacks, we can explore better solutions for managing configuration files in deployments, such as mounting config volumes.\n* **12-Factor App Philosophy:** While we acknowledge the benefits of environment-based configuration, the practical limitations and need for flexibility outweigh strict adherence to this principle in this case. \n\nThis decision prioritizes a pragmatic approach that balances simplicity, flexibility, and developer experience while leaving room for future improvements in deployment and configuration management.\n","GenTime":"2024-08-19 15:33:47","text":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n\n##Decision\nWe will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n"}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"Prediction":"## Decision\n\nWe will adopt Prettier as our default code formatter for all new and existing projects. This decision aims to:\n\n* **Enforce a consistent code style across all our projects**, improving code readability and maintainability.\n* **Reduce time spent on code style discussions and manual formatting**, allowing developers to focus on more critical tasks.\n* **Align with industry best practices** by leveraging a widely adopted and well-maintained tool. \n\nWe will integrate Prettier into our development workflow through:\n\n* **Editor integrations:** Encourage developers to use Prettier extensions for their preferred IDEs to format code on save.\n* **Pre-commit hooks:** Implement pre-commit hooks to automatically format code before each commit, ensuring that only consistently formatted code reaches the repository.\n* **CI\/CD pipeline:** Include a Prettier check in our CI\/CD pipeline to prevent merging code with formatting errors. \n\nWhile Prettier's opinionated nature minimizes configuration, we will define a minimal configuration file (`.prettierrc`) in the root of each project to ensure consistency across development environments and to customize any specific formatting preferences. \n","GenTime":"2024-08-19 15:33:52","text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n"}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"Prediction":"## Decision\n\n**We will use the Scala programming language to develop LaTiS.** This decision is driven by the confluence of Scala's suitability to the problem domain and practical considerations:\n\n* **Scala's hybrid nature** allows us to leverage the strengths of both object-oriented and functional paradigms. This is crucial for LaTiS, as it needs to handle complex data structures (benefiting from OOP) and perform intricate data transformations (benefiting from FP).\n* **Strong typing** in Scala promotes code correctness and maintainability. This aligns with our emphasis on explicit data transformations and reasoning about code behavior.\n* **Scala's growing popularity**, particularly within the data science and scientific computing communities, ensures a larger pool of potential developers. \n* **The functional programming aspects** of Scala, combined with its strong type system, make it well-suited for processing large datasets and expressing complex computations in a clear and concise manner.\n* **Scala's support for DSLs** allows us to create expressive and tailored syntax for specific problem domains within LaTiS. \n","GenTime":"2024-08-19 15:33:57","text":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n\n\n##Decision\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n"}
{"File Name":"cf-k8s-networking\/0013-rename-master-branch.md","Context":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n","Decision":"We are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","tokens":63,"id":2251,"Prediction":"## Decision\n\nThe decision has been made to rename the `master` branch to `main` in all Cloud Foundry repositories. This change aligns with the broader software development community's efforts to use more inclusive terminology and foster a welcoming environment for all contributors. \n","GenTime":"2024-08-19 15:33:59","text":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n\n\n##Decision\nWe are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n"}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"Prediction":"## Decision\n\nWe will create and maintain our own set of geometries representing municipality boundaries. These geometries will be specifically tailored to match the boundaries used for splitting road segments, ensuring accurate municipality association. \n\nThis decision is driven by the following factors:\n\n* **Data Inaccessibility:** The geometries in the municipality registry are not publicly accessible.\n* **Boundary Mismatch:** The existing municipality boundaries do not align with the boundaries used for segmenting roads, making them unreliable for our use case.\n* **Asynchronous Updates:** Changes in the official municipality boundaries may not always be reflected in road segment updates, requiring independent boundary management.\n\nThis approach provides a reliable and consistent method for determining the municipality of road segments, even when they lack explicit left\/right street information. \n","GenTime":"2024-08-19 15:34:03","text":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n\n\n##Decision\nWe keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n"}
{"File Name":"external-service-operator\/0004-ports-are-never-stored-as-named-ports.md","Context":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":70,"id":1735,"Prediction":"## Decision\n\nWhile the Kubernetes API allows storing ports as strings, we will **store ports as numeric values** for Backends outside the cluster. \n\nThis decision is based on the following:\n\n* **Clarity and Accuracy:**  Representing ports numerically accurately reflects their nature and usage for external Backends, which are accessed via IP addresses and port numbers. String representations are misleading and unnecessary in this context.\n* **Consistency:** Using numeric values for ports aligns with standard networking practices and ensures consistency within the system, even though the Kubernetes API offers flexibility.\n* **Preventing Misinterpretation:** Storing ports as strings might lead to misinterpretations by users or other tools expecting standard numeric port representations.\n\nAlthough the External Service Operator doesn't directly utilize the string representation for endpoint generation, storing ports numerically promotes better data integrity and understanding. \n","GenTime":"2024-08-19 15:34:07","text":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"Prediction":"## Decision\n\nWe will refactor the FxA authorization flow to minimize the times users are asked for their password. The new flow will adhere to the following principles:\n\n1. **Password Required:** Users MUST be prompted for their password when authorizing an RP that requires encryption keys (e.g., Sync, Lockwise, Send).\n2. **Password Not Required:** Users SHOULD NOT be prompted for their password when authorizing an RP that does not require encryption keys (e.g., Monitor, AMO) if they have an active session. Existing session tokens will be used to obtain new OAuth tokens.\n3. **Bug Fix:** The identified bug in state management logic that triggers unnecessary password prompts will be fixed.\n\nThis decision prioritizes user experience by minimizing friction during the authorization process. It also aims to improve the clarity and predictability of the flow for both users and developers. \n","GenTime":"2024-08-19 15:34:11","text":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n\n\n##Decision\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n"}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"Prediction":"## Decision \n\n(Write your decision here. Be concise and clear. This section should describe what the decision was.) \n","GenTime":"2024-08-19 15:34:12","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file living in the `\/docs\/adr` directory of the code repository, following a standardized template (e.g., MADR). \n","GenTime":"2024-08-19 15:34:14","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"Prediction":"## Decision\n\nWe will provide a Node.js library, leveraging Passport.js, to simplify authentication and authorization for Node.js users. \n\nThis library will:\n\n* **Offer pre-built Passport.js strategies** for our authentication system.\n* **Abstract away complexities** of communication and token handling.\n* **Provide clear and concise documentation** tailored for Node.js and Passport.js developers.\n\nFor users leveraging other languages and frameworks, we will provide:\n\n* **A well-documented REST API** with clear examples for various use cases. \n* **SDKs in popular languages** as needed, based on user demand and resource availability. \n","GenTime":"2024-08-19 15:34:18","text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n"}
{"File Name":"beis-report-official-development-assistance\/0030-run-data-migrations-manually.md","Context":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n","Decision":"With this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","tokens":69,"id":2396,"Prediction":"## Decision\n\nWe will move away from using the Data Migrate gem for database migrations. Instead, we will implement a new system with the following characteristics:\n\n* **Explicit Execution:** Migrations will be run as a separate, explicit step in the deployment process. This ensures developers are aware of migrations being applied and can address any issues proactively. \n* **Version Control:**  Migration code will be stored in version control, allowing for easy tracking, rollback, and collaboration on database changes.\n* **Robust Error Handling:** The new system will include comprehensive error handling and logging to provide clear insights into any issues encountered during migration execution.\n\nThis approach mitigates the risks associated with the Data Migrate gem by:\n\n* **Increasing Visibility:** Making migrations an explicit step increases awareness and allows for better monitoring.\n* **Improving Control:** Versioning and a dedicated system provide more control over the migration process.\n* **Enhancing Reliability:** Robust error handling reduces the likelihood of unexpected issues and simplifies debugging if they occur. \n","GenTime":"2024-08-19 15:34:22","text":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n\n##Decision\nWith this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n"}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"Prediction":"## Decision\n\nThe project will be implemented using the following technology stack:\n\n- **Frontend:** React with TypeScript\n- **State Management:** Redux\n- **Backend:** Node.js\n- **Package Management:** NPM\n- **Module Bundler:** Webpack\n\nThis decision is based on the desire to leverage modern web technologies and build a robust foundation for the project. React provides a component-based architecture for building user interfaces, while TypeScript enhances code quality and maintainability through static typing. Redux offers a centralized store for managing application state, promoting predictability and testability. Node.js enables server-side rendering and API development, while NPM and Webpack streamline dependency management and module bundling.\n","GenTime":"2024-08-19 15:34:26","text":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n\n\n##Decision\nI decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n"}
{"File Name":"lbh-frontend-react\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":1408,"Prediction":"## Decision\n\nWe will adopt Prettier for all projects within the organization. This will enforce a consistent coding style across all projects, improving code readability and maintainability. This decision aligns with Prettier's widespread adoption and status as an industry standard. \n","GenTime":"2024-08-19 15:34:27","text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n"}
{"File Name":"skypy\/adr-02.md","Context":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"Prediction":"## Decision\n\nWe will adopt the **Mpc** convention for units, i.e., **excluding the factor h**. \n\n## Reasoning\n\nWhile using **Mpc\/h** offers flexibility in propagating results across different Hubble constant values, we prioritize consistency and minimizing confusion by aligning with the **astropy** library's default unit of **Mpc**. \n\nThis decision promotes:\n\n* **Interoperability:** Using the standard unit simplifies integration with **astropy** and other astronomical software that adhere to this convention.\n* **Readability:** Expressing distances directly in Mpc enhances the clarity and interpretability of code and results for users familiar with standard astronomical units.\n* **Reduced Cognitive Load:** Eliminating the need to track and manage the h factor simplifies calculations and reduces the potential for errors.\n\nWe acknowledge that this choice may require additional steps when working with data or simulations that utilize the Mpc\/h convention. However, we believe the benefits of consistency and interoperability outweigh this minor inconvenience. \n\nFurthermore, we will provide clear documentation and helper functions within our codebase to facilitate the conversion between Mpc and Mpc\/h when necessary. \n","GenTime":"2024-08-19 15:34:33","text":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n\n\n##Decision\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n"}
{"File Name":"verify-self-service\/0003-use-pundit-for-rbac.md","Context":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n","Decision":"After some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","tokens":97,"id":3950,"Prediction":"## Decision\n\nWe will implement Role Based Access Control (RBAC) using a combination of JSON Web Tokens (JWT) for storing user roles and a centralized policy management system. \n\n**Authentication:**\n\n* Users will authenticate using industry-standard protocols such as OAuth 2.0 or OpenID Connect. \n* Upon successful authentication, the authorization server will issue a JWT containing the user's information, including their assigned roles. \n\n**Authorization:**\n\n* The application will validate the JWT on every request and extract the user's roles.\n* A centralized policy management system will be implemented to define permissions and map them to roles. \n* This system will provide an interface for administrators to create, edit, and delete roles and permissions.\n* Before accessing a protected resource, the application will consult the policy management system to determine if the user's roles have the required permissions.\n\n**Role Information Storage:**\n\n* User roles will be stored within the JWT issued upon successful authentication. \n* The policy management system will store the mapping between roles and permissions.\n\n**Team Structure:**\n\n* A dedicated security team will be responsible for:\n    * Managing the authentication and authorization infrastructure.\n    * Defining and maintaining the RBAC policy.\n    * Monitoring and responding to security events.\n* Application development teams will be responsible for:\n    * Integrating with the authentication and authorization system.\n    * Enforcing access control based on user roles and permissions. \n\n**Justification:**\n\n* **JWT for role storage:**  JWTs provide a secure and standardized way to transmit user information, including roles. This approach simplifies role management and eliminates the need for a separate role database.\n* **Centralized policy management:** Centralizing policy management provides a single source of truth for access control rules, simplifying administration and reducing the risk of inconsistencies.\n* **Dedicated security team:**  Having a dedicated security team ensures that RBAC is implemented and maintained effectively, minimizing security risks.\n\nThis approach provides a robust and flexible RBAC solution that is easy to manage and scales well with the application's growth. \n","GenTime":"2024-08-19 15:34:42","text":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n\n##Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n"}
{"File Name":"TANF-app\/010-assign-superuser.md","Context":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n","Decision":"The Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","tokens":35,"id":3792,"Prediction":"## Decision\n\nWe will create a custom Django management command to create a superuser. This command will be accessible programmatically, allowing us to trigger it during the application's startup sequence in production. This approach provides the following benefits:\n\n* **Security:** We avoid storing superuser credentials in configuration files or environment variables, which could be exposed accidentally.\n* **Flexibility:**  The command can be customized to prompt for user details or accept them as arguments, providing flexibility for different deployment scenarios. \n* **Maintainability:** Using a management command keeps the superuser creation logic within the Django ecosystem, promoting consistency and easier maintenance.\n\n\n**Implementation Details:**\n\n1. **Create the command:** A new management command will be created under the appropriate Django app directory (e.g., `management\/commands\/create_superuser.py`).\n2. **Command Logic:** The command will utilize Django's built-in `User.objects.create_superuser()` method to create the user with the provided details.\n3. **Production Integration:**  We will invoke the command within a script executed during the container startup or within the deployment pipeline, ensuring the superuser is available when the application starts in production.\n\n**Alternatives Considered:**\n\n- **Pre-creating the superuser:** This approach lacks flexibility and introduces security risks if the database is shared across environments.\n- **Using a third-party library:** While libraries exist for this purpose, they introduce additional dependencies and might not align with our specific requirements. \n\n**Consequences:**\n\n- **Development Impact:** Minor development effort is required to create and integrate the management command.\n- **Operational Impact:** This approach ensures a consistent and secure way to create the initial superuser in production.\n\nThis decision ensures a secure and manageable way to create the initial superuser in production, avoiding security risks associated with hardcoded credentials while maintaining flexibility for different deployment scenarios. \n","GenTime":"2024-08-19 15:34:50","text":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n\n\n##Decision\nThe Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n"}
{"File Name":"govuk-aws\/0032-transfer-artefact-binary.md","Context":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n","Decision":"We decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","tokens":95,"id":4057,"Prediction":"## Decision\n\nWe will implement a tagging strategy for binary artifacts stored in the AWS S3 bucket. Each artifact will be tagged with the following key-value pairs:\n\n* **Application:** Name of the application (e.g., \"govuk-integration\")\n* **Environment:** Environment the artifact was built for (e.g., \"Integration\", \"Staging\", \"Production\")\n* **Build Number:** Jenkins build number from the **Deploy_App** job \n\nThis approach offers several benefits:\n\n* **Clear identification:** Easily identify the correct artifact for deployment based on application, environment, and build number.\n* **Simplified retrieval:**  Streamline artifact retrieval for deployment scripts using tag-based searches.\n* **Version control:** Maintain a history of deployments by referencing specific build numbers. \n* **Automated cleanup:** Implement lifecycle policies based on tags to automatically delete older artifacts and optimize storage costs.\n\n**Implementation Details:**\n\n* The **Deploy_App** Jenkins job will be updated to tag artifacts uploaded to the S3 bucket.\n*  Deployment scripts will be modified to retrieve artifacts from S3 using the relevant tags. \n\nThis tagging strategy ensures a robust and scalable solution for managing binary artifacts throughout the application lifecycle. \n","GenTime":"2024-08-19 15:34:56","text":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n\n\n##Decision\nWe decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n"}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"Prediction":"## Decision\n\n**Adopt a trunk-based development model with a focus on small, frequent, and focused pull requests.**\n\nThis decision aims to address the pain points identified with the current PR flow by:\n\n* **Reducing PR size:** Encourage developers to break down large changes into smaller, more manageable PRs.\n* **Speeding up integration:** Frequent merging to the main branch reduces integration conflicts and accelerates development cycles. \n* **Improving PR quality:** Smaller, focused PRs are easier to review and test, leading to higher code quality.\n* **Increasing transparency and collaboration:** Continuous integration makes progress more visible and encourages ongoing feedback and collaboration. \n\nThis decision aligns with the concerns raised by developers, QA, and design in the mentioned discussions. By moving towards a more streamlined and collaborative development process, we aim to improve the overall development experience and deliver higher-quality software.\n","GenTime":"2024-08-19 15:35:00","text":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n\n\n##Decision\nThese specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n"}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"Prediction":"## Decision\n\nWe will update the UI repository and all UI applications and libraries to React 17 and adopt the following approach for replacing the `Responsive` component from `react-semantic-ui`:\n\n**Migrate to a custom `Responsive` component within our UI library.** \n\nThis decision is based on the following rationale:\n\n* **Control and Maintainability:**  Creating a custom solution gives us full control over its implementation, maintenance, and future evolution.  We can tailor it specifically to our needs and ensure it aligns with our coding standards and best practices. \n* **Avoiding External Dependencies:** While external libraries like `@artsy\/fresnel` offer solutions, introducing a new dependency for this specific functionality adds complexity.  Our UI library is already a central dependency; housing this component within it keeps related code consolidated. \n* **Gradual Migration Path:**  By providing a drop-in replacement for the `react-semantic-ui` `Responsive` component within our UI library, we allow for a smoother migration process for other teams. They can upgrade at their own pace while having a familiar API.  \n\n**Implementation Details:**\n\n1. **Component Creation:**  Develop a new `Responsive` component within our UI library ([https:\/\/github.com\/decentraland\/ui](https:\/\/github.com\/decentraland\/ui)). This component will:\n    * Use CSS media queries or a suitable alternative for responsiveness.\n    * Offer a similar API to the deprecated `react-semantic-ui` `Responsive` component to minimize breaking changes.\n    * Potentially provide additional features or improvements based on our needs.\n\n2. **Documentation and Examples:** Provide clear documentation and usage examples for the new `Responsive` component within the UI library.\n\n3. **Gradual Migration:** Encourage teams to migrate to the new `Responsive` component. We can provide migration guides and support as needed.\n\n4. **Deprecation:**  Once the migration is complete, we can deprecate and eventually remove the legacy `Responsive` component from our UI library.\n\n**Long-Term Benefits:**\n\n* A more maintainable and scalable UI codebase.\n* A consistent approach to responsiveness across our applications.\n* Reduced reliance on external dependencies. \n\nThis approach strikes a balance between a smooth migration path and long-term maintainability. \n","GenTime":"2024-08-19 15:35:10","text":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n\n##Decision\nThe option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n"}
{"File Name":"smjs\/2018121601-multiple-usages-of-the-processor.md","Context":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n","Decision":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","tokens":161,"id":3981,"Prediction":"## Decision\n\nWe decided **not** to use the same definition of Processor for all the use cases described. While the core functionality of fetching and executing instructions remains, the control flow and interaction with the user (or other components) differ significantly. Encapsulating all these variations within a single Processor class would lead to:\n\n* **Increased complexity:** The Processor would become bloated with conditional logic and state management for each use case, harming maintainability and readability.\n* **Reduced flexibility:** Adding new features or use cases would require modifying the core Processor, potentially introducing bugs and making it harder to reason about the code.\n* **Violation of the Single Responsibility Principle:** The Processor should be responsible for executing instructions, not for managing program execution flow or user interaction.\n\nInstead, we will define a **core Processor interface** responsible for fetching and executing instructions. We will then create **separate adapter classes** for each use case (e.g., `VMProcessor`, `REPLProcessor`, `DebugProcessor`). These adapters will implement the Processor interface and handle the specific control flow, input\/output, and debugging features for their respective use cases. \n\nThis approach provides several benefits:\n\n* **Improved modularity and maintainability:** Each use case is encapsulated in its own adapter, making the code easier to understand, modify, and test.\n* **Increased flexibility:** New use cases can be added by simply implementing the Processor interface without modifying existing code.\n* **Clear separation of concerns:** The core Processor focuses on its primary responsibility, while adapters handle use case-specific logic. \n\nThis approach allows us to leverage the core functionality of the Processor while accommodating the diverse requirements of different use cases in a clean and maintainable way. \n","GenTime":"2024-08-19 15:35:18","text":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n\n\n##Decision\nWe will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"Prediction":"## Decision\n\nWe will require **PHP 8.0** as a minimum version.\n\nThis decision balances the following factors:\n\n* **Modern features**: PHP 8.0 introduces several valuable features like constructor property promotion, the nullsafe operator, and union types that improve developer experience and code quality.\n* **Security**: Older PHP versions are no longer actively supported and may expose security vulnerabilities.\n* **Maintenance burden**: Supporting a wide range of PHP versions significantly increases the complexity and cost of maintaining the library.\n* **Adoption rate**: PHP 8.0 has seen significant adoption, and most modern projects are likely to be using at least this version.\n\nBy choosing PHP 8.0, we strike a balance between leveraging modern features, ensuring security, and minimizing maintenance overhead while maintaining reasonable accessibility for potential users.\n\nWe acknowledge that this decision may exclude users on older PHP versions. However, we believe that the benefits of using a modern PHP version outweigh the potential loss of a few users. We will clearly document the minimum PHP version requirement in our README and composer.json file. \n","GenTime":"2024-08-19 15:35:23","text":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n\n\n##Decision\nFor the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n"}
{"File Name":"PIMS\/database.md","Context":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n","Decision":"Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","tokens":49,"id":3966,"Prediction":"## Decision\n\nWe will utilize **PostgreSQL** as the database for PIMS. \n\n## Reasons\n\n* **Relational Data:** PostgreSQL is a robust, open-source relational database management system (RDBMS) well-suited for handling relational data with constraints.\n* **Linux Docker Container on OpenShift:**  PostgreSQL is highly container-friendly and has excellent official Docker images optimized for performance and security, making deployment on OpenShift straightforward.\n* **Entity Framework Core 3.1 Support:**  EF Core 3.1 provides a mature and fully-featured provider for PostgreSQL, ensuring seamless integration and efficient data access.\n* **Cost-Effective:** PostgreSQL is open-source and free to use, minimizing licensing costs for the project.\n\n## Considered Options\n\n* **MySQL:** Another popular open-source RDBMS, but PostgreSQL offers more advanced features and is generally favored for its data integrity and extensibility.\n* **MS SQL Server:** While a powerful database, licensing costs and reliance on Windows-based containers make it less suitable for the specified environment.\n\n## Consequences\n\n* **Positive:**\n    * Robust and reliable data storage solution.\n    * Simplified deployment and management using Docker on OpenShift.\n    * Efficient and well-supported integration with Entity Framework Core 3.1.\n    * Cost-effective choice due to its open-source nature.\n* **Negative:**\n    *  Development team may require familiarization with PostgreSQL if they are primarily experienced with other database systems. However, the prevalence of PostgreSQL and readily available resources mitigate this concern.\n\nThis decision aligns with the technical requirements and constraints of PIMS while leveraging a widely-used, powerful, and cost-effective database solution.\n","GenTime":"2024-08-19 15:35:30","text":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n\n##Decision\nOriginally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n"}
{"File Name":"simple-server\/014-region-level-sync.md","Context":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n","Decision":"- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","tokens":282,"id":1708,"Prediction":"## Decision\n\nWe will change the app's synchronization behavior to be block-specific. Instead of syncing an entire district's patient data, the app will only sync data for the block(s) a user is assigned to. This approach addresses the performance issues stemming from large database sizes on user devices, particularly lower-end phones. \n\n**Implementation Details:**\n\n* **User Block Assignment:**  We will introduce a mechanism for assigning users to specific blocks within a district. This could be managed through the admin console or potentially self-selected by the user upon login.\n* **Data Partitioning:** The synchronization process will be modified to select and download only the patient data belonging to the user's assigned block(s).\n* **Cross-Block Search (Future Consideration):**  While initial implementation will focus on block-specific data, we will investigate and potentially implement mechanisms for cross-block patient search in the future to accommodate the small percentage of patients who visit facilities across blocks. This could involve:\n    * A centralized search index for all patients in the district.\n    * A background process that periodically checks for patients with visits in multiple blocks and updates the user's local database.\n    * An option for users to manually trigger a search in other blocks.\n\n**Trade-offs:**\n\n* **Pros:**\n    * Significantly reduces the amount of data stored on user devices, improving performance, especially on lower-end phones.\n    * Simplifies data management and synchronization.\n* **Cons:**\n    * Introduces a small risk of users not finding patients who have visited facilities outside of their assigned block.\n    * Requires implementing a mechanism for assigning users to blocks. \n\n**Monitoring:**\n\n* We will closely monitor app performance after deployment to ensure the desired improvements are achieved. \n* User feedback will be crucial in assessing the impact of the change and identifying any usability issues related to block-specific data access. \n* We will track the usage patterns of cross-block patient searches to inform the priority of implementing a more robust solution for this scenario. \n","GenTime":"2024-08-19 15:35:39","text":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n\n\n##Decision\n- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n"}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"Prediction":"## Decision\n\nWe will implement a new **Composite Asset Processor** within the DLCS. This processor will be responsible for:\n\n1. **Identifying composite artifacts:** Determining if an ingested object is a composite artifact based on its format and metadata.\n2. **Extracting embedded resources:** Extracting individual resources from the composite artifact.\n3. **Ingesting extracted resources:**  Treating each extracted resource as a separate entity for ingestion into the DLCS, including generating unique identifiers and metadata inheritance.\n4. **Managing relationships:**  Establishing and maintaining the relationship between the original composite artifact and its extracted resources within the DLCS metadata model. \n\nThis approach allows us to leverage existing DLCS functionality for individual resources while accommodating the specific requirements of composite objects. \n","GenTime":"2024-08-19 15:35:43","text":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n\n\n##Decision\nDelivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n"}
{"File Name":"alfresco-anaxes-shipyard\/0004-k8s-cluster-creation.md","Context":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n","Decision":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","tokens":101,"id":3600,"Prediction":"## Decision\n\nWe will recommend **minikube** for development and testing purposes due to its ease of use on local machines. \n\nFor production environments, we will recommend using **Amazon Elastic Kubernetes Service (EKS)**.  This choice is based on the following factors:\n\n* **Widely adopted and well-documented:** EKS is a managed Kubernetes service from AWS, making it a robust and reliable option.\n* **Integration with AWS ecosystem:**  EKS seamlessly integrates with other AWS services, simplifying operations and deployments.\n* **User-friendly interface:** The AWS console provides a straightforward way to provision and manage EKS clusters, even for users with limited technical expertise.\n\nWhile other solutions listed in the Kubernetes on AWS documentation offer valuable features, EKS strikes a balance between functionality, ease of use, and integration with the AWS ecosystem, making it the most suitable recommendation for our customer base. \n","GenTime":"2024-08-19 15:35:47","text":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n\n\n##Decision\nWe will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n"}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"Prediction":"## Decision\n\nWe will use **Cypress** as our primary integration test framework for this project. \n\n## Justification\n\nWhile Selenium and TestCafe are both valid options, particularly for extensive cross-browser testing, Cypress offers several advantages that align better with our development workflow and priorities:\n\n* **Developer Friendliness:** Cypress's focus on developer experience with its convenient tooling, native application access, and excellent documentation makes it easier to write, run, and debug tests, leading to higher developer adoption and satisfaction.\n* **Speed and Efficiency:** Cypress's architecture allows for faster test execution and reduces the reliance on brittle workarounds often needed with Selenium. This translates to faster feedback loops during development.\n* **Modern JS Framework Support:**  Cypress is built to work seamlessly with modern JavaScript frameworks like Angular, Vue.js, and React.js, which are commonly used in single-page applications.\n* **Active Community & Future Proofing:** Being a newer framework with an active community, Cypress benefits from continuous development and improvement, making it a future-proof choice.\n\n## Consequences\n\n* **Limited Cross-Browser Testing Scope:**  While Cypress supports Chrome-based browsers and Firefox, it currently has more limited cross-browser support compared to Selenium or TestCafe. \n* **Potentially Steeper Learning Curve:** Developers familiar with Selenium might require some time to learn Cypress's specific APIs and concepts.\n\n## Mitigation\n\n* **Targeted Cross-Browser Testing:** To address the limited cross-browser support, we can strategically employ Selenium or a cloud-based testing service like SauceLabs or BrowserStack for critical cross-browser compatibility testing, focusing on specific browser\/OS combinations.\n* **Knowledge Sharing & Training:** We will invest in knowledge sharing and potentially organize training sessions to help developers quickly learn and adopt Cypress effectively. \n\nThis decision prioritizes developer productivity and fast feedback loops during development while acknowledging the importance of targeted cross-browser testing. \n","GenTime":"2024-08-19 15:35:56","text":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n\n\n##Decision\nAfter playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n"}
{"File Name":"jupyter-nbrequirements\/0000-dependencies-management-jupyter-notebooks.md","Context":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n","Decision":"* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","tokens":140,"id":1228,"Prediction":"## Decision\n\nTo ensure reproducibility of Jupyter Notebooks, we will adopt the following approach:\n\n1. **Dependency Management:** Each notebook will be accompanied by a dedicated dependency management file (e.g., `requirements.txt` for pip, `environment.yml` for conda). This file will explicitly list all direct and transitive dependencies required to run the notebook.\n\n2. **Containerization:** We will utilize containerization technology, such as Docker, to create isolated and reproducible environments for running the notebooks. Each notebook, along with its dependencies defined in the management file, will be packaged into a Docker image.\n\n3. **Version Control:** Both the notebooks and their corresponding dependency files will be version-controlled using a system like Git. This ensures that every change is tracked, and previous versions can be easily retrieved.\n\n4. **Execution Environment:**  Notebooks will be executed within their respective containerized environments. This can be facilitated through various methods, such as:\n    - Cloud-based solutions like Binder or Google Colab, which automatically build and launch containers from Git repositories.\n    - Local execution using Docker or similar containerization tools.\n\n## Consequences\n\nThis approach offers several benefits:\n\n- **Reproducibility:**  By packaging dependencies with notebooks and using containerization, we create consistent and self-contained environments, eliminating discrepancies caused by different system setups or library versions.\n- **Traceability:**  Version control allows tracking changes to both notebooks and dependencies, aiding in understanding modifications and their impact.\n- **Security:** Containerization provides isolation, limiting the impact of potentially vulnerable code within the notebook on the host system.\n- **Collaboration:** Sharing notebooks with others becomes simpler and more reliable, as the necessary execution environment is bundled.\n\nHowever, there are some potential drawbacks:\n\n- **Increased Storage:** Storing container images might require more storage space compared to storing notebooks alone.\n- **Complexity:**  Introducing containerization might introduce a slight learning curve for users unfamiliar with the technology.\n\nDespite these drawbacks, the benefits of reproducibility, traceability, and security outweigh the costs, making this the preferred approach for managing Jupyter Notebooks.\n","GenTime":"2024-08-19 15:36:05","text":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n\n##Decision\n* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n"}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will be a separate Markdown file, stored in the `\/docs\/adr` directory of the repository. \n","GenTime":"2024-08-19 15:36:06","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"stac-ml-aoi-extension\/0004-multiple-label-items.md","Context":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n","Decision":"`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","tokens":375,"id":2286,"Prediction":"## Decision\n\nWe will limit each `ml-aoi` Item to a single label link. \n\n## Reasons\n\nWhile supporting multiple labels directly within the `ml-aoi` Item offers flexibility and avoids preprocessing, the added complexity outweighs the benefits in most use cases. \n\n* **Simplicity and Performance:** Copying label item metadata directly to the `ml-aoi` Item simplifies data access and processing, eliminating the need to follow links. \n* **Reusability:**  This approach naturally extends the `ml-aoi` Item to function as a `label` Item, promoting reusability with existing tooling.\n* **Manageable Complexity:** Handling multiple label sources and their metadata within the `ml-aoi` Item introduces significant complexity for both data producers and consumers. \n\n## Consequences\n\n* **Preprocessing for Multi-class Classification:**  Users requiring multi-class datasets will need a preprocessing step to combine labels into a single `label` Item. This process, while adding a step, is well-defined and can be standardized. \n* **Limited External Label Integration:** Directly referencing external labels through links is not supported.  However, mechanisms for incorporating external data can be explored through alternative means (e.g., standardized IDs for cross-referencing).\n\n## Status\n\nAccepted. \n\n## Notes\n\nWe will provide clear documentation and potentially helper functions to facilitate the preprocessing step for multi-class datasets.  Future extensions can explore more sophisticated mechanisms for handling multi-label scenarios, if necessary, without compromising the core simplicity and performance of the current design. \n","GenTime":"2024-08-19 15:36:14","text":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n\n\n##Decision\n`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n"}
{"File Name":"digital-paper-edit-client\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":4171,"Prediction":"## Decision\n\nWe will implement a simple solution to start: a field for estimated voice-over time will be added to the rough cut estimation form. This field will allow users to input their estimated time directly. \n\nThis decision prioritizes ease of implementation and allows for user-dependent accuracy. We recognize that this approach may not be as accurate or extensible as other solutions. However, it provides a basic framework that we can iterate upon based on user feedback and future requirements. \n\nWe will monitor the usage and feedback of this feature closely. If there is a significant need for greater accuracy or automation in voice-over time estimation, we will revisit this decision and consider more complex solutions, such as integration with voice-over software or AI-based estimation tools.\n","GenTime":"2024-08-19 15:36:17","text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n"}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"Prediction":"## Decision\n\nWe will implement a **hybrid push\/pull synchronization mechanism with configurable broadcast options.**\n\nThis approach addresses the limitations of the current system and fulfills the outlined use cases:\n\n* **Push-based Synchronization:**  Users can push changes from any server (listed or unlisted) to a central repository (e.g., the DAO or a dedicated message queue). This enables any server to act as a source of updates, including \"local-staging\" environments.\n* **Pull-based Synchronization:** Servers can subscribe to specific content categories or channels from the central repository and receive updates relevant to them. This maintains efficient synchronization for servers with limited resources.\n* **Configurable Broadcast:**  Content creators can specify whether changes should be:\n    * **Globally Broadcast:**  Sent to all subscribing servers.\n    * **Selectively Broadcast:**  Sent to a specific subset of servers (e.g., a test group or a particular region). \n    * **Private:**  Kept on the originating server and only synchronized upon request.\n\nThis solution provides flexibility and scalability while addressing the concerns regarding server limitations and enabling new use cases like local staging and controlled content distribution. \n","GenTime":"2024-08-19 15:36:23","text":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n\n\n##Decision\n* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"Prediction":"## Decision\n\nUse Unix timestamps (number of milliseconds since 1970-01-01) for representing all date\/time values in the REST API.\n","GenTime":"2024-08-19 15:36:24","text":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\n\n##Decision\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n"}
{"File Name":"community\/0007-github-actions.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n","Decision":"We will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","tokens":222,"id":3456,"Prediction":"## Decision\n\nWe will migrate our automated build processes from Travis CI to Github Actions for all Libero products. \n","GenTime":"2024-08-19 15:36:25","text":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n\n\n##Decision\nWe will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"Prediction":"## Decision\n\nTo ensure zero downtime during database migrations, we will adopt a two-phase deployment strategy:\n\n**Phase 1: Backward Compatible Changes**\n\n*   All database changes must first be backward compatible with the existing production code.\n*   This means new columns should allow null values, new tables should not be immediately referenced, and data changes should be introduced gradually.\n*   Thorough testing should be conducted to ensure backward compatibility.\n\n**Phase 2: Application Deployment and Finalization**\n\n*   Once the backward-compatible database changes are deployed, the updated application code, which leverages these changes, can be deployed.\n*   After successful deployment and testing of the new application version, the database changes can be finalized.\n*   Finalization may involve steps like making columns non-nullable, adding foreign key constraints, or backfilling data.\n\nThis approach decouples database migrations from application deployments, allowing for controlled and safe updates without disrupting the live system. \n","GenTime":"2024-08-19 15:36:30","text":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n\n\n##Decision\nWhere possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"Prediction":"## Decision\n\nBased on the context provided and the considerations outlined, the following decisions have been made for the implementation of a Secret Provider:\n\n**Only Exclusive Secret Stores:**\n\n*   Each service will have its own exclusive secret store. This promotes security by isolating secrets on a per-service basis, reducing the blast radius in case of a compromise.\n\n**Abstraction Interface:**\n\n*   A common interface `SecretProvider` will be defined, outlining methods for accessing secrets. This interface will remain consistent across different implementations and languages.\n\n**Implementation:**\n\n*   **Factory Method and Bootstrap Handler:** A factory method `NewSecretProvider` will be responsible for creating instances of the `SecretProvider` based on the service context (e.g., environment variables). This factory method will be invoked during the bootstrap process of each service.\n*   **Caching of Secrets:** To improve performance, implementations of `SecretProvider` can choose to cache retrieved secrets. This caching should be configurable to allow for flexibility based on security and performance needs.\n*   **Insecure Secrets:** The `InsecureSecrets` configuration will be supported, allowing developers to override secrets from configuration for development and testing purposes.  These secrets should **never** be used in production environments.\n\n**Handling on-the-fly changes to `InsecureSecrets`:**\n\n*   For development convenience, changes to `InsecureSecrets` will be reflected in the running application without requiring a restart. The mechanism for this (e.g., file watch, signal handling) will be implementation-specific.\n\n**Mocks:**\n\n*   To facilitate testing, a mock implementation of the `SecretProvider` interface will be provided. This mock implementation will allow for injecting controlled secrets within tests.\n\n**Where will `SecretProvider` reside?**\n\n*   **Go Services:** The `SecretProvider` implementation and its related components will reside within a common package that can be imported by all Go services.\n*   **C Device Service:**  A suitable mechanism (e.g., shared library, common module) will be determined to make the `SecretProvider` functionality accessible to the C device service.\n\n**Consequences:**\n\n*   **Improved Security:** Service-specific secret stores enhance security by limiting the impact of potential compromises.\n*   **Increased Maintainability:** A unified interface and factory method promote code reusability and simplify secret management across services.\n*   **Flexibility:**  The design allows for different implementations of the `SecretProvider` interface, enabling adaptation to various secret storage mechanisms. \n*   **Developer Experience:** `InsecureSecrets` and mock implementations facilitate local development and testing by providing easy access to and control over secrets.\n","GenTime":"2024-08-19 15:36:41","text":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n"}
{"File Name":"fundraising-application\/005_URL_Schema.md","Context":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n","Decision":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","tokens":128,"id":1522,"Prediction":"## Decision\n\nWe will **not refactor the existing URL schema** at this time. \n\nWhile a consistent URL schema can improve readability and maintainability, the current inconsistencies do not pose a significant risk to the application's functionality or performance.  \n\nFurthermore, SEO is not a priority for the Fundraising application as the primary traffic driver is external banners. \n\nWe acknowledge that a more consistent URL schema would be ideal for future development and maintainability. If SEO becomes a priority or the application undergoes significant refactoring in the future, we will re-evaluate this decision. \n","GenTime":"2024-08-19 15:36:44","text":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n\n\n##Decision\nWe will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n"}
{"File Name":"SoundCloudVisualizer\/0004-use-gulp-to-build-source-code.md","Context":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n","Decision":"[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","tokens":81,"id":1576,"Prediction":"## Decision\n\nWe will use **Gulp** as our task runner to bundle and minify JavaScript files for the following reasons:\n\n* **Simpler configuration:** Gulp uses code-driven configuration with Node.js streams, making it more intuitive and easier to understand than Grunt's declarative JSON configuration. This will be beneficial for developers joining the project and needing to understand or modify the build process.\n* **Faster build times:** Gulp's use of Node.js streams for file operations allows it to perform tasks like copying and minifying files faster than Grunt, especially for larger projects. \n* **Growing community and plugin ecosystem:** While both Gulp and Grunt have large communities and extensive plugin ecosystems, Gulp's popularity was increasing at the time of this decision, suggesting better long-term support and more readily available resources.\n\nThis decision will improve the maintainability and scalability of the application by:\n\n* **Automating the process of bundling and minifying JS files**, reducing the number of HTTP requests and improving page load times.\n* **Providing a centralized location for managing build-related tasks**, making it easier to add new features and optimize the build process.\n\nWe will use Gulp in conjunction with plugins like **gulp-concat** to combine JavaScript files and **gulp-uglify** to minify them. \n","GenTime":"2024-08-19 15:36:50","text":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n\n\n##Decision\n[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n"}
{"File Name":"heptaconnect-docs\/2020-04-30-contracts-and-interfaces.md","Context":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n","Decision":"The approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","tokens":39,"id":3203,"Prediction":"## Decision\n\nWe will adopt the **plugin architecture** pattern to structure our code and ensure extensibility without interference. This decision stems from the requirement to enable the addition and modification of features with minimal impact on existing code. \n\nThis means:\n\n1. **Core Functionality:** Define a core application with well-defined interfaces and abstract classes. This core should encapsulate the fundamental aspects of the system that are unlikely to change frequently.\n\n2. **Plugin Interface:** Establish a clear and stable plugin interface. This interface will define how plugins interact with the core application, specifying the data exchanged and the functions available. \n\n3. **Independent Plugins:**  Develop individual functionalities as separate plugins. Each plugin will implement the defined interface, ensuring seamless integration with the core.  Plugins can be developed, tested, and deployed independently, promoting modularity.\n\n4. **Discovery Mechanism:** Implement a mechanism (e.g., configuration files, dependency injection) for the core application to discover and load available plugins.\n\n## Consequences\n\n**Pros:**\n\n* **High Extensibility:**  New functionality can be added as plugins without modifying the core application.\n* **Reduced Coupling:** Plugins and the core interact through a defined interface, minimizing dependencies and potential conflicts.\n* **Improved Maintainability:**  Smaller, modular components (plugins) are easier to understand, test, and maintain.\n* **Flexibility:**  Plugins can be added or removed at runtime, allowing for dynamic customization.\n\n**Cons:**\n\n* **Increased Complexity:** Designing a robust plugin system adds architectural complexity compared to a monolithic approach.\n* **Performance Overhead:** Plugin loading and communication might introduce minor performance overhead compared to direct function calls.\n* **Security Considerations:**  Properly defining the plugin interface and security policies is crucial to prevent malicious or insecure plugins from compromising the system.\n\nThis decision aligns with the goal of creating an extensible system by promoting modularity and reducing dependencies between different parts of the codebase.  \n","GenTime":"2024-08-19 15:36:58","text":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n\n\n##Decision\nThe approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n"}
{"File Name":"cf-k8s-networking\/0007-maintain-generated-istio.md","Context":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n","Decision":"We decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","tokens":133,"id":2236,"Prediction":"## Decision \ud83d\udca1\n\nMigrate Istio installation and lifecycle management from `cf-for-k8s` to `cf-k8s-networking`. This will allow the networking team to have direct control over Istio configuration and enable faster iteration on networking features. \n","GenTime":"2024-08-19 15:37:00","text":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n\n\n##Decision\nWe decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n"}
{"File Name":"architecture\/0008-code-owners.md","Context":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n","Decision":"Code ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","tokens":343,"id":1431,"Prediction":"## Decision\n\nTo encourage a wider sense of ownership and maintainability across our integrations, we will implement the following:\n\n**1.  Code Owner Policy:**\n\n*   We will formally define the role and responsibilities of a \"Code Owner\" within our contributor documentation. \n*   This definition will emphasize that code ownership signifies both an honorable contribution and, importantly, a commitment to ongoing maintenance and support.\n*   It will clarify that code ownership is not exclusive and encourages shared ownership for better knowledge distribution and sustainability.\n\n**2. Automated Code Owner Suggestion:**\n\n*   We will implement a mechanism (e.g., GitHub Action, bot) that automatically suggests adding the contributor as a code owner for the integration they contribute to (new integration, feature, or refactor).\n*   This suggestion will be presented to the contributor during the pull request process.\n*   Contributors will have the option to accept or decline the suggestion, but declining will require a brief justification.\n\n**3. Code Owner Review and Outreach:**\n\n*   We will initiate a review of existing integrations with single or no code owners.\n*   We will proactively reach out to past contributors and encourage them to consider adding themselves as code owners. \n*   If past contributors are unavailable, we will actively seek new contributors to take on code ownership.\n\n**4. Code Owner Recognition:**\n\n*   We will explore ways to better recognize and appreciate our Code Owners, such as dedicated acknowledgements on the website, repository, or during community events.\n\nBy implementing these measures, we aim to foster a more inclusive and sustainable development model for Home Assistant integrations. \n","GenTime":"2024-08-19 15:37:07","text":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n\n\n##Decision\nCode ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"Prediction":"## Decision\n\nTo enable the custom AMF validator to display the location of errors in the source file, we will implement a mechanism to track and propagate source location information through the AMF parsing and JSON-LD emission process.\n\nThis could involve:\n\n**1. Augmenting the AMF Model:**\n\n* Extend the internal representation of AMF nodes to include source location metadata (e.g., file name, line number, column number). \n* Modify the AMF parser to populate this metadata during parsing.\n\n**2. Propagating Location Information during JSON-LD Emission:**\n\n* Update the JSON-LD emitter to include the source location metadata alongside the corresponding JSON-LD nodes. This might involve adding custom properties or annotations to the generated JSON-LD.\n\n**3. Utilizing Location Information in the Validator:**\n\n* Modify the custom AMF validator to access and display the source location information associated with each validation error.\n\n**Alternatives Considered:**\n\n* **Passing file information separately to the validator:** This approach would require significant changes to the validator's API and complicate its usage.\n* **Relying solely on line numbers:** While simpler to implement, this approach would lack the granularity of column information, making it harder to pinpoint the exact error location.\n\n**Consequences:**\n\n* **Increased Complexity:** The AMF model, parser, and JSON-LD emitter will require modifications, increasing the complexity of the codebase.\n* **Potential Performance Impact:** Adding and processing source location metadata might slightly impact performance.\n\n**Rationale:**\n\nThis approach provides a robust and accurate way to track and display the source location of errors, significantly improving the usability of the custom AMF validator and simplifying debugging for developers. The benefits of improved error reporting outweigh the potential increase in complexity and performance impact. \n","GenTime":"2024-08-19 15:37:15","text":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n\n\n##Decision\nA new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"Prediction":"## Decision\n\nWe will suppress SonarQube's RSPEC-2142 warning globally for our project. \n\nWhile we acknowledge the importance of preserving the interrupted status for thread pool management as highlighted by the rule, the current limitations with JUnit's thread reuse make it impractical to address this issue directly. Silencing the warning locally could lead to overlooking genuine cases where the interrupted status should be propagated. \n\nThis decision prioritizes the stability of our CI builds and avoids introducing potentially disruptive changes solely to satisfy a static analysis tool. \n\nWe will revisit this decision if:\n\n* We migrate to a testing framework that handles thread interruptions more reliably. \n* SonarQube or JUnit provide better mechanisms to address this specific conflict.\n* We implement a robust thread pooling mechanism in the future where propagating interruption becomes essential for our application's logic.\n\nUntil then, we accept the trade-off of suppressing the warning and acknowledge the potential risk of masking genuine thread interruption issues. We will rely on careful code reviews and thorough testing to mitigate this risk. \n","GenTime":"2024-08-19 15:37:20","text":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n\n\n##Decision\nWe will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"Prediction":"## Decision\n\nWe will use **MQTT** as a protocol and **Eclipse Mosquitto** as a broker. \n\n## Justification\n\n- **MQTT** is a lightweight protocol, specifically designed for resource-constrained devices often found in IoT deployments. This addresses the \"Applicability regarding IoT projects\" driver.\n- While not its primary use case, **MQTT** can handle the streaming of images\/frames.  The \"Possibility to use it to stream frames\/images\" driver is satisfied with some caveats:\n    -  Consider compressing image data before publishing.\n    -  Large or frequent image streams might necessitate performance optimizations or alternative solutions depending on the specific requirements.\n- **Eclipse Mosquitto** is lightweight, open-source, and has excellent community support. It also offers official Docker images, simplifying deployment on Kubernetes and addressing the \"Ease of deployment on Kubernetes\" driver. \n- While the ADR doesn't specify the team's existing knowledge, the widespread adoption of MQTT and the availability of resources make it a good choice even without prior experience. This caters to the \"Existing knowledge of the team\" driver by minimizing the learning curve. \n\n## Considered Alternatives\n\n- **Kafka**: More robust and feature-rich but might be overkill for the current needs, potentially impacting resource usage on devices.\n- **RabbitMQ**:  A versatile option, but its resource requirements might be higher compared to Mosquitto, especially in resource-constrained environments.\n\n## Consequences\n\n- Choosing a lightweight approach might require future optimization or reconsideration if the application's needs evolve significantly (e.g., extremely high throughput image streaming).\n- Team members without prior MQTT experience will need to dedicate time for learning. However, the readily available resources and the technology's relative simplicity should mitigate this. \n","GenTime":"2024-08-19 15:37:28","text":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n\n\n##Decision\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n"}
{"File Name":"react-template\/0005-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":3323,"Prediction":"## Decision\n\nWe will use Jest as our test framework. \n","GenTime":"2024-08-19 15:37:29","text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n"}
{"File Name":"interlok\/0006-workflow-callback.md","Context":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n","Decision":"Chosen option: Modify AdaptrisMessageListener to have callbacks.\\n","tokens":171,"id":2343,"Prediction":"## Decision\n\nGiven the constraints of SQS Dead Letter Queues requiring messages to not be deleted from the source queue and the limitations of our JMS implementation, we will implement a separate mechanism for handling messages that fail processing by the workflow. \n\nInstead of relying on SQS Dead Letter Queues, we will adopt the following approach:\n\n1. **Workflow-level Error Handling:** Implement robust error handling within the workflow itself. This includes catching exceptions, retrying failed tasks, and logging errors. \n\n2. **Error Queue:** Upon encountering an unrecoverable error in the workflow, the message will be published to a dedicated \"Error Queue\" along with relevant error context.\n\n3. **Error Monitoring and Remediation:**  Establish monitoring and alerting for the Error Queue. A separate process or system will be responsible for consuming messages from the Error Queue, investigating the issues, and taking appropriate remediation actions (e.g., manual intervention, retrying after fixing the issue, etc.).\n\nThis approach provides several benefits:\n\n* **Decoupling:** Decouples error handling from message consumption and SQS queue mechanics.\n* **Flexibility:** Offers greater flexibility in handling different types of errors and implementing custom retry logic.\n* **Visibility:** Provides a centralized location for monitoring and managing errors.\n\nThis decision avoids reliance on SQS-specific features and leverages application-level error handling for greater control and visibility. \n","GenTime":"2024-08-19 15:37:35","text":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n\n\n##Decision\nChosen option: Modify AdaptrisMessageListener to have callbacks.\\n"}
{"File Name":"my-notes\/prettier.md","Context":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"Prediction":"## Decision\n\nReplace Standard with Prettier for code formatting.\n\n### Reasoning\n\nWhile Standard offers a \"zero-configuration\" experience, the identified conflicts with Prettier within VS Code present a significant hurdle for developer experience. \n\nPrettier offers wider editor integration and community support, making it a more robust and less disruptive choice for our development workflow.  \n\nThis decision prioritizes a smoother development experience and wider tool compatibility over the specific stylistic choices enforced by Standard. \n","GenTime":"2024-08-19 15:37:38","text":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n\n\n##Decision\nReplace Standard with Prettier\\n"}
{"File Name":"holochain-rust\/0010-write-our-own-dht-implementation.md","Context":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n","Decision":"We will build our own DHT implementation in Rust from scratch.\\n","tokens":110,"id":1494,"Prediction":"## Decision\n\nWe will develop a custom distributed hash table (DHT) implementation in Rust, tailored specifically for Holochain's needs. This decision is driven by the following factors:\n\n* **Language incompatibility:** The existing Go-based Kademlia implementation is not directly portable to Rust. \n* **Holochain-specific requirements:** The current implementation has numerous Holochain-specific modifications, making it difficult to maintain and evolve independently.\n* **Future-proofing:** Developing a custom DHT provides greater flexibility to integrate future features, such as the World Model, CRDT handling, and advanced graph properties, without being constrained by existing implementations.\n* **Control and optimization:**  Building our own DHT allows for fine-grained control over performance optimizations and security considerations specifically tailored to Holochain's architecture and use cases. \n","GenTime":"2024-08-19 15:37:42","text":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n\n\n##Decision\nWe will build our own DHT implementation in Rust from scratch.\\n"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"Prediction":"## Decision\n\nWe will adopt the following approach for migrating client-side queries to use Protocol Buffers:\n\n1. **Define Protobuf messages for queries and responses:** For each existing query route, define corresponding Protobuf messages for both the query input and the response output. These messages should be defined in a dedicated `.proto` file within the Cosmos SDK repository, under a package dedicated to client-side interactions.\n2. **Implement Protobuf encoding and decoding in the client libraries:** Update the client libraries to support encoding queries as Protobuf messages and decoding responses received in Protobuf format. This will involve adding new methods or modifying existing ones to work with both legacy and Protobuf-encoded data.\n3. **Maintain backward compatibility during the transition period:**  During the migration, the client libraries should maintain backward compatibility with the existing REST API. This can be achieved by supporting both Protobuf and legacy encoding\/decoding for queries and responses. The client libraries should default to using Protobuf encoding but allow users to opt-out and use the legacy format if needed.\n4. **Introduce a feature flag for enabling\/disabling Protobuf encoding:** A feature flag will be introduced in the client libraries to control whether Protobuf encoding is enabled or disabled by default. This allows for a gradual rollout of the new encoding scheme and provides users with control over the transition process.\n5. **Deprecate and remove legacy encoding in a future release:** Once the migration to Protobuf encoding is complete and the ecosystem has adapted, the legacy encoding mechanism will be deprecated and eventually removed in a future release of the Cosmos SDK. This ensures a clean and consistent API surface based on Protobuf.\n\nThis approach balances the need for a smooth transition with the long-term goal of adopting Protocol Buffers as the primary encoding format for client-side interactions in the Cosmos SDK.\n","GenTime":"2024-08-19 15:37:50","text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n\n\n##Decision\n### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n"}
{"File Name":"libelektra\/error_handling.md","Context":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n","Decision":"- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","tokens":44,"id":1292,"Prediction":"## Decision\n\nTo address ambiguous return values, we will implement a dedicated error handling mechanism that provides fine-grained error codes and messages. \n\nInstead of relying solely on return values, functions prone to ambiguous results will return a structured object or tuple. This structure will contain:\n\n1. **Status Code:** An enum value representing success or a specific error condition.\n2. **Return Value:** The actual data to be returned, only populated on success. This could be null or an empty structure for operations not returning data.\n3. **Error Information:** Optional field containing details about the error. This might be a human-readable message, an internal error code for debugging, or both.\n\nThis approach achieves the following:\n\n* **Clear Error Differentiation:** Enables distinguishing between different failure modes, like \"invalid name\" vs. \"key locked.\"\n* **Improved Debugging:** Provides more informative error messages, facilitating troubleshooting.\n* **Cleaner Code:** Separates error handling logic from the regular execution flow.\n\n**Example:**\n\n```python\n# Instead of:\ndef modify_key_name(key, new_name):\n  # ... logic ...\n  return False  # Could mean invalid name or key locked\n\n# New approach:\ndef modify_key_name(key, new_name):\n  # ... logic ...\n  if invalid_name:\n    return {\"status\": \"error\", \"error_code\": \"INVALID_NAME\", \"message\": \"Invalid characters in new key name\"}\n  elif key_locked:\n    return {\"status\": \"error\", \"error_code\": \"KEY_LOCKED\", \"message\": \"Key is currently locked\"}\n  else:\n    # ... update key name ...\n    return {\"status\": \"success\", \"value\": True}\n```\n","GenTime":"2024-08-19 15:37:58","text":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n\n\n##Decision\n- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"Prediction":"## Decision\n\nThis starter seed will **not** assert an opinion on state management libraries. \n\nDevelopers will be free to choose the state management solution that best fits their project's needs and their own preferences. \n\n### Justification\n\nWhile providing a pre-configured state management solution can be convenient, it can also lead to:\n\n* **Increased learning curve:**  Developers unfamiliar with the chosen library may face a steeper learning curve. \n* **Limited flexibility:**  Pre-determined choices may not be suitable for all projects and could limit flexibility when requirements change.\n* **Potential for \"outdated\" choices:** The rapidly evolving landscape of state management could mean a pre-selected library becomes outdated quickly.\n\nBy leaving this decision open, we encourage:\n\n* **Flexibility:** Developers can choose the best tool for their specific project needs and scale their solution as required.\n* **Exploration and learning:** This approach encourages developers to explore different state management options and expand their skillset.\n* **Simpler maintenance:**  Avoiding dependencies on specific libraries simplifies maintenance and reduces potential conflicts in the future.\n\n### Consequences\n\n* **Increased initial setup time:** Developers will need to invest some time in researching and integrating their chosen state management solution.\n* **Potential for inconsistency:** Different projects may use different approaches to state management.\n\nTo mitigate these consequences, the starter seed will:\n\n* Provide clear documentation and examples demonstrating how to integrate various popular state management libraries.\n* Offer guidance and best practices for choosing and implementing state management solutions.\n\nThis decision will be revisited periodically to ensure it remains aligned with the evolving landscape of state management and the needs of the developer community. \n","GenTime":"2024-08-19 15:38:05","text":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n\n\n##Decision\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n"}
{"File Name":"CCS-Architecture-Decision-Records\/0004-language-choices-for-new-development.md","Context":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n","Decision":"New services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","tokens":329,"id":1000,"Prediction":"## Decision\n\nCCS will adopt a \"preferred language\" approach for new services, focusing on a limited set of programming languages while maintaining flexibility for specific use cases.\n\n**Preferred Languages:**\n\n* **Python:**  Will be the primary language for new backend services due to its prevalence in government, large community support, extensive libraries for data analysis and machine learning, and strong alignment with the CCS Digital Marketplace.\n* **JavaScript (Node.js):** Will be the preferred language for new frontend services and APIs to align with the GOV.UK Frontend Toolkit and modern web development practices.\n\n**Acceptable Languages:**\n\n* Existing systems will continue to be supported in their current languages.\n* Justified exceptions to the preferred languages can be made on a case-by-case basis, considering the factors outlined in the context section (e.g., unique skill set requirements, specific library dependencies, integration needs). These exceptions require architectural review and approval.\n\n**Justification:**\n\nThis approach balances flexibility with maintainability:\n\n* **Reduces Cognitive Load:** Focusing on a core set of languages streamlines development, onboarding, and knowledge sharing within CCS.\n* **Leverages Existing Expertise:**  The selected languages align with existing skills within CCS and the broader government technology landscape.\n* **Supports Future Maintainability:**  A limited language set simplifies future support, reduces dependency on individual suppliers, and allows for easier talent acquisition.\n* **Allows for Innovation:**  The \"acceptable language\" provision ensures CCS can leverage specialized tools and technologies when necessary. \n\n**Implementation:**\n\n* The above decision will be documented in the CCS technology strategy and guidelines.\n*  Guidance will be provided to teams on language selection, outlining the justification process for using an \"acceptable\" language.\n* Training and upskilling opportunities will be offered to support the preferred languages.\n\nThis approach will be reviewed periodically to ensure its continued alignment with CCS's strategic objectives and the evolving technology landscape. \n","GenTime":"2024-08-19 15:38:13","text":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n\n\n##Decision\nNew services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n"}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"Prediction":"## Decision\n\nArchive the `decentraland\/standards` repository. Keep both the `decentraland\/adr` and `decentraland\/proposals` repositories untouched. \n\nThis decision leverages the functionality of ADRs to encompass the existing standards, consolidates documentation, and maintains the community-driven proposals repository. \n","GenTime":"2024-08-19 15:38:15","text":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n\n\n##Decision\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n"}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"## Context\\nContext for rate limiting pattern\\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"Prediction":"## Decision\n\n**Decision:** Implement rate limiting using a token bucket algorithm at the API gateway level.\n\n**Reason:** \n\n* **Centralized Control:** Implementing at the API gateway provides a single point of control for all incoming requests, simplifying management and ensuring consistency.\n* **Protection against Spikes:** The token bucket algorithm allows for burst traffic while effectively throttling excessive requests, preventing overload and ensuring service availability. \n* **Flexibility:**  Token bucket provides flexibility to configure different rate limits for various endpoints or user groups based on their specific needs and service level agreements.\n\n**Alternatives Considered:**\n\n* **Leaky bucket algorithm:** Less flexible in handling bursts of traffic compared to token bucket.\n* **Fixed window counter:**  Prone to spikes at the beginning of each window.\n* **Sliding window log:**  More resource intensive than token bucket.\n* **Implementing rate limiting at the application level:**  Introduces code complexity and inconsistencies across different parts of the application.\n\n**Consequences:**\n\n* **Improved System Stability:**  Protects against rate-based attacks and prevents service degradation due to overload.\n* **Fair Resource Allocation:** Ensures fair usage for all users and prevents a single client from monopolizing resources. \n* **Potential Increased Latency:**  Clients exceeding the rate limit might experience increased latency or rejected requests.\n* **Implementation Overhead:**  Requires initial setup and configuration of the rate limiting mechanism at the API gateway.\n\n**Status:** Accepted\n\n**Last Updated:** 2023-10-26 \n","GenTime":"2024-08-19 15:38:22","text":"## Context\\nContext for rate limiting pattern\\n\n\n##Decision\nProsa about the decision in the ADD\\n"}
{"File Name":"pottery\/0004-store-shards-in-time-stamped-directories-and-files.md","Context":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n","Decision":"Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","tokens":56,"id":2508,"Prediction":"## Decision\n\nWe will implement a hierarchical directory structure for storing project history shards.  Shards will be grouped into subdirectories based on time periods, such as by year and month. This approach addresses the performance and usability concerns associated with storing all shards in a single directory. \n\n**Example structure:**\n\n```\nproject_history\/\n  - 2023\/\n    - 01\/  \n    - 02\/\n    - ...\n  - 2024\/\n    - 01\/\n    - ... \n```\n\nThis hierarchical structure offers several advantages:\n\n* **Improved Performance**: File system operations like listing and searching will be significantly faster, especially on older file systems not optimized for large directories.\n* **Enhanced Organization**:  The directory structure provides a logical and chronological organization of project history, making it easier to navigate and locate specific time periods.\n* **Simplified Maintenance**:  Archiving or deleting older history becomes more manageable as operations can be performed on specific subdirectories instead of the entire dataset. \n\nThis decision prioritizes scalability and maintainability of the project history system as it grows over time. \n","GenTime":"2024-08-19 15:38:27","text":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n\n\n##Decision\nStore shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n"}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"Prediction":"## Decision\n\nGiven the context and decision drivers, the following decision has been made:\n\nA new event, \"ResultsRendered\", will be emitted after the search results have been rendered. This event will carry a boolean payload indicating whether the \"Viewed by Customer\" feature is enabled (based on the UserActions component state). \n\nAn event listener will be implemented on all result templates. Upon receiving the \"ResultsRendered\" event, the listener will:\n\n1. Check if the \"Viewed by Customer\" feature is enabled.\n2. Check if the `ViewedByCustomer` component already exists within the template.\n3. If enabled and not already present, the listener will dynamically add the `ViewedByCustomer` component to the template. \n\nThis approach ensures that:\n\n* The `ViewedByCustomer` component is added consistently to all result templates, including those added later.\n* Duplicate components are avoided.\n* The feature can be toggled on\/off via the UserActions component. \n","GenTime":"2024-08-19 15:38:32","text":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n\n\n##Decision\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"Prediction":"## Decision\n\nWe will implement partial updates using the PATCH HTTP method with JSON Merge Patch (RFC 7396) as the payload format. \n\n## Justification\n\n* **Standard Compliance:** JSON Merge Patch is a well-defined and widely accepted standard for expressing partial updates.\n* **Simplicity:** The format is easy to understand and implement for both client and server.\n* **Flexibility:**  It allows for adding, replacing, or removing object members in a single request.\n* **Efficiency:**  Only the modified fields need to be transmitted, reducing bandwidth consumption compared to full updates.\n\n## Consequences\n\n* **Implementation Effort:** Minimal effort is required to implement JSON Merge Patch support in most frameworks.\n* **Client Compatibility:** Clients need to support the PATCH method and JSON Merge Patch format. \n* **Error Handling:** Clear error messages should be provided for invalid merge patches or conflicts.\n* **Versioning:** Consider implementing an optimistic locking mechanism (e.g., ETags or version numbers) to handle concurrent updates and prevent data loss. \n\n## Alternatives Considered\n\n* **PUT with full object representation:** This approach is simpler but less efficient as it requires transmitting the entire object even for small updates.\n* **Custom API endpoints for each field:** This offers flexibility but increases complexity and can lead to inconsistent API design.\n* **GraphQL:** Offers powerful and flexible data fetching and manipulation capabilities but requires a significant paradigm shift and potentially higher initial implementation effort.\n\n## Status\n\nAccepted\n\n## Notes\n\nWe will revisit this decision if:\n\n* JSON Merge Patch proves insufficient for our needs.\n* A more suitable standard for partial updates emerges.\n* We encounter significant compatibility issues with clients.\n","GenTime":"2024-08-19 15:38:40","text":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n\n\n##Decision\nA merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n"}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"Prediction":"## Decision\n\nThe scratch app will communicate with a locally running web server to execute native program calls. This approach avoids the need for Electron, minimizing the application's size and dependency on specific platforms. The web server will act as an intermediary, receiving requests from the scratch app, invoking the necessary native programs (arduino-cli and scratch-to-c), and returning the results to the app.\n","GenTime":"2024-08-19 15:38:42","text":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n\n\n##Decision\nWe will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n"}
{"File Name":"unit-e-project\/ADR-0002.md","Context":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n","Decision":"We outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","tokens":52,"id":3768,"Prediction":"## Decision\n\nWe will use the following terminology across the entire codebase for roles in the Esperanza protocol:\n\n* **Validator:** Replaces terms like \"staking\" and \"miner.\"  A validator is any entity actively participating in the consensus mechanism by locking up tokens and performing validation tasks.\n* **Proposer:**  Remains as \"proposer.\" A proposer is the specific validator chosen to propose a new block at a particular slot.\n\n**Rationale:**\n\n* **Clarity and Consistency:**  Using \"validator\" consistently avoids ambiguity with terms like \"staking\" which can refer to both the act of locking tokens and the entity doing so. This aligns with common terminology used in other Proof-of-Stake systems. \n* **Specificity:**  \"Miner\" is inappropriate for a Proof-of-Stake system.  \"Validator\" accurately reflects the role in Esperanza.\n* **Familiarity:** \"Proposer\" is already a well-understood term in blockchain and doesn't carry the same ambiguity as \"miner\" or \"staking\".\n\n**Consequences:**\n\n* **Refactoring Effort:** Existing code will need to be refactored to reflect this decision.\n* **Improved Maintainability:** Consistent naming will increase code readability and reduce the potential for errors.\n* **Easier Onboarding:**  New developers will have an easier time understanding the codebase with clear and consistent terminology. \n\n**Implementation:**\n\n* **Code Style Guide:** Update the style guide to enforce the use of \"validator\" and \"proposer.\"\n* **Refactoring:**  Gradually refactor existing code to align with the new terminology.  Consider using automated tools where possible.\n* **Code Reviews:** Pay close attention to terminology used during code reviews to ensure consistency. \n","GenTime":"2024-08-19 15:38:50","text":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n\n\n##Decision\nWe outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"Prediction":"## Decision\n\nDXC will adopt a hybrid approach to operating system (OS) images in AWS, leveraging a combination of AWS Managed Images, curated Marketplace images, and custom-built images. This approach balances standardization, security, flexibility, and cost-effectiveness.\n\n**Rationale:**\n\n* **AWS Managed Images:** Utilizing AWS Managed Images for widely used operating systems like Amazon Linux 2 and Windows Server will simplify management, patching, and ensure optimal performance with AWS services. \n* **Curated Marketplace Images:** For specialized software stacks or applications with pre-configured dependencies, leveraging trusted and verified images from the AWS Marketplace offers a faster deployment path and reduces configuration overhead.\n* **Custom-built Images:** In specific cases where unique security hardening, compliance requirements, or specific software configurations are necessary, building and maintaining custom images will be permitted. However, this approach will be subject to stricter governance, security reviews, and documentation requirements to mitigate potential risks associated with image sprawl and maintenance overhead. \n\n**This decision entails the following actions:**\n\n* **Develop a clear image selection process:** Define criteria and guidelines for choosing between AWS Managed Images, Marketplace Images, and custom builds based on factors like security posture, compliance requirements, application dependencies, and cost considerations.\n* **Establish an image governance model:** Define processes for building, testing, documenting, versioning, and securing custom images. Implement security scanning and vulnerability management for all images.\n* **Implement a centralized image repository:** Utilize AWS Systems Manager Parameter Store or similar mechanisms to centrally manage and share approved images across different teams and projects.\n* **Promote the use of Infrastructure as Code (IaC):** Encourage the use of IaC tools like AWS CloudFormation or Terraform to define and automate image creation, configuration, and deployment processes for improved consistency and repeatability.\n* **Regularly review and optimize image usage:** Periodically assess image usage patterns, identify and deprecate unused or outdated images, and promote the adoption of standardized images to minimize security risks and operational complexity. \n\n**This decision aims to achieve the following benefits:**\n\n* **Improved Security:** Utilizing AWS Managed Images and trusted Marketplace images with established security postures enhances the baseline security of deployed workloads.\n* **Increased Agility:** Provides flexibility to leverage pre-configured images for faster deployment while maintaining the ability to customize for specific requirements.\n* **Reduced Costs:** Minimizes operational overhead by utilizing pre-built images and reducing the need for extensive image management.\n* **Enhanced Governance:** Implementing standardized processes and tooling for image management ensures better control, visibility, and compliance across the organization. \n","GenTime":"2024-08-19 15:39:00","text":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n\n\n##Decision\nWe will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n"}
{"File Name":"mat-process-utils\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3763,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight mechanism to document significant architectural decisions. Each record will follow a standardized template (e.g., MADR) and be stored in a dedicated version-controlled directory within the project repository. \n","GenTime":"2024-08-19 15:39:02","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"lobiani\/0004-accept-coupling-with-spring-boot-in-backend.md","Context":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n","Decision":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","tokens":127,"id":1035,"Prediction":"## Decision\n\nWe will **not introduce an additional abstraction layer on top of Spring Boot\/Framework**. \n\nWhile we acknowledge the general principle of reducing coupling with 3rd party libraries, we believe that in this specific case, the practical benefits of directly using Spring Boot outweigh the potential drawbacks of increased coupling. \n\nThis decision is based on the following reasons:\n\n* **Spring Boot's design already emphasizes abstraction and extensibility:**  Introducing another layer of abstraction might lead to unnecessary complexity and overhead without providing significant benefits. \n* **The Spring ecosystem offers a rich set of tools and libraries:** Leveraging these directly will contribute to faster development, improved maintainability, and easier integration with other Spring-based projects. \n* **The risk of needing to replace Spring Boot entirely in the future is deemed low:**  Spring Boot is a mature and widely adopted framework with a strong community and ongoing support. \n\nHowever, to maintain a degree of flexibility and avoid tight coupling within our codebase, we will adhere to the following guidelines:\n\n* **Favor dependency injection and interface-based programming:** This will allow us to substitute specific Spring components or modules with alternatives if needed in the future. \n* **Encapsulate direct usage of Spring annotations and classes within specific modules or layers:**  This will help to isolate the impact of potential future changes in Spring and make it easier to migrate or adapt the codebase.\n\nThis approach allows us to benefit from the strengths of Spring Boot while still maintaining a level of flexibility and control over our codebase. \n","GenTime":"2024-08-19 15:39:09","text":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n\n\n##Decision\nWe accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n"}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as the mechanism to capture significant architectural decisions made during the project.  Each ADR will be a single markdown file stored within a dedicated \"docs\/adr\" directory at the root of the project repository. \n","GenTime":"2024-08-19 15:39:11","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"alfa\/adr-002.md","Context":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"Prediction":"## Decision\n\nWe will decouple the accessibility assessment engine from the browser environment and transition towards a static analysis approach. This will involve the following:\n\n1. **Developing a standalone component:** This component will be responsible for fetching the website resources (HTML, CSS, JavaScript, etc.), parsing them into a structured format (e.g., a DOM-like representation), and executing relevant JavaScript within a controlled environment.\n\n2. **Migrating accessibility rules:** The accessibility testing logic currently implemented within the browser extension will be adapted to work on this static representation of the website. This may involve leveraging existing libraries for DOM manipulation and CSS parsing.\n\n3. **Building a deterministic execution environment:** This environment will ensure consistent execution of JavaScript by controlling network requests, timing functions, and other potential sources of non-determinism.\n\n4. **Developing a mechanism for capturing and replaying network requests:** This will allow us to precisely control the responses to network requests made by the website's JavaScript, ensuring consistent behavior across assessments. \n\nThis approach provides several advantages:\n\n* **Reproducibility:** By eliminating browser-specific inconsistencies, we can guarantee that the same input will always produce the same assessment result. \n* **Scalability:** Analyzing a static representation of the website is generally faster than running assessments within a full browser environment.\n* **Maintainability:** Decoupling from the browser simplifies testing and debugging efforts, leading to a more robust and reliable solution.\n\nThis decision does not preclude the use of browser-based tools for other purposes, such as manual testing or user feedback collection. The browser extension may still be used to gather initial website data or provide a user interface for interacting with the static analysis results.\n","GenTime":"2024-08-19 15:39:18","text":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n\n\n##Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n"}
{"File Name":"buy-for-your-school\/0005-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":1246,"Prediction":"## Decision\n\nTo mitigate the risk of inefficient database queries being deployed, we will implement a two-pronged approach:\n\n**1. Automated Query Analysis:** Integrate a static analysis tool into our CI\/CD pipeline. This tool will scan code changes for potential database query performance issues. It will flag queries that:\n\n* Perform N+1 selects.\n* Lack appropriate indexes.\n* Utilize inefficient join operations.\n* Are overly complex and could benefit from simplification.\n\nThe tool will block merging code with flagged issues into the main branch until addressed, ensuring only performant code reaches production.\n\n**2. Developer Education and Best Practices:** \n\n*  We will provide developers with clear documentation and training materials on writing efficient database queries. \n*  Code reviews will explicitly focus on database interaction, encouraging reviewers to scrutinize queries for potential optimizations.\n*  We will establish coding standards and best practices for database interactions, such as utilizing ORMs effectively, avoiding unnecessary data fetching, and leveraging database-specific optimization features.\n\nThis multi-faceted approach will proactively prevent inefficient queries from reaching production while fostering a culture of performance awareness among developers. \n","GenTime":"2024-08-19 15:39:24","text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n"}
{"File Name":"my-notes\/styled-components.md","Context":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"Prediction":"### Decision\n\nDon't start using Styled Components for now. \n\n### Reasoning\n\nWhile Styled Components offer maintainability advantages over traditional CSS, the inability to get tests working reliably outweighs those benefits at this time.  A core principle of software development is to have confidence in our code, and reliable testing is essential for that. \n","GenTime":"2024-08-19 15:39:26","text":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n\n\n##Decision\nDon't start using Styled Components\\n"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project.  Each ADR will be a single Markdown file stored within a dedicated \"adr\" directory at the root level of the project repository. \n\nWe chose this approach for the following reasons:\n\n* **Simplicity:** ADRs are easy to create, maintain, and understand. They promote a lightweight and consistent approach to documenting architectural decisions.\n* **Traceability:** Storing ADRs within the project repository ensures that the decision-making process is transparent and readily available to all team members.\n* **Version Control:** Using a version control system for ADRs allows us to track changes over time and understand the rationale behind specific decisions.\n\nWe will use the MADR template for our ADRs, which includes the following sections:\n\n* **Title:** A concise and descriptive title for the decision.\n* **Status:** The current status of the decision (e.g., Proposed, Accepted, Rejected, Superseded).\n* **Context:** The background and motivation for the decision.\n* **Decision:** The actual decision made.\n* **Consequences:** The known positive and negative consequences of the decision.\n\nThis ADR serves as a starting point, and we may refine our approach to documenting architectural decisions as the project evolves. \n","GenTime":"2024-08-19 15:39:32","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"Prediction":"## Decision\n\nWe will implement a new module, tentatively named the \"authorization\" module, to handle the delegation of specific actions from one account to another. This module will address the use cases of delegated voting and \"sub-keys\" functionality.\n\nThis decision is based on the following:\n\n* **Clear Need:** There is a demonstrable need for delegating specific actions like voting and spending, as evidenced by community requests and the \"sub-keys\" proposal. \n* **Modular Design:** A dedicated module promotes modularity and separation of concerns within the software architecture.\n* **Extensibility:** A dedicated module allows for future expansion and support for additional delegatable actions beyond voting and spending. \n\nThis module will be designed in conjunction with the `fee_grant` module and the `group` module to provide a comprehensive \"sub-keys\" solution.  The initial implementation will be based on the work done by the Gaian's team at Hackatom Berlin 2019, and will be further developed to meet the specific requirements outlined in the use cases. \n","GenTime":"2024-08-19 15:39:36","text":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\n\n##Decision\nWe will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n"}
{"File Name":"WorkoutLog\/0009_one_time_events_livedata.md","Context":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n","Decision":"A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","tokens":49,"id":1231,"Prediction":"## Decision\n\nWe will implement a mechanism to consume LiveData events, preventing their re-emission after lifecycle events like configuration changes (e.g., screen rotation) or navigation events. \n\nTwo primary approaches will be considered:\n\n**1. SingleLiveEvent Wrapper:**\n\n*   Utilize a wrapper class around LiveData, like `SingleLiveEvent`. This wrapper ensures that each observer receives the event only once. \n*   This approach involves minimal boilerplate code and is relatively straightforward to implement.\n\n**2. Event Wrapper with Consumption Status:**\n\n*   Create a custom event wrapper class that includes a flag indicating whether the event has been consumed. \n*   The ViewModel will set the flag to `true` after posting the event. Observers will check this flag before processing the event. \n\n**Decision Rationale:**\n\nThe choice between the two approaches will depend on factors like project complexity and existing architecture. If the project already utilizes a similar event wrapper, extending it might be more efficient. Otherwise, the `SingleLiveEvent` approach offers a simpler solution. \n\n**Consequences:**\n\n*   **Reduced Bugs:** Prevents unintended side effects from repeated event handling.\n*   **Increased Code Complexity (Slightly):** Introduces new classes or logic for event handling.\n*   **Potential Performance Overhead (Minimal):** Negligible impact due to the lightweight nature of the solution. \n\n**Note:** We will need to ensure proper testing of the chosen implementation to guarantee the desired behavior across different lifecycle scenarios. \n","GenTime":"2024-08-19 15:39:43","text":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n\n\n##Decision\nA new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n"}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"Prediction":"## Decision\n\nWe will use a two-step process to convert Jupyter notebooks to PDF in AutoFlow:\n\n1. **Convert Jupyter notebook to ASCIIDoc using nbconvert.**\n2. **Convert ASCIIDoc to PDF using Asciidoctor PDF.**\n\nThis approach avoids the need for a full LaTeX installation in the AutoFlow container, resulting in a smaller container size. \n","GenTime":"2024-08-19 15:39:45","text":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n\n\n##Decision\nWe will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n"}
{"File Name":"heptaconnect-docs\/2021-10-29-flow-components-are-not-crud.md","Context":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n","Decision":"Receivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","tokens":250,"id":3205,"Prediction":"## Decision\n\nWe will **not limit HEPTAconnect flow components to CRUD or BREAD operations.** \n\nWhile the current explorers, emitters, and receivers resemble aspects of these paradigms, we acknowledge their broader applicability and the potential for misuse within a strictly CRUD\/BREAD context. \n\nThis decision is driven by the following factors:\n\n* **Flexibility:** HEPTAconnect's value proposition lies in its flexibility to connect various systems. Limiting flow components to CRUD\/BREAD would hinder its ability to interact with APIs and systems that operate outside these paradigms. \n* **Future-Proofing:**  As HEPTAconnect evolves, we may encounter use cases that demand flow components beyond CRUD\/BREAD operations.  \n* **Existing Examples:** The presence of webhooks and status reporters already demonstrates that HEPTAconnect's architecture accommodates components outside of CRUD\/BREAD.\n\n**Moving forward, we will:**\n\n* **Embrace a broader definition of flow components** that encompasses data manipulation, system interaction, and event-driven communication.\n* **Provide clear documentation and examples** to guide users on proper utilization of flow components and discourage potential misuse. \n* **Explore alternative approaches** for handling \"deletion\" operations that are more robust and secure than relying on custom commands within existing components. \n","GenTime":"2024-08-19 15:39:51","text":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n\n\n##Decision\nReceivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n"}
{"File Name":"launcher\/2018-03-29_add_crc_and_uuid_to_log_lines.md","Context":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n","Decision":"Launcher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","tokens":236,"id":2549,"Prediction":"## Decision\n\n**Add a UUID and a pre-calculated CRC hash to each individual osquery log line.**\n\n**Rationale:**\n\n* **Improved Uniqueness and Deduplication:** The UUID guarantees each log line is uniquely identifiable, even if the content is identical to other log lines. This enables accurate deduplication downstream, regardless of potential duplicate message delivery.\n* **Efficient Data Comparison:** The CRC hash allows for efficient identification of log lines with identical data without requiring a full field-by-field comparison. This optimizes downstream processing and analysis.\n* **Enhanced Visibility and Troubleshooting:** Unique identifiers for each log line provide granular visibility into osquery events, aiding in debugging, anomaly detection, and trend analysis.\n\n**Implementation Details:**\n\n* The UUID will be generated using a standard UUID generation algorithm (e.g., UUIDv4) and added as a new field to the log line JSON object.\n* The CRC hash will be calculated over the log line data (excluding the UUID field) before adding the UUID. This ensures the hash represents the data content consistently.\n* The chosen CRC algorithm should be efficient and provide a good balance between collision resistance and performance (e.g., CRC32).\n* Downstream log processing pipelines and tools will be updated to utilize the UUID and CRC hash for deduplication, analysis, and reporting. \n\n**Consequences:**\n\n* **Increased Log Volume:** Adding the UUID and CRC hash will slightly increase the size of each log line. \n* **Implementation Effort:**  Updating the osquery log generation mechanism and potentially downstream systems will require development effort.\n\n**Trade-offs:**\n\n* The benefits of improved uniqueness, deduplication, and analysis efficiency outweigh the minor increase in log volume and implementation effort. This decision prioritizes data integrity and analytical capabilities.\n","GenTime":"2024-08-19 15:39:59","text":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n\n\n##Decision\nLauncher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n"}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"Prediction":"## Decision\n\nWe will use **environment variables** to provide the address of the FaaS gateway and the function name to instances of the generic component. \n\n## Reasoning\n\n* **Widely Supported:**  Environment variables are a fundamental mechanism supported by virtually all programming languages and runtime environments, making this solution compatible with any technology used for the generic component. \n* **Easy Integration:** Injecting environment variables is a standard feature in MICO and container orchestration platforms like Kubernetes. This ensures straightforward integration without complex configurations.\n* **Proven and Secure:** Environment variables are a well-established mechanism for configuring applications, particularly in cloud-native environments. They offer a degree of security as they are typically managed by the operating system or container orchestration platform and are not embedded directly within the application code. \n\n## Consequences\n\n* **Flexibility:** Environment variables allow for dynamic configuration changes without recompiling or redeploying the generic component. \n* **Centralized Management:**  Environment variables can be managed centrally, simplifying the configuration and deployment process for different EAI pattern implementations.\n* **Security Considerations:** While generally secure, sensitive information like FaaS gateway credentials might require additional security measures like secrets management if directly stored in environment variables. \n","GenTime":"2024-08-19 15:40:04","text":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n\n\n##Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n"}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"## Context\\nSome rules regarding usage of health-module apis.\\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"Prediction":"## Decision\n\nWe will implement a dedicated authorization layer for all health-module APIs. This layer will enforce the following rules:\n\n* **User Consent:** Access to any health data requires explicit consent from the user. This consent will be granular, allowing users to grant or revoke access to specific data types (e.g., heart rate, sleep data) for specific applications or purposes.\n* **Role-Based Access Control (RBAC):**  Different user roles will have varying levels of access. For instance:\n    * **Patients:** Can only access their own data.\n    * **Healthcare Providers:** Can access data of patients under their care, subject to user consent.\n    * **System Administrators:** Have full access for maintenance and troubleshooting purposes.\n* **Data Minimization:** APIs will only return the minimum necessary data required for the specific request. \n* **Auditing and Logging:**  All API calls, including the user, data accessed, and timestamp, will be logged for audit and security purposes.\n\nThis approach ensures that health data is accessed and used securely and ethically, respecting user privacy and complying with relevant regulations.\n","GenTime":"2024-08-19 15:40:10","text":"## Context\\nSome rules regarding usage of health-module apis.\\n\n\n##Decision\nInput for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n"}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"Prediction":"## Decision\n\nWe will adopt Domain-Driven Design (DDD) as a core principle for this software project. This decision is driven by the numerous benefits DDD offers, aligning perfectly with our project goals and context:\n\n* **Improved Communication and Collaboration:** By establishing a ubiquitous language, DDD will bridge the communication gap between technical and business teams, fostering a shared understanding of the domain.\n* **Maintainable and Scalable Codebase:** Modeling our codebase after the business domain ensures clarity, making it easier to understand, maintain, and scale the software as the business evolves.\n* **Reduced Complexity:** DDD encourages breaking down complex systems into manageable, domain-specific modules, simplifying development and problem-solving.\n* **Alignment with Business Goals:** By prioritizing the core domain and logic, we ensure the software directly addresses the most critical business needs and adapts effectively to changing requirements. \n\nWhile DDD offers significant advantages, we acknowledge it requires a significant upfront investment in domain modeling and collaboration. To mitigate this, we will:\n\n* **Prioritize close collaboration** between developers and domain experts throughout the project lifecycle.\n* **Iteratively refine our domain model** based on continuous feedback and evolving understanding.\n* **Invest in training and resources** to ensure the development team is well-versed in DDD principles and practices. \n\nWe believe that the long-term benefits of DDD, including increased maintainability, scalability, and alignment with business goals, far outweigh the initial investment. \n","GenTime":"2024-08-19 15:40:16","text":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n\n\n##Decision\nWe In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n"}
{"File Name":"meadow\/0009-tailwind-css-framework.md","Context":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n","Decision":"Use the Tailwind CSS framework for design and layout.\\n","tokens":70,"id":3829,"Prediction":"## Decision\n\nAdopt Tailwind CSS as the primary styling framework for the project. This decision is driven by the desire for increased flexibility, faster iteration cycles, and reduced overhead associated with overriding pre-defined styles inherent in opinionated frameworks like Bootstrap. The utility-first approach offered by Tailwind aligns better with our need for granular control over the UI, enabling us to build custom designs efficiently without fighting against framework constraints. \n","GenTime":"2024-08-19 15:40:19","text":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n\n\n##Decision\nUse the Tailwind CSS framework for design and layout.\\n"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"Prediction":"## Decision\n\n**Implement a basic validator consensus key rotation feature in the Cosmos SDK, without modifying the Tendermint consensus logic.** \n\nThis decision prioritizes simplicity and leverages the existing Tendermint framework. \n\n**Key points:**\n\n* **Scope:** Implement a single consensus key rotation mechanism. Multiple consensus keys will be considered as a future enhancement.\n* **Implementation:** Focus on Cosmos SDK changes. No changes required to Tendermint's core consensus logic. \n* **Functionality:** Allow validator operators to seamlessly rotate their consensus keys without interrupting validator participation.\n* **Rationale:**\n    * Aligns with the long-standing request for improved validator key management (as seen in  https:\/\/github.com\/tendermint\/tendermint\/issues\/1136).\n    * Minimizes complexity and potential for introducing bugs by leveraging Tendermint's agnostic approach to key mapping. \n    * Provides a foundation for future enhancements, such as supporting multiple consensus keys. \n","GenTime":"2024-08-19 15:40:23","text":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n\n\n##Decision\n### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n"}
{"File Name":"heroes-board-game\/0002-tests-definition-and-scope.md","Context":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n","Decision":"* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","tokens":44,"id":3166,"Prediction":"## Decision\n\nKeep acceptance tests embedded within the game server application, but expose their results and reporting in a manner accessible and understandable to end-users through the UI layer. \n\n## Reasons\n\n* **Direct Feedback Loop:** Embedding tests within the game server provides the most direct feedback loop for developers on the functionality and stability of core game mechanics. This allows for quicker identification and resolution of issues directly impacting the game experience.\n* **End-User Relevance:** While the tests reside within the server, their results can be translated into meaningful information for end-users. This can be achieved by presenting test outcomes (e.g., success\/failure, performance metrics) through in-game dashboards, logs, or notifications within the UI.\n* **Flexibility and Control:**  This approach offers flexibility. Developers can run the full suite of embedded tests for in-depth analysis while still surfacing relevant subset results to end-users based on their needs and permissions.\n\n## Considered Options\n\n* **Moving Acceptance Tests to the UI Layer:** While this might seem aligned with user understanding, it creates several drawbacks:\n    * **Increased Complexity:** Separating tests from the core logic can lead to complex synchronization and maintenance challenges.\n    * **Limited Scope:** UI-level tests often focus on interface interactions and may not adequately cover core game logic and server interactions.\n\n## Consequences\n\n* **Development Effort:** Additional development effort is required to expose test results through the UI in a user-friendly manner.\n* **Potential for Misinterpretation:** Clear and concise presentation of test results is crucial to avoid user confusion or misinterpretation.\n\nThis approach balances the need for robust technical testing with the importance of providing end-users with meaningful insights into game stability and performance. \n","GenTime":"2024-08-19 15:40:31","text":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n\n\n##Decision\n* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n"}
{"File Name":"CCS-Architecture-Decision-Records\/0008-cmp-use-shared-store-for-capturing-buyer-needs.md","Context":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n","Decision":"Use a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","tokens":123,"id":1001,"Prediction":"## Decision\n\nWe will implement a two-pronged approach for storing data related to buyer needs and agreement details:\n\n**1. Buyer Needs Repository:** \n\n* A centralized repository will store all buyer-provided data during the discovery and negotiation phases. \n* This repository will be structured to capture common patterns and data elements across different agreements, allowing for analysis and automation.\n* Data models will be designed to accommodate both structured data (e.g., multiple-choice selections) and unstructured data (e.g., free-text responses).\n* This repository will facilitate:\n    * **Automated Matching:** Matching buyer needs to relevant products and supplier offerings.\n    * **Data Analysis:** Identifying trends and patterns in buyer requirements.\n    * **Personalization:** Tailoring the buyer journey based on their specific needs and preferences.\n\n**2. Agreement Records:**\n\n* Once a buyer commits to a specific agreement and call-off, finalized details will be stored in dedicated agreement records. \n* These records will contain specific contractual information, pricing, and other relevant details for the agreed-upon services.\n* The buyer needs repository can be referenced to provide context and history for the agreed-upon terms.\n\n**Data Synchronization:**\n\n* A clear process will be defined for transferring relevant data from the buyer needs repository to the agreement records upon call-off confirmation.\n* Data integrity and consistency will be maintained between the two systems.\n\nThis approach balances the need for flexibility during the exploration phase with the requirement for structured and secure storage of finalized agreement information. The buyer needs repository will act as a valuable knowledge base, driving automation and improving the overall buyer experience.\n","GenTime":"2024-08-19 15:40:38","text":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n\n##Decision\nUse a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"Prediction":"## Decision\n\nWe will adopt a combination of static analysis tools and code formatters to enforce code style and best practices:\n\n* **Linting:** We will use [ESLint](https:\/\/eslint.org\/) (JavaScript\/TypeScript), [pylint](https:\/\/pylint.org\/) (Python), or a similar language-specific linter to identify stylistic errors, potential bugs, and anti-patterns. \n* **Formatting:** We will use [Prettier](https:\/\/prettier.io\/) for automatic code formatting across all languages where it is supported.  Where Prettier lacks support, we will adopt a language-specific formatter like [Black](https:\/\/black.readthedocs.io\/en\/stable\/) (Python) or similar. \n* **Integration:** We will integrate these tools into our development workflow through:\n    * **Pre-commit hooks:** Ensure code adheres to standards before every commit.\n    * **CI\/CD pipeline:** Enforce these standards on every pull request and merge.\n    * **IDE integration:** Provide developers with real-time feedback within their IDEs.\n\nThis approach will help us:\n\n* **Improve code consistency:** By enforcing a unified style guide.\n* **Reduce bugs:** By identifying potential issues early on.\n* **Enhance maintainability:** By making the codebase easier to read and understand.\n* **Increase developer productivity:** By automating repetitive formatting tasks.\n\n## Consequences\n\n* **Initial setup effort:** Configuring the tools and integrating them into the development workflow will require some initial effort.\n* **Potential conflicts:** There might be cases where the linter and the formatter have conflicting rules. These will need to be addressed on a case-by-case basis.\n* **Learning curve:** Developers will need to familiarize themselves with the chosen tools and their configurations.\n\nDespite these consequences, the benefits of improved code quality, maintainability, and developer productivity outweigh the costs. This decision fosters a more robust and maintainable codebase for the long term. \n","GenTime":"2024-08-19 15:40:46","text":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n\n\n##Decision\nWe will use isort, black, and flake8.\\n"}
{"File Name":"stamper\/0003-spring-boot-as-web-framework.md","Context":"## Context\\nWe need to choose a web framework for the app.\\n","Decision":"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","tokens":15,"id":1243,"Prediction":"## Decision\n\nWe will use **React** as our web framework. \n","GenTime":"2024-08-19 15:40:47","text":"## Context\\nWe need to choose a web framework for the app.\\n\n\n##Decision\nWe use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n"}
{"File Name":"snippets-service\/0005-frequency-capping.md","Context":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n","Decision":"We decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","tokens":208,"id":3229,"Prediction":"## Decision\n\nWe will implement **Frequency Capping** in the Snippet Service utilizing the new capabilities provided by Firefox Messaging Center and the Firefox Telemetry Pipeline. \n\nWe will support both **Global Frequency Capping** and **User Frequency Capping**. \n\n* **Global Frequency Capping** will limit the total number of impressions a snippet can have across all users.\n* **User Frequency Capping** will limit the number of times a specific user sees a particular snippet.\n\nThis approach provides content managers with granular control over content exposure and allows us to drive engagement while respecting user experience. \n","GenTime":"2024-08-19 15:40:51","text":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n\n##Decision\nWe decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n"}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"Prediction":"## Decision\n\n**Adopt Architectural Decision Records (ADRs) as a lightweight, modular approach to documenting architectural decisions within the agile development process.**\n\nThis decision is driven by the need to maintain just enough architectural documentation to support understanding and maintainability of the project without introducing unnecessary overhead. \n\nHere's why ADRs fit the agile context:\n\n* **Modular and Concise:** ADRs are small, self-contained documents that focus on a single decision, making them easy to write, read, and update. \n* **Traceable Rationale:** They capture the context, forces, and consequences of a decision, aiding in understanding the \"why\" behind architectural choices. \n* **Living Documentation:** ADRs can be easily updated as the project evolves and new information arises, preventing them from becoming obsolete. \n* **Improved Onboarding:** ADRs provide a valuable knowledge base for new team members, allowing them to quickly understand the architectural direction and reasoning. \n\nBy using ADRs, we aim to strike a balance between agility and architectural awareness, promoting informed decision-making and reducing the risk of knowledge loss or architectural drift.\n","GenTime":"2024-08-19 15:40:56","text":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n"}
{"File Name":"TANF-app\/015-terraform-service-provisioning.md","Context":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n","Decision":"[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","tokens":148,"id":3794,"Prediction":"## Decision\n\nWe will implement Infrastructure-as-Code (IaC) using Terraform to automate the provisioning and management of Cloud.gov-brokered services (S3 and RDS PostgreSQL) for the TANF application. \n\n## Reasons\n\n* **Automation and Repeatability:** IaC eliminates manual processes, reduces human error, and ensures consistent deployments across multiple environments (development, testing, staging, production). \n* **Documentation:** Terraform code serves as living documentation of the infrastructure configuration.\n* **Version Control:** Storing IaC scripts in version control allows for tracking changes, rollbacks, and collaboration among team members.\n* **Security:** Terraform enables us to define and enforce security policies consistently across our infrastructure.\n* **Integration with CI\/CD:** We can easily integrate Terraform with our existing CI pipeline to automate provisioning as part of the deployment process.\n\n## Consequences\n\n* **Learning Curve:**  The team will need to invest time in learning Terraform.\n* **Maintenance Overhead:**  We will need to maintain the Terraform codebase.\n\n## Status\n\nAccepted\n\n## Next Steps\n\n1. **Tooling Selection:**  Evaluate and choose specific Terraform modules for interacting with Cloud.gov and its brokered services.\n2. **Develop Terraform Scripts:**  Write Terraform code to define the desired state of S3 buckets (including appropriate security configurations for sensitive data) and the RDS PostgreSQL instance.\n3. **Integrate with CI\/CD:**  Incorporate Terraform scripts into the CI pipeline to automate provisioning during deployments.\n4. **Documentation:**  Create documentation on how to use the new Terraform-based provisioning process.\n5. **Training:** Provide training on Terraform basics to the development team. \n","GenTime":"2024-08-19 15:41:03","text":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n\n\n##Decision\n[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n"}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"Prediction":"## Decision\n\nWe will persist data related to supported currencies using a **relational database**. \n\n## Justification\n\nWhile text files and NoSQL databases offer potential solutions, a relational database provides the most suitable approach for managing currency data due to the following reasons:\n\n* **Data Integrity:** Relational databases excel at enforcing data integrity through constraints, ensuring data consistency and accuracy. This is crucial for currency data where errors can have significant financial implications.\n* **Structured Format:** Currency data typically has a well-defined structure (e.g., currency code, symbol, name). Relational databases naturally lend themselves to storing and querying such structured data efficiently.\n* **Relationships:**  While not immediately apparent, there might be future needs to represent relationships with currency data (e.g., exchange rates between currencies). Relational databases are designed to handle such relationships effectively.\n* **Transactionality:** Currency-related operations often require atomicity and consistency, which are well-supported by relational database transactions. \n* **Mature Ecosystem:** Relational databases have a mature ecosystem of tools and libraries for development, administration, and backup.\n\nWhile text files offer simplicity, they lack robust data integrity mechanisms and can be inefficient for querying. NoSQL databases, while flexible, introduce unnecessary complexity for this specific use case. \n\n**Specific Considerations:**\n\n* The chosen relational database should support standard SQL for portability and ease of use.\n* The schema design should prioritize data integrity and efficiency for common operations like currency lookups and updates.\n\nThis decision provides a balance between simplicity, maintainability, and the specific requirements of managing currency data effectively.\n","GenTime":"2024-08-19 15:41:10","text":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n\n\n##Decision\nI decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"Prediction":"## Decision \n\nWe will implement a new mechanism for service discovery and secure communication that will replace the existing usage of the centralized registry client. This new mechanism will be based on [Specific technology or approach, e.g., a service mesh like Istio or a decentralized approach using a gossip protocol], and will offer the following benefits:\n\n* **Decentralization:** Eliminating the single point of failure and bottleneck presented by the centralized registry.\n* **Improved Security:** Enabling secure communication channels between services without relying on a single security proxy.\n* **Simplified Architecture:** Reducing the complexity associated with managing and configuring the registry client and security proxies.\n\nThis decision will involve the following steps:\n\n1. **Technology Selection:**  We will evaluate and choose the most suitable [Specific technology or approach] based on our specific needs and constraints.\n2. **Pilot Implementation:** We will conduct a pilot implementation of the chosen solution in a controlled environment to validate its feasibility and performance.\n3. **Incremental Rollout:** We will gradually roll out the new solution to the rest of the system, carefully monitoring its impact and addressing any issues that arise.\n4. **Deprecation of Existing Infrastructure:** Once the new solution is fully operational, we will deprecate and decommission the existing registry client and security proxy infrastructure. \n\nThis approach allows us to benefit from the advantages of a more modern and robust service discovery and security mechanism while minimizing disruption to the existing system during the transition period. \n","GenTime":"2024-08-19 15:41:16","text":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n\n\n##Decision\n- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n"}
{"File Name":"knot-documentation\/ARC-002-fog-cloud-sync.md","Context":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n","Decision":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","tokens":132,"id":3337,"Prediction":"## Decision \n\n**Implement a tiered synchronization approach for device contextual representation and data between the fog and cloud, prioritizing efficiency and scalability.**\n\n**Tier 1: On-Demand Synchronization for Critical Data**\n\n* **Device registration and schema information:** Synchronize with the cloud immediately upon any change. \n* **Real-time or time-sensitive data:**  Synchronize with the cloud as soon as it becomes available, ensuring minimal latency.\n\n**Tier 2: Batched Synchronization for Non-Critical Data**\n\n* **Historical or non-time-sensitive data:**  Aggregate and synchronize with the cloud in batches, leveraging off-peak hours or periods of network stability. This reduces the load on the network and messaging queues.\n* **Device status updates (non-critical):**  Utilize a similar batched approach to minimize overhead.\n\n**Key Considerations:**\n\n* **Data Prioritization:** Clearly define criteria for classifying data as critical or non-critical to ensure efficient resource utilization.\n* **Connectivity Status:** Implement mechanisms to buffer data locally and synchronize it with the cloud when connectivity is restored in case of intermittent connections.\n* **Data Consistency:** Employ appropriate strategies (e.g., timestamps, versioning) to maintain data consistency between the fog and cloud.\n* **Scalability:** Design the solution with future scalability in mind, allowing for potential adjustments in synchronization frequency and data prioritization as the system grows. \n\nThis approach prioritizes efficiency by only immediately synchronizing critical information, while less critical data is batched for optimized transmission. This balances data availability with resource constraints, proving suitable for the initial integration phase. As the system evolves, the specific data classification criteria and synchronization frequencies can be adjusted to accommodate changing needs. \n","GenTime":"2024-08-19 15:41:24","text":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n\n\n##Decision\n* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"Prediction":"## Decision\n\nA Marain instance is comprised of the following components:\n\n**1. Core Services:**\n\n* **API Gateway:** A single entry point for all external communication with the instance, responsible for authentication, authorization, request routing, and rate limiting.\n* **Data Store:** A persistent data storage solution, such as a relational database or a NoSQL database, responsible for storing all instance data.\n* **Message Queue:** An asynchronous messaging system, such as RabbitMQ or Kafka, used for inter-service communication and event propagation within the instance.\n* **Identity Provider:** A service responsible for user authentication and authorization, potentially integrated with existing identity providers.\n\n**2. Functional Services:**\n\n* **Service A:** (Replace with the actual name and brief description of a functional service within Marain)\n* **Service B:** (Replace with the actual name and brief description of another functional service within Marain)\n* **...** (Add more functional services as needed)\n\n**3. Infrastructure:**\n\n* **Compute Resources:** Virtual machines, containers, or serverless functions that host the core and functional services.\n* **Networking:** Virtual networks, load balancers, and DNS configurations that enable communication between services and with the outside world.\n* **Monitoring and Logging:** Tools and services for monitoring the health and performance of the instance, collecting logs, and alerting on issues.\n\n**4. Configuration and Deployment:**\n\n* **Configuration Management:** Tools and practices for managing the configuration of services and infrastructure within the instance.\n* **Deployment Pipeline:** An automated process for deploying and updating services and infrastructure in a controlled and repeatable manner.\n\n**5. Data:**\n\n* **Instance Data:** All data stored within the instance's data store, including user data, application data, and configuration settings.\n\n**Clarifications:**\n\n* Each Marain instance is isolated from other instances, meaning they have their own dedicated set of components and data.\n* Functional services represent distinct functionalities provided by the Marain instance and can vary based on specific requirements.\n* The specific technologies and tools used for each component can be chosen based on performance, scalability, and cost considerations.\n\nThis definition provides a clear and unambiguous understanding of the constituent parts of a Marain instance.\n","GenTime":"2024-08-19 15:41:34","text":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n\n\n##Decision\nThe `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"Prediction":"## Decision\n\nWe will proceed with **Option 2: Use an external SaaS offering like Quay**. \n\nWhile this option introduces additional configuration and procurement overhead, it provides the most robust and feature-rich solution for our needs. Here's why:\n\n* **Reduced Development and Maintenance:** Relying on a mature SaaS solution like Quay significantly reduces the development and maintenance burden compared to building and maintaining a custom solution (Option 1) or managing a self-hosted solution (Option 3).\n* **Comprehensive Feature Set:** Quay offers built-in support for public repositories, Docker Content Trust (image signing and verification), vulnerability scanning, and other features that align with our requirements.\n* **Security and Reliability:** Quay has a strong track record of security and reliability, providing a trusted platform for storing and distributing our container images.\n\nWhile using a SaaS solution introduces external dependencies, the benefits in terms of reduced operational overhead, faster time to market, and access to a comprehensive feature set outweigh the drawbacks. We will carefully evaluate and select a SaaS provider that meets our security, compliance, and performance requirements. \n","GenTime":"2024-08-19 15:41:38","text":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n\n##Decision\nWe will run a self hosted set of Docker tools\\n"}
{"File Name":"beis-report-official-development-assistance\/0009-use-pundit-to-manage-permissions.md","Context":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n","Decision":"We will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","tokens":81,"id":2397,"Prediction":"## Decision\n\nWe will implement a lightweight, service-agnostic role-based access control (RBAC) system within our application. \n\nThis approach offers several advantages:\n\n* **Decoupling:**  It avoids tight coupling to Auth0 and allows for flexibility in switching or extending authentication providers in the future.\n* **Granular Control:** We retain control over defining and managing roles and permissions specific to our application's needs.\n* **Simplicity:** A lightweight RBAC system can be implemented efficiently without excessive complexity.\n\n**Implementation Details:**\n\n1. **Role Definition:** Define roles and their corresponding permissions within our application. This can be achieved using configuration files, database tables, or in-code declarations.\n2. **User-Role Mapping:** Upon successful authentication (regardless of the provider), retrieve the user's assigned roles. This information can be stored in JWT tokens, a separate database table, or fetched from an external user management system.\n3. **Authorization Enforcement:**  Implement authorization checks within the application logic to ensure users only access resources permitted by their assigned roles.\n\nThis decision prioritizes flexibility, maintainability, and security by providing a tailored, service-agnostic access control solution.\n","GenTime":"2024-08-19 15:41:44","text":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n\n##Decision\nWe will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n"}
{"File Name":"Wikibase\/0009-refactor-hooks-for-testability.md","Context":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n","Decision":"It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","tokens":496,"id":1339,"Prediction":"## Decision\n\nWe will implement a hybrid approach for testing the `RepoHooks` class that combines elements of both solutions considered:\n\n1. **Gradual Refactoring to Injectable Services:** We will incrementally refactor the logic within `RepoHooks` methods into separate, testable service classes. These services can then be injected into a wrapper class responsible for interfacing with the hooks. This approach allows us to gradually improve testability without requiring a complete overhaul upfront.\n\n2. **Context Object for Dependencies:**  Similar to the `WikibaseClient` pattern, we will introduce a context object that can hold dependencies like the `WikibaseRepo` instance. Initially, this object can default to using the singleton instance. However, during testing, we can inject a mock or stub of `WikibaseRepo` into this context object, providing control over its behavior. \n\nThis hybrid approach offers several advantages:\n\n- **Incremental Improvement:** We can gradually increase test coverage without a massive, risky refactoring effort.\n- **Flexibility:** The context object provides a flexible mechanism for managing dependencies, allowing us to introduce mocks and stubs during testing.\n- **Future-Proofing:** This strategy aligns with the long-term goal of enabling dependency injection in hooks (as per the RFC). Once implemented, our refactored services will be well-positioned to take advantage of this functionality. \n\nThis approach represents a pragmatic compromise that balances the need for improved testing with the constraints of the existing codebase. It allows us to move towards a more testable architecture incrementally while laying the groundwork for future improvements. \n","GenTime":"2024-08-19 15:41:51","text":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n\n\n##Decision\nIt was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"Prediction":"## Decision\n\nAdopt Testcontainers framework to manage the PostgreSQL dependency for Service Tests. This framework provides lightweight, throwaway instances of databases (including PostgreSQL) within Docker containers, eliminating the need for manual setup or persistent instances. \n","GenTime":"2024-08-19 15:41:53","text":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n\n##Decision\nChosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n"}
{"File Name":"govuk-aws\/0015-dns-infrastructure.md","Context":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n","Decision":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","tokens":154,"id":4056,"Prediction":"## Decision\n\nWe will implement a split DNS infrastructure with the following characteristics:\n\n* **Internal DNS Zone:** An internal DNS zone (e.g., `.internal`) will be used for resolving internal infrastructure services like Puppet, Graphite, and Logstash. This zone will only be accessible from within our network.\n* **Public DNS Zone:** A public DNS zone (e.g., `service.gov.uk`) will host public-facing services and applications like alerts.integration, deploy.integration, and www-origin. This zone will be managed by a DNS provider and will be publicly resolvable.\n* **Environment-Specific Subdomains:**  For each new piece of infrastructure or environment, we will create environment-specific subdomains under both the internal and public DNS zones (e.g., `test.service.gov.uk`, `staging.service.gov.uk`). This will allow for direct access and testing of specific environments.\n* **DNS-Based Traffic Routing:** We will leverage DNS records (e.g., CNAME, A records) to control which stack is serving the active version of a service or application. This allows us to redirect traffic between different environments or instances for testing, deployments, and failover scenarios.\n* **Publishing Domain Alias:** We will configure the publishing domain (publishing.service.gov.uk) as an alias (CNAME) pointing to the active production environment's public DNS endpoint. This ensures consistent user experience when browsing the site regardless of underlying infrastructure changes.\n\nThis approach allows us to meet the requirements of secure internal service discovery, flexible environment management, controlled traffic routing, and consistent public access while maintaining separation between internal and public DNS namespaces. \n","GenTime":"2024-08-19 15:42:00","text":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n\n\n##Decision\n![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n"}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"Prediction":"## Decision\n\nWe will refactor the Elm code execution process to eliminate the need for manual `ElmContext` instantiation and the implicit modification of the `window` object. Instead, we will introduce an explicit `run` function:\n\n```typescript\nfunction run(elmCode: string): ElmContext \n```\n\nThis function will:\n\n1. **Compile** the provided `elmCode` using the `Compiler` from `elm-spec-core`.\n2. **Create** a new `ElmContext` internally. \n3. **Evaluate** the compiled Elm code within the context of the newly created `ElmContext`, ensuring that all dependencies and mocked objects are correctly injected. \n4. **Return** the created `ElmContext` instance, allowing access to its functionalities (e.g., assertions, spies). \n\nThis approach offers several benefits:\n\n* **Simplified API:**  Test writers only need to call a single `run` function, eliminating the need for manual `ElmContext` instantiation and reducing the potential for errors. \n* **Improved clarity:**  The execution flow becomes explicit and easier to understand. \n* **Testability:**  The `run` function itself becomes easily testable, allowing for better isolation and verification of the execution process.\n\nThis change will require updates to `elm-spec-runner`, `karma-elm-spec-framework`, and the harness documentation. However, it will significantly simplify the user experience and make the testing process more intuitive. \n","GenTime":"2024-08-19 15:42:06","text":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n\n\n##Decision\nWe should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n"}
{"File Name":"devops-challenge\/0010-feat-ci-cd-with-circleci.md","Context":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n","Decision":"Try first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","tokens":168,"id":3482,"Prediction":"## Decision\n\nWe will use **CircleCI** as our CI\/CD solution for the following reasons:\n\n* **Existing Knowledge:**  The team has prior experience with CircleCI, reducing the learning curve and accelerating implementation. \n* **Free Tier Availability:** CircleCI's free plan allows us to begin testing and development without immediate cost concerns, crucial for evaluating its suitability for our project.\n* **Jenkins Complexity:** While Jenkins and Jenkins X are powerful, the initial setup issues encountered, combined with the existing team's familiarity with CircleCI, make CircleCI a more pragmatic choice for now. \n\n**We will revisit the decision if CircleCI proves insufficient for our needs in the future.**  Factors that may trigger a reevaluation include:\n\n* **Scalability Concerns:** If the project grows significantly and CircleCI's limitations impact performance or cost.\n* **Specific Jenkins X Features:** If our needs evolve to require unique capabilities offered only by Jenkins X, further investigation would be warranted. \n\n**Next Steps:**\n\n1. **Formalize CircleCI Configuration:**  Define pipelines for building, testing, and deploying applications to the Kubernetes cluster.\n2. **Integrate with Existing Workflow:**  Seamlessly connect CircleCI with our code repository and development processes.\n3. **Document CircleCI Setup and Usage:**  Create internal documentation to onboard team members and ensure maintainability.\n","GenTime":"2024-08-19 15:42:13","text":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n\n\n##Decision\nTry first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n"}
{"File Name":"lbh-frontend-react\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":1404,"Prediction":"## Decision\n\nWe will use ESLint to enforce code style and quality rules in our JavaScript and TypeScript codebase. \n\nWe will configure ESLint using a dedicated configuration file, likely `.eslintrc.js`, to define our chosen ruleset and any project-specific exceptions. We will integrate ESLint with our code editor for real-time feedback during development and include it as part of our continuous integration pipeline to prevent code with linting errors from being merged into the main branch. \n\nFor TypeScript specific linting, we will leverage the officially supported  [eslint-plugin-typescript](https:\/\/typescript-eslint.io\/) plugin. \n","GenTime":"2024-08-19 15:42:16","text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n"}
{"File Name":"titania-os\/0002-docker-for-dapps.md","Context":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n","Decision":"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","tokens":31,"id":2573,"Prediction":"## Decision\n\nWe will use **Docker containers** to define and isolate each dApp within Titania. \n\n## Reasoning\n\n* **Isolation:** Docker provides strong isolation between dApps, ensuring that code, dependencies, and resources are separate. This prevents conflicts and enhances security.\n* **Scalability:**  Containers are lightweight and easily scalable, allowing us to efficiently manage resources and handle varying demands from different dApps.\n* **Portability:** Docker containers are highly portable and can run consistently across different environments, simplifying deployment and reducing potential compatibility issues.\n* **Dependency Management:** Each dApp can define its specific dependencies within its container, avoiding version conflicts and ensuring consistent behavior.\n* **Development Efficiency:**  Containers provide a consistent and reproducible development environment, simplifying the process of building, testing, and deploying dApps. \n* **Community Support:** Docker has a large and active community, providing access to extensive documentation, tools, and support resources.\n\n## Consequences\n\n* **Increased Complexity:** Managing Docker containers adds some complexity to the system, particularly in terms of orchestration and networking.\n* **Resource Overhead:** While containers are lightweight, they still consume some resources. This overhead needs to be carefully considered, especially in resource-constrained environments. \n* **Security Considerations:** While Docker provides isolation, it's crucial to implement proper security measures within containers and the host system to mitigate potential vulnerabilities.\n\n## Alternatives Considered\n\n* **Virtual Machines:**  While VMs offer strong isolation, they come with significant resource overhead compared to containers.\n* **Process Isolation:** Techniques like namespaces and cgroups provide some degree of isolation at the process level but are less robust than containers. \n* **Language-Specific Sandboxes:**  Some languages offer built-in sandboxing mechanisms. However, these are often language-specific and may not provide sufficient isolation for multi-dApp environments.\n\n## Status\n\nAccepted\n\n## Notes\n\nWe will need to investigate and select a suitable container orchestration platform (e.g., Kubernetes, Docker Swarm) to manage the lifecycle of dApp containers within Titania.  Additionally, we need to establish security best practices for building and deploying containers. \n","GenTime":"2024-08-19 15:42:25","text":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n\n\n##Decision\nWe will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n"}
{"File Name":"ibc-go\/adr-015-ibc-packet-receiver.md","Context":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":341,"id":1442,"Prediction":"## Decision\n\nThis ADR makes the following decisions:\n\n1. **Utilize `baseapp.router` for packet routing:** Instead of implementing a separate routing module as defined in ICS 26, leverage the existing routing functionality provided by `baseapp.router` to route IBC packets to their respective application modules. This promotes code reuse and reduces complexity. \n\n2. **Perform packet verification in `AnteHandler`:** Integrate the verification steps of the ICS 26 routing module callbacks into the `AnteHandler` of the application. This enhances developer experience by minimizing boilerplate code and centralizing verification logic.\n\n3. **Introduce `CodeTxBreak` for atomic multi-message transactions:** Implement a new error code, `CodeTxBreak`, specifically designed to handle atomic multi-message transactions involving IBC packets. This code allows for graceful handling of failures in application-specific message execution within an atomic batch, preserving IBC-related state changes while preventing further execution of the transaction upon encountering an error. \n\nThis approach aims to simplify the integration of ICS 26 while ensuring the integrity of IBC operations within the context of atomic multi-message transactions. \n","GenTime":"2024-08-19 15:42:30","text":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n\n\n##Decision\n`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n"}
{"File Name":"james\/0040-quality-levels-definitions.md","Context":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2150,"Prediction":"## Decision\n\nWe will define three tiers of quality for James Server artifacts and features: **Production Ready**, **Experimental**, and **Incubating**.\n\n**1. Production Ready:**\n\n* **Artifacts:** Deemed suitable for use in production environments. This implies:\n    * Thorough testing with high code coverage.\n    * Stable API with clear versioning and backward compatibility guarantees.\n    * Comprehensive documentation including setup, usage, and troubleshooting.\n    * Performance testing and optimization for typical use cases.\n    * Security review and vulnerability mitigation strategies in place.\n* **Features:**  Considered stable and reliable for production use. This implies:\n    * Feature has been available and stable for a significant period.\n    * Minimal known bugs and issues.\n    * Clear documentation and configuration guidelines.\n    * Backwards compatibility maintained for configurations.\n\n**2. Experimental:**\n\n* **Artifacts:** Functional but may have limitations or be subject to significant changes. This implies:\n    * Testing and documentation are in progress and may be incomplete.\n    * API and behavior may change significantly between releases.\n    * Performance and security may not be fully optimized or assessed.\n* **Features:**  New functionality undergoing active development and testing. This implies:\n    *  API and behavior may be unstable and subject to change.\n    *  Thorough testing and documentation may be lacking.\n    *  Performance and resource usage may not be optimized.\n\n**3. Incubating:**\n\n* **Artifacts:** Early stage development with potential to become experimental or production ready. This implies:\n    * Functionality is basic and may be incomplete.\n    * API and behavior are highly subject to change.\n    * Minimal testing and documentation. \n* **Features:**  Concepts or ideas under consideration for future development. This implies:\n    *  May not have any working implementation yet.\n    *  Primarily for community discussion and feedback.\n\n**Applying these levels:**\n\n* All artifacts and features will be clearly labeled with their quality level in documentation, release notes, and potentially within the code itself.\n* Default configurations will prioritize Production Ready artifacts and features.\n* Users will be explicitly informed of the risks associated with using Experimental or Incubating artifacts and features.\n* Regular reviews will be conducted to reassess the quality level of artifacts and features, potentially promoting them to a higher level or deprecating them as needed. \n","GenTime":"2024-08-19 15:42:40","text":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n\n\n##Decision\nFor a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n"}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"Prediction":"## Decision\n\nWe will adopt a build process that transpiles modern JavaScript code to a widely compatible ECMAScript version (e.g., ES5) using Babel. This process will include the following:\n\n* **Transpilation:** Convert modern JavaScript syntax and features (e.g., async\/await, arrow functions, classes) to their ES5 equivalents, ensuring compatibility with older browsers and environments.\n* **Linting:** Integrate a linter like ESLint to enforce consistent code style, identify potential errors, and promote best practices within the codebase.\n* **Minification:** Utilize a tool like UglifyJS or Terser to minify the transpiled code, reducing its file size for faster downloads.\n* **Bundling:** Bundle multiple JavaScript files into a single file using a tool like Webpack or Parcel to minimize HTTP requests and improve page load times.\n\nThis approach balances the desire for modern JavaScript development with the need to support older environments while also ensuring code quality and optimized delivery.\n","GenTime":"2024-08-19 15:42:45","text":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n\n##Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n"}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"Prediction":"## Decision \n\n(Please provide the context so I can give you a relevant architectural decision) \n","GenTime":"2024-08-19 15:42:46","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n"}
{"File Name":"libelektra\/warning_array.md","Context":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n","Decision":"The format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","tokens":46,"id":1303,"Prediction":"## Decision\n\nMigrate away from the legacy array notation (`#00`, etc.) for storing warnings to a more robust and scalable approach. This new approach should address the following:\n\n1. **Unlimited Capacity:** Remove the current limitation of 100 entries.\n2. **Elektra Conformity:**  Align the storage format with established Elektra best practices and guidelines as described in the [array.md](array.md) document.\n3. **Maintainability:**  Choose a representation that is easy to understand, parse, and manipulate programmatically.\n\nPossible solutions could include:\n\n* **Nested Keys:** Utilize a hierarchical structure with dedicated keys for each warning. \n* **Metadata:**  Embed warning information within the metadata of relevant keys.\n* **Dedicated Warning Key:** Introduce a specific key to store all warnings, potentially leveraging a structured format like JSON or YAML if necessary.\n\nThe specific solution will be chosen based on factors like the complexity of the warnings, performance considerations, and impact on existing code. \n","GenTime":"2024-08-19 15:42:51","text":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n\n##Decision\nThe format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n"}
{"File Name":"amf\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":1397,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions along with their context and consequences. \n\nThis will involve:\n\n* **Format:** We will use Markdown files for ADRs, following a standardized template (e.g., MADR  - https:\/\/github.com\/adr\/madr). \n* **Storage:** ADRs will be version-controlled alongside the project's source code in a dedicated directory (e.g., \/docs\/adr).\n* **Process:**  A new ADR will be created for each significant architectural decision. Each ADR will be reviewed and merged through the team's standard code review process.\n* **Tooling (Optional):** We may consider using ADR tooling (e.g., adr-tools) to help manage and automate the ADR lifecycle. \n \nThis decision aims to:\n\n*  Improve the transparency and traceability of architectural decisions.\n*  Facilitate communication and knowledge sharing within the team.\n*  Support future maintenance and evolution of the system. \n","GenTime":"2024-08-19 15:42:56","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"Prediction":"## Decision\n\nWe will use **Scrapy** for scraping data from the Koshvani platform and **JSON** for storing the data in a machine-readable format. \n\n## Justification\n\n* **Scrapy**:\n    * **Powerful and Efficient**: Scrapy is a robust and efficient web scraping framework specifically designed for data extraction. Its asynchronous architecture allows it to crawl websites and extract data rapidly.\n    * **Data Extraction Features**: It provides built-in mechanisms for selecting, extracting, and cleaning data using CSS or XPath selectors, making it easy to target specific information on the Koshvani platform.\n    * **Scalability**: Scrapy can handle large-scale scraping tasks, which is crucial if the Koshvani platform contains a significant amount of data.\n    * **Pythonic API**:  Its Pythonic API allows for easy integration with other Python libraries and tools, enabling us to further process and analyze the scraped data. \n* **JSON**:\n    * **Universally Readable**: JSON is a widely-supported data format, ensuring compatibility with various programming languages and tools we might use in the future.\n    * **Lightweight and Human-Readable**:  Its simple structure is easy to understand and parse, both for machines and humans, which is helpful for debugging and data exploration.\n    * **Flexible Data Structures**: JSON supports nested objects and arrays, allowing us to represent the hierarchical relationships within the Koshvani platform data effectively.\n\n\n## Considered Alternatives\n\n* **BeautifulSoup**:  While simpler to learn, it might not be as performant or scalable as Scrapy for large-scale scraping.\n* **Selenium**: Ideal for dynamically generated content, but overkill if the Koshvani platform is statically rendered.\n* **CSV**: Less flexible than JSON for representing complex data structures that may exist within the Koshvani platform. \n\n## Consequences\n\n* **Positive**:\n    * **Efficient and Reliable Data Acquisition**: Scrapy's robust features ensure efficient and reliable data extraction from the Koshvani platform.\n    * **Clean and Structured Data**:  The combination of Scrapy's data extraction capabilities and JSON's structured format will provide us with clean, organized, and easily analyzable data. \n* **Negative**:\n    * **Maintenance**:  Changes in the Koshvani platform's structure might require updates to the Scrapy spider.\n    * **Rate Limiting**:  We need to implement measures to respect the Koshvani platform's rate limits to avoid being blocked.\n\n## Action Items\n\n1. **Install Necessary Libraries**:  Install Scrapy and any JSON libraries needed.\n2. **Develop Scrapy Spider**: Create a Scrapy spider tailored to extract the desired data from the Koshvani platform.\n3. **Handle Rate Limiting**:  Implement mechanisms (e.g., delays, rotating proxies) to adhere to the platform's rate limits.\n4. **Data Validation**:  Include data validation steps to ensure the scraped data conforms to the expected format and structure.\n5. **Documentation**: Document the scraping process, including data fields, spider logic, and any platform-specific considerations. \n","GenTime":"2024-08-19 15:43:09","text":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n\n\n##Decision\n- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n"}
{"File Name":"macondo\/0003-remote-repositories-management.md","Context":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n","Decision":"- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","tokens":92,"id":2191,"Prediction":"## Decision\n\nWe will implement a plugin system for commands, allowing users to extend the CLI tool's functionality beyond the built-in commands. This will involve the following:\n\n1. **Defining a Plugin Interface:**  A clear and well-documented interface will specify how plugins should be structured, registered, and interact with the core application. This interface will likely involve methods for:\n    - Fetching available commands and their definitions.\n    - Handling the execution of specific commands.\n    - Providing help\/documentation for commands.\n\n2. **Supporting Multiple Repository Types:**  The plugin system will support loading command repositories from various sources:\n    - **Remote Repositories:** \n        - HTTP\/HTTPS URLs pointing to YAML manifests containing command definitions.\n        - GitHub repositories containing YAML manifests (potentially using a specific file naming convention).\n    - **Local Repositories:**\n        - Local files (YAML, JSON, etc.) containing command definitions.\n        - Local directories containing multiple command definition files.\n\n3. **Plugin Management Commands:**  New CLI commands will be added for managing plugins:\n    - `add-repository <source>`:  Adds a new repository from the specified source (URL, GitHub repo, local file\/directory).\n    - `remove-repository <source>`: Removes a repository.\n    - `list-repositories`: Lists all registered repositories.\n    - `update-repository <source>`:  Updates a repository from its source.\n\n4. **Command Resolution:** When a user invokes a command:\n    - The CLI tool will first search for the command within the built-in set.\n    - If not found, it will iterate through the registered plugins in a defined order (configurable by the user) and attempt to locate the command within each plugin's available commands.\n    - Once found, the corresponding plugin will handle the command's execution.\n\n5. **Security Considerations:**  Mechanisms will be implemented to ensure the security and integrity of plugins:\n    - Plugin repositories can be pinned to specific versions or commits.\n    - Options for sandboxing plugin execution can be explored.\n    - Mechanisms to verify the authenticity and integrity of plugins (e.g., code signing) can be considered. \n\nThis plugin system will provide flexibility and extensibility, allowing users to tailor the CLI tool to their specific needs by incorporating commands from various sources. \n","GenTime":"2024-08-19 15:43:19","text":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n\n\n##Decision\n- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"Prediction":"## Decision\n\nWe will provide a Platform SDK and a set of tools to enable service teams to interact with the Cloud Platform and manage their Kubernetes resources. \n\n**The Platform SDK will offer two levels of abstraction:**\n\n1. **High-Level APIs:** These APIs will abstract away the complexities of Kubernetes and provide a PaaS-like experience for deploying and managing simple applications. Teams can define their application using a simple configuration file, and the platform will handle the deployment, scaling, and lifecycle management of the application. \n\n2. **Low-Level APIs:** For teams requiring more control, we will provide low-level APIs that map directly to Kubernetes resources. This will allow teams to manage their deployments, services, pods, and other Kubernetes resources directly. \n\n**The Platform Tooling will include:**\n\n* **CLI:** A command-line interface for interacting with the Platform SDK.\n* **Web Console:** A graphical user interface for visualizing and managing resources.\n* **CI\/CD Integration:** Tools and documentation for integrating deployments with popular CI\/CD systems.\n\n**Multi-tenancy will be addressed using Kubernetes Namespaces:**\n\nEach service team will have dedicated namespaces for their development, staging, and production environments. This will ensure logical separation of resources and prevent conflicts between teams.  \n\n**Security policies will be enforced at the namespace level**: \n\n This will allow for granular control over network access, resource limits, and other security-related settings.\n\nThis approach balances ease of use with flexibility, allowing teams to choose the level of control they need. It also ensures proper isolation between teams and environments. \n","GenTime":"2024-08-19 15:43:26","text":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\n\n##Decision\n1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n"}
{"File Name":"celestia-core\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":379,"id":4113,"Prediction":"## Decision\n\nCurrently, we will **not be addressing light client amnesia attacks**.\n\nThis decision is based on the following factors:\n\n* **Complexity:** Implementing a robust solution to prevent and punish amnesia attacks introduces significant complexity. This complexity extends to both the consensus algorithm and the light client implementation. \n* **Cost-Benefit Analysis:**  The attack vector for amnesia attacks is currently considered low. Exploiting this vulnerability requires a high degree of network control and sophisticated manipulation. The resources and effort required to execute such an attack are substantial. \n* **Prioritization:** Given the limited development resources and the current threat landscape, we prioritize other security vulnerabilities and features with a higher impact.\n\n## Rationale\n\nWhile we acknowledge the potential risk of light client amnesia attacks, we believe that the complexity and cost associated with implementing a solution at this time outweigh the potential benefits. \n\nOur current focus is on strengthening the core consensus algorithm and addressing more immediate security concerns. We believe that a more measured approach to handling amnesia attacks is appropriate, allowing us to gather more data on the feasibility and impact of such attacks in real-world scenarios.\n\n## Future Work\n\nThis ADR doesn't completely dismiss the need to address amnesia attacks in the future. We propose the following steps:\n\n* **Further Research:**  We will continue to research and monitor the evolution of amnesia attacks and potential mitigation strategies. \n* **Community Engagement:** We will actively engage with the broader research community to discuss and collaborate on solutions for light client security, including amnesia attacks. \n* **Re-evaluation:** We will revisit this decision periodically, taking into account advancements in research, changes in the threat landscape, and feedback from the community.\n\nThis approach allows us to maintain a balance between security and practicality, ensuring that we address the most critical threats while remaining agile and responsive to emerging challenges. \n","GenTime":"2024-08-19 15:43:34","text":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n\n\n##Decision\nThe decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n"}
{"File Name":"elife-xpub\/0004-code-structure.md","Context":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n","Decision":"In summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","tokens":257,"id":3776,"Prediction":"## Decision\n\nWe will adopt a lightweight, custom Data Access Layer (DAL) structure. This will involve:\n\n1. **Dedicated Data Access Objects (DAOs):** Each entity\/table will have a corresponding DAO class responsible for all database interactions related to that entity. This promotes separation of concerns and keeps data access logic contained.\n2. **No Base \"Model\" Class:**  We avoid inheriting from a base \"Model\" class to prevent unnecessary complexity and potential conflicts with our chosen validation approach.\n3. **Clear Interface for DAOs:** Each DAO will expose a consistent interface (e.g., `get`, `create`, `update`, `delete`) for interacting with the database, regardless of the underlying implementation.\n4. **Database Agnostic Code:** The DAL will abstract away the specific database technology being used. This allows for potential future changes in the database without requiring major code rewrites.\n5. **Query Builder Compatibility:** The chosen structure will allow for the integration of a query builder like Knex.js where necessary for complex queries, while keeping simpler queries within the DAOs themselves.\n\nThis approach offers a good balance between structure, flexibility, and maintainability:\n\n* **Simplicity:**  It avoids the overhead of a full-blown ORM, reducing the learning curve and potential for issues.\n* **Control:**  Provides direct control over database interactions and query optimization.\n* **Maintainability:**  Enforces a clear separation of concerns, making the codebase easier to understand, maintain, and scale.\n","GenTime":"2024-08-19 15:43:41","text":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n\n##Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"Prediction":"## Decision\n\n**Adopt a microservices architecture for the data scraping, analysis, and imputation pipeline.**\n\n## Reasons\n\n* **Scalability and Maintainability:** Microservices allow for independent development, deployment, and scaling of individual components for each Expenditure sub-section. This aligns with the phased approach and the long-term project scope.\n* **Data Isolation and Integrity:** Separate microservices for scraping, analysis, and imputation ensure data integrity and minimize the impact of errors or changes in one section on others. \n* **Flexibility and Reusability:** Microservices promote code reusability. For example, the imputation service can be reused for other sections or datasets.\n* **Technology Agnostic:**  Microservices allow for using different technologies best suited for each component, e.g., Python for scraping, R for analysis, etc.\n\n## Consequences\n\n* **Increased Architectural Complexity:** Microservices introduce complexity in terms of communication, orchestration, and monitoring.\n* **Development Overhead:** Initial development effort might be higher compared to a monolithic architecture.\n* **Operational Complexity:** Deploying and managing multiple services requires robust DevOps practices. \n","GenTime":"2024-08-19 15:43:47","text":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n\n\n##Decision\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"Prediction":"## Decision\n\n**Introduce a message broker to support asynchronous communication between services within the SDR ecosystem.** This will involve selecting a suitable message broker technology and implementing it strategically to handle asynchronous tasks such as:\n\n* Event reporting and propagation\n* Long-running job status updates\n* Inter-service communication where synchronous responses are not strictly required\n\nThis decision aims to address the limitations of the current synchronous HTTP-based architecture and achieve the following benefits:\n\n* **Improved Decoupling:** Services will no longer require direct knowledge of or dependencies on numerous other services.\n* **Enhanced Scalability:**  The asynchronous nature of message queues will allow services to scale independently and handle load fluctuations more effectively.\n* **Increased Resilience:**  The system will be more tolerant of service outages as message queues can buffer messages until dependent services are available.\n* **Simplified Development:** The need for complex error handling and retry logic related to network unreliability will be reduced.\n* **Elimination of Redundant Solutions:** Existing ad-hoc implementations of message broker-like functionality can be retired. \n","GenTime":"2024-08-19 15:43:51","text":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n\n\n##Decision\nTBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"Prediction":"## Decision\n\nAdopt a spectrum-based spacing system using multiples of a base unit. This system will define spacing values as multiples of a fundamental unit (e.g., 4px or 8px).  This allows for a wider range of spacing options and avoids the ambiguity of relative terms. \n","GenTime":"2024-08-19 15:43:53","text":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n\n\n##Decision\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n"}
{"File Name":"Corvus.Extensions.Newtonsoft.Json\/0001-json-framework-independence.md","Context":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n","Decision":"We have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","tokens":132,"id":2585,"Prediction":"## Decision\n\nWe will adopt a strategy of **abstraction** to handle the need to work with both Json.NET and `System.Text.Json` without tightly coupling our libraries to either one. This approach will involve the following:\n\n1. **Defining an internal abstraction layer:** We will create an internal interface within our libraries that defines the JSON operations our code requires (e.g., serialization, deserialization, specific data access methods).\n\n2. **Implementing adapters:** For each supported JSON library (Json.NET and `System.Text.Json`), we will create concrete implementations of the internal interface. These adapters will handle the actual interaction with the specific library.\n\n3. **Using dependency injection:**  Our libraries will not directly depend on any JSON library. Instead, they will accept an instance of the internal interface through dependency injection. This allows the consuming application to choose and inject the preferred JSON library implementation.\n\n4. **Providing optional convenience packages:** To simplify integration, we can offer optional NuGet packages for each supported JSON library. These packages will contain the required adapter implementations and any necessary configuration logic.\n\n\nThis approach provides several benefits:\n\n* **Flexibility:** Applications have the freedom to choose their preferred JSON library without any restrictions imposed by our libraries.\n* **Testability:** By relying on an internal abstraction, our libraries' core logic can be easily tested with mocked or custom JSON implementations.\n* **Maintainability:** Decoupling our libraries from concrete JSON implementations simplifies maintenance and future migrations.\n* **Future-proofing:**  This approach makes it easier to adopt new JSON libraries or serialization formats in the future by simply creating new adapter implementations. \n","GenTime":"2024-08-19 15:44:00","text":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n\n\n##Decision\nWe have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n"}
{"File Name":"thing\/0002-authentication-via-touchstone-saml.md","Context":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n","Decision":"We will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","tokens":144,"id":2582,"Prediction":"## Decision\n\nWe will implement end-user authentication using SAML directly within the application, rather than relying on Apache httpd with mod_shib. \n\nThis decision is based on the following factors:\n\n* **Successful SAML Proof-of-Concept:** We have already demonstrated that SAML integration is feasible within our application's containerized environment.\n* **Flexibility and Control:** Implementing SAML in-app provides greater control over the authentication flow and customization options compared to relying on external Apache configurations.\n* **Modern Approach:** Direct SAML integration aligns with current best practices for authentication in modern web applications. \n\nWhile this approach deviates from MIT's officially supported mod_shib solution, the benefits of flexibility, control, and alignment with modern practices outweigh the reliance on a specific Apache module.  We will ensure the chosen SAML library is robust, well-maintained, and provides adequate security measures. \n","GenTime":"2024-08-19 15:44:05","text":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n\n\n##Decision\nWe will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n"}
{"File Name":"experimenter\/0007-doc-hub-url.md","Context":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n","Decision":"We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","tokens":45,"id":1823,"Prediction":"## Decision\n\nWe will host the experimenter docs hub at the following URL: [Choose one option and replace  placeholder, removing brackets]:\n\n**Option 1: Subdomain:** `docs.[company\/product domain].com` \n* **Example:** `docs.example-company.com` \n* **Pros:** Clear, memorable, and reinforces brand\/product association.\n* **Cons:** Requires setting up and maintaining a subdomain.\n\n**Option 2: Subdirectory:**  `[company\/product domain].com\/docs`\n* **Example:**  `example-company.com\/docs` \n* **Pros:** Simple to implement, no additional DNS configuration. \n* **Cons:**  May be less prominent than a dedicated subdomain.\n\n**Option 3: Existing platform:** Integrate into existing documentation platform at `[existing platform URL]\/[experimenter-docs]`\n* **Example:** `docs.example-company.com\/products\/experimenter`\n* **Pros:** Leverages existing infrastructure and user familiarity.\n* **Cons:** May not offer the same flexibility or prominence as a dedicated site. \n\n**Rationale:** \n\n[Explain the reasons for selecting the chosen URL structure.  Consider factors like brand consistency, SEO, user experience, and technical feasibility.]\n\nThis decision aligns with our goal of having a permanent, memorable, and stable URL for the experimenter docs hub that integrates well with our overall data information architecture. \n","GenTime":"2024-08-19 15:44:11","text":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n\n\n##Decision\nWe will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document the architectural decisions made on this project. Each significant decision will be captured in a separate Markdown file following a standardized template. \n","GenTime":"2024-08-19 15:44:13","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"Wikibase\/0000-use-cache-instead-wb_terms-for-data-needed-for-display.md","Context":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n","Decision":"Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","tokens":330,"id":1341,"Prediction":"## Decision\n\n**Deprecate and eventually remove the `wb_terms` table.** \n\n**Reasoning:**\n\n* The original performance justification for `wb_terms` as a search index is no longer valid. \n* Maintaining `wb_terms` introduces significant complexity and is unsustainable in the long run.\n* Wikibase already leverages MediaWiki's caching mechanisms (e.g., memcached), mitigating the need for `wb_terms`.\n* Lexeme and form display, which do not utilize `wb_terms`, haven't exhibited significant performance issues.\n* Adapting `wb_terms` for lexemes and forms would require semantic changes and increase complexity.\n\n**Migration strategy:**\n\n1. **Identify all usages of `wb_terms` for short form rendering of items and properties.**\n2. **Develop and implement alternative approaches for obtaining short form data**, leveraging existing MediaWiki caching mechanisms and potentially optimizing entity loading procedures.\n3. **Gradually transition away from using `wb_terms`**, thoroughly testing performance at each stage.\n4. **Once all dependencies on `wb_terms` are removed, decommission and remove the table.**\n\n**Monitoring:**\n\n* Closely monitor performance during and after the migration to ensure there are no regressions.\n* Track the impact of this decision on the overall maintainability and complexity of the codebase. \n","GenTime":"2024-08-19 15:44:19","text":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n\n\n##Decision\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"Prediction":"## Decision\n\nWe will adopt static type checking for the Python codebase using a tool like MyPy. \n\n## Reasons\n\n* **Improved Code Quality:** Static type checking will help identify type errors during development, reducing the risk of runtime errors and improving overall code reliability. \n* **Enhanced Maintainability:** Type annotations improve code readability and understanding, making it easier for developers to maintain and extend the codebase over time.\n* **Existing Annotations:** Leveraging existing type annotations minimizes the initial effort required to implement static analysis. \n\n## Consequences\n\n* **Increased Development Time:**  Initially, developers will need to address type errors identified by the static type checker. This may require additional time for code refactoring and adding type annotations where they are missing.\n* **Learning Curve:**  Developers may need time to learn and adapt to the use of static type checking tools and practices.\n* **Potential for False Positives:**  Static type checkers may occasionally raise false positives, flagging code as incorrect when it is not.  We will need to establish strategies for managing these situations.\n\n## Status \n\nAccepted\n\n## Notes\n\nWe will gradually integrate static type checking into our development workflow, starting with critical modules and expanding coverage over time. We will also provide training and resources to help developers adopt and benefit from static typing. \n","GenTime":"2024-08-19 15:44:25","text":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n\n\n##Decision\nType-check via [mypy](http:\/\/mypy-lang.org\/).\\n"}
{"File Name":"FindMeFoodTrucks\/Architecture Style.md","Context":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n","Decision":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","tokens":58,"id":1094,"Prediction":"## :dart: Decision\n\nAfter careful consideration of the project requirements and constraints, we have decided to adopt the **[Choose one of the listed Architecture Styles or specify \"a combination of\" followed by the chosen styles ]** architecture style for this application. \n\nThis decision is based on the following factors:\n\n* **[Reason 1 justifying the chosen Architecture style(s)]:** \n    * [Explain how the chosen style aligns with this specific reason and project needs]\n* **[Reason 2 justifying the chosen Architecture style(s)]:**\n    * [Explain how the chosen style aligns with this specific reason and project needs]\n* **[Reason 3 justifying the chosen Architecture style(s) - optional]:**\n    * [Explain how the chosen style aligns with this specific reason and project needs] \n\nThe other architecture styles were ruled out due to the following reasons:\n\n* **[Briefly mention any specific reason for not choosing other styles, if applicable.  For example:  \"While microservices offer great scalability, the added complexity is unnecessary at this stage\" ]** \n\nThis decision will be revisited if the project requirements or constraints change significantly during the development lifecycle. \n","GenTime":"2024-08-19 15:44:31","text":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n\n\n##Decision\nA simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n"}
{"File Name":"iampeterbanjo.com\/0011-artist-images-from-spotify.md","Context":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n","Decision":"Spotify is the only viable alternative.\\n","tokens":148,"id":1598,"Prediction":"## Decision\n\nWe will use Spotify's API to retrieve artist images for Korin. \n\n## Reasons\n\n- **High-quality images:** Spotify is known for its high-quality artist images, which will significantly improve Korin's aesthetics.\n- **Reliable API:** Spotify's API is well-documented, stable, and widely used, ensuring consistent and reliable access to artist information.\n- **Popularity:** Spotify is a dominant player in the music streaming industry, increasing the likelihood of finding images for a broad range of artists.\n\n## Consequences\n\n- **ID Mapping:**  We will need to implement a mechanism to map IMDB IDs to Spotify IDs. This might require additional API calls or data processing.\n- **API Limits:** We need to be mindful of Spotify's API rate limits and implement appropriate caching and error handling mechanisms.\n- **Spotify Dependency:** Korin will become reliant on Spotify's API for artist images. Any changes or disruptions to Spotify's service could impact Korin. \n\n## Status\n\nAccepted. \n\n## Notes\n\nWe acknowledge that using Spotify's API introduces a new dependency and requires ID mapping. However, we believe the benefits of having high-quality artist images outweigh these drawbacks. We will prioritize implementing robust error handling and caching to mitigate the impact of potential API limitations. We will also investigate alternative ID mapping solutions to minimize the number of API calls.\n","GenTime":"2024-08-19 15:44:37","text":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n\n\n##Decision\nSpotify is the only viable alternative.\\n"}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"Prediction":"## Decision\n\nWe will follow a global versioning strategy for our API. This means:\n\n* **A single version number will be applied to the entire API.**  This version will be reflected in the URL (e.g., `\/v1\/`, `\/v2\/`).\n* **Breaking changes will always result in a new API version.** We will strive to minimize breaking changes.\n* **Old API versions will be supported for a defined period after a new version is released.**  This will give clients time to migrate to the new version. We will provide clear communication and support during the migration period. \n* **We will thoroughly document all API versions and their differences.**\n\nThis approach prioritizes:\n\n* **Simplicity:** One version number for the entire API is easy to understand and manage.\n* **Clarity:**  Breaking changes are immediately visible through the version number in the URL. \n* **Compatibility:**  Supporting older versions allows us to introduce changes without forcing immediate upgrades for all clients. \n\nWe acknowledge that maintaining multiple API versions adds complexity. However, we believe this is outweighed by the benefits of clear versioning, particularly for significant architectural changes like switching from REST to GraphQL. \n","GenTime":"2024-08-19 15:44:42","text":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n\n\n##Decision\nAll API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n"}
{"File Name":"admin-react-components\/0003-component-bundling.md","Context":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n","Decision":"We'll package components with their own inline styles. No external dependencies needed.\\n","tokens":38,"id":1631,"Prediction":"## Decision\n\nWe will package exported components with **zero shared dependencies**. \n\nThis decision prioritizes:\n\n* **Ease of integration:** Consuming applications won't need to resolve potential dependency conflicts, simplifying their integration process.\n* **Version control:** We maintain full control over our component's dependencies and their versions, preventing breakages due to dependency mismatches in consuming applications.\n* **Reduced bundle size:**  Avoiding shared dependencies generally leads to smaller bundle sizes, improving application load times for users.\n\n**However, it's important to acknowledge the tradeoffs:**\n\n* **Potential code duplication:** Some code might be duplicated across the component and consuming applications if they rely on different versions of the same external library.\n* **Increased maintenance overhead:** We become responsible for managing all dependencies within each component, potentially increasing our maintenance effort.\n\n**To mitigate these tradeoffs, we will:**\n\n* **Document clearly:**  Provide comprehensive documentation outlining all dependencies of each component and their versions. \n* **Versioning strategy:**  Employ a robust versioning strategy (like semantic versioning) to ensure predictable updates and minimize breaking changes for consuming applications.\n* **Regularly evaluate:** Periodically reassess if the benefits of zero dependencies outweigh the costs, especially as the project evolves. \n","GenTime":"2024-08-19 15:44:48","text":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n\n\n##Decision\nWe'll package components with their own inline styles. No external dependencies needed.\\n"}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"Prediction":"## Decision\n\n**Implement a centralized accessor method within `event-routing-backends` to retrieve data from Open edX events. This method will enforce a strict key-based access pattern and provide logging and exception handling to mitigate the risk of PII leakage.**\n\nThis decision encompasses the points outlined in your proposed solution:\n\n1. **Centralized Access:**  All base and event transformers will be required to use the accessor method (`method_`) to retrieve data from Open edX events. This enforces a single point of control for data access.\n2. **Strict Key Validation:** The accessor will strictly enforce the presence of required keys as defined by the xAPI or Caliper specifications. If a required key is missing, the method will throw an exception, halting the transformation process and logging the error.\n3. **Handling Optional Keys:**  For optional keys, the accessor will log instances where the key is missing from the Open edX event but will return `None` to allow the transformation to proceed. This ensures visibility into potential data gaps without disrupting the process.\n4. **Future Enhancement:** Investigate and evaluate the effectiveness of fuzzy comparison techniques to identify potential PII leakage risks associated with transformed event keys. This evaluation will inform the potential incorporation of fuzzy matching in the future. \n\nThis approach prioritizes a proactive and maintainable solution for minimizing PII leakage risks within `event-routing-backends`. \n","GenTime":"2024-08-19 15:44:54","text":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n\n\n##Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n"}
{"File Name":"TruBudget\/0004-ressource-level-lifetime.md","Context":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n","Decision":"### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","tokens":69,"id":1216,"Prediction":"## Decision\n\nIntroduce a finite state machine (FSM) mechanism to model the lifecycle of resources. This mechanism will allow defining:\n\n* **States:** Representing the distinct stages in a resource's lifetime (e.g., \"Created\", \"Validated\", \"Published\", \"Archived\").\n* **Transitions:** Defining the allowed movements between states, along with any conditions or triggers that need to be met (e.g., \"Publish\" transition from \"Validated\" to \"Published\" triggered by user action).\n* **Actions:** Specifying operations to execute upon entering or exiting a state (e.g., logging, notification, data transformation).\n\nThis FSM will be implemented as a generic framework allowing for custom state definitions, transitions, and actions, thus enabling adaptation to diverse use-cases and resource types.\n\n\n## Consequences\n\n**Pros:**\n\n* **Flexibility:**  Easily adapt to different resource lifecycles and business rules.\n* **Clarity and Maintainability:** Provides a structured and explicit representation of resource lifecycles.\n* **Encapsulation:** Business logic related to resource state transitions is centralized.\n* **Extensibility:** New states and transitions can be added without modifying the core framework.\n\n**Cons:**\n\n* **Complexity:**  Introduces a new abstraction layer, potentially increasing complexity for simple use-cases.\n* **Overhead:** FSM implementation and state management may introduce minor performance overhead.\n\n## Status\n\nProposed\n\n## Open Questions\n\n* What is the most suitable approach for persisting and managing the state of resources?\n* How will the FSM framework integrate with existing systems and workflows?\n* What level of granularity is required for defining states and transitions for different resource types? \n","GenTime":"2024-08-19 15:45:02","text":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n\n\n##Decision\n### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"Prediction":"## Decision\n\nWe will implement a fallback solution for handling pointer events in browsers with insufficient PointerEvents API support, specifically targeting Safari and iOS. \n\n**Rationale:**\n\n- While PointerEvents API offers a streamlined approach, its limited adoption in Safari and iOS necessitates a fallback mechanism to ensure consistent functionality across target platforms. \n-  Even though mobile is a secondary concern, desktop Safari users are still a significant demographic. Lack of support on this platform cannot be disregarded.\n-  Directly relying on `movementX\/Y` is not feasible due to its experimental nature and inconsistent behavior in Safari.\n\n**Implementation details:**\n\n1.  **Primary Implementation (Modern Browsers):** Utilize the PointerEvents API to capture pointer events (e.g., `pointermove`, `pointerdown`, `pointerup`) for mouse and pointer input.\n2.  **Fallback Implementation (Safari and iOS):**\n    -   Implement event listeners for traditional mouse events (e.g., `mousemove`, `mousedown`, `mouseup`).\n    -   Track previous mouse\/pointer coordinates manually to calculate movement deltas as a replacement for `movementX\/Y`.\n3.  **Browser Detection:**  Utilize feature detection to identify browsers with inadequate PointerEvents API support and switch to the fallback mechanism accordingly.\n\nThis approach prioritizes the use of the modern and concise PointerEvents API while ensuring compatibility with environments where it falls short. \n","GenTime":"2024-08-19 15:45:08","text":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n\n##Decision\nAll pointer event interactions have a mouse event fallback.\\n"}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"Prediction":"## Decision\n\nA shared network file system will be used to store the `sqw` and `nxspe` data files required for Horace, Herbert, and Euphonic testing. \n\nThis approach offers several advantages:\n\n* **Centralized storage:**  Provides a single source of truth for the test data, ensuring consistency across projects.\n* **Version control:** The network file system can be backed up and versioned independently, preserving data integrity and allowing for rollbacks if needed.\n* **Reduced duplication:**  Avoids duplicating large data files across repositories, saving storage space.\n* **Simplified access:** Provides a common access point for all projects requiring the data.\n\nFurther details regarding the specific implementation:\n\n* **Location:** The specific network file system (e.g., NFS, SMB) and its location will be determined based on organizational infrastructure and best practices. \n* **Organization:** A clear folder structure within the network file system will be established to logically organize the data files by project (Horace, Herbert, Euphonic) and data type (`sqw`, `nxspe`).\n* **Access control:**  Appropriate access permissions will be configured to ensure data security and prevent unauthorized modifications. \n\nThis decision allows for efficient and manageable storage and access to the large data files required for testing across multiple projects. \n","GenTime":"2024-08-19 15:45:14","text":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n\n\n##Decision\nThe data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n"}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"Prediction":"## Decision\n\nWe will implement two separate Custom Resource Definitions (CRDs) for interacting with Kafkarator:\n\n1. **KafkaratorTopic:** This CRD will be used to define and manage Kafka topics and their configurations within Kafkarator. \n2. **KafkaratorACL:** This CRD will be used to define and manage Access Control Lists (ACLs) for Kafka topics managed by Kafkarator.\n\n## Rationale\n\nThis approach addresses the identified concerns by providing:\n\n* **Separation of Concerns:** Two dedicated CRDs clearly separate the configuration of topics and ACLs, making the API easier to understand and use.\n* **Flexibility:** Developers can deploy and manage topics and ACLs independently, providing granular control over their Kafka resources.\n* **NAIS Compatibility:** Using separate CRDs aligns with the NAIS deployment model, allowing developers to include both resource definitions in their deployment configurations. \n\nThis approach avoids piggybacking on existing NAIS CRDs, ensuring that Kafkarator's API remains focused and specific to its functionality. \n","GenTime":"2024-08-19 15:45:19","text":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n\n##Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n"}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate Markdown file versioned within the code repository. \n","GenTime":"2024-08-19 15:45:20","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"paas-csls-splunk-broker\/ADR001-syslog-http-to-csls-adapter.md","Context":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n","Decision":"We will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","tokens":248,"id":3290,"Prediction":"## Decision\n\nWe will develop a minimal log shipping component deployed to GOV.UK PaaS. This component will:\n\n1. **Receive logs in syslog format** from application syslog drains. This leverages existing GOV.UK PaaS logging infrastructure.\n2. **Parse syslog messages** into structured data.\n3. **Format structured data into AWS CloudWatch Logs format.**\n4. **Publish formatted logs to the GDS Cyber Security AWS Kinesis stream.**\n\nThis approach offers several advantages:\n\n* **Leverages existing infrastructure:** Utilizes GOV.UK PaaS syslog drains and GDS Cyber Security's Kinesis stream.\n* **Minimizes complexity:** Introduces a single, purpose-built component with a focused scope.\n* **Improves testability:** Allows for focused testing of syslog parsing and CloudWatch Logs formatting.\n* **Reduces application overhead:** Avoids requiring applications to manage multiple logging transports or sidecar containers.\n\nThis decision prioritizes simplicity, maintainability, and integration with existing infrastructure over the flexibility offered by more complex solutions like Fluentd. \n","GenTime":"2024-08-19 15:45:25","text":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n\n\n##Decision\nWe will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n"}
{"File Name":"meadow\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3854,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions made during the project. \n\nEach ADR will follow a standardized template (e.g., MADR, Nygard) and be stored in a version-controlled repository alongside the codebase.\n","GenTime":"2024-08-19 15:45:27","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"Prediction":"## Decision\n\nWe will adopt a serverless architecture for our application's backend infrastructure. \n\nThis decision is driven by the following factors:\n\n* **Focus on Business Logic:** Serverless computing allows our developers to focus primarily on writing and deploying code, rather than managing infrastructure. This aligns with our goal of rapid development and iteration.\n* **Scalability and Flexibility:** The inherent scalability of serverless platforms allows us to easily handle fluctuations in user demand without manual intervention. This ensures optimal performance and cost efficiency.\n* **Reduced Operational Costs:** With serverless, we only pay for the actual compute time consumed, eliminating the cost of idle resources. This is expected to lead to significant cost savings compared to traditional infrastructure.\n* **Faster Time to Market:** The simplified deployment model of serverless platforms enables us to release new features and updates more frequently, giving us a competitive edge. \n\nWe acknowledge that serverless architectures may not be suitable for all applications and will carefully evaluate its suitability for specific components and functionalities. We will address potential challenges such as vendor lock-in and cold starts through careful planning and architectural design choices.\n","GenTime":"2024-08-19 15:45:32","text":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n\n\n##Decision\nWe will use serverless infrastructure components where possible.\\n"}
{"File Name":"libelektra\/highlevel_help_message.md","Context":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n","Decision":"Ignore missing `require`d keys (in help mode), but fail for every other error.\\n","tokens":31,"id":1274,"Prediction":"## Decision\n\nWe will refactor the error handling in the application to ensure the help message can always be printed, even after errors in `kdbOpen` or `kdbGet`. This will involve the following:\n\n1. **Decoupling Help Message from Error Handling:**  Instead of coupling the printing of the help message to the success or failure of `kdbOpen` and `kdbGet`, we will separate this logic. \n2. **Centralized Error Handling:** Implement a centralized error handling mechanism that can be invoked from anywhere in the code. This could be a function call or exception handling, depending on the programming language and existing error handling strategy.\n3. **Conditional Help Message:**  Within the error handler, we will check if the help flag (e.g., `-h` or `--help`) was passed as a command-line argument. If so, print the help message regardless of any other errors.\n\nThis approach provides a clean separation of concerns and ensures the help message remains accessible even in error scenarios. \n","GenTime":"2024-08-19 15:45:37","text":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n\n\n##Decision\nIgnore missing `require`d keys (in help mode), but fail for every other error.\\n"}
{"File Name":"govuk-aws\/0012-security-groups-in-terraform.md","Context":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n","Decision":"Security groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","tokens":89,"id":4032,"Prediction":"## Decision\n\nWe will define security groups in Terraform using **separate rules**. \n\n## Justification\n\nWhile the in-line approach offers brevity and co-location of resource definition and rules, the operational drawbacks outweigh the benefits. Specifically:\n\n* **Reduced Blast Radius**:  Modifying a single rule with the separate approach only triggers a change to that specific rule. In contrast, the in-line approach necessitates the recreation of the entire security group, potentially impacting other resources associated with it. This granular control is crucial for minimizing disruptions in a production environment.\n* **Improved Readability and Maintainability**: Separating rules from the security group definition improves the overall readability of the Terraform codebase. It allows for better organization and makes it easier to understand the security posture applied to a particular resource. This is particularly valuable as the infrastructure and its associated security policies grow in complexity.\n\nThe increased boilerplate associated with separate rules is deemed acceptable considering the long-term benefits related to stability and maintainability. We can mitigate this drawback by using Terraform modules to encapsulate the logic for creating security groups and their rules. \n","GenTime":"2024-08-19 15:45:42","text":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n\n\n##Decision\nSecurity groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n"}
{"File Name":"educational-platform\/0002-integration-events-implementation.md","Context":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n","Decision":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","tokens":21,"id":2499,"Prediction":"## Decision\n\n**Option Selected:** Utilize a message broker for asynchronous communication.\n\n**Reasoning:** \n\n* **Scalability and Reliability:** Message brokers are designed to handle high volumes of messages and provide reliable delivery mechanisms, ensuring event propagation even in case of temporary service outages.\n* **Loose Coupling:**  They promote loose coupling between services by abstracting the sender and receiver. This allows for independent development and evolution of services within the application.\n* **Flexibility and Extensibility:**  Message brokers facilitate easy integration of new services and event types, supporting future growth and changes in application requirements.\n* **Fault Tolerance:** Features like message queues and retry mechanisms enhance fault tolerance by preventing data loss and ensuring eventual consistency in case of transient failures.\n\n**Alternatives Considered:**\n\n* **Direct HTTP Communication:** While simpler to implement initially, it lacks the scalability, reliability, and loose coupling offered by a message broker.\n* **Shared Database:** Using a shared database for event communication introduces tight coupling and potential performance bottlenecks.\n\n**Consequences:**\n\n* **Increased Complexity:** Introducing a message broker adds complexity to the infrastructure and requires expertise in its setup and management.\n* **Operational Overhead:**  Monitoring and maintaining the message broker introduces additional operational tasks.\n\n**Chosen Message Broker:**\n\n*[Specify the chosen message broker (e.g., RabbitMQ, Kafka, etc.) based on your specific needs and constraints. Provide a brief justification for your choice.]*\n","GenTime":"2024-08-19 15:45:48","text":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n\n\n##Decision\nWe will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n"}
{"File Name":"registraties\/001-use-existing-apis.md","Context":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n","Decision":"The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","tokens":112,"id":1122,"Prediction":"## Decision\n\nWe will develop a dedicated \"Address Details\" page within the Registraties project. This page will fetch and display data from the specified APIs (BAG, BRK, HR) based on a user-provided address or verblijfsobject identifier. \n\nThe data displayed will be read-only and presented in a clear and user-friendly format. No data manipulation or storage beyond caching will be implemented at this stage. \n\nThis approach allows us to leverage existing data sources without introducing complex data synchronization or transformation logic initially.\n","GenTime":"2024-08-19 15:45:51","text":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n\n\n##Decision\nThe project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n"}
{"File Name":"bosh-bootloader\/0002-replace-go-bindata-with-packr2.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n","Decision":"Use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","tokens":126,"id":3873,"Prediction":"## Decision\n\nWe will replace go-bindata with an alternative solution for embedding static assets into our Go binary. \n\nThis decision is driven by: \n\n* **Concerns about long-term maintenance of go-bindata:** The original author's departure and subsequent changes in ownership raise concerns about the project's future and stability. \n* **Complexity in development workflows:** The existing usage of go-bindata introduces complexities in managing which assets are bundled depending on the environment (test vs. production), making it harder to reason about the codebase. \n","GenTime":"2024-08-19 15:45:54","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n\n##Decision\nUse [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"Prediction":"## Decision\n\nWe will **no longer enforce a default ECR lifecycle policy of 100 images per repository**. \n\nWhile this policy aimed to address storage limitations and potential cost concerns, it introduced a risk of unintended image deletion for teams without established cleanup mechanisms. \n\nInstead, we will pursue the following:\n\n* **Increase awareness:** We will enhance documentation and onboarding materials to clearly communicate the AWS ECR image limit and the importance of implementing image lifecycle management. \n* **Provide guidance and best practices:** We will offer resources and best practices for teams to define and implement their own ECR lifecycle policies based on their specific needs. This will include examples and recommendations for image tagging, retention periods, and automated cleanup processes.\n* **Explore tooling and automation:** We will investigate potential tooling or automation solutions that can assist teams in managing their ECR image lifecycle effectively. This could include integrations with CI\/CD pipelines or dedicated image management tools.\n\nBy empowering teams to manage their own ECR lifecycle policies, we provide flexibility while mitigating the risk of unintended image deletion and ensuring a more robust and scalable solution for container image management.\n","GenTime":"2024-08-19 15:45:59","text":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n\n\n##Decision\nAfter discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n"}
{"File Name":"community\/dr-013-Allowed_UI_technology_in_the_Console_UI.md","Context":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n","Decision":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","tokens":159,"id":3460,"Prediction":"## Decision\n\n**We will use React as the framework for developing the Console UI and its plugins.**\n\nThis decision is based on the following factors:\n\n* **Popularity and Community Support:** React is a widely adopted and well-supported JavaScript framework with a large and active community. This ensures access to a wealth of resources, libraries, and tools, simplifying development and troubleshooting.\n* **Component-Based Architecture:** React's component-based architecture aligns well with the plugin-based extensibility planned for the Console UI.  This allows for modular development and easier integration of new features and plugins.\n* **Performance:** React's virtual DOM and efficient rendering mechanisms contribute to a performant user interface, important for a smooth administrative experience. \n* **Learning Curve:** While any new framework requires a learning curve, React is considered relatively easy to learn and use, especially for developers already familiar with JavaScript. \n* **Alignment with Industry Trends:** Choosing a popular and well-established framework like React increases the attractiveness of Kyma to developers and ensures access to a larger talent pool.\n\n**Consequences:**\n\n* **Positive:**\n    * Simplified development process due to a unified framework.\n    * Improved maintainability and scalability of the Console UI.\n    * Access to a large talent pool of React developers.\n* **Negative:**\n    * Potential limitations for developers preferring or requiring alternative frameworks for plugin development.\n\n**To mitigate the negative consequences**:\n\n* Provide clear documentation and examples for developing plugins with React. \n* Offer support and guidance to developers transitioning to React. \n*  Consider offering wrappers or adapters for other popular frameworks in the future, based on demand and feasibility. \n","GenTime":"2024-08-19 15:46:06","text":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n\n\n##Decision\nThe decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n"}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"Prediction":"## Decision\n\nA structured logging format will be adopted across all components of Hamlet Deploy. This format will use a common schema based on key-value pairs, allowing for consistent parsing and interpretation regardless of the originating language. \n\n**Specifics:**\n\n* **Format:** JSON (JavaScript Object Notation) will be used as the standardized format due to its readability, parsability, and wide support across languages. \n* **Schema:** Each log message will contain the following key-value pairs:\n    * `timestamp`: ISO 8601 formatted timestamp.\n    * `level`: Severity level of the message (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL).\n    * `component`: Name of the Hamlet Deploy component originating the message.\n    * `message`:  A concise, human-readable description of the event.\n    * `error_code`: (Optional) A unique code to identify the specific error, if applicable.\n    * `details`: (Optional)  Additional information relevant to the event, such as stack traces or variable values.\n\n**Example:**\n\n```json\n{\n  \"timestamp\": \"2023-10-27T10:30:00Z\",\n  \"level\": \"ERROR\",\n  \"component\": \"Deployment Engine\",\n  \"message\": \"Failed to connect to remote server\",\n  \"error_code\": \"ERR_CONN_TIMEOUT\",\n  \"details\": {\n    \"hostname\": \"example.com\",\n    \"port\": \"22\",\n    \"timeout\": \"30s\"\n  }\n}\n```\n\n## Consequences\n\n**Positive:**\n\n* **Improved Error Diagnosis:** Standardized format allows for easier parsing and analysis of logs, enabling faster troubleshooting and issue resolution for both users and developers.\n* **Simplified Log Management:** A unified format allows for centralized logging and analysis using existing tools compatible with JSON.\n* **Enhanced User Experience:**  Clearer and more informative error messages improve the user experience by providing actionable insights.\n\n**Negative:**\n\n* **Development Effort:**  Minor modifications are required to adapt existing logging mechanisms in each component. \n* **Log Size:** JSON formatted logs might be slightly larger than simple text-based logs.\n\n**Overall:**  The benefits of structured logging significantly outweigh the minor drawbacks, promoting maintainability, and improving the user and developer experience in the long term. \n","GenTime":"2024-08-19 15:46:17","text":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n\n\n##Decision\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n"}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"Prediction":"## Decision\n\nWe will store the small amount of session data client-side within an encrypted cookie.  \n\n**Reasoning:**\n\n* **Reduced Infrastructure:**  Leveraging client-side storage eliminates the need for dedicated server-side session management. \n* **Security:**  While client-side, encryption will be employed to protect the sensitivity of the data.\n* **Authenticity:** To prevent tampering, a message authentication code (MAC) will be included within the cookie. This will allow the server to verify the integrity of the data.\n\n**Consequences:**\n\n* **Complexity:** Implementing encryption and authentication mechanisms adds complexity to the session management logic.\n* **Cookie Size:** While the data is small, encryption and MACs will increase cookie size.  We need to remain mindful of cookie size limits (4KB) and potential performance impacts, especially on mobile devices. \n* **Security Considerations:**  The chosen encryption and MAC algorithms must be robust and kept up-to-date to mitigate security vulnerabilities. Key management and rotation strategies should be defined and enforced. \n","GenTime":"2024-08-19 15:46:21","text":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n\n\n##Decision\nTo use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n"}
{"File Name":"meadow\/0011-yarn.md","Context":"## Context\\nWe discussed the relative merits of different JS package managers.\\n","Decision":"Use `yarn` instead of `npm` in all dev, test, and build environments.\\n","tokens":15,"id":3827,"Prediction":"## Decision\n\nWe will be using **[package manager name]** for managing JavaScript packages in this project. \n","GenTime":"2024-08-19 15:46:23","text":"## Context\\nWe discussed the relative merits of different JS package managers.\\n\n\n##Decision\nUse `yarn` instead of `npm` in all dev, test, and build environments.\\n"}
{"File Name":"runbooks-mkdocs\/0003-use-plantuml-for-diagramming.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":1145,"Prediction":"## Decision\n\n**Decision:**  We will defer making a decision on this issue until more information is gathered. \n\n**Reason:** Without a clear understanding of the context, issue, and potential constraints, it is premature to make an informed architectural decision. \n\n**Next Steps:** \n\n* Clearly define the issue motivating this decision. \n* Identify and document the context surrounding this issue. \n* Outline any constraints that might influence the decision-making process.\n","GenTime":"2024-08-19 15:46:25","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"apply-for-teacher-training\/0010-cookie-sessions.md","Context":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n","Decision":"Use session cookies.\\n","tokens":264,"id":4011,"Prediction":"## Decision\n\nWe will implement **storage-based cookies** for user sessions.\n\n## Justification\n\nWhile cookie-based sessions offer simplicity, the security risks associated with session hijacking and the inability to remotely sign out users are unacceptable for our application. Storage-based cookies, despite the minor performance overhead and reliance on additional infrastructure, provide the necessary security and functionality. \n\nThis decision prioritizes:\n\n* **Security:** Preventing session hijacking through cookie stealing is paramount. Storage-based cookies allow us to invalidate sessions server-side, mitigating this risk.\n* **User Experience:** Offering remote sign-out functionality enhances security and provides users with greater control over their accounts.\n\nWe acknowledge the following trade-offs:\n\n* **Performance:** Storage-based cookies introduce a slight performance overhead compared to cookie-based sessions due to the need to interact with a database or caching service.\n* **Complexity:**  Managing sessions in a separate data store adds complexity to the application.\n\nWe will mitigate these trade-offs by:\n\n* **Efficient Implementation:** Employing efficient database queries or caching mechanisms (e.g., Redis) to minimize the performance impact of storage-based cookies.\n* **Robust Monitoring:** Implementing monitoring and alerting for the performance and availability of the session storage mechanism.\n\nThis approach strikes a balance between security, user experience, and performance, making it the most suitable option for our application's needs. \n","GenTime":"2024-08-19 15:46:31","text":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n\n\n##Decision\nUse session cookies.\\n"}
{"File Name":"Head-Start-TTADP\/0006-database.md","Context":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n","Decision":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","tokens":90,"id":1167,"Prediction":"## Decision\n\nWe will use **PostgreSQL** as the relational database management system for the tta smarthub platform. \n\nThis decision is based on the following factors:\n\n* **Data Structure:** The platform's data is primarily structured, aligning well with PostgreSQL's relational model.\n* **Query and Reporting Needs:** PostgreSQL's powerful querying capabilities and robust reporting features make it suitable for the platform's frequent query and reporting requirements.\n* **Data Integrity:** PostgreSQL's strong commitment to data integrity through features like foreign keys, constraints, and ACID properties ensures data reliability and consistency.\n* **Scalability:** While both MySQL and PostgreSQL offer scaling options, PostgreSQL's architecture, particularly its ability to handle larger datasets and more complex queries, provides a greater potential for future scalability as the tta smarthub platform grows.\n* **Open Source and Community Support:** PostgreSQL benefits from a vibrant open-source community and extensive documentation, ensuring ongoing support and resources. \n\nWhile MySQL is also a viable option, PostgreSQL's strengths in handling structured data, complex queries, and its focus on data integrity make it the more suitable choice for the tta smarthub platform's long-term data storage and management needs. \n","GenTime":"2024-08-19 15:46:36","text":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n\n##Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n"}
{"File Name":"cafebabel.com\/0005-flask-architecture.md","Context":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n","Decision":"An architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","tokens":58,"id":3237,"Prediction":"## Decision\n\nWe will use the Application Factory pattern to structure our Flask application. This involves defining a function (the factory) that creates and configures the Flask app instance, along with its extensions and blueprints, before returning it. \n\n## Justification\n\nThe Application Factory pattern offers several advantages over directly instantiating the Flask app:\n\n* **Improved organization and maintainability:**  Centralizing app creation and configuration logic makes the codebase cleaner and easier to navigate. This avoids messy structures and simplifies the process of adding, removing, or modifying components. \n* **Enhanced testability:**  By decoupling app creation from the global scope, we can easily create multiple instances with different configurations for testing purposes.\n* **Better configuration management:**  The factory function provides a single entry point for loading configuration files or environment variables, making it more manageable and less error-prone.\n* **Clear separation of concerns:** The factory function clearly separates the responsibility of creating and configuring the app from other application logic.\n* **Scalability:** As the application grows, the factory pattern facilitates modularity and reusability by allowing different configurations for various environments (development, testing, production).\n\nWhile using the Application Factory pattern introduces some initial setup overhead, the long-term benefits in terms of maintainability, testability, and scalability outweigh the initial effort.\n","GenTime":"2024-08-19 15:46:42","text":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n\n\n##Decision\nAn architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"Prediction":"## Decision\n\nWe will use **Python** to develop the Kubernetes operator. \n\nWhile Golang is a popular choice for Kubernetes operator development, our team's lack of proficiency in Golang would result in a steeper learning curve and potentially slower development. \n\nAlthough Rust could be a valuable learning experience, prioritizing a language familiar to the team (Python) is crucial for the project's timeline and maintainability. Our existing experience with Python in FIAAS further strengthens this decision, as we can leverage existing knowledge and potentially share code or tooling. \n","GenTime":"2024-08-19 15:46:45","text":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n\n\n##Decision\nWe will use Python for Kafkarator.\\n"}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"Prediction":"## Decision\n\nWe will use the following schema for naming graphics files:\n\n`<figure-number>-<short-description>.<extension>`\n\nWhere:\n\n* `<figure-number>` is a sequential number corresponding to the order of appearance in the document.\n* `<short-description>` is a brief, lowercase, hyphen-separated description of the image content.\n* `<extension>` is the appropriate file extension for the image format (e.g., .png, .jpg, .svg).\n\n**Example:** \n\n`1-user-interface-diagram.png`\n\n**Rationale:**\n\nThis schema balances human readability with machine parsability. The sequential numbering allows for easy referencing and ordering within Jekyll, while the descriptive slugs improve understandability for content creators. \n","GenTime":"2024-08-19 15:46:49","text":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n\n\n##Decision\n* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n"}
{"File Name":"libelektra\/null_pointer_checks.md","Context":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n","Decision":"Rejected (keep checks) due to time constraints\\n","tokens":17,"id":1317,"Prediction":"## Decision\n\nWe will implement a hybrid approach for argument checking to balance performance and correctness. \n\n**Specifically:**\n\n1. **Critical functions:** Functions crucial for data integrity, security, or system stability will continue to perform exhaustive argument checking. This ensures robustness in core functionalities.\n\n2. **Performance-sensitive functions:** Functions identified through profiling as performance bottlenecks will implement lightweight argument checking. This may involve:\n    *  Checking only the most critical arguments.\n    *  Using assertions instead of exceptions for faster failure detection in debug builds.\n    *  Employing static analysis tools to catch errors during development.\n\n3. **Other functions:** All remaining functions will retain their current argument checking for development and debugging. We will investigate the option to disable these checks in optimized builds or production environments if performance becomes a concern.\n\n4. **Documentation:** All functions will clearly document their level of argument checking and any potential risks associated with relaxed checks.\n","GenTime":"2024-08-19 15:46:53","text":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n\n\n##Decision\nRejected (keep checks) due to time constraints\\n"}
{"File Name":"dos-server\/adr-1-metadata.md","Context":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n","Decision":"Descriptive metadata will not be stored by DOS.\\n","tokens":59,"id":3372,"Prediction":"## Decision\n\nDescriptive metadata will not be stored directly within the Digital Object Service (DOS). Instead, DOS will utilize a dedicated Metadata Service to manage and provide access to descriptive metadata. This approach adheres to the principle of avoiding data duplication and promotes consistency and maintainability of descriptive metadata across the system. \n\n**Further details:**\n\n* The Metadata Service will be responsible for storing, indexing, and retrieving descriptive metadata.\n* DOS will interact with the Metadata Service via a well-defined API to associate, retrieve, and update descriptive metadata for digital objects.\n* The specific implementation details of the Metadata Service (e.g., database technology, API design) will be determined in a separate ADR.\n* Administrative and structural metadata, which are closely tied to the internal representation and management of digital objects within DOS, will continue to be stored within DOS. \n","GenTime":"2024-08-19 15:46:57","text":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n\n\n##Decision\nDescriptive metadata will not be stored by DOS.\\n"}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"Prediction":"## Decision\n\nRemove the `name` field from the base classes of `Driver`, `Processor`, `Hardware Platform`, and `Platform`. \n\n**Justification:**\n\n- **Redundancy:** The `name` field is often duplicated or unused, leading to data redundancy and potential inconsistencies.\n- **Unnecessary Complexity:** Storing unused data increases complexity without providing any benefit. \n- **Improved Maintainability:** Removing the unused fields simplifies the codebase and makes it easier to maintain.\n\n**Consequences:**\n\n- **Refactoring Required:** The codebase needs to be refactored to remove references to the `name` field in the affected classes.\n- **Potential Impact on Existing Users:** If any users rely on accessing the `name` field directly, they will need to adapt their code. However, this impact is expected to be minimal since the field is either unused or redundant. \n\n**Alternatives Considered:**\n\n- **Keeping the `name` field but marking it as deprecated:** This would signal that the field is no longer in use but would still require maintaining it in the codebase.\n- **Moving the `name` field to a separate component:** This would add complexity without addressing the core issue of redundancy and unused data.\n\n**Overall, removing the `name` field from the base classes is the most straightforward and efficient solution to address the identified issues.** \n","GenTime":"2024-08-19 15:47:03","text":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n\n\n##Decision\nThese names will no longer be required by the base classes.\\n"}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"Prediction":"## Decision\n\nTo resolve the deadlock situation and ensure a reliable deployment process, we will implement the following:\n\n1. **Remove the dependency of the Connaisseur Pod's `Ready` state on the existence of the MutatingWebhookConfiguration.** This will allow the Pods to report `Ready` once their internal services are operational, even if the webhook is not yet configured.\n\n2. **Implement a separate health check endpoint within the Connaisseur Pod.** This endpoint will be responsible for verifying the connectivity and functionality of the MutatingWebhookConfiguration. External monitoring tools and Kubernetes liveness\/readiness probes can utilize this endpoint to determine the overall health of the Connaisseur deployment.\n\n3. **Update documentation to clarify the deployment order and the meaning of the Pod's `Ready` state.**  It should be explicitly mentioned that the `Ready` state indicates the Pod's operational status but not necessarily the full functionality of Connaisseur, which depends on the webhook being configured.\n\nThis approach avoids the deadlock, provides a clear indication of the system's health, and ensures a more robust and reliable deployment process. \n","GenTime":"2024-08-19 15:47:08","text":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n\n\n##Decision\nWe chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"Prediction":"## Decision\n\nAfter carefully analyzing our architectural drivers, which prioritize **[List the top 3-5 architectural drivers, e.g., scalability, performance, maintainability, security, cost-effectiveness]**, we have decided to adopt a **[Choose an architecture style, e.g., microservices, event-driven, serverless, layered]**.\n\n## Reasoning\n\nThis architecture style aligns well with our drivers by providing the following benefits:\n\n* **[Driver 1 addressed]**: Explain how the chosen style addresses this driver. For example, \"Microservices enable independent scaling of specific functionalities based on demand, enhancing scalability.\"\n* **[Driver 2 addressed]**:  Explain how the chosen style addresses this driver. For example, \"Event-driven architecture promotes loose coupling, simplifying maintenance and updates.\" \n* **[Driver 3 addressed]**:  Explain how the chosen style addresses this driver. For example, \"Serverless architecture reduces operational overhead and infrastructure costs, optimizing cost-effectiveness.\"\n\nWe considered alternative architectural styles such as **[List 1-2 alternative styles]** but ultimately decided against them for the following reasons:\n\n* **[Alternative 1 dismissed]**: Explain why this alternative was not chosen. For example, \"While a monolithic architecture is simpler to implement initially, it lacks the scalability and maintainability required for our long-term vision.\"\n* **[Alternative 2 dismissed]**: Explain why this alternative was not chosen. For example, \"A layered architecture, although promoting modularity, might introduce performance bottlenecks in highly concurrent scenarios.\"\n\n## Consequences\n\nThe decision to adopt a **[Chosen style]** architecture will result in the following consequences:\n\n* **Positive**: List the positive consequences, such as improved scalability, maintainability, performance, security, and reduced costs.\n* **Negative**: List potential negative consequences, such as increased complexity, need for specialized skills, or potential vendor lock-in. Be sure to include mitigation strategies for each negative consequence.\n\nThis architectural decision will be revisited and potentially revised if our requirements or priorities change significantly in the future. \n","GenTime":"2024-08-19 15:47:17","text":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n\n\n##Decision\nWe will use ports&adapters architecture style\\n"}
{"File Name":"smjs\/2018120802-implement-clean-architecture.md","Context":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n","Decision":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","tokens":21,"id":3982,"Prediction":"## Decision\n\nWe will prioritize flexibility and ease of experimentation over strict adherence to a single technology stack or architecture.  This means:\n\n* **Modular Design:** We will design the system in a modular way, with clear separation of concerns and well-defined interfaces. This will allow us to easily swap out different components and experiment with different technologies without impacting the entire system.\n* **Proof of Concept Focus:** Initial implementations can prioritize demonstrating the viability of a concept over production-ready code.  This allows for faster iteration and exploration.\n* **Documentation of Decisions:** While we prioritize flexibility, it is important to document the rationale behind technology and design choices.  This creates a record of what was tried, why, and what was learned for future reference. \n\nThis approach allows us to maximize the learning potential of the project and explore a wider range of possibilities. \n","GenTime":"2024-08-19 15:47:21","text":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n\n\n##Decision\nTo standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n"}
{"File Name":"dilay\/0003-use-compile-api-to-transfter-class.md","Context":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n","Decision":"Decision here...\\n","tokens":54,"id":1337,"Prediction":"## Decision\n\nWe will utilize the `ts-morph` library to interact with the TypeScript Compiler API. This decision is based on the following:\n\n* **Ease of Use:** `ts-morph` provides a higher-level, object-oriented API compared to the lower-level TypeScript Compiler API. This abstraction simplifies code navigation, manipulation, and analysis, reducing development time and complexity.\n* **Rich Functionality:** The library offers a wide range of features beyond the basic Compiler API, including advanced code transformations, project-wide analysis, and code generation capabilities.\n* **Active Maintenance and Community:** `ts-morph` is an actively maintained project with a strong community. This ensures ongoing bug fixes, feature updates, and access to support resources.\n\nWhile using the TypeScript Compiler API directly offers more fine-grained control, the improved developer experience and added functionality of `ts-morph` outweigh the benefits in this scenario. \n","GenTime":"2024-08-19 15:47:25","text":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n\n\n##Decision\nDecision here...\\n"}
{"File Name":"meadow\/0012-websockets.md","Context":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n","Decision":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","tokens":27,"id":3845,"Prediction":"## Decision\n\nWe will implement a publish-subscribe messaging system using WebSockets to provide live updates to the front-end. \n\nThis decision is driven by the need for real-time, bi-directional communication between the server and the client. \n\nHere's a breakdown of the decision:\n\n**Technology:** WebSockets\n\n**Rationale:**\n* **Real-time updates:** WebSockets provide a persistent, bi-directional communication channel, allowing the server to push updates to the front-end as soon as they occur, without the need for polling. \n* **Efficiency:** Compared to polling, WebSockets offer lower latency and reduced overhead, resulting in a more responsive and efficient user experience.\n* **Scalability:**  Modern WebSocket implementations are designed for scalability and can handle a large number of concurrent connections.\n\n**Alternatives Considered:**\n\n* **Polling:**  While simpler to implement, polling introduces significant latency and overhead, especially when frequent updates are required.\n* **Server-Sent Events (SSE):**  SSE is suitable for unidirectional communication from the server to the client. However, our use case requires bi-directional communication for features like validation feedback during sheet ingest.\n\n**Consequences:**\n\n* **Increased complexity:** Implementing a messaging system introduces complexity compared to simple polling mechanisms.\n* **Infrastructure considerations:**  Deploying and managing a WebSocket server requires additional infrastructure considerations.\n\n**Next Steps:**\n\n1.  **Select a WebSocket library\/framework:**  Evaluate and choose a suitable WebSocket library\/framework for our technology stack.\n2.  **Design the message structure:**  Define a clear and consistent message format for communication between the front-end and back-end.\n3.  **Implement the messaging system:**  Develop the necessary components for handling WebSocket connections, publishing messages, and subscribing to topics.\n4.  **Update the front-end:** Implement the client-side logic for connecting to the WebSocket server, handling incoming messages, and updating the UI accordingly. \n","GenTime":"2024-08-19 15:47:34","text":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n\n\n##Decision\nWe will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n"}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"Prediction":"## Decision\n\nWe will restrict Scratch variables on the ftduino platform to 32-bit floats. This decision sacrifices some precision and range compared to standard Scratch, but significantly improves performance, conserves memory, and avoids complex 64-bit emulation on the ftduino.\n","GenTime":"2024-08-19 15:47:36","text":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n\n\n##Decision\nWe will use 32-bit floats to store any numbers.\\n"}
{"File Name":"android-guidelines\/0007-use-deprecated-annotation-with-description.md","Context":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n","Decision":"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","tokens":24,"id":3863,"Prediction":"## Decision\n\n**Implement a policy requiring all usages of deprecated annotations to include a comment explaining the reason for deprecation and suggesting an alternative approach or migration path.** \n\nThis comment should be mandatory and enforced, possibly through a linting rule or a compiler warning. \n","GenTime":"2024-08-19 15:47:38","text":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n\n\n##Decision\nWhen we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n"}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"Prediction":"## Decision\n\n**Adopt the context-based theming approach as the new standard for styling and theming components.** \n\nThis decision is based on the following advantages over the approach outlined in ADR-0002:\n\n* **Dynamic Theming:** The new approach allows for dynamic theme changes at runtime by simply updating the context object. \n* **Separation of Concerns:** By leveraging the context object, we achieve a cleaner separation between behavioral and stylistic component configurations.\n* **Improved Code Clarity:** Retrieving theme attributes through the context object improves code readability and maintainability compared to the previous solution.\n\nThis decision considers the potential learning curve associated with understanding and implementing context-based theming. However, we believe the long-term benefits of this approach outweigh the initial investment. \n\nThe live demo provides a clear and concise example of how to implement this new theming strategy.  We recommend using the demo as a reference implementation for future component development.\n","GenTime":"2024-08-19 15:47:42","text":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n\n\n##Decision\n### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n"}
{"File Name":"buy-for-your-school\/0013-use-dfe-sign-in-as-auth-provider.md","Context":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n","Decision":"We are going to use DSI as our single sign-on provider using the OIDC standard.\\n","tokens":126,"id":1252,"Prediction":"## Decision\n\n**Use the DfE Sign-in (DSI) platform for authenticating school buying professionals.**\n\n**Reasons:**\n\n* **DfE Governance Mandate:** DSI is the mandated authentication service for DfE applications, even if user data within DSI is not directly utilized.\n* **Security Standards:** While not explicitly stated in the technical guidance, DSI is likely to have been developed with robust and modern security standards, given its central role in DfE's digital ecosystem.\n* **Future Compatibility:** Using DSI ensures compatibility with future DfE applications and services, potentially simplifying integrations and data sharing.\n\n**Consequences:**\n\n* **Increased Complexity:** Integrating with DSI, despite not requiring its user data features, introduces additional complexity compared to a simpler passwordless solution.\n* **Dependence on DSI:** The service becomes reliant on the availability and performance of DSI.\n* **Limited Control:** Using a centrally mandated service like DSI reduces control over specific authentication features and customization options.\n\n**Alternatives Considered:**\n\n* **Developing a custom passwordless authentication system:** This would offer greater control and potentially simplify the authentication flow. However, it was deemed non-compliant with DfE governance and would likely require significant development and maintenance effort.\n\n**Open Questions:**\n\n* **DSI Integration Guidance:** Clarification is needed regarding specific DSI integration recommendations and best practices, given the lack of guidance in the current technical documentation. \n* **Performance Impact:** Assess the potential impact of DSI integration on the service's performance and response times. \n","GenTime":"2024-08-19 15:47:49","text":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n\n##Decision\nWe are going to use DSI as our single sign-on provider using the OIDC standard.\\n"}
{"File Name":"application-services\/0004-early-startup-experiments.md","Context":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n","Decision":"* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","tokens":170,"id":2067,"Prediction":"## Decision\n\nWe will implement a system that allows pre-populating experiment data before the user's first run. This will involve the following steps:\n\n1. **Pre-population mechanism:**  We will create a mechanism to generate and store experiment assignments before a user's first run. This could involve:\n    * **Backend generation:**  Assigning experiment variants on the backend during user creation and storing this information.\n    * **Device-side generation:** Using a deterministic algorithm on the device to pre-assign experiment variants based on factors like device ID or install timestamp. This approach minimizes backend changes but requires careful consideration to avoid bias.\n\n2. **Nimbus Integration:** We will integrate this pre-populated experiment data with Nimbus. On the first run, Nimbus will read and apply the pre-assigned experiment variant.  Subsequent runs will behave as usual, fetching experiment data from the designated source. \n\n3. **Experiment Configuration:** A new field will be added to experiment configurations to enable pre-population. This allows experimenters to opt-in to pre-population on an experiment-by-experiment basis.\n\n4. **Monitoring and Rollback:** We will implement monitoring to track any discrepancies or unexpected behavior resulting from pre-population.  A straightforward mechanism for disabling pre-population (e.g., feature flag) will be put in place for quick rollback if necessary.\n\n## Consequences\n\nThis approach addresses the decision drivers in the following ways:\n\n* **Availability of experiments early on:**  Experiments are readily available from the first run.\n* **No impact on data analysis:** Pre-population only affects the initial assignment and doesn't alter the experiment data itself, ensuring consistent analysis.\n* **Flexibility in creating experiments:** Experimenters can choose to utilize pre-population based on their needs.\n* **Ability to quickly disable experiments:**  The rollback mechanism allows for quick disabling of pre-population if issues arise. \n* **Simplicity of releases:** This solution minimizes complexity by leveraging existing systems with minimal changes.\n* **Mobile's expectations of Nimbus:**  The solution maintains Nimbus's idempotency as pre-populated data is only used on the first run.\n\n**Potential drawbacks:**\n\n* **Complexity:** While designed to be minimally invasive, this approach introduces some complexity to the system.\n* **Bias:** Device-side pre-population requires careful design to avoid introducing bias into experiment assignments.\n\nThis approach offers a balance between achieving the desired functionality and minimizing disruption to the existing system. We believe it provides the best way to achieve the goal of running experiments from the user's first interaction. \n","GenTime":"2024-08-19 15:48:00","text":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n\n\n##Decision\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n"}
{"File Name":"adr-viewer\/0006-accessibility-as-a-first-class-concern.md","Context":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n","Decision":"Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","tokens":40,"id":3815,"Prediction":"## Decision\n\n**Adopt an accessibility-first design approach for data visualization and interaction.**\n\nThis will involve:\n\n1. **Moving away from relying solely on color to convey information.** We will implement alternative methods like:\n    * **Shape variations:** Different record types can be represented by distinct shapes.\n    * **Icons and symbols:** Universally understood icons can represent specific data categories.\n    * **Text labels:**  Clear and concise labels will accompany visual elements.\n    * **Pattern variations:** Different patterns (e.g., stripes, dots) can distinguish record types.\n\n2. **Providing multiple interaction modalities.** Users will have flexibility in how they interact with the tool, including:\n    * **Keyboard navigation:**  Ensure all functionality is accessible via keyboard.\n    * **Screen reader compatibility:** Implement appropriate ARIA attributes and semantic HTML to allow screen readers to interpret and navigate the interface effectively.\n    * **Customization options:** Allow users to adjust font sizes, contrast levels, and color palettes to suit their needs.\n\n3. **Conducting accessibility audits.** We will regularly assess the tool using accessibility testing tools and involve users with diverse needs in usability testing to identify and address potential barriers. \n","GenTime":"2024-08-19 15:48:05","text":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n\n\n##Decision\nAccessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n"}
{"File Name":"containers\/2-.md","Context":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n","Decision":"======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","tokens":34,"id":2619,"Prediction":"## Decision\n======\nWe will use a Fedora-based image for our application deployment. \n","GenTime":"2024-08-19 15:48:06","text":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n\n\n##Decision\n======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n"}
{"File Name":"TANF-app\/001-ADR-template.md","Context":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":37,"id":3784,"Prediction":"## Decision\n\nThis ADR documents the process for recording architectural decisions for this project. We will utilize a lightweight ADR format inspired by the RFC process and Michael Nygard's template. Each decision will be documented in a separate Markdown file within the `\/docs\/adr` directory of the project repository.\n\nThis approach was chosen to:\n\n* **Improve transparency and understanding:**  Documenting decisions provides a clear and accessible history of why the system is designed the way it is.\n* **Facilitate communication and collaboration:** ADRs provide a centralized location for stakeholders to understand and discuss architectural choices.\n* **Support maintainability and evolution:** Having a record of past decisions aids in understanding the implications of future changes and avoids repeating past mistakes.\n* **Ensure accountability and traceability:** ADRs clearly link decisions back to their context and rationale, fostering accountability.\n\nThis decision addresses the need to record architectural decisions by establishing a standardized process and location for documentation.\n","GenTime":"2024-08-19 15:48:10","text":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"Prediction":"## Decision\n\nWe will use a lightweight approach to documenting architectural decisions using Markdown Architectural Decision Records (ADRs). Each decision will be recorded in a separate Markdown file adhering to a consistent template.  This lightweight approach is favored over a heavyweight approach (e.g., using a dedicated tool) due to its simplicity, ease of use, and integration with our existing version control system.\n","GenTime":"2024-08-19 15:48:12","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"Prediction":"## Decision\n\nWe will use **absolute indexing** for accessing pixels within the SQW object. \n\n## Reasons\n\n* **Simplicity for the user:** Absolute indexing provides a consistent and intuitive way to access pixel data, regardless of the underlying file-backed implementation. Users won't need to keep track of page boundaries or perform relative index calculations.\n* **Flexibility:**  Absolute indexing allows for easier implementation of features that require accessing data across page boundaries, such as image processing algorithms or region of interest selection.\n* **Abstraction:** By abstracting away the file-backed implementation detail, we maintain flexibility to change the paging mechanism in the future without impacting the user interface.\n\n## Consequences\n\n* **Increased implementation complexity:**  The SQW object will be responsible for managing page loading and translating absolute indices to relative indices internally.\n* **Potential performance overhead:** Accessing pixels that are not currently in memory will incur a performance penalty due to disk I\/O. However, we will mitigate this through efficient caching strategies and pre-fetching mechanisms. \n\n## Status\n\nAccepted. \n","GenTime":"2024-08-19 15:48:17","text":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n\n\n##Decision\nThere will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n"}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n","Decision":"Done.\\n","tokens":42,"id":2997,"Prediction":"## Decision\n\nWe will use AWS ECS (Elastic Container Service) to run our Docker containers. This managed service eliminates the need to provision and manage our own Docker infrastructure. We will create an ECS cluster to group and manage our containers. This approach allows us to leverage the scalability, reliability, and security features provided by AWS ECS. \n","GenTime":"2024-08-19 15:48:19","text":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n\n\n##Decision\nDone.\\n"}
{"File Name":"lobiani\/0005-use-cqrs-architectural-style.md","Context":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n","Decision":"We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","tokens":45,"id":1031,"Prediction":"## Decision\n\n**Employ a CQRS (Command Query Responsibility Segregation) pattern to decouple read and write operations.**\n\nThis decision addresses the challenge of balancing read and write scalability and maintainability in a complex system by separating the concerns:\n\n* **Command side (Write):** Focuses on handling commands that change the application's state. It uses a model optimized for write operations, ensuring data consistency and integrity.\n* **Query side (Read):** Focuses on handling queries for retrieving data. It can use a different model, optimized for efficient read operations, potentially using denormalized data or caching strategies.\n\n**Rationale:**\n\n* **Improved performance:** Decoupling reads and writes allows for independent scaling and optimization of each operation.\n* **Enhanced scalability:** Each side can be scaled independently based on its specific load requirements.\n* **Better maintainability:** Separate models for reads and writes simplify development and reduce the impact of changes on other parts of the system.\n* **Increased flexibility:** Different data stores or technologies can be used for the command and query sides based on their specific needs.\n\n**Trade-offs:**\n\n* **Increased complexity:** CQRS introduces additional layers and components, requiring careful design and implementation.\n* **Eventual consistency:** Asynchronous communication between the command and query sides can lead to eventual consistency, requiring careful handling of data synchronization.\n\n**Consequences:**\n\n* Developers need to be familiar with the CQRS pattern and its implications.\n* Monitoring and management of the system might become more complex due to the separated components.\n* Specific strategies for handling eventual consistency and data synchronization need to be implemented.\n\n**Overall, CQRS offers a powerful approach for achieving both read and write scalability and maintainability in complex systems. However, its inherent complexities need to be carefully considered and addressed during design and implementation.** \n","GenTime":"2024-08-19 15:48:27","text":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n\n\n##Decision\nWe will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n"}
{"File Name":"testy\/0007-support-node-versions-with-at-least-security-updates.md","Context":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n","Decision":"Only support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","tokens":20,"id":1048,"Prediction":"## Decision\n\nWe will maintain a clear and concise versioning policy with a defined End-of-Life (EOL) cycle for our software. \n\n**Versioning Scheme:**\n\nWe will adopt [Semantic Versioning](https:\/\/semver.org\/) (Major.Minor.Patch) for our software:\n\n* **Major:** Breaking changes, potentially requiring code updates in integrations.\n* **Minor:** New features and enhancements, backward compatible with previous minor versions.\n* **Patch:** Bug fixes and security patches, backward compatible with previous patch versions.\n\n**Supported Versions:**\n\n* We will provide **active support** for the latest major version and the previous major version. \n    * Active support includes bug fixes, security patches, and responding to user inquiries.\n* We will provide **limited support** for older major versions until their End-of-Life date.\n    * Limited support includes only critical security patches. \n\n**End-of-Life (EOL) Policy:**\n\n* A major version will reach its EOL date **12 months** after the release of the subsequent major version. \n* We will announce the EOL date of a version at least **3 months** prior to its occurrence through our communication channels (website, release notes, etc.).\n\n**Communication:**\n\n* Supported versions and their EOL dates will be clearly documented in our:\n    * **README file** in the software repository.\n    * **Website documentation** dedicated to versioning and support policy.\n    * **Release notes** for every new release.\n\n**Updating the Policy:**\n\nThis policy will be reviewed and updated at least once a year or as needed based on the software's evolution and user feedback. \n","GenTime":"2024-08-19 15:48:35","text":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n\n\n##Decision\nOnly support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\\nOpen Lobby must have open stable API.\\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"Prediction":"## Decision\n\nWe will implement an open and stable API for the Open Lobby feature. This API will adhere to the following principles:\n\n* **Versioning:** The API will use semantic versioning (e.g., v1.0.0) to clearly communicate breaking changes and ensure backward compatibility where possible. \n* **Documentation:** Comprehensive and up-to-date documentation will be provided, including API endpoints, request\/response formats, authentication methods, and usage examples. The documentation will be publicly accessible.\n* **Stability guarantees:** We will strive to maintain backward compatibility for the API within major versions. Breaking changes will be introduced only after a deprecation period and will be clearly communicated in the documentation and release notes.\n* **Technology selection:**  We will choose a widely adopted and well-supported technology for the API, such as RESTful API with JSON or GraphQL. The chosen technology will be based on factors such as ease of use, industry standards, and team familiarity. \n\n\n## Consequences\n\n**Positive:**\n\n* **Increased flexibility and extensibility:**  An open API allows third-party developers and internal teams to integrate with the Open Lobby feature and build custom experiences. \n* **Potential for ecosystem growth:**  A well-documented and stable API can foster the development of a thriving ecosystem around the Open Lobby, leading to innovative integrations and increased adoption. \n* **Improved testability:**  A well-defined API facilitates automated testing and integration testing, improving the overall quality and stability of the Open Lobby feature.\n\n**Negative:**\n\n* **Increased development and maintenance effort:**  Developing and maintaining a stable API requires careful planning, design, and ongoing support. \n* **Security considerations:**  Exposing an API introduces potential security risks. We will need to implement appropriate security measures, such as authentication, authorization, and input validation, to mitigate these risks.\n* **Version management complexity:** As the API evolves, managing different versions and ensuring backward compatibility can become challenging. \n\n**Note:** This decision will be reviewed and updated as needed based on future requirements and feedback.\n","GenTime":"2024-08-19 15:48:43","text":"## Context\\nOpen Lobby must have open stable API.\\n\n\n##Decision\nOpen Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n"}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"## Context\\nDefine a strategy for deprecations.\\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"Prediction":"## Decision\n\nWe will adopt a **phased approach to deprecations** with clear communication, sufficient migration time, and alternatives provided. This approach aims to minimize disruption for users while allowing us to modernize and improve the software.\n\n**Specifically, we will:**\n\n1. **Announce:** Publicly announce the deprecation in the release notes, documentation, and potentially through in-app notifications. Clearly state the deprecated feature, the planned removal date, and the reason for deprecation.\n2. **Provide Alternatives:** Offer clear alternatives to the deprecated functionality. This could involve migrating to a new API, utilizing a different feature, or adopting a completely new approach. \n3. **Migration Period:** Provide a reasonable migration period based on the complexity of the change. This should be a minimum of two release cycles but could be longer for significant changes.\n4. **Support:** Continue to provide support for the deprecated feature during the migration period. This includes addressing bug fixes and answering user questions.\n5. **Remove:** After the deprecation period, remove the deprecated feature from the codebase. This ensures maintainability and reduces technical debt.\n6. **Documentation:** Update all relevant documentation to reflect the deprecation and removal of the feature.\n\n**Tooling:**\n\n*  We will leverage automated tooling for:\n    * Tracking deprecated code sections.\n    * Generating deprecation warnings during compilation or at runtime.\n    * Identifying usage of deprecated functionalities in our codebase and in user projects (where possible). \n\n**Communication Channels:**\n\n* Release notes\n* Official documentation\n* Blog posts (for major deprecations)\n* Forum announcements\n* In-app notifications (if applicable)\n\nThis strategy strives for a balance between encouraging users to adopt newer technologies while minimizing disruption and providing ample opportunity for a smooth transition. \n","GenTime":"2024-08-19 15:48:51","text":"## Context\\nDefine a strategy for deprecations.\\n\n\n##Decision\n### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n"}
{"File Name":"api-catalogue\/0001-remove-tech-docs-gem-dependency.md","Context":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n","Decision":"Remove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","tokens":228,"id":3586,"Prediction":"## Decision\n\nWe will migrate the API catalogue website away from using the Tech Docs Gem (TDG) and Middleman. \n\nThis decision is driven by the following factors:\n\n* **Data source mismatch:** The TDG is designed for markdown-based documentation, while our data source is now a CSV. This mismatch makes it difficult to effectively utilize the TDG's features.\n* **Limited customization:**  Overriding TDG's default templates and adding page-specific styling or JavaScript is proving to be cumbersome. This lack of flexibility hinders our ability to tailor the website to our specific needs.\n* **Performance issues:**  The TDG's approach to generating the Table of Contents is inefficient for our use case, leading to significantly increased build times. \n* **Dependency constraints:** The TDG's strict dependency versions restrict our ability to control updates, potentially leaving us vulnerable to security issues and lacking support. \n\nBy migrating away from the TDG, we aim to:\n\n* **Improve website performance:** By adopting a more efficient approach to generating dynamic content and managing dependencies.\n* **Increase flexibility and customization:**  This allows us to tailor the website's design, functionality, and user experience to better suit the API catalogue's requirements.\n* **Simplify maintenance:**  Migrating to a solution better aligned with our data source and technology stack will reduce complexity and ease future development.\n\nThe specific alternative solution and migration plan will be detailed in a separate Architectural Decision Record. \n","GenTime":"2024-08-19 15:48:58","text":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n\n##Decision\nRemove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n"}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"Prediction":"## Decision\n\nWe will introduce a node pre-processing step within Nd4j's model import framework to handle automatic graph upgrades. This pre-processing step will be driven by annotations accompanying the imported model. These annotations will specify the necessary transformations to ensure compatibility with the current Nd4j version. \n\nThis approach offers several advantages:\n\n* **Automation:** Automating the upgrade process simplifies model integration and reduces manual intervention.\n* **Flexibility:** Annotations provide a flexible mechanism to define and apply various transformations. \n* **Maintainability:**  This approach centralizes upgrade logic within the pre-processing step, improving maintainability and reducing code duplication. \n\nWe will implement this by:\n\n1. **Defining an Annotation Schema:**  A well-defined schema will be created to express version-specific transformations within the model import annotations. \n2. **Developing a Pre-processing Module:** This module will parse annotations and execute the corresponding graph transformations before feeding the model to the Nd4j execution engine.\n3. **Integrating with Existing Framework:** The pre-processing module will be seamlessly integrated into the existing model import framework.\n\nThis decision allows us to address the compatibility challenges posed by evolving protobuf-based frameworks while maintaining the flexibility and extensibility of Nd4j's model import capabilities. \n","GenTime":"2024-08-19 15:49:03","text":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n\n\n##Decision\nIn order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"Prediction":"## Decision\n\nWe will use Heroku as the platform for deploying our application. \n\n## Reasons\n\n* **Simplicity:** Heroku's PaaS offering significantly reduces the complexity of setting up and managing infrastructure compared to self-hosting with container orchestration platforms. This aligns with our goal of a simple setup process.\n* **Cost-effectiveness:** Heroku's free tier, including its container registry and PostgreSQL database, allows us to get started without incurring any hosting costs. \n* **Convenience:** Heroku's built-in support for deploying Docker images and automatic configuration of environment variables like \"DATABASE_URL\" further simplifies the deployment process.\n\n## Consequences\n\n* **Vendor lock-in:** Choosing Heroku means tying our deployment process to their platform, potentially making it harder to migrate in the future if needed.\n* **Scalability limitations:** While the free tier is sufficient for initial development, we may need to consider Heroku's paid plans or alternative solutions as our application grows and requires more resources or surpasses the database row limit.\n* **Reduced control:** Using a PaaS means having less control over the underlying infrastructure and system configurations compared to self-hosting.\n\n## Status\n\nAccepted.\n\n## Notes\n\nWe will monitor the application's resource usage and evaluate the need to switch to a paid Heroku plan or a different hosting solution as the project evolves. \n","GenTime":"2024-08-19 15:49:10","text":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n\n##Decision\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n"}
{"File Name":"tendermint\/adr-003-abci-app-rpc.md","Context":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n","Decision":"We dont expose an RPC server on any of our ABCI-apps.\\n","tokens":161,"id":1985,"Prediction":"## Decision\n\nWe will **not** have the ABCI-server expose its own RPC and act as a proxy to Tendermint. \n\nInstead, **Tendermint will expose its own RPC server**, acting as the primary interface for clients. This decision is driven by the following factors:\n\n* **Importance of standardized RPC:** A unified and well-defined Tendermint RPC interface enables the creation of a generic light client compatible with any application built on the platform. This fosters code reusability and promotes standardization across the ecosystem.\n* **Proof Handling:** Clients require direct communication with Tendermint for proof verification.  Delegating this responsibility to individual app developers would introduce unnecessary complexity and potential inconsistencies.\n* **Security:** While arguably more complex, securing two separate servers (ABCI and Tendermint) provides a more robust security model.  Isolating the application from direct client interaction limits the potential attack surface.\n\n**Trade-offs:**\n\n* **Querying complexity:**  Using Tendermint RPC for all interactions might necessitate passing queries through the `Query` interface, potentially making it less intuitive compared to direct ABCI interaction.\n\n**Mitigation:**\n\n* We will explore ways to streamline the query process for improved developer experience while maintaining the benefits of a standardized RPC interface.  This could involve providing helper libraries or clearly documented best practices. \n","GenTime":"2024-08-19 15:49:15","text":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n\n##Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n"}
{"File Name":"dp\/0009.md","Context":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"Prediction":"## Decision\n\n**Do not collapse dimension option and code nodes in the graph database. Maintain separate nodes for dimension options and their corresponding codes.**\n\n## Justification\n\nWhile there is overlap between the information stored on a dimension option and its corresponding code, collapsing them into a single node introduces several disadvantages:\n\n* **Increased complexity:** Queries become more complex as they need to differentiate between relationships intended for the dimension option and those intended for the code.\n* **Reduced flexibility:**  Separating the nodes allows for greater flexibility in modeling future relationships. For example, a dimension option might be linked to external resources or analytical tools, which wouldn't be appropriate to directly link to a code.\n* **Data integrity:** Mixing information from different domains (dataset-specific dimension options vs. general code definitions) within a single node can lead to data integrity issues and make it harder to maintain data consistency.\n\nMaintaining separate nodes for dimension options and codes ensures cleaner data modeling, easier querying, and greater flexibility for future expansion. \n\n## Consequences\n\n* **Slight increase in storage space:** Storing some redundant information across two nodes. However, this is minimal compared to the benefits of separation. \n* **Potential for minor redundancy in relationships:** Both the dimension option and code nodes might have relationships to the same nodes (e.g., \"belongs to codelist\"). This redundancy can be minimized with careful database design. \n","GenTime":"2024-08-19 15:49:22","text":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n\n\n##Decision\nDimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n"}
{"File Name":"git-en-boite\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3505,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions made during the project. Each ADR will consist of a Markdown file with a consistent structure (e.g., title, context, decision, consequences, status) stored within a dedicated \"docs\/adr\" directory in the code repository. \n","GenTime":"2024-08-19 15:49:24","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"planet4-docs\/adr-0001-use-gitbook-for-technical-documentation.md","Context":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Decision":"* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","tokens":492,"id":4087,"Prediction":"## Decision \n\nAfter considering the pros and cons of each option, we have decided to move all Technical documentation (Development & Infrastructure) to Gitbook. \n","GenTime":"2024-08-19 15:49:25","text":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n\n\n##Decision\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n"}
{"File Name":"apply-for-teacher-training\/0003-initial-datamodel.md","Context":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n","Decision":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","tokens":33,"id":4021,"Prediction":"## Decision\n\nWe will adopt a **microservices architecture** with a **domain-driven design** approach for data modeling. \n\nThis decision is based on the following rationale:\n\n* **Scalability and Flexibility:**  Microservices allow for independent scaling and deployment of individual components, providing flexibility as the application grows and evolves.\n* **Maintainability:**  Smaller, decoupled services are easier to understand, maintain, and debug than a large monolithic codebase.\n* **Team Autonomy:** Different teams can own and develop individual services, fostering faster development cycles and specialized expertise.\n* **Domain Alignment:**  Domain-driven design ensures the data model closely reflects the business domain, improving communication and reducing complexity.\n\n## Consequences\n\n* **Increased Complexity:** Microservices introduce distributed system complexities like inter-service communication, data consistency, and fault tolerance.\n* **Infrastructure Overhead:**  Deploying and managing multiple services requires robust infrastructure and tooling.\n* **Initial Investment:**  Setting up a microservices architecture requires upfront investment in design, development practices, and infrastructure. \n\nDespite these challenges, the benefits of scalability, maintainability, and team autonomy outweigh the drawbacks, making this approach suitable for the anticipated growth and evolution of the application.\n","GenTime":"2024-08-19 15:49:31","text":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n\n##Decision\nWe discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n"}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"Prediction":"## Decision \n\n(Please provide the context so I can give you a relevant decision) \n","GenTime":"2024-08-19 15:49:32","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n"}
{"File Name":"volley-management\/0007-follow-optimistic-concurrency.md","Context":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n","Decision":"* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","tokens":86,"id":2526,"Prediction":"## Decision\n\nWe will use **optimistic locking** to handle concurrent data modification. \n\n## Considered Options\n\n* **Optimistic Locking:**  A version field (or timestamp) is added to the data. When updating, we check if the version we loaded is still the current one. If not, the update fails, indicating a concurrent modification.\n* **Pessimistic Locking:** A lock is acquired on the data at the beginning of the transaction. This prevents other transactions from modifying the data until the lock is released.\n\n## Decision Outcome\n\nWe choose optimistic locking for the following reasons:\n\n* **Performance:** In most cases, concurrent modifications are rare. Optimistic locking avoids the overhead of acquiring and releasing locks in these common scenarios, leading to higher throughput.\n* **Maintainability:**  Optimistic locking generally requires less code compared to implementing a robust pessimistic locking mechanism, especially when considering potential issues like deadlocks.\n\n## Pros and Cons of the Chosen Option\n\n**Pros:**\n* Better performance in scenarios with infrequent concurrent modifications.\n* Simpler implementation compared to pessimistic locking.\n\n**Cons:**\n* Requires careful handling of optimistic lock failures. The application must implement retry mechanisms or inform users about the conflict.\n* May not be suitable for applications with very high contention rates, where optimistic locking could lead to excessive retries.\n\n## Future Work\n\n* Monitor the system performance and the frequency of optimistic lock failures.\n* Implement a robust strategy for handling optimistic lock failures, including retry mechanisms and user notifications.\n*  Evaluate the potential need for pessimistic locking for specific, highly contended resources if necessary. \n","GenTime":"2024-08-19 15:49:39","text":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n\n##Decision\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n"}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"Prediction":"## Decision\n\nWe will introduce a new concept of \"component-level permissions\" to address the issue of certain components requiring broad access privileges within a module, without granting full module-level access. \n\nInstead of assigning permissions at the module level, we will define finer-grained permissions at the component level. This allows us to grant the necessary privileges to specific components, like the rule builder or media manager, without granting access to the entire module. \n\nFor example:\n\n* Instead of granting \"full access\" to the \"Content Management\" module, we can grant \"rule management\" permissions to the rule builder component and \"media library access\" permissions to the media manager component.\n\nThis approach provides a more granular and secure access control mechanism, ensuring users only have access to the specific functionalities they need. \n","GenTime":"2024-08-19 15:49:43","text":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n\n\n##Decision\nTo avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"Prediction":"## Decision\n\nWe will refactor the classifier's subject queue to support browsing within a subject set. This will involve:\n\n1. **Storing a local copy of the subject set data:** Instead of discarding subjects after classification, we will store them locally within the classifier. This will allow users to revisit previously viewed subjects.\n2. **Implementing navigation controls:** We will add navigation controls within the classifier to allow users to move freely within the subject set (e.g., \"previous,\" \"next,\" \"go to subject\"). \n3. **Updating data fetching from the API:** The interaction with the Panoptes API will be modified to support fetching a batch of subjects instead of a single subject at a time. This will allow for smoother browsing experience and reduce the load on the API.\n4. **Maintaining synchronization with the server:**  While enabling local browsing, we will need to implement a mechanism to ensure that the locally stored data stays synchronized with the server. This could be achieved by periodically checking for updates or using websockets for real-time updates.\n\nThis approach provides a better user experience by allowing volunteers to browse and compare subjects within a set before making a classification. It also offers greater flexibility and control over the classification process. \n","GenTime":"2024-08-19 15:49:49","text":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n\n\n##Decision\n- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n"}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"Prediction":"## Decision\n\nWe will adopt a cloud-based data warehouse solution, specifically **[Cloud Provider Here - e.g. Google BigQuery, Amazon Redshift, Snowflake etc.]**, combined with a data pipeline tool like **[Data Pipeline Tool Here - e.g. Apache Airflow, Google Cloud Dataflow, AWS Glue]**. \n\n## Decision Rationale\n\nThis decision addresses the identified needs and prioritizes the key drivers in the following ways:\n\n* **Flexibility:** Cloud data warehouses offer scalability and schema flexibility, allowing us to adapt to evolving data needs and analytics requirements without significant infrastructure overhead.\n* **Integration:** These platforms provide native integrations with various data sources, including databases, SaaS applications, and streaming platforms, facilitating efficient data ingestion.\n* **Implementation Difficulty:** While setting up the initial pipeline and data transformations requires effort, cloud-based solutions offer managed services and intuitive interfaces, reducing the overall implementation complexity compared to building a custom solution. \n* **Future Expansion:** This infrastructure allows building an analytics product for customers by leveraging the platform's existing features for secure data sharing, access control, and potentially even embedded analytics dashboards.\n\n**Alternatives Considered:**\n\n* **Building a custom data warehouse solution:** While offering more control, it requires significant development time and resources, contradicting the need for a quick and adaptable solution.\n* **Continuing with the existing read-replica setup:** This approach lacks the scalability, flexibility, and advanced analytics capabilities required for complex analysis and customer-facing products.\n\n**Consequences of Decision:**\n\n* **Cost:** Cloud-based solutions come with subscription costs, potentially higher than maintaining a minimal on-premise setup. However, the scalability and managed services can lead to cost savings compared to managing and scaling a custom solution in the long term.\n* **Vendor Lock-in:** Choosing a specific cloud provider and tooling may lead to vendor lock-in. However, adopting platform-agnostic data formats and pipeline orchestration can mitigate this risk.\n\n**Next Steps:**\n\n1. **Evaluate and select a specific cloud data warehouse provider** based on factors such as pricing, features, integration capabilities, and data residency requirements.\n2. **Choose a suitable data pipeline tool** that seamlessly integrates with the chosen data warehouse and supports the required data sources and transformation capabilities.\n3. **Design and implement the initial data pipeline** to ingest data from critical sources into the data warehouse, prioritizing data relevant for customer analytics.\n4. **Establish data governance and security policies** to ensure data quality, integrity, and access control, especially for customer-facing applications.\n5. **Iteratively develop and deploy analytics dashboards and reports** within the chosen platform to address internal and customer-facing analytics needs.\n\nBy adopting this approach, Bud can establish a robust, scalable, and adaptable analytics infrastructure that supports its current and future growth while enabling the development of innovative customer-facing analytics products.\n","GenTime":"2024-08-19 15:50:00","text":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n\n\n##Decision\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n"}
{"File Name":"eq-author-app\/0002-routing-mk2.md","Context":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n","Decision":"Implement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","tokens":51,"id":2111,"Prediction":"## Decision\n\nWe will implement a **rule-based routing engine** to handle the complex branching logic required for the survey. \n\nThis engine will:\n\n1. **Represent routing rules using a structured format**, such as a JSON schema or a domain-specific language (DSL). This format will allow users to define conditions based on:\n    *  Answers to previous questions.\n    *  User attributes.\n    *  Calculated values.\n2. **Employ a rule evaluator** that can parse and evaluate boolean expressions defined in the chosen format. This evaluator will determine the appropriate next step based on the respondent's answers and the defined rules.\n3. **Support complex branching scenarios** including:\n    *  **Conditional branching:** Navigate to different sections based on specific answers.\n    *  **Skip logic:** Bypass irrelevant questions based on previous responses.\n    *  **Dynamic routing:** Calculate scores or values based on answers and route accordingly.\n4. **Provide an interface for administrators** to easily create, edit, and manage routing rules for different surveys. This interface could be a graphical user interface (GUI) or a command-line interface (CLI).\n\n## Consequences\n\nThis decision will lead to the following consequences:\n\n* **Increased Flexibility:** The system will be able to accommodate a wide range of routing scenarios and complex branching logic.\n* **Improved User Experience:** Respondents will experience a more personalized and efficient survey-taking process.\n* **Reduced Development Time:** Using a dedicated routing engine will simplify development and maintenance of the survey routing logic.\n* **Increased Complexity:** Implementing a rule-based routing engine adds complexity to the system compared to a simpler approach. This may require more sophisticated development and testing efforts.\n* **Potential Performance Impact:** Complex routing rules with extensive evaluations could impact the performance of the survey application. This needs to be considered during implementation and optimization.\n","GenTime":"2024-08-19 15:50:08","text":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n\n\n##Decision\nImplement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"Prediction":"## Decision\n\nIntroduce a new class \"Drain\" in Maud, independent of the existing \"Reaction\" class, to represent drains in the ODE system. This decision is made despite the fact that drains, like reactions, influence the ODE system, due to the following reasons:\n\n* **Conceptual Clarity:** Drains are fundamentally different from enzymatic reactions. They act as boundary conditions and do not rely on enzyme kinetics. Separating them enhances code readability and maintainability.\n* **Post-processing Advantages:**  Excluding drains from enzyme-dependent analysis techniques simplifies these processes and avoids potential misinterpretations. \n* **Prior Specification:** Treating drains as priors in Bayesian inference allows for informative prior predictive checks, reflecting their known influence on the experimental conditions.\n\nThis approach enables a clear separation between enzymatic reactions and drains, improving the flexibility and interpretability of Maud while ensuring compatibility with Bayesian analysis methods. \n","GenTime":"2024-08-19 15:50:12","text":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n\n\n##Decision\nDrains will not be considered as a reaction class.\\n"}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"Prediction":"## Decision\n\nIntroduce a new database entity to store sorting options. This entity will allow shop owners to define custom sorting options for product listings and search result pages through the administration interface. \n\nThe entity will contain the following information:\n\n* **Name:** (e.g., \"Price - Low to High\", \"Newest Arrivals\")\n* **Priority:**  A numerical value to determine the display order in the administration and storefront.\n* **Availability:** Define if the sorting option is available for product listings, search results, or both.\n* **Default for Listings:** A boolean flag indicating if this is the default sorting option for product listings. Only one option can be marked as default.\n* **Default for Search:** A boolean flag indicating if this is the default sorting option for search results. Only one option can be marked as default.\n* **Sort Criteria:** A JSON field that defines the actual sorting logic. This could be a simple key-value pair for basic sorting (e.g., `{ \"price\": \"ASC\" }`) or a more complex structure for multi-level sorting or custom logic.\n\nThe existing service-based approach for defining sorting options will be deprecated but remain functional for backward compatibility. \n\n**Default Sorting:**\n\n* **Product Listings:** Shop owners can set a default sorting option through the administration interface.\n* **Search Results & Suggest Route:** \"Top Results\" remains the default, sorting products by `_score`.\n\n**Implementation Details:**\n\n* The administration interface will be extended to allow managing these sorting options.\n* The storefront will be updated to retrieve and use the defined sorting options. \n* A migration script will be provided to transfer existing service-based sorting options to the new entity format.\n\nThis approach provides a more user-friendly and flexible way for shop owners to manage sorting options while ensuring backward compatibility.\n","GenTime":"2024-08-19 15:50:20","text":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n\n\n##Decision\nFrom now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n"}
{"File Name":"gp-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":96,"id":3470,"Prediction":"## Decision\n\nWe will use **Express.js** as our web framework for this project. \n\nThis decision builds upon the platform-level choice of Node.js and leverages the team's existing expertise in Express.js, minimizing the learning curve and enabling us to focus on delivering domain-specific features efficiently. \n","GenTime":"2024-08-19 15:50:22","text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n"}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will follow a standardized template, including context, decision, status, consequences, and other relevant information.  This will help us maintain a clear and traceable history of why we structured the software the way we did. \n","GenTime":"2024-08-19 15:50:24","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"LogLady\/0004-implement-styled-components.md","Context":"## Context\\nComplicated with uses of different css rules\\n","Decision":"Only uses styled-components for design\\n","tokens":12,"id":1855,"Prediction":"## Decision\n\nAdopt a CSS methodology like BEM (Block Element Modifier) or ITCSS (Inverted Triangle CSS) to organize and maintain consistent styling throughout the application. \n\n**Reasoning:**\n\n- A defined structure for writing CSS will combat the complexity arising from numerous and potentially conflicting rules.\n- Methodologies like BEM and ITCSS promote reusability, readability, and maintainability, crucial aspects for large and evolving codebases. \n- Choosing a widely used approach facilitates onboarding new developers and ensures a shared understanding of the stylesheet organization.\n\n**Consequences:**\n\n- **Positive:**\n    - Improved CSS maintainability and scalability.\n    - Reduced style conflicts and unexpected behavior.\n    - Increased code readability and collaboration efficiency.\n- **Negative:**\n    - Initial learning curve for developers unfamiliar with the chosen methodology.\n    - Slight overhead in writing more verbose class names.\n\n**Alternatives:**\n\n- **Continue with the existing approach:** This is not recommended due to the stated complexity and potential for further issues as the codebase grows.\n- **Utilize a CSS preprocessor (like Sass or Less):** While helpful, preprocessors alone don't enforce a structural methodology. They can, however, complement BEM or ITCSS.\n\n**Next Steps:**\n\n1. **Choose a specific CSS methodology** based on team preferences and project needs.\n2. **Define clear guidelines and documentation** for the chosen methodology. \n3. **Train the team** on the new approach and best practices.\n4. **Gradually refactor existing CSS** or implement the methodology in new features. \n","GenTime":"2024-08-19 15:50:31","text":"## Context\\nComplicated with uses of different css rules\\n\n\n##Decision\nOnly uses styled-components for design\\n"}
{"File Name":"molgenis-service-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":1328,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will be a self-contained Markdown file that includes context, decision, status, and consequences. We will store these ADRs in a dedicated \"docs\/adr\" directory within the project repository. \n","GenTime":"2024-08-19 15:50:33","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n"}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"Prediction":"## Decision\n\nImplement a base abstract class for presenters' delegates that defines a common interface with default implementations for optional methods. This approach provides several advantages:\n\n* **Consistency:** Enforces a consistent interface across all presenter delegates, reducing the likelihood of errors due to inconsistent method signatures or behaviors.\n* **Flexibility:** Allows for optional methods in the delegate protocol, enabling presenters to expose functionality only when needed without breaking implementations in existing delegates.\n* **Code Reusability:**  Provides a central location for common delegate logic, reducing code duplication and improving maintainability. \n* **Reduced Boilerplate:** Simplifies delegate implementations by providing default behaviors for common tasks, allowing developers to focus on specific logic.\n\nConcrete delegate classes can then inherit from this abstract base class and override only the necessary methods, benefiting from the shared structure and reducing the potential for inconsistencies. \n","GenTime":"2024-08-19 15:50:37","text":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n\n\n##Decision\nWe decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n"}
{"File Name":"agentframework\/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md","Context":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n","Decision":"We will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","tokens":31,"id":2360,"Prediction":"## Decision\n\nWe will **not** implement interceptors by directly modifying existing classes in ES5. \n\nThis decision is made for the following reasons:\n\n* **Reversibility and Side Effects:**  Modifying existing classes directly is not easily reversible and can lead to unforeseen side effects, especially if other parts of the application or third-party libraries rely on the original class behavior. \n* **Compatibility Issues:** As noted in the context, this approach can cause compatibility problems with other frameworks that might be using or extending the same classes. \n* **Maintainability:**  Direct modification can make the codebase harder to understand and maintain, particularly as the project grows.\n\n## Alternatives Considered\n\n1. **Wrapper Classes:**  Instead of direct modification, we could create wrapper classes that extend the original classes and implement the interceptor functionality. This approach provides better isolation and avoids direct modification of existing code.\n2. **Composition over Inheritance:** We could favor composition over inheritance and implement interceptors as separate objects that can be dynamically attached to target objects. This offers flexibility and avoids the rigidity of inheritance. \n3. **ES6 Proxies (If Possible):**  If the project allows for the use of ES6, Proxies offer a cleaner and more robust way to intercept and customize object behavior without directly modifying existing code.\n\n## Decision Outcome\n\nWe will explore and evaluate the alternatives mentioned above, prioritizing solutions that emphasize code isolation, compatibility, and maintainability. The specific approach adopted will depend on factors like project constraints, performance requirements, and the complexity of the interceptor logic. \n","GenTime":"2024-08-19 15:50:44","text":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n\n\n##Decision\nWe will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n"}
{"File Name":"content-data-api\/adr-007-etl-publishing-api-content-store.md","Context":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n","Decision":"Addressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","tokens":33,"id":1857,"Prediction":"## Decision\n\nWe will implement a webhook-based solution to update the Content Items' dimension in near real-time. \n\nWhen a content item is edited, the CMS will trigger a webhook notification. Our application will receive this notification and update the corresponding entry in the Content Items' dimension. This approach offers several benefits:\n\n* **Near real-time updates:**  The dimension reflects changes shortly after content edits, improving data freshness and reducing discrepancies.\n* **Scalability:** Webhooks provide an efficient way to handle updates without continuous polling.\n* **Loose coupling:** The CMS and the data warehouse remain loosely coupled, simplifying maintenance and reducing dependencies. \n\n[1]: This is a placeholder URL and should be replaced with the actual Trello card URL.\n","GenTime":"2024-08-19 15:50:47","text":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n\n\n##Decision\nAddressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n"}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"Prediction":"## Decision\n\nWe will adopt Prettier as our default code formatter for all projects.  This decision aims to improve code readability and maintainability by enforcing a consistent code style across the entire codebase. \n","GenTime":"2024-08-19 15:50:49","text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n"}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"Prediction":"## Decision\n\nWhen a fetcher throws an exception, the URL used for fetching will be included in the exception message. This approach addresses both Claim 1 and Claim 2 while adhering to the Decision Drivers.\n\n**Justification:**\n\n* **Improved Debugging:** Including the URL in the exception message directly provides immediate context when errors occur, eliminating the need for separate log searching.\n* **Code Clarity:** This method avoids additional logging code, maintaining code simplicity and readability.\n* **Best Practices:** Aligns with the recommended practice of enriching exception messages with relevant context for effective debugging. \n\n**Consequences:**\n\n* **Increased Exception Verbosity:** Exception messages will be slightly longer, but the added context outweighs this minor drawback.\n* **Potential Sensitive Information:**  If the URL contains sensitive data, appropriate sanitization or masking techniques should be implemented to prevent exposure in logs or error messages. \n","GenTime":"2024-08-19 15:50:53","text":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n\n\n##Decision\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n"}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"Prediction":"## Decision\n\nWe will implement a generic relationship model using a graph database structure. This will allow us to:\n\n* **Represent both parent-child and family relationships:**  Nodes in the graph can represent individuals, while edges can define the type of relationship (e.g., \"mother_of\", \"child_of\", \"head_of_household\", \"spouse\").\n* **Easily navigate between related individuals:**  The graph structure enables efficient traversal, making it simple to switch between a mother, her children, and other family members during data entry and viewing. \n* **Accommodate complex family structures:**  The flexibility of the graph model allows for diverse family structures beyond the nuclear family, such as extended families, blended families, and same-sex couples. \n* **Support future relationship types:**  The generic nature of the model makes it easily extensible to incorporate new relationship types as needed.\n\n**Implementation Details:**\n\n* Each individual will be represented by a node in the graph database.\n* Each relationship will be represented by a directed edge connecting two individual nodes.\n* Edges will be labeled with the relationship type (e.g., \"mother_of\", \"child_of\").\n* A \"family\" can be defined as a subgraph within the larger graph, with the head of the household as a central node.\n\n**Alternatives Considered:**\n\n* **Relational Database with Foreign Keys:** This approach lacks the flexibility and efficiency for navigating complex relationships and can lead to performance issues.\n* **Hierarchical Data Structure:**  While suitable for parent-child relationships, this model struggles with more complex family structures.\n\n**Consequences:**\n\n* **Increased Complexity:** Graph databases can be more complex to implement and manage than traditional relational databases.\n* **Learning Curve:** Developers will require training and experience working with graph databases.\n\n**Overall, the benefits of a flexible and performant relationship model outweigh the potential drawbacks. The graph database approach provides a robust and future-proof solution for representing and managing complex family relationships within the mother program.** \n","GenTime":"2024-08-19 15:51:01","text":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n\n##Decision\nCreate an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n"}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"Prediction":"## Decision\n\nTo improve performance and reduce the overhead of lock management, AtlasDB will adopt asynchronous lock release for immutable timestamp locks and row\/cell locks upon transaction commit. \n\nThis means that:\n\n1. **Asynchronous Release:** Instead of releasing locks synchronously after a transaction commits, the release operations will be dispatched asynchronously without waiting for their completion. \n2. **Unified Release:** Instead of two separate calls for releasing immutable timestamp locks and row\/cell locks, a single asynchronous operation will release both types of locks.\n\nThis decision prioritizes performance improvement by reducing the latency experienced by users upon transaction commit. The asynchronous nature of the release operation means that the user doesn't bear the cost of waiting for the lock service to acknowledge the release. Furthermore, combining the release of both lock types into a single operation reduces the communication overhead with the lock service.\n\nWhile this decision introduces the possibility of locks being held slightly longer than in the synchronous approach, the performance benefits are deemed to outweigh this minor tradeoff. Additionally, since correctness is not contingent on immediate lock release, the system remains robust even in failure scenarios such as client crashes.\n","GenTime":"2024-08-19 15:51:06","text":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n\n\n##Decision\nInstead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n"}
{"File Name":"dogma\/0011-message-timing-information.md","Context":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n","Decision":"The sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","tokens":101,"id":1610,"Prediction":"## Decision\n\nWe will **expose message timing information via the API**. \n\nWhile we acknowledge the benefits of modeling time explicitly within messages for time-based business logic, we recognize that relying solely on this approach has limitations:\n\n* **Reduced Observability:**  Not exposing timing information hinders our ability to monitor and debug message flow within the system. This information is crucial for performance analysis, identifying bottlenecks, and understanding system behavior.\n* **Limited External Integration:** Third-party tools and services often rely on message timestamps for tasks like event correlation, log analysis, and real-time dashboards. Not providing this information limits our integration options and could necessitate cumbersome workarounds. \n* **Potential Inefficiency:** For non-time-critical functionalities, retrieving timestamps via the API can be more efficient than requiring applications to inject and manage potentially redundant timestamps within messages.\n\n**Implementation Details:**\n\n* We will expose relevant timestamps via the API for each message, including (but not limited to) creation time, queue entry time, processing start time, and completion time.\n* The API will provide a clear and consistent way to access this information.\n* Documentation will clearly articulate the meaning and usage of each timestamp.\n\n**Mitigation:**\n\n* We will continue to encourage the practice of modeling time within messages for business logic explicitly reliant on timing.\n* We will provide clear guidance and best practices for using both internal message timestamps and API-exposed timing information. \n\nThis decision prioritizes observability, flexibility, and integration capabilities while acknowledging the importance of explicit time modeling within messages for certain use cases. \n","GenTime":"2024-08-19 15:51:13","text":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n\n\n##Decision\nThe sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"Prediction":"## Decision\n\nWe will separate the build and deploy stages into distinct pipelines. This will allow for configuration changes to be deployed rapidly without requiring a full rebuild of the application. The build pipeline will produce an immutable artifact (e.g., a Docker image or deployment package), which will then be promoted through different environments (e.g., staging, production) by the deploy pipeline. \n","GenTime":"2024-08-19 15:51:15","text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n"}
{"File Name":"Head-Start-TTADP\/0011-monitoring.md","Context":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n","Decision":"TTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","tokens":20,"id":1171,"Prediction":"## Decision\n\nWe will implement a centralized logging and monitoring system for TTA Smart Hub. This system will collect logs and metrics from all components of the Smart Hub, including:\n\n*   **Software Components:** Backend services, APIs, databases, etc.\n*   **Hardware Components:** Smart devices, gateways, network equipment, etc.\n\nThe collected data will be used to:\n\n*   **Monitor System Health:** Detect anomalies, track performance metrics, and visualize system behavior.\n*   **Troubleshoot Issues:** Quickly identify root causes of errors, performance bottlenecks, and other issues.\n*   **Improve System Reliability:**  Proactively identify and address potential problems before they impact users.\n\n**Justification:**\n\nCentralized logging and monitoring offers several advantages:\n\n*   **Improved Visibility:** Provides a unified view of the entire system's health and performance.\n*   **Faster Problem Resolution:** Enables quicker identification and diagnosis of issues through correlated logs and metrics.\n*   **Increased Uptime:** Proactive monitoring helps prevent downtime by identifying and addressing potential problems early on.\n*   **Data-Driven Optimization:** Collected data can be analyzed to identify areas for improvement in system performance, reliability, and resource utilization.\n\n**Options Considered:**\n\n*   **Developing a custom monitoring solution in-house:** This option provides greater flexibility but requires significant development effort and ongoing maintenance.\n*   **Using a managed logging and monitoring service:** This option offers a quicker and easier setup with reduced maintenance overhead but may come with higher costs and less customization.\n\n**Decision Outcome:**\n\nWe will opt for a **managed logging and monitoring service** due to the following reasons:\n\n*   **Faster Implementation:** Allows for rapid deployment and quicker time-to-value.\n*   **Reduced Maintenance:** Offloads the burden of managing and scaling the monitoring infrastructure.\n*   **Lower Initial Costs:** Eliminates upfront development costs associated with building a custom solution.\n\nWe will evaluate and select a specific managed service based on factors like:\n\n*   **Features and capabilities:** Support for various log sources, real-time monitoring, alerting, dashboards, and reporting.\n*   **Scalability and reliability:** Ability to handle the current and future volume of logs and metrics.\n*   **Cost-effectiveness:** Pricing model aligned with our budget and usage patterns.\n\n**Next Steps:**\n\n*   **Research and select a suitable managed logging and monitoring service.**\n*   **Define monitoring requirements and metrics to track.**\n*   **Implement the chosen solution and integrate it with all Smart Hub components.**\n*   **Establish alerting and escalation procedures.**\n*   **Continuously review and optimize the monitoring setup based on usage patterns and evolving needs.** \n","GenTime":"2024-08-19 15:51:27","text":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n\n##Decision\nTTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n"}
{"File Name":"meadow\/0022-iiif-manifests.md","Context":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n","Decision":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","tokens":55,"id":3852,"Prediction":"## Decision\n\n**Option 1: Implement Bypass Logic in Donut**\n\n* Add logic within Donut to identify Meadow-generated manifests (e.g., based on a specific prefix in the manifest ID) and directly serve them from S3 without further processing.\n\n**Pros:**\n* Minimal code changes required.\n* Leverages existing caching infrastructure.\n\n**Cons:**\n* Introduces Meadow-specific logic into Donut, increasing coupling.\n* Donut still needs to process the request, even if minimally.\n\n**Option 2: Route Meadow Manifests Directly to S3**\n\n* Configure a separate routing rule (e.g., using AWS API Gateway) to directly send requests for Meadow-generated manifests to the S3 bucket.\n\n**Pros:**\n* Clear separation of concerns between Donut and Meadow.\n* More efficient as requests are not unnecessarily processed by Donut.\n\n**Cons:**\n* Requires additional infrastructure setup and maintenance.\n* May require changes to how Meadow manifests are identified (e.g., through a specific URL pattern).\n\n**Decision:**\n\nWe will implement **Option 2: Route Meadow Manifests Directly to S3**. This option offers better separation of concerns and improved performance. \n\n**Rationale:**\n\nWhile Option 1 offers a quick solution, it introduces unnecessary coupling between Donut and Meadow. Option 2 promotes a cleaner architecture and avoids adding Meadow-specific logic to Donut. \n\n**Consequences:**\n\n* We will need to configure a separate routing rule for Meadow manifests. \n* We need to ensure that Meadow manifest URLs are easily identifiable for routing purposes. \n* This change will require coordination between the Meadow and platform teams. \n\nThis decision will be reviewed if:\n\n* The performance difference between options is negligible.\n* Maintaining a separate routing rule becomes overly complex. \n","GenTime":"2024-08-19 15:51:34","text":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n\n\n##Decision\n- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n"}
{"File Name":"CAFE5\/0002-calculate-prior-from-user-input.md","Context":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n","Decision":"The prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","tokens":40,"id":1747,"Prediction":"## Decision\n\nWe will use a **uniform prior distribution** for calculating probabilities. \n\n## Reasoning\n\n* **Simplicity and Transparency:** A uniform prior assigns equal probability to all possible values within a defined range. This approach is easy to understand and explain to users, minimizing potential confusion.\n* **Minimal Bias:** By assuming equal initial probabilities, we avoid introducing strong biases that might unfairly influence the posterior probabilities and subsequent decisions. This is crucial for providing users with results that align with their expectations.\n* **Objectivity:** In the absence of strong prior knowledge, a uniform prior offers a neutral starting point, allowing the data to speak for itself and shape the posterior distribution. This fosters trust in the system's ability to provide objective probability estimates.\n\n## Consequences\n\n* **Sensitivity to Data:** With a uniform prior, the calculated probabilities will be heavily influenced by the observed data. If the dataset is small or unrepresentative, the results might be less reliable.\n* **Potential for Revision:**  As more data becomes available, we can revisit this decision and consider incorporating informative priors based on domain expertise or user feedback to refine the probability calculations.\n\n## Alternatives Considered\n\n* **Informative priors:** While informative priors can leverage existing knowledge and potentially lead to more accurate estimates, they require careful selection and justification. Without sufficient information or domain expertise, specifying an informative prior could introduce unintended biases or misrepresent the true underlying probabilities. \n\n## Status\n\nAccepted\n\n## Notes\n\nThis decision will be reviewed and potentially revised as we gather more data and gain a better understanding of user expectations and typical data patterns. We will monitor the system's performance and user feedback to identify any issues related to the chosen prior distribution. \n","GenTime":"2024-08-19 15:51:42","text":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n\n\n##Decision\nThe prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n"}
{"File Name":"where-away\/0004-test-with-jest.md","Context":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n","Decision":"Use Jest for testing.\\n","tokens":31,"id":2273,"Prediction":"## Decision\n\nWe will use Jest as our primary JavaScript testing framework. \n","GenTime":"2024-08-19 15:51:42","text":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n\n\n##Decision\nUse Jest for testing.\\n"}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"Prediction":"## Decision\n\nWe will adopt Kafkarator as our Kafka client library to facilitate the migration to Aiven's hosted Kafka solution. \n\nThis decision is driven by the following factors:\n\n* **Simplification of integration:** Kafkarator abstracts away much of the complexity of interacting with Kafka, allowing developers to focus on application logic rather than low-level Kafka configurations. This reduces manual effort and promotes code-based configuration.\n* **Cloud migration strategy:** Choosing a cloud-agnostic library like Kafkarator aligns with the strategic direction of moving all infrastructure to the cloud. This provides flexibility in case of future cloud provider changes.\n* **Reduced operational overhead:** Utilizing Aiven's hosted Kafka eliminates the need for on-premise Kafka cluster maintenance. This frees up resources and allows the team to focus on core business value. \n\nThis decision will be re-evaluated if:\n\n* Kafkarator proves unsuitable for handling the scale and specific requirements of our applications.\n* Significant drawbacks or limitations arise from using Kafkarator with Aiven's hosted Kafka solution. \n* A more compelling alternative solution emerges that better addresses the needs of the project and organization. \n","GenTime":"2024-08-19 15:51:48","text":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n\n##Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n"}
{"File Name":"aspan-server\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":1893,"Prediction":"## Decision\n\nWe will **not** utilize import\/export syntax for modularizing GraphQL SDL definitions at this time. \n\nWhile this feature offers potential benefits in terms of code organization and reusability, several factors weigh against its adoption:\n\n* **Limited Tooling and Ecosystem Support:**  Many popular GraphQL tools and libraries do not fully support or have limitations when working with modularized SDL. This could lead to tooling issues, reduced functionality, and potential conflicts.\n* **Increased Complexity for Simple Use Cases:** For projects with relatively small schemas, introducing module imports\/exports may add unnecessary complexity without providing significant benefits.\n* **Potential Performance Overhead:** Depending on the implementation and tooling, resolving imports at build time or runtime could introduce a performance overhead.\n\n**Alternative Solution:**\n\nInstead of relying on language-level modules, we will adopt the following approach for managing our GraphQL SDL:\n\n* **Schema Stitching\/Federation:** For larger projects or microservices architecture, we will consider utilizing schema stitching or federation techniques. This allows combining multiple independent GraphQL schemas into a unified graph, offering flexibility and scalability.\n* **Logical Organization within Codebase:** We will organize our SDL files and types logically within our codebase. This could involve grouping related types together in files or directories based on domain concepts or functionality. \n* **Clear Naming Conventions:** Employing descriptive and consistent naming conventions for types, fields, and other elements will further improve code readability and maintainability.\n\n**Future Considerations:**\n\nWe will re-evaluate the use of GraphQL modules as the ecosystem matures, tooling improves, and if our project's scale and complexity warrant the need for this level of modularity.  \n","GenTime":"2024-08-19 15:51:55","text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"Prediction":"## Decision\n\nWe will transition from using static IAM user credentials to IAM roles and instance profiles for all AWS API access from our deployment tooling and Cloud Foundry components running on EC2 instances. This approach eliminates the need to manage and store long-lived access keys, significantly reducing the risk of accidental credential leakage.\n\nThis decision aligns with AWS best practices for securing API access and minimizes our exposure to security vulnerabilities associated with static credentials. \n","GenTime":"2024-08-19 15:51:57","text":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n\n\n##Decision\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n"}
{"File Name":"marsha\/0002-videos-languages.md","Context":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"Prediction":"## Decision\n\n**Accessibility First Approach for Video Content**\n\nTo ensure Marsha is accessible from the outset, we will adopt an \"Accessibility First\" approach for video content. This means considering accessibility requirements throughout the design and development process.\n\n**Specific Decisions:**\n\n1. **Multi-Track Audio:** The video player will support multiple audio tracks. This allows content creators to provide:\n    * The original audio track\n    * Audio descriptions for visually impaired users\n    * Audio in different languages\n2. **Comprehensive Timed Text Support:**  The video player will support multiple timed text tracks, including:\n    * Captions for the deaf and hard of hearing\n    * Subtitles in different languages\n    * Transcripts for searchability and accessibility\n3. **Integrated Sign Language Videos:**  We will provide two options for sign language interpretation:\n    * **Option 1: Picture-in-Picture:** Offer a secondary, smaller video window within the main video player displaying the sign language interpretation. This provides a more integrated viewing experience.\n    * **Option 2: Separate Video Track:** Offer sign language interpretations as separate video tracks selectable within the player, similar to audio tracks. This allows for flexibility and user preference. \n4. **Multi-Lingual Sign Language Support:** The platform will accommodate multiple sign language videos for a single video, catering to different spoken languages and regional variations in sign language.\n\n**Rationale:**\n\n* Prioritizing accessibility from the beginning ensures a more inclusive platform for all users.\n* Supporting multiple audio and timed text tracks caters to a wider audience, including those with disabilities, language differences, and learning preferences.\n* Providing both integrated and separate options for sign language videos allows for user flexibility and preference.\n\n**Consequences:**\n\n* Increased development effort and resources required to implement and maintain accessibility features.\n* Potential need for larger storage capacity to accommodate multiple audio, timed text, and sign language video files.\n* The user interface of the video player needs careful design to accommodate multiple tracks and options without overwhelming users.\n\nThis decision aligns with our commitment to creating an inclusive and accessible platform for all users.\n","GenTime":"2024-08-19 15:52:06","text":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n\n\n##Decision\nWe decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n"}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"Prediction":"## Decision: \n\n**Do not officially support or provide binary packages for google-cloud-cpp.**\n\nThis means:\n\n* The project will not build or distribute binary packages (e.g., dpkg, rpm, vcpkg) directly.\n* The project will not officially test binary distributions of the code.\n* The project will not host configuration files for building binary packages.\n\nHowever:\n\n* The project will strive for code that is easily packageable by third parties.\n* The project will consider reasonable requests and contributions to improve the experience of building binary packages by third parties. \n\nThis decision avoids endorsing specific package managers while enabling community contributions for binary distribution. \n","GenTime":"2024-08-19 15:52:09","text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n"}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"Prediction":"## Decision\n\nWe will use Rollup to bundle the library into multiple formats:\n\n* **CommonJS (CJS):** For use in Node.js and other CJS environments.\n* **ES Modules (ESM):** For use in modern browsers and Node.js versions supporting ESM.\n* **UMD (Universal Module Definition):** For compatibility with both CJS and AMD environments.\n\nThis approach will ensure the library can be consumed by a wide range of JavaScript and TypeScript applications.\n\n**Rationale:**\n\n* Rollup's tree-shaking capabilities will help keep the bundle size small by eliminating unused code.\n* Rollup's plugin ecosystem allows for easy customization and integration with other tools.\n* The multiple output formats maximize compatibility with different JavaScript environments.\n\n**Consequences:**\n\n* Requires adding Rollup as a development dependency.\n* Configuration files will need to be created and maintained for Rollup.\n* Developers will need to be familiar with Rollup's configuration and usage.\n\n**Alternatives Considered:**\n\n* **Webpack:** Another popular bundler, but might be overkill for this specific use case.\n* **No bundling:** This would require consumers to set up their own bundling pipeline.\n\n**Status:** Accepted\n\n**Date:** 2023-10-26\n","GenTime":"2024-08-19 15:52:15","text":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\n\n##Decision\nWe will build distributables using Rollup.js.\\n"}
{"File Name":"paas-csls-splunk-broker\/ADR007-paas-taking-ownership.md","Context":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n","Decision":"The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","tokens":94,"id":3291,"Prediction":"## Decision\n\nWe will migrate the service broker out of the `alphagov\/tech-ops` repository and into its own dedicated repository. This will provide the following benefits:\n\n* **Improved code isolation and maintainability:** Separating the broker code from the larger Tech Ops codebase will improve code clarity and simplify maintenance. This will also allow for independent versioning and releases of the broker.\n* **Enhanced security posture:**  A dedicated repository allows for more granular control over access and permissions, aligning better with the principle of least privilege, especially important given Cyber Security's involvement.\n* **Clearer ownership and accountability:**  A dedicated repository clarifies ownership of the broker code and infrastructure, promoting better accountability for its development and maintenance.\n* **Improved developer experience:**  A dedicated repository offers a cleaner and more focused development environment for engineers working on the broker, improving productivity and reducing cognitive overhead.\n\nWe will also migrate the build and deployment pipeline from the Tech Ops multi-tenant Concourse to a dedicated Concourse pipeline within the new repository. This further reinforces the separation of concerns and simplifies pipeline management. \n\nThis decision aligns with our overall strategy of modularizing our infrastructure and applications for improved agility, security, and maintainability. \n","GenTime":"2024-08-19 15:52:21","text":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n\n##Decision\nThe GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"Prediction":"## Decision\n\nWe will utilize a dedicated JSON serialization library instead of implementing custom serialization\/deserialization logic. This decision is driven by the following factors:\n\n* **Reduced Development Time:**  Leveraging an established library significantly reduces the time and effort required for implementing, testing, and maintaining serialization\/deserialization code. \n* **Improved Code Maintainability:**  Using a library leads to cleaner and more understandable code, simplifying future maintenance and extension.\n* **Reduced Risk of Errors:** Established libraries are typically well-tested and robust, minimizing the risk of introducing serialization\/deserialization bugs.\n\n**Library Selection:**\n\nThe specific library choice will be based on the project's programming language and ecosystem. Popular options include:\n\n* **JavaScript:**  `JSON.stringify()` and `JSON.parse()` (built-in) or libraries like `gson` or `jackson`.\n* **Java:** `Jackson`, `Gson`, or `JSON-P`.\n* **Python:**  The `json` module (built-in).\n\n**Implementation Details:**\n\n* We will define data classes or structures representing the workspace elements.\n* The chosen JSON library will be used to serialize and deserialize these data structures to and from JSON strings.\n* Error handling will be implemented to gracefully manage potential serialization\/deserialization issues. \n\n**Consequences:**\n\n* **Dependency:** Introducing a new dependency on the chosen JSON library.\n* **Learning Curve:**  A minimal learning curve may be associated with understanding the library's API.\n\nThis decision favors development efficiency, code quality, and reduced risk by leveraging the benefits of a dedicated JSON library.\n","GenTime":"2024-08-19 15:52:28","text":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n\n\n##Decision\nIn order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n"}
{"File Name":"celestia-core\/adr-060-go-api-stability.md","Context":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n","Decision":"From Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","tokens":336,"id":4112,"Prediction":"## Decision\n\nWe will adopt the following rules for our Go API to ensure backwards compatibility:\n\n**1. Public API Definition:**\n\n* Any **exported** identifier (types, variables, constants, functions, methods) within a package listed in the `README` as public, is considered part of our public API. This includes code in `_test.go` files that can be accessed via the public API.\n* All other code, including **private** identifiers and code within the `internal\/` directory, is considered private API.\n\n**2. Backwards Compatibility Guarantees:**\n\n* We will follow semantic versioning for all public APIs.\n* **Breaking changes** to the public API will only be introduced in major releases (e.g., from 1.x.x to 2.0.0).\n* We **do not guarantee** backwards compatibility for private APIs. These are subject to change or removal at any time.\n\n**3. Public API Evolution:**\n\n* We will strive to add new functionality to the public API in a backwards-compatible way whenever possible. \n* If a breaking change is absolutely necessary, it will be handled according to semantic versioning with a major version bump.\n* Deprecated functionality will be marked as such in the documentation and may be removed in a future major release.\n\n**4. Documentation:**\n\n* Our `README` will clearly list all packages considered part of the public API.\n* All public APIs will be thoroughly documented with clear explanations of their behavior and usage.\n\n**5. External Projects:**\n\n* External projects should only depend on the public API defined in the `README` and documented accordingly.\n* We encourage external projects to vendor their dependencies on Tendermint to avoid unexpected breakage due to changes in our private APIs.\n\nBy adhering to these rules, we aim to provide a stable and predictable API surface for external projects while retaining the flexibility to evolve our codebase internally.\n","GenTime":"2024-08-19 15:52:36","text":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n\n\n##Decision\nFrom Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n"}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"Prediction":"## Decision\n\nWe will adopt a two-pronged approach to managing third-party dependencies:\n\n**1. Production Environments:**\n\n* Prioritize minimizing dependencies to improve availability and reduce potential points of failure. \n* Where feasible and cost-effective, consider caching data or replicating functionality of critical third-party services to reduce reliance.\n* Thoroughly evaluate the reliability, security, and performance implications of all third-party services used in production.\n\n**2. Build and Development Environments:**\n\n*  Leverage third-party components and services freely to accelerate development and reduce overhead.\n*  Focus on ease of use and integration rather than minimizing dependencies in these environments.\n*  Maintain clear documentation of all third-party dependencies and their purpose in the development process.\n\nThis approach balances the need for high availability and resilience in production with the benefits of leveraging external services for rapid development and iteration. \n","GenTime":"2024-08-19 15:52:40","text":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n\n\n##Decision\nDuring runtime, we will only depend on resources services from the Google Cloud Platform.\\n"}
{"File Name":"govuk-aws\/0038-mongo_replacement_by_documentdb.md","Context":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n","Decision":"The approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","tokens":145,"id":4047,"Prediction":"## Decision\n\nMigrate existing GOV.UK MongoDB clusters to AWS DocumentDB (with MongoDB compatibility). \n\n## Reasons\n\n* **Reduced operational burden:** Utilizing AWS DocumentDB, a fully managed service, aligns with the TechOps principle of leveraging AWS managed services. This reduces the operational overhead associated with managing MongoDB clusters, including patching, upgrades, backups, and scaling. \n* **Potential Cost Savings:**  While a detailed cost analysis is required, managed services like DocumentDB can potentially offer cost savings compared to self-managed MongoDB clusters, especially when considering operational expenses.\n* **Improved Performance and Scalability:** DocumentDB provides enhanced performance and scalability features, potentially benefiting applications with demanding performance requirements.\n* **MongoDB Compatibility:**  DocumentDB's compatibility with MongoDB APIs simplifies the migration process, minimizing code changes and reducing migration risks.\n\n## Consequences\n\n* **Migration Effort:** Migrating existing databases to DocumentDB will require planning, testing, and potential downtime.\n* **Feature Differences:** While DocumentDB is highly compatible, some MongoDB features may not be available or function identically, necessitating application code adjustments.\n* **Vendor Lock-in:**  Migrating to DocumentDB increases dependency on AWS services. \n\n## Action Items\n\n1. **Conduct a feasibility study:** Analyze the compatibility of DocumentDB with existing GOV.UK applications and data.\n2. **Perform a cost-benefit analysis:**  Compare the costs of self-managed MongoDB and DocumentDB, factoring in operational expenses and potential performance gains. \n3. **Develop a migration plan:** Define the migration strategy, timeline, and resources required for a phased migration approach.\n4. **Test and validate:**  Thoroughly test the migrated applications and data in a staging environment to ensure functionality and performance. \n","GenTime":"2024-08-19 15:52:47","text":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n\n\n##Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n"}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"Prediction":"## Decision\n\nWe will decouple the Tendermint ABCI application hash from the consensus state root. Instead of a one-to-one relationship, the application hash will be derived from multiple sub-roots, potentially including:\n\n* **Consensus state root:** This will remain for backward compatibility and to represent the overall state of the application after block execution.\n* **Event root:** A separate Merkle root will be constructed from all events emitted within a block. This will enable light clients to verify the inclusion of specific events without downloading the entire state.\n* **Tx execution results root:**  A root could be derived from the execution results of all transactions in a block. This would enable verification of specific transaction outcomes.\n\nThe specific sub-roots included, and the derivation logic for the final application hash will be configurable based on the needs of the application. \n\nThis approach offers flexibility and allows for future extensibility without breaking changes. For instance, new sub-roots could be added later to support additional use cases. \n","GenTime":"2024-08-19 15:52:52","text":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n\n\n##Decision\nThe proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"Prediction":"## Decision\n\nWe will use **Git** as our version control system and **GitHub** as our online platform for hosting our repository and collaborating with the wider community. \n\n","GenTime":"2024-08-19 15:52:53","text":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\n\n##Decision\nWe use git for version control and GitHub for collaboration.\\n"}
{"File Name":"adr-tools\/0005-help-comments.md","Context":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n","Decision":"Write usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","tokens":64,"id":3566,"Prediction":"## Decision\n\nWe will adopt a hybrid approach:\n\n1.  **In-code comments:** We will maintain concise usage documentation within the script files themselves using comments. This documentation will focus on providing essential information for developers and users who are already familiar with the command-line interface, such as:\n    *   A brief description of the script's purpose.\n    *   Basic usage examples for common scenarios.\n    *   Explanations of key arguments and options.\n\n2.  **`help` Subcommand:** The `help` subcommand will provide comprehensive documentation, encompassing and expanding upon the information available in the in-code comments. This will serve as the primary source of information for new users and those seeking detailed guidance. The `help` subcommand documentation will include:\n    *   Detailed explanations of all arguments and options, including default values, accepted ranges, and potential interactions.\n    *   Advanced usage examples and workflows.\n    *   Troubleshooting tips and common error messages.\n    *   Links to external resources, such as the project's GitHub repository or official documentation website.\n\nThis approach balances the need for readily accessible in-code documentation with the benefits of a dedicated `help` subcommand for comprehensive information and discoverability. \n","GenTime":"2024-08-19 15:52:59","text":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n\n\n##Decision\nWrite usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n"}
{"File Name":"connecting-to-services\/0003-use-postcodes-io-for-postcode-lookup.md","Context":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n","Decision":"[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","tokens":68,"id":2204,"Prediction":"## Decision\n\nWe will use a third-party geocoding API to resolve postcodes and outcodes to latitude\/longitude coordinates. This approach offers several advantages:\n\n* **Simplicity:**  It avoids the complexity of building and maintaining our own geocoding solution.\n* **Accuracy:** Reputable geocoding providers offer high accuracy and up-to-date data.\n* **Scalability:** Third-party APIs typically handle large volumes of requests, ensuring our application scales effectively.\n\nWe will evaluate and select a provider based on the following criteria:\n\n* **Accuracy and Coverage:**  Prioritize providers with comprehensive and accurate UK postcode data.\n* **Performance:**  Choose a provider with low latency and high availability to ensure a responsive user experience.\n* **Cost:**  Balance features and performance with budget constraints.\n* **Rate Limiting:** Understand the API request limits to avoid disruptions and plan for potential scaling needs.\n\n**Implementation Details:**\n\n* We will integrate the chosen geocoding API into our backend service.\n* A dedicated function will handle postcode\/outcode resolution, caching results to optimize performance and reduce API calls. \n* Error handling will be implemented to gracefully manage invalid input or API issues, providing informative messages to the user. \n\nThis approach allows us to focus on the core functionality of finding nearby services while leveraging a reliable and efficient solution for postcode resolution. \n","GenTime":"2024-08-19 15:53:05","text":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n\n\n##Decision\n[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n"}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"Prediction":"## Decision\n\nTo address the limitations of manual image building and deployment, we will implement a CI\/CD pipeline for Connaisseur. This pipeline will automate the build, test, sign, and push processes for Connaisseur images.\n\nThis decision offers several benefits:\n\n* **Increased Development Velocity:** Developers can focus on code contributions rather than manual deployment tasks. \n* **Improved Release Cadence:**  Automated builds and deployments enable more frequent and consistent releases.\n* **Enhanced Reliability:** Automated testing within the pipeline ensures the stability and quality of releases.\n* **Version Consistency:** Automated releases ensure that the published Connaisseur image always corresponds to the latest version referenced in the Helm chart.\n* **Scalability:** The CI\/CD pipeline can seamlessly scale with the growing number of team members and contributions.\n\nWe will investigate and select a suitable CI\/CD platform (e.g., GitHub Actions, GitLab CI, Jenkins) that integrates seamlessly with our existing infrastructure and workflows.  The pipeline will be configured to:\n\n1. **Trigger on Pull Requests and Merges:** Automate the build and testing processes for each code change.\n2. **Execute Automated Tests:** Ensure the quality and functionality of the software with every change.\n3. **Sign Images:**  Maintain the security and trustworthiness of Connaisseur images.\n4. **Push Images to Registry:** Automatically publish the built and signed images to a designated container registry.\n5. **Update Helm Chart:** Automatically update the Helm chart version to match the latest successful build.\n\nThis automated approach eliminates manual intervention, reduces errors, and allows for faster and more reliable software delivery. \n","GenTime":"2024-08-19 15:53:12","text":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n\n\n##Decision\nFor choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"Prediction":"## Decision\n\nTo address the issues of tight coupling, dependency pollution, and overly broad feature dependencies, we will refactor the OpenHAB build and runtime structure to achieve the following:\n\n**1. Strict API\/Implementation Separation:**\n\n* **Separate Modules:** Introduce new modules for internal implementations, clearly separating them from API modules. For example, `org.openhab.core.items.internal` will reside in a separate module from `org.openhab.core.items`.\n* **Build Tool Enforcement:** Utilize the build tool (e.g., Maven) to enforce strict dependency rules. API modules can only depend on other API modules, while internal modules can depend on both APIs and other internal modules.\n* **OSGi Metadata:** Leverage OSGi metadata (e.g., `Export-Package`) to explicitly control package visibility, ensuring that internal packages remain hidden at runtime.\n\n**2. Fine-grained Feature Dependencies:**\n\n* **Decoupled Features:**  Decompose monolithic features like `openhab-core-base` and `openhab-runtime-base` into smaller, more focused features. \n* **Targeted Dependencies:** Define feature dependencies based on actual functional requirements. For instance, an extension requiring only the REST API should depend solely on the relevant REST feature, not the entire core.\n\n**3. Restructured Modules and Bundles:**\n\n* **Modularization:** Analyze the existing codebase and decompose it into smaller, more cohesive modules based on functionality. This will further improve separation of concerns and reduce unnecessary dependencies.\n* **Bundle Optimization:**  Align bundle boundaries with the new module structure, ensuring each bundle contains a well-defined set of related functionalities.\n\n**Example:**\n\nThe `[openhab thing core]` and `[openhab rest core]` modules could be restructured as follows:\n\n```\n[openhab-api]             <--- [openhab-rest-api]\n       ^                         ^\n       |                         |\n[openhab-thing-core]     <--- [openhab-rest-core] \n       ^                         ^\n       |                         |\n[openhab-thing-internal] <--- [openhab-rest-internal]\n```\n\nThis structure ensures that:\n\n* `openhab-rest-api` only exposes public API interfaces.\n* `openhab-rest-core` implements the API and can depend on both API and internal modules.\n* Consumers of the REST API only depend on `openhab-rest-api`, avoiding exposure to internal implementations.\n\nThis approach fosters a cleaner architecture, reduces coupling, and improves maintainability by:\n\n* **Encapsulation:** Hiding implementation details and preventing unintended dependencies.\n* **Modularity:** Promoting code reuse and independent development of components.\n* **Flexibility:** Enabling easier extension and customization through well-defined APIs. \n","GenTime":"2024-08-19 15:53:24","text":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n\n\n##Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n"}
{"File Name":"drt-v2\/0006-physical-deployment-one-jvm-per-port.md","Context":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n","Decision":"Use a jvm per airport\\n","tokens":33,"id":1912,"Prediction":"## Decision\n\nDRTv2 will move away from a single normalized port model. Instead, we will adopt a type-based approach where ports are categorized into different types, each with its own specific schema and supported features. This will allow us to:\n\n* **Accommodate port-specific requirements:**  Each port type can have its unique attributes and relationships, catering to specific data structures and functionalities.\n* **Improve data accuracy and consistency:** By enforcing type-specific schemas, we ensure data integrity and reduce ambiguity.\n* **Simplify feature development:**  Adding new features or modifying existing ones becomes more straightforward as changes are localized to specific port types. \n\nWe will define a base set of common port types based on existing data and use cases. New types can be introduced as needed, and the system will be designed to be extensible for future requirements. \n","GenTime":"2024-08-19 15:53:28","text":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n\n\n##Decision\nUse a jvm per airport\\n"}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"Prediction":"## Decision\n\nWe will implement a distributed locking mechanism using Apache ZooKeeper to prevent concurrent table creation in Cassandra. Before creating a table, each Cassandra node will attempt to acquire a lock associated with the table name in ZooKeeper. \n\n* If a node successfully acquires the lock, it proceeds with table creation. \n* If another node attempts to create a table with the same name, it will be blocked until the lock is released. \n* Once the table creation is complete, the lock is released, allowing other nodes to create tables.\n\nThis approach ensures that only one node can create a table with a given name at any time, preventing the data loss scenario described.\n","GenTime":"2024-08-19 15:53:31","text":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n\n\n##Decision\nAll schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n"}
{"File Name":"saas-platform-frontend\/0006-use-antdesign-as-the-ui-framework.md","Context":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n","Decision":"We use Ant Design as the UI Framework.\\n","tokens":28,"id":3718,"Prediction":"## Decision\n\nWe will use **React** as our UI framework. \n\n## Reasons\n\n* **Large and active community**: React boasts a massive and active developer community. This translates to readily available resources, comprehensive documentation, numerous third-party libraries, and strong community support for troubleshooting and knowledge sharing. \n* **Component-based architecture**: React's component-based approach promotes reusability, maintainability, and a streamlined development process. \n* **Performance**: React leverages a virtual DOM and efficient rendering algorithms, resulting in performant and responsive user interfaces.\n* **Tooling and ecosystem**:  React offers excellent tooling with Create React App for easy project setup and features a rich ecosystem of libraries and tools addressing various needs, including state management (Redux, Zustand), routing (React Router), and testing.\n* **Industry adoption**: React is widely adopted by leading tech companies, which further strengthens its credibility, ensures long-term support, and provides access to a large talent pool. \n\n## Considered Alternatives\n\n* **Vue.js**:  While offering simplicity and ease of learning, Vue.js has a smaller community compared to React, potentially limiting readily available resources and support.\n* **Angular**: Although Angular provides a comprehensive framework, its steeper learning curve and opinionated structure might slow down initial development speed. \n\n## Consequences\n\n* **Learning curve**:  While React offers numerous advantages, developers unfamiliar with the framework will require dedicated time for learning its concepts and best practices. \n* **Potential for library fatigue**: The vast React ecosystem, while beneficial, can lead to decision fatigue when choosing among numerous libraries for similar functionalities.\n\nThis decision aligns with our need for a UI framework that offers both rich functionality and a strong community, ultimately leading to faster UI development and a more robust application in the long run. \n","GenTime":"2024-08-19 15:53:39","text":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n\n\n##Decision\nWe use Ant Design as the UI Framework.\\n"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"Prediction":"## Decision\n\n**Option 2: Creating a separate table for periodic tasks.**\n\n## Reasons\n\nWhile adding fields to the existing `job` table (Option 1) might seem simpler, it introduces unnecessary complexity and potential performance bottlenecks:\n\n* **Schema clarity:** Separating periodic tasks into their own table improves database schema organization and clarity. It logically separates immediate tasks from those with scheduled execution times. \n* **Query efficiency:** Searching and filtering on periodic tasks becomes more efficient with a dedicated table. This is crucial for identifying due tasks without impacting the performance of querying immediate tasks.\n* **Data integrity:** A separate table allows for specific constraints and data types relevant to periodic tasks (e.g., execution interval, last run timestamp). This enhances data integrity and validation.\n* **Future extensibility:**  A separate table provides flexibility for future features specific to periodic tasks, such as user-defined schedules, different recurrence patterns, or advanced scheduling options.\n\n## Consequences\n\n* **Increased complexity:**  Requires creating and managing an additional table, potentially with relationships to other tables.\n* **Migration effort:**  If an existing system needs migration, moving data and adapting existing queries will be necessary.\n\nHowever, these consequences are outweighed by the benefits listed above, especially considering the long-term maintainability, scalability, and potential for future expansion. \n","GenTime":"2024-08-19 15:53:45","text":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n\n\n##Decision\nThe 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n"}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"Prediction":"## Decision\n\nWe will implement a more granular breakdown of tasks and steps within the software's workflow. This will involve decomposing larger tasks into smaller, more manageable sub-tasks. \n","GenTime":"2024-08-19 15:53:46","text":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n\n\n##Decision\n* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n"}
{"File Name":"openfido-app-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2472,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file living in the \/docs\/adrs directory of our project's repository using a consistent naming convention like ADR-0001-Decision-Title.md. \n\nWe choose ADRs because they provide a transparent and accessible way to document architectural thinking, facilitate communication within the team and with stakeholders, and enable us to understand the rationale behind our architectural choices over time.\n","GenTime":"2024-08-19 15:53:49","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"FindMeFoodTrucks\/Messaging choice.md","Context":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n","Decision":"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","tokens":20,"id":1091,"Prediction":"## :dart: Decision\n\nWe will implement all Web APIs asynchronously. \n\n","GenTime":"2024-08-19 15:53:50","text":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n\n\n##Decision\nConsidering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"Prediction":"## Decision\n\nWe will use **Infrastructure as Code (IaC)** to provision servers and software for the datalabs project.  This approach offers several advantages over manual configuration or using a GUI-based tool:\n\n* **Reproducibility:** IaC allows us to define our infrastructure in code, which makes it easy to reproduce the same environment on demand. This is essential for ensuring consistency across different environments (development, testing, production) and for disaster recovery.\n* **Automation:**  IaC enables us to automate the provisioning process, reducing the risk of human error and freeing up our team to focus on higher-level tasks.\n* **Version Control:**  Storing our infrastructure code in a version control system allows us to track changes over time, revert to previous versions if necessary, and collaborate more effectively. \n* **Documentation:** IaC code serves as documentation of our infrastructure, making it easier for new team members to understand how things are set up.\n\nWe will evaluate and choose a specific IaC tool (e.g., Terraform, Pulumi, AWS CloudFormation, Azure Resource Manager) based on factors such as:\n\n* **Supported platforms:**  The tool should support the cloud providers and services we plan to use.\n* **Ease of use:** The tool should have a clear syntax and be easy to learn and use.\n* **Community support:** A strong community can provide valuable resources and support.\n* **Cost:**  Some tools may have licensing costs associated with them.\n\nThis decision will be reviewed and potentially revised as the datalabs project evolves and our needs change. \n","GenTime":"2024-08-19 15:53:57","text":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\n\n##Decision\nWe will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n"}
{"File Name":"govuk-aws\/0008-postgres-on-puppetmaster.md","Context":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n","Decision":"The Puppetmaster will continue to use a local instance of PostgreSQL.\\n","tokens":110,"id":4061,"Prediction":"## Decision\n\nWe will migrate the Puppetmaster's database to an RDS instance instead of hosting PostgreSQL on the Puppetmaster itself. \n\n## Reasons\n\n* **Reduced operational overhead:** Utilizing RDS eliminates the need for us to manage PostgreSQL directly, including tasks like installation, configuration, patching, backups, and scaling. This frees up our team to focus on higher-level tasks related to Puppet itself.\n* **Improved reliability and scalability:** RDS offers built-in redundancy, backups, and monitoring, enhancing the reliability of our Puppetmaster setup.  It also provides easy scalability options should our needs grow.\n* **Integration with AWS ecosystem:** Choosing RDS aligns with our strategy of leveraging AWS services and benefits from seamless integration with other tools and services within the AWS ecosystem.\n\n## Consequences\n\n* **Migration effort:** Migrating the existing database to RDS requires dedicated time and effort for tasks like data transfer, configuration changes, and testing.\n* **Potential vendor lock-in:** While RDS offers significant advantages, it increases our reliance on AWS services, potentially limiting future flexibility if we need to switch cloud providers.\n* **Cost implications:** While RDS removes the overhead of managing our own database, it introduces a recurring cost for the service. We need to carefully evaluate the cost-benefit analysis of using RDS. \n\n## Action Items\n\n1. **Cost analysis:** Compare the costs of self-hosting PostgreSQL vs. using RDS, considering factors like instance size, storage, and backup options.\n2. **Migration plan:** Develop a detailed migration plan outlining the steps involved in moving the database to RDS, including data transfer, configuration, and testing.\n3. **Terraform configuration:** Create the necessary Terraform code to provision the RDS instance and configure it for use with the Puppetmaster.\n4. **Puppet code updates:** Update the Puppetmaster's configuration to connect to the RDS instance as its database backend.\n5. **Documentation:** Document the new architecture, including the RDS configuration and any changes made to the Puppetmaster. \n","GenTime":"2024-08-19 15:54:05","text":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n\n\n##Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\\n"}
{"File Name":"saas-platform-frontend\/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md","Context":"## Context\\nI need to decided with which tools I build my react app.\\n","Decision":"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","tokens":17,"id":3721,"Prediction":"## Decision\n\nWe will use **Vite** as our build tool for the React application. \n\nThis decision is based on the following factors:\n\n* **Performance:** Vite is known for its exceptional speed, particularly during development due to its use of native ES modules and on-demand serving. This will lead to faster development cycles and a more productive development experience.\n* **Simplicity:** Vite offers a simpler configuration and setup compared to alternatives like Create React App. Its intuitive API and minimal configuration overhead will allow us to focus on building the application logic rather than wrestling with tooling.\n* **Modern Features:** Vite provides built-in support for modern web development features like Hot Module Replacement (HMR), TypeScript, JSX, and CSS Modules out of the box. This ensures a smooth development experience with the latest technologies.\n* **Growing Ecosystem:**  While relatively new, Vite boasts a rapidly expanding ecosystem of plugins and integrations, providing flexibility and extensibility for future needs.\n\nWhile Create React App (CRA) is a popular and well-established choice, we believe Vite's advantages in performance and a modern development experience make it a more suitable option for our needs. \n","GenTime":"2024-08-19 15:54:11","text":"## Context\\nI need to decided with which tools I build my react app.\\n\n\n##Decision\nI build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n"}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"Prediction":"## Decision:  We will utilize the pre-configured PHP-Apache Docker image to host the PHP web application. \n\n**Justification:** This approach leverages existing resources and minimizes setup complexity due to the readily available Docker image bundling PHP and Apache. This aligns with the team's current expertise and allows for rapid deployment. \n\n**Alternatives Considered:** \n\n* **Other web servers (Nginx, Caddy):**  While potentially more performant, exploring and configuring these options would require additional time and expertise. \n* **Manual Apache and PHP installation:** This offers greater customization but introduces complexity and potential for configuration errors. \n\n**Rationale for Chosen Decision:** Given the project constraints, utilizing the pre-built PHP-Apache Docker image presents the most pragmatic approach, optimizing for development speed and ease of deployment. \n\n**Important Notes:**  While no immediate drawbacks are foreseen, it's essential to acknowledge that long-term scalability and performance needs might necessitate revisiting this decision in the future.  We will monitor the application's performance and re-evaluate if necessary. \n","GenTime":"2024-08-19 15:54:16","text":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n\n\n##Decision\n### Consequences: No forseen consequences or drawbacks in using apache.\\n"}
{"File Name":"celestia-core\/adr-005-decouple-blockid-and-partsetheader.md","Context":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n","Decision":"While we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","tokens":132,"id":4152,"Prediction":"## Decision\n\nWe will deprecate the use of the `PartSetHeader` in the `BlockID` for committing to the block data. This change will be implemented in the following phases:\n\n**Phase 1: Deprecation in New Components**\n\n* All new components and functionalities will not use the `PartSetHeader` in the `BlockID` for data commitment.\n* The `DataHash` will be the sole source of truth for verifying the integrity of the block data.\n\n**Phase 2: Gradual Removal from Existing Components**\n\n*  We will identify and analyze the existing components that currently rely on the `PartSetHeader` in the `BlockID`.\n*  For each component, we will evaluate the effort required to remove the dependency and the potential impact on other parts of the system.\n* We will prioritize the removal based on the ease of implementation and the criticality of the component. \n\n**Phase 3: Complete Removal**\n\n* Once the dependency on the `PartSetHeader` in the `BlockID` has been removed from all relevant components, we will completely remove the field from the `BlockID` struct.\n\n**Maintaining `PartSetHeader` in `Proposal`**\n\n* We will keep the `PartSetHeader` in the `Proposal` struct for the time being to support block propagation during consensus. \n* The use of `PartSetHeader` in `Proposal` will be revisited and potentially phased out when a more efficient block propagation mechanism is implemented.\n\nThis phased approach allows us to gradually remove the dependency on the `PartSetHeader` in the `BlockID` while minimizing disruption to existing functionalities.\n","GenTime":"2024-08-19 15:54:23","text":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n\n\n##Decision\nWhile we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n"}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"Prediction":"## Decision\n\n**(Replace this with a concise description of the decision that was made.)** \n\nFor example: \n\n* \"We will use PostgreSQL as our primary database.\"\n* \"We will adopt a microservices architecture based on RESTful APIs.\"\n* \"We will utilize Kubernetes for container orchestration.\" \n","GenTime":"2024-08-19 15:54:25","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"dogma\/0012-identifier-comparison.md","Context":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n","Decision":"In keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","tokens":109,"id":1618,"Prediction":"## Decision\n\nIdentifiers will be compared using a byte-wise comparison of their UTF-8 representation. This means two identifiers are considered equal if and only if their UTF-8 representations are identical.\n\n## Consequences\n\n* **Performance:** Byte-wise comparison is very efficient and straightforward to implement.\n* **Security:** No normalization is performed, preventing potential security vulnerabilities related to Unicode normalization attacks.\n* **Simplicity:** This approach is easy to understand and reason about, aligning with the immutability of the identifiers.\n* **Interoperability:**  UTF-8 is a widely supported encoding, minimizing potential interoperability issues across different systems.\n\n**Justification:**\n\nGiven the constraints of the context:\n\n* **Immutability and code generation:** The fact that identifiers are mostly immutable and generated in the source code reduces the need for complex normalization procedures often required for user-provided data.\n* **Performance sensitivity:** Comparing identifiers is likely a frequent operation. A byte-wise comparison is the most performant approach.\n* **Security:** Avoiding normalization prevents vulnerabilities related to Unicode attacks.\n\nTherefore, byte-wise comparison of UTF-8 representation provides the best trade-off between performance, simplicity, and security. \n\n**Alternatives considered:**\n\n* **Case-insensitive comparison:** This was rejected as it could lead to ambiguity and unwanted matches. \n* **Unicode normalization:**  While this could offer greater flexibility in handling different representations of the same character, it comes with performance costs and potential security implications that are not justified in this context.\n\nThis decision will be reviewed if the requirements around identifier generation or comparison change significantly in the future. \n","GenTime":"2024-08-19 15:54:32","text":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n\n\n##Decision\nIn keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n"}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"Prediction":"## Decision\n\nWe will develop a generic parser for the `*_ALL.DBF|*_ALL.SHP` files that leverages the `RECORDTYPE` column to identify changes. This parser will handle the non-unique primary key situation by considering all records with the same primary key but different `RECORDTYPE` values as part of the same change.\n\nTo address the specific complexity of the `WEGSEGMENT_ALL.DBF` file, we will implement special logic within the generic parser. This logic will prioritize the `EVENTIDN` column over the `WS_OIDN` column as the primary key when `RECORDTYPE` is `ADDED` and `EVENTIDN` is not `0`. This ensures correct identification and referencing of road segments in this specific scenario. \n\nThis approach allows us to maintain a single parser for all `*_ALL.DBF|*_ALL.SHP` files while accommodating the unique characteristics of the `WEGSEGMENT_ALL.DBF` file. This promotes code maintainability and reduces complexity. \n","GenTime":"2024-08-19 15:54:37","text":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n\n\n##Decision\nModifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n"}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"Prediction":"## Decision\n\nWe will utilize Azure Active Directory (Azure AD) as the identity management system for the frontend application, providing single sign-on (SSO) capabilities for both backstage and SEPA users. This decision leverages Azure's integration with Microsoft Dynamics 365 and capitalizes on SEPA's existing Active Directory infrastructure.\n","GenTime":"2024-08-19 15:54:39","text":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n\n##Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n"}
{"File Name":"pomi-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2487,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file living in the `\/docs\/adr` directory of our repository, following a standardized template (e.g., MADR or lightweight Markdown). \n","GenTime":"2024-08-19 15:54:41","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"claim-additional-payments-for-teaching\/0003-use-rubocop-for-linting.md","Context":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n","Decision":"We will use RuboCop for linting our Ruby code\\n","tokens":24,"id":2101,"Prediction":"## Decision\n\nWe will use RuboCop for linting our Ruby code. \n","GenTime":"2024-08-19 15:54:42","text":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n\n\n##Decision\nWe will use RuboCop for linting our Ruby code\\n"}
{"File Name":"opg-metrics\/0002-aws-infrastructure.md","Context":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n","Decision":"To run in a fully managed AWS cloud environment using Terraform to manage it.\\n","tokens":31,"id":1545,"Prediction":"## Decision\n\nWe will utilize a serverless architecture based on AWS Lambda for the following reasons:\n\n* **Lightweight:** Lambda functions are inherently lightweight, as they only consume resources during execution. This aligns with the requirement for a lightweight system.\n* **Fully Managed:** AWS Lambda is a fully managed service, abstracting away infrastructure management concerns like provisioning, scaling, and patching. This fulfills the \"fully managed\" requirement.\n* **Easy Integration:** Lambda integrates seamlessly with other AWS services, providing a rich ecosystem for building complex systems. Additionally, Lambda supports various programming languages and event triggers, enabling smooth integration with existing tools and workflows. \n\nThis approach allows us to focus on developing business logic without the overhead of managing servers, resulting in faster development cycles and reduced operational complexity. \n","GenTime":"2024-08-19 15:54:46","text":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n\n\n##Decision\nTo run in a fully managed AWS cloud environment using Terraform to manage it.\\n"}
{"File Name":"mediawiki-extensions-Wikispeech\/0001-store-synthesized-audio-in-mediawiki-extension.md","Context":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n","Decision":"Files are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","tokens":488,"id":3351,"Prediction":"## Decision\n\nThe synthesized audio will be stored in a dedicated storage service, decoupled from the Speechoid service. This storage service will be configurable, allowing for different backends like Swift (for Wikimedia) or other storage solutions as needed. \n\nHere's a breakdown of how this addresses the identified issues:\n\n* **Storage Agnosticism:** The configurable nature of the storage service allows for flexibility in choosing the backend, catering to both Wikimedia's Swift requirements and potential future needs.\n* **Stateless Speechoid:**  By delegating storage responsibility, Speechoid remains stateless and focused solely on audio synthesis.\n* **Metadata Preservation:**  The storage service will store utterance metadata along with the audio. This allows for serving the audio directly from the cache without needing to re-synthesize if the text hasn't changed, even if the metadata (e.g., voice) is requested.\n* **RevisionDelete Compliance:**  No text content will be stored in the storage service. Instead, a unique, non-reversible identifier will be used to reference the audio. This identifier will be generated by the service responsible for content storage (e.g., MediaWiki) and passed to Speechoid along with the text to be synthesized. This ensures that the storage service remains oblivious to the actual text content.\n* **Cache Management:**  The storage service will implement a mechanism for cache expiry and invalidation. This mechanism can be configured to support different policies:\n    * **Time-based expiry:** Automatically remove audio after a predefined period.\n    * **Targeted invalidation:**  Allow for explicit removal of audio associated with a specific identifier when a text segment is modified.\n    * **Lexicon-based invalidation:** Allow for invalidation of audio based on changes in the phonetic lexicon, triggering re-synthesis with updated pronunciation.\n\n* **Utterance Reusability:** Utterances will be stored and indexed independently of specific pages or revisions, maximizing reusability across the platform. The storage service will utilize the aforementioned unique identifiers to retrieve the audio, regardless of the context.\n\nThis approach provides a flexible, scalable, and privacy-conscious solution for storing and serving synthesized audio. It adheres to Wikimedia's requirements while remaining adaptable for other use cases. \n","GenTime":"2024-08-19 15:54:55","text":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n\n\n##Decision\nFiles are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n"}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"Prediction":"## Decision\n-------\n**To ensure traceability and compatibility as event transformers evolve, we will implement a versioning system for both xAPI and Caliper events.** This system adheres to the following principles:\n\n1. **Versioning:** Each event will explicitly state the version of the transformer used in its generation.\n2. **Format:** The version will follow the pattern \"transformer-name@semantic-version\", e.g., \"event-routing-backends@v1.2.3\".\n3. **Location:** \n    * For xAPI: The version will be nested within the statement's context extensions under the key \"https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version\".\n    * For Caliper: The version will be directly under the \"extensions\" object with the key \"transformerVersion\".\n4. **Semantic Versioning:** We will adhere to semantic versioning (SemVer) for the transformer version (X.Y.Z):\n    * **Major (X):** Incremented for breaking changes like spec updates, key removals\/renames, or value modifications in the transformer.\n    * **Minor (Y):** Incremented for backwards-compatible additions like new keys, events, or transformations.\n    * **Patch (Z):** Incremented for bug fixes and non-behavior-altering changes.\n5. **Changelogs:** Detailed changelogs will be meticulously maintained for both xAPI and Caliper transformers, documenting all modifications.\n\nThis approach provides a clear, consistent, and informative way to track transformer versions and understand the potential impact of changes on event consumers. \n","GenTime":"2024-08-19 15:55:02","text":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n\n##Decision\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n"}
{"File Name":"runner\/0276-problem-matchers.md","Context":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n","Decision":"### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","tokens":227,"id":3734,"Prediction":"## Decision\n\nWe will adopt VSCode's problem matcher concept for Setup Actions. Each Setup Action can optionally register problem matchers which will be active for the duration of the job. \n\nThis approach provides several benefits:\n\n* **Improved Developer Experience:** By surfacing granular error messages within the UI, developers can more easily diagnose and fix build failures.\n* **Familiarity and Reusability:** Leveraging the existing VSCode problem matcher ecosystem allows us to avoid reinventing the wheel and benefits from community contributions.\n* **Flexibility:**  The extensible nature of problem matchers enables support for various compilers, linters, and other tools.\n\nWe will implement this by:\n\n1. **Extending Setup Actions:**  Add a mechanism for Setup Actions to register problem matchers.\n2. **Problem Matcher Management:**  Implement functionality to activate and deactivate problem matchers based on registered Setup Actions.\n3. **Integration with CI Output:** Update the CI build output parsing logic to leverage active problem matchers for enhanced error reporting. \n","GenTime":"2024-08-19 15:55:07","text":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n\n\n##Decision\n### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n"}
{"File Name":"beis-report-official-development-assistance\/0022-use-govuk-paas-for-hosting.md","Context":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n","Decision":"Host all environments excpet local development on BEIS own GOVUK PaaS account.\\n","tokens":44,"id":2410,"Prediction":"## Decision\n\nThe BEIS service will be hosted on GOV.UK PaaS. \n","GenTime":"2024-08-19 15:55:08","text":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n\n\n##Decision\nHost all environments excpet local development on BEIS own GOVUK PaaS account.\\n"}
{"File Name":"site-stitcher\/001-generate-static-pages.md","Context":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n","Decision":"With static pages the above can be achieved. Easily.\\n","tokens":222,"id":1931,"Prediction":"## Decision\n\n**Embrace minimalism and prioritize core web technologies to create a fast, accessible, and lightweight website with minimal dependencies.**\n\nThis decision entails:\n\n* **Prioritizing HTML, CSS, and vanilla JavaScript:** We will strive to utilize core web technologies whenever possible, minimizing reliance on external libraries and frameworks. This will result in a smaller footprint, faster loading times, and improved accessibility.\n* **Rigorously evaluating dependencies:** Each potential dependency will undergo a thorough evaluation to determine its necessity and impact on performance. We will prioritize minimal, purpose-built libraries over large, feature-rich ones. If a small amount of custom code can replace a dependency, that option will be favored. \n* **Optimizing for performance:**  Website performance will be a top priority, ensuring fast loading times, smooth interactions, and a low Time to Interactive. This will include optimizing images, minimizing HTTP requests, and utilizing efficient JavaScript code.\n* **Ensuring accessibility:** We will adhere to web accessibility guidelines (WCAG) to guarantee the website is usable by everyone, regardless of ability.\n* **Leveraging static site generation and GitHub Pages:** We will employ static site generation for its speed and security benefits. Utilizing GitHub Pages as a hosting platform provides a simple, free, and reliable solution.\n\nThis approach aligns with the project goals of simplicity, performance, accessibility, and minimal infrastructure. It promotes a deeper understanding of web technologies and avoids the bloat often associated with complex JavaScript tooling. \n","GenTime":"2024-08-19 15:55:14","text":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n\n\n##Decision\nWith static pages the above can be achieved. Easily.\\n"}
{"File Name":"digital-paper-edit-client\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":4167,"Prediction":"## Decision\n**Utilize a modular, versioned schema approach:**\n\n1. **Define separate SQL schema files for each milestone.**  For example, `v1_transcript_editing.sql`, `v2_transcript_annotations.sql`, and `v3_paper_editing.sql`. \n2. **Each schema file builds upon the previous one.**  Include necessary `CREATE TABLE` statements for the new milestone, along with any ALTER TABLE statements to adjust existing tables.\n3. **Use a database migration tool.** Consider tools like Flyway or Liquibase to manage applying these schema changes incrementally in a controlled and versioned manner for both PostgreSQL and SQLite.\n4. **Leverage DB Designer for visual design and SQL generation.** Export the SQL for each milestone version to the respective schema file.\n\nThis approach offers several benefits:\n\n* **Clear versioning:**  Easily track database changes aligned with project milestones.\n* **Incremental updates:**  Apply only the changes needed for each milestone, simplifying rollbacks if required.\n* **Reusability:** Adapt the core schema across PostgreSQL (web app) and SQLite (desktop) by applying relevant versioned changes.\n* **Maintainability:** Modular schema files enhance readability and understanding.\n","GenTime":"2024-08-19 15:55:20","text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n"}
{"File Name":"ReportMI-service-manual\/0006-use-terraform-to-create-and-document-infrastructure.md","Context":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n","Decision":"We will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","tokens":208,"id":2062,"Prediction":"## Decision\n\nWe will use **Terraform** to manage the infrastructure for the Data Submission Service. \n\n## Reasons\n\n* **Cloud Agnostic:** Terraform's cloud-agnostic nature allows us to define our infrastructure once and deploy it across multiple cloud providers. This avoids vendor lock-in and offers flexibility in choosing or switching providers in the future.\n* **Infrastructure as Code:** Terraform embraces the Infrastructure as Code (IaC) principle. This means our infrastructure is defined in code and version-controlled, enabling better tracking, reproducibility, and collaboration among developers.\n* **Simplified Management:**  Terraform simplifies infrastructure management by handling the differences between cloud providers. This abstraction reduces the complexity and effort required to manage infrastructure across different environments. \n* **Improved Efficiency:** Automating infrastructure provisioning with Terraform saves time and reduces the risk of manual errors compared to manual scripting or using cloud-specific solutions.\n\n## Alternatives Considered\n\n* **Cloud-Specific Solutions (e.g., CloudFormation, Resource Manager):** While these offer tight integration with their respective cloud platforms, they lead to vendor lock-in and require re-implementation when switching providers.\n* **Manual Scripting:**  Although possible, it's time-consuming, error-prone, and difficult to maintain across multiple cloud providers.\n\n## Consequences\n\n* **Learning Curve:**  Teams will need to invest time in learning Terraform, but the long-term benefits outweigh the initial learning curve.\n* **Tooling Dependency:**  We become reliant on Terraform for our infrastructure management. However, Terraform is a widely adopted and actively maintained tool, mitigating this risk.\n\nThis decision aligns with our goals of maintaining a flexible, maintainable, and cloud-agnostic infrastructure for the Data Submission Service. \n","GenTime":"2024-08-19 15:55:28","text":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n\n\n##Decision\nWe will use Terraform to create the infrastructure for the Data Submission\\nService.\\n"}
{"File Name":"CAFE5\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1751,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions throughout the project lifecycle. Each ADR will capture the context, decision, status, and consequences of a particular architectural choice. We will utilize a lightweight Markdown-based template stored within the project repository for easy access and version control.\n","GenTime":"2024-08-19 15:55:30","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"opg-data\/0005-content-structure.md","Context":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n","Decision":"Our structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","tokens":21,"id":2181,"Prediction":"## Decision\n\nWe will use the OpenAPI Specification (OAS) version 3.0.x to define our API contract. \n\n## Reasons\n\n* **Industry Standard:** OAS is a widely adopted and recognized standard for describing RESTful APIs. This ensures better interoperability with various tools and technologies.\n* **Tooling and Automation:**  OAS has a rich ecosystem of tools for documentation generation, code generation (both server and client), testing, and validation. This promotes efficiency and consistency throughout the development lifecycle.\n* **Clear Contract:** OAS provides a clear, human-readable, and machine-readable contract that accurately describes the API endpoints, request\/response structures, data types, and other relevant details.\n* **Version Control:** OAS definitions can be version-controlled, allowing us to track changes, manage compatibility, and evolve the API contract over time.\n* **Community Support:** OAS has a large and active community, providing access to resources, support, and best practices.\n\n## Consequences\n\n* **Learning Curve:** Adopting OAS may require some initial learning and training for team members unfamiliar with the specification.\n* **Maintenance Overhead:** While OAS offers automation benefits, keeping the API specification in sync with code changes requires discipline and potentially dedicated tooling.\n\n## Status\n\nAccepted\n\n## Notes\n\nWe will consider using a dedicated API design and management tool to assist with the creation, editing, validation, and documentation generation from our OAS definition. \n","GenTime":"2024-08-19 15:55:36","text":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n\n\n##Decision\nOur structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n"}
{"File Name":"otm-docs\/api_framework.md","Context":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"Prediction":"## Decision\n\nWe will adopt **[Framework Name]** as our PHP framework for this project.\n\n**Reasoning:**\n\nWhile pure PHP offers flexibility, a framework provides pre-built components and functionalities, streamlining development and ensuring best practices. [Framework Name] was chosen for the following reasons:\n\n* **[Reason 1]:**  (e.g., Large community support, extensive documentation)\n* **[Reason 2]:**  (e.g., Suitable for [project type] development, like API or web app)\n* **[Reason 3]:**  (e.g., Performance benefits, security features, specific library availability)\n\n**Alternatives Considered:**\n\n* **[Alternative Framework 1]:**  Briefly mention why it was considered and why it was rejected (e.g., Steep learning curve, not ideal for [project type])\n* **[Alternative Framework 2]:**  Briefly mention why it was considered and why it was rejected (e.g., Smaller community, lacks desired features)\n\n**Consequences:**\n\n* **Positive:** Faster development, maintainability, code reusability, scalability.\n* **Negative:** Potential learning curve for developers unfamiliar with the chosen framework. Mitigation: Allocate time for framework familiarization.\n\nThis decision will be revisited if [Conditions for revisiting the decision], such as encountering performance issues or if project requirements drastically change. \n","GenTime":"2024-08-19 15:55:42","text":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n\n\n##Decision\nLaravel 7\\n"}
{"File Name":"clone_difitalcitizenship\/0002-use-openapi-to-defined-the-api-specs.md","Context":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n","Decision":"We use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","tokens":20,"id":1180,"Prediction":"## Decision\n\nWe will use OpenAPI Specification (OAS, formerly known as Swagger) to define the API specifications for our services. \n\n## Status\n\nAccepted\n\n## Consequences\n\n**Pros:**\n\n* **Standardization:** OAS is a widely adopted industry standard for defining RESTful APIs, ensuring consistency and interoperability.\n* **Tooling & Ecosystem:** A rich ecosystem of tools exists for OAS, including documentation generators (e.g., Swagger UI), client SDK generators, and API mocking tools.\n* **Early Validation & Feedback:** Defining APIs upfront with OAS allows for early validation of design choices and facilitates communication among developers and stakeholders. \n* **Code Generation:**  OAS specifications can be used to automatically generate server stubs and client SDKs, reducing boilerplate code and potential errors. \n* **Simplified Documentation:**  OAS specifications act as living documentation that stays in sync with API changes.\n\n**Cons:**\n\n* **Learning Curve:**  While OAS is relatively straightforward to learn, there is an initial learning curve for developers unfamiliar with the specification. \n* **Verbosity:** OAS specifications can be verbose, especially for complex APIs. However, the benefits often outweigh this drawback.\n\n## Alternatives Considered:\n\n* **API Blueprint:** Another popular API specification language, but with a smaller community and tooling ecosystem compared to OAS.\n* **RAML:**  (RESTful API Modeling Language) offers strong features for API design but might be overly complex for simpler APIs.\n* **No formal specification:** Defining APIs informally can lead to inconsistencies, miscommunication, and difficulties in maintaining API documentation. \n\n## Decision Drivers:\n\n* **Industry Standard & Tooling:** The widespread adoption of OAS and its mature tooling ecosystem were key factors in this decision.\n* **Focus on RESTful APIs:** Our current focus on RESTful APIs aligns well with OAS's strengths.\n* **Need for clear documentation and collaboration:** OAS promotes clear API documentation and facilitates communication among team members.\n\n## Considered Options:\n\nWe considered using other API specification languages, but ultimately decided on OAS due to its strong industry adoption, tooling, and suitability for RESTful APIs. \n","GenTime":"2024-08-19 15:55:51","text":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n\n##Decision\nWe use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n"}
{"File Name":"easyvista_toolkit\/0001_Using_globalVariable.md","Context":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n","Decision":"We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","tokens":79,"id":2329,"Prediction":"## Decision\n\nWe will implement a caching mechanism within the cmdlet to store and retrieve friendly names for GUIDs used by the EasyVista REST API. \n\n## Reasons\n\n* **Improved User Experience:** Autocompletion with friendly names significantly enhances the usability of the cmdlets compared to requiring users to look up and input GUIDs.\n* **Environment Variable Limitations:**  Given the reported limitations of environment variables with hashtables, directly using them is not a feasible solution. \n* **Caching for Performance:**  Caching the friendly names avoids repeated API calls for the same GUIDs, improving the performance of the autocompletion feature.\n\n## Details\n\n1. **Caching Strategy:**  Implement a simple in-memory cache within the cmdlet's scope. This cache will store a mapping of GUIDs to their friendly names. \n2. **Cache Population:**\n    * When the cmdlet is invoked and requires autocompletion for a specific parameter, it will first check the cache. \n    * If the GUID is not found in the cache, the cmdlet will make a request to the EasyVista API to retrieve the friendly name.\n    * The retrieved friendly name and corresponding GUID will be stored in the cache.\n3. **Cache Invalidation (Optional):** Depending on the frequency of changes to these mappings in EasyVista, consider implementing a cache invalidation strategy (e.g., time-based expiry).\n\n## Consequences\n\n* **Increased Cmdlet Complexity:** The cmdlet code will become slightly more complex to handle the caching logic.\n* **Potential for Stale Data (if using cache invalidation):** If cache invalidation is implemented, there's a chance of displaying outdated friendly names if the mappings change in EasyVista. This trade-off is in favor of performance. \n","GenTime":"2024-08-19 15:55:58","text":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n\n\n##Decision\nWe will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n"}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"Prediction":"## Decision\n\nWe will develop automated tests for the SAML compliance tool. \n\nThese tests will serve three primary purposes:\n\n1. **Internal Compliance Validation:** Regularly test our own implementation against the compliance tool, ensuring alignment with the SAML profile. This acts as a form of continuous integration for the Verify platform itself.\n2. **Exemplar for Relying Parties:** The tests will act as publicly available examples, demonstrating to relying parties how to effectively test their integrations against the compliance tool.  \n3. **User Experience Insights:** Developing and maintaining these tests will provide invaluable insights into the user experience of the compliance tool. This will expose areas for improvement in terms of usability, clarity, and functionality. \n\nThis approach allows us to improve the onboarding experience for relying parties, increase confidence in the compliance tool's efficacy, and proactively maintain alignment with the SAML profile. \n","GenTime":"2024-08-19 15:56:03","text":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n\n\n##Decision\nWe'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n"}
{"File Name":"bookit-api\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":3217,"Prediction":"## Decision\n\nWe will use Okta as our Identity as a Service (IDaaS) provider for authentication and authorization. We will leverage the Microsoft social OpenConnect ID provider for user authentication, effectively using Okta as a proxy for Microsoft identity management.\n\nThis decision is based on the following factors:\n\n* **Cost-Effectiveness:** Okta's free tier accommodates our anticipated user base.\n* **Simplified Integration:**  Okta seamlessly integrates with our Spring Boot application through existing libraries and provides a React client SDK, reducing development effort. \n* **Enhanced Security:** Okta offers a robust and secure platform for managing identities.\n* **Scalability & Flexibility:** Okta allows us to easily add support for additional social logins, Active Directory integration, and user role\/group management in the future. \n\nThe successful spike demonstrated the feasibility of integrating Okta within our technology stack. \n","GenTime":"2024-08-19 15:56:07","text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n"}
{"File Name":"uniprot-rest-api\/0003-spring-framework.md","Context":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n","Decision":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","tokens":35,"id":1477,"Prediction":"## Decision\n\nWe will adopt a programming framework for this project. \n\n**Reasoning:**\n\nGiven the potential benefits outlined in the context, namely increased productivity, reduced codebase size, enhanced reliability, and access to pre-built features, leveraging a framework aligns with our goals of efficient and robust software development. \n\n**Further Considerations:**\n\nThis decision necessitates further investigation to determine the **specific framework** that best aligns with our project requirements. This selection process will consider factors such as:\n\n* **Project scope and complexity:** Different frameworks are suited for different project scales and architectures.\n* **Language compatibility:**  The chosen framework must align with our chosen programming language(s).\n* **Learning curve and team expertise:** We need to assess the team's familiarity with different frameworks to minimize onboarding time.\n* **Community support and documentation:** A vibrant community and comprehensive documentation are crucial for troubleshooting and long-term project maintainability.\n* **Performance and scalability:**  The framework should support the desired performance and scalability goals of the application.\n* **Licensing and costs:**  We need to consider any licensing restrictions or associated costs with the framework.\n\n A thorough evaluation of these factors will guide us towards selecting the most appropriate framework for this project. \n","GenTime":"2024-08-19 15:56:13","text":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n\n\n##Decision\nWe have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n"}
{"File Name":"beis-report-official-development-assistance\/0025-use-terraform-to-set-up-and-deploy-to-the-paas.md","Context":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n","Decision":"- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","tokens":107,"id":2409,"Prediction":"## Decision\n\nWe will use Terraform to manage our Cloud Foundry organization, spaces, and application deployments. We will transition from our current shell script-based approach to leverage the benefits of Terraform's declarative syntax, infrastructure-as-code capabilities, and integration with the Cloud Foundry provider. \n","GenTime":"2024-08-19 15:56:14","text":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n\n\n##Decision\n- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"Prediction":"## Decision\n\n**Adopt a layered approach to security in the Cosmos SDK that combines both object-capability principles and explicit permission management.**\n\n**Rationale:**\n\nWhile a pure object-capability model offers strong security guarantees, the current state of the Cosmos SDK and the need for a thriving module ecosystem necessitate a more pragmatic approach. A layered approach allows us to improve security incrementally while providing a clear path towards a more robust system in the future.\n\n**This approach consists of:**\n\n**1. Short-Term (v1.0 stabilization):**\n\n* **Stabilize Module Interfaces:** Prioritize stabilizing core module interfaces (like `x\/bank`) for v1.0 release. This will provide a stable foundation for module developers.\n* **Introduce Explicit Module Permissions:** Expand upon the existing module permission system to provide more granular control over actions like minting, burning, and balance manipulation.\n* **Enforce Permissions at Keeper Level:** Implement checks within keeper methods to enforce these permissions, preventing unauthorized access even when directly calling the keeper.\n* **Documentation and Best Practices:** Clearly document the permission system, security implications, and best practices for module developers to encourage secure module development.\n\n**2. Long-Term (Post v1.0):**\n\n* **Explore Object Capabilities:**  Investigate integrating more robust object-capability concepts into the SDK, potentially leveraging capabilities for inter-module communication and access control. This could involve defining specific capability tokens for actions instead of relying on module names.\n* **Refactor Core Modules:** Gradually refactor core modules to align with object-capability principles where applicable, improving encapsulation and minimizing direct access to sensitive functionalities.\n\n**Consequences:**\n\n* **Improved Security:** The layered approach enhances security compared to the current state by introducing stricter access control and reducing the impact of faulty or malicious modules. \n* **Ecosystem Growth:** Stabilizing the SDK and providing clear security guidelines fosters a more robust and trustworthy module ecosystem.\n* **Incremental Adoption:** This approach allows for incremental implementation and adoption of more advanced security concepts like object-capabilities, minimizing disruption for existing modules and developers.\n\n**This approach provides a balanced solution by addressing immediate security needs while paving the way for a more secure and robust Cosmos SDK in the future.** \n","GenTime":"2024-08-19 15:56:24","text":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n\n\n##Decision\nBased on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n"}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"Prediction":"## Decision\n\nWe will use `SqlStreamStore` for storing our events. \n\n## Reasoning\n\nWhile `GetEventStore` is likely a robust solution, the operational overhead of self-hosting and the uncertainty regarding storing business-critical data outside of `MSSQL` in `AIV` pose significant concerns. \n\n`SqlStreamStore` offers a compelling alternative by:\n\n* **Leveraging existing infrastructure:** It integrates directly with `MSSQL`, eliminating the need for additional infrastructure and aligning with our current data storage practices. \n* **Providing essential features:** It supports catch-up subscriptions, a crucial requirement for our async event handlers.\n* **Offering community support:** Its open-source nature and active community provide a safety net for potential issues and reduce reliance on a single vendor.\n* **Reducing operational burden:** By utilizing existing infrastructure and leveraging community support, we minimize the operational overhead associated with managing a separate event store.\n\nThis approach allows us to adopt event sourcing with catch-up subscriptions without introducing significant operational complexity or deviating from established data management practices within `AIV`. \n","GenTime":"2024-08-19 15:56:29","text":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n\n\n##Decision\nWe will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences. Each decision will be documented in a separate Markdown file following a consistent template. \n","GenTime":"2024-08-19 15:56:30","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"nhsuk-prototype-kit-version-one\/0003-use-npm-scripts-and-gulp-for-running-tasks.md","Context":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n","Decision":"We will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","tokens":49,"id":3682,"Prediction":"## Decision\n\nWe will implement a task runner to automate the process of setting up and running the prototype kit. This task runner will be responsible for:\n\n* **Managing dependencies:** Installing, updating, and removing project dependencies from a central location.\n* **File scaffolding:** Automating the process of moving files from dependencies into the appropriate locations within the app file structure. \n* **Running the application:**  Providing a simple command to build and launch the application, including any necessary pre-processing or compilation steps. \n\nThis approach will simplify the setup process for new developers, reduce the potential for errors, and ensure consistency across different development environments.\n","GenTime":"2024-08-19 15:56:33","text":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n\n##Decision\nWe will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"Prediction":"## Decision\n\nWe will adopt **NestJS** as the application framework for all new FxA backend services and for the migration of the four newest existing services (admin-server, support-panel, event-broker, gql-api). \n\n## Justification\n\nNestJS offers a compelling solution that directly addresses the identified problems and fulfills the desired decision drivers:\n\n* **Documented application architecture:** NestJS enforces a clear and well-documented architecture based on established patterns like MVC (Model-View-Controller), promoting consistency and maintainability. This makes it easier for new developers to understand the structure and contribute effectively. \n* **Tooling and reduced boilerplate:**  NestJS provides a powerful CLI and tooling that automates common tasks, reducing boilerplate code and ensuring consistent code style. This allows developers to focus on business logic rather than repetitive setup. \n* **Modern paradigms and DI:** NestJS embraces Dependency Injection as a core principle, simplifying object creation, management, and testability. This directly addresses the challenges of plumbing objects through initialization chains.\n* **Training and onboarding:** The well-defined structure and abundant learning resources available for NestJS (documentation, tutorials, community support) will significantly reduce the learning curve for new developers.\n* **Migration path for legacy applications:** While not a primary focus initially, NestJS's modular design allows for incremental adoption. Components from legacy applications can be gradually migrated and integrated into the new architecture over time.\n* **Comprehensive feature set:**  NestJS provides built-in support or readily available modules for:\n    * **Exception handling:** Streamlined mechanisms for handling errors gracefully.\n    * **Validation:**  Integration with validation libraries for data integrity.\n    * **Service instantiation (DI):**  A cornerstone of the framework.\n    * **Authentication:**  Facilitates implementation of various authentication schemes.\n    * **RESTful APIs (Swagger\/OpenAPI):**  Built-in support and easy integration with Swagger\/OpenAPI for API documentation.\n    * **GraphQL:**  First-class support for building GraphQL APIs.\n    * **Use of components\/services in scripts:** The modular and DI-driven nature allows for flexible component usage in various contexts, including scripts.\n\n**Alternatives Considered:**\n\n* **Continuing with the ad-hoc approach:**  This is not a viable option as it perpetuates the existing problems and incurs significant technical debt.\n* **Other Node.js frameworks (Express.js, Koa.js):**  While these frameworks provide a foundation, they often lack the opinionated structure and comprehensive features of NestJS, requiring more manual configuration and decision-making. \n\n**Consequences:**\n\n* **Positive:**  Improved code maintainability, reduced onboarding time, enhanced developer productivity, and a more robust and scalable backend architecture.\n* **Negative:**  Initial investment in learning and migrating to NestJS. However, this is significantly outweighed by the long-term benefits and reduced technical debt.\n\nThis decision establishes a clear direction for the evolution of FxA's backend services, promoting a consistent, maintainable, and scalable architecture that supports the growth and evolution of the platform. \n","GenTime":"2024-08-19 15:56:47","text":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n\n\n##Decision\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n"}
{"File Name":"fundraising-application\/003_Validation.md","Context":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n","Decision":"For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","tokens":225,"id":1532,"Prediction":"## Decision\n\nWe decided to **write our own validation logic** and implement it **within the domain layer**. \n\nThis decision is based on the following rationale:\n\n* **Domain Ownership:**  Validation rules are inherently tied to the business rules of our domain. Placing validation within the domain layer ensures that these rules are encapsulated and enforced consistently, regardless of how the data is presented or manipulated by external layers. \n* **Flexibility:** Writing our own validation logic provides greater control and flexibility in defining and implementing custom validation rules specific to our application's needs.  This is particularly relevant given the anticipated simplicity of most validation tasks.\n* **Avoiding External Dependencies:** While using an established library like Symfony Validation has its advantages, we prioritized minimizing external dependencies to reduce potential maintenance overhead and compatibility issues in the long run.\n* **Simplicity and Transparency:** Given the anticipated straightforward nature of our validation requirements, a lightweight, custom implementation will be easier for all developers to understand, maintain, and adapt as needed, without requiring them to learn a complex external library.\n\n**Implementation Details:**\n\n* Validation logic will be implemented within domain objects themselves or as separate classes within the domain layer, depending on the complexity and context of the validation rules.\n* We will follow a consistent approach to validation across the application to ensure maintainability and readability. This may involve defining custom exceptions or utilizing a simple, lightweight validation result object.\n\n**Future Considerations:**\n\n* As the application grows and validation requirements become more complex, we may re-evaluate the need for an external validation library. However, we believe that the benefits of a domain-centric approach to validation outweigh the initial overhead of a custom implementation. \n","GenTime":"2024-08-19 15:56:54","text":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n\n##Decision\nFor each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n"}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will document the context and consequences of a specific decision, providing a rationale and a clear audit trail. \n","GenTime":"2024-08-19 15:56:55","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"Prediction":"## Decision\n\nWe will adopt a standardized key derivation scheme for all applications interacting with the Oasis Network. This scheme will be based on well-established standards within the blockchain ecosystem, such as BIP-32 (Hierarchical Deterministic Wallets) and BIP-39 (Mnemonic code for entropy). \n\nThis decision entails:\n\n1. **Defining a specific derivation path** according to BIP-44  to ensure interoperability with existing wallets and tools. This path will be unique to the Oasis Network.\n2. **Supporting both 12 and 24-word mnemonics** for seed generation, allowing users to choose their preferred level of security and convenience.\n3. **Updating existing applications and SDKs** to implement this standardized key derivation scheme. This includes providing clear documentation and examples for developers. \n4. **Encouraging hardware wallet manufacturers** to add support for the Oasis Network using the standardized derivation path.\n\nThis decision does not preclude the future adoption of additional key management schemes, such as multi-signature wallets or threshold signatures. \n","GenTime":"2024-08-19 15:57:00","text":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n\n\n##Decision\n### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n"}
{"File Name":"tech-radar\/adr-4-structure_of_principle.md","Context":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","Decision":"We propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","tokens":382,"id":3925,"Prediction":"## Decision\n\nWe will adopt the proposed structure for principle artefacts, including:\n\n* **Context:** Providing background and rationale for the pattern.\n* **Business Strategy Link:**  Where possible, include URIs linking to specific business strategy bullet points that the principle supports.\n* **Problems:** Outlining the challenges the pattern addresses.\n* **Rationale:**  Explaining the reasoning behind the pattern choice. \n* **Examples:**  Illustrating the pattern's application. \n* **Discussion Points:** Highlighting areas requiring further consideration or debate.\n* **Case by Case Topics:** Including specific instructions or contact information for unique scenarios, like legacy API integration. \n\nThis structure will be mandatory for all new architectural patterns and a target for refactoring existing ones. \n","GenTime":"2024-08-19 15:57:04","text":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n\n\n##Decision\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"Prediction":"## Decision\n\nGiven the constraints of RDS and the limitations of PostgreSQL's permission model, we will implement a dedicated schema per application within a shared RDS instance. \n\nThis decision is based on the following:\n\n* **Isolation:** This approach provides the necessary isolation between applications sharing the same database instance. Each application will have its own schema, preventing conflicts arising from table alterations, deletions, or schema customizations. \n* **Security:** By limiting access to specific schemas, we enhance security by preventing unintended or malicious actions from one application affecting another.\n* **Migration Compatibility:** With dedicated schemas, each application can independently manage its migrations without interfering with other applications' database objects.\n* **Maintainability:**  This approach offers a cleaner and more manageable solution by logically separating application data within the database.\n\n**Implementation Details:**\n\n1. **Schema Creation:** The RDS broker will create a dedicated schema for each application upon binding to the database instance.\n2. **User Permissions:** The broker will grant the application user full access (read, write, alter, drop) to their designated schema only.\n3. **Application Modification:** Applications must be modified to work within their designated schemas. This includes prefixing table names with the schema name in queries and ensuring all database operations target the correct schema.\n\nThis decision avoids overly complex workarounds and leverages PostgreSQL's built-in schema system to provide a robust and scalable solution for multi-tenant database management. \n","GenTime":"2024-08-19 15:57:10","text":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n\n##Decision\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n"}
{"File Name":"james\/0015-objectstorage-blobid-list.md","Context":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2122,"Prediction":"## Decision\n\nGiven the performance limitations of ObjectStorage's `exist` method and the cost associated with redundant writes, we will implement a **probabilistic approach** to detect and prevent duplicate blob writes, combined with asynchronous cleanup of duplicates. \n\nThis will involve:\n\n1. **Generating a deterministic hash (e.g., MD5, SHA-256) of the blob content.**  This hash will serve as a unique identifier for the blob, regardless of its source or intended recipients.\n2. **Storing the blob hash in a fast, read-optimized data store (e.g., Redis, in-memory cache).** This will allow for quick lookups to determine if a blob with the same hash already exists. \n3. **Before writing a blob to ObjectStorage, check if its hash exists in the read-optimized store.**\n    * If the hash exists, skip writing the blob and use the existing blob's reference. \n    * If the hash does not exist, write the blob to ObjectStorage and store the hash in the read-optimized store.\n4. **Implementing an asynchronous background process to identify and remove duplicate blobs in ObjectStorage.** This process can utilize the hash information stored in the read-optimized store to detect duplicates and safely remove redundant data. \n\n**Reasons for this decision:**\n\n* **Performance:** Probabilistic approaches using hashes offer a good balance between accuracy and performance. While not completely foolproof, they significantly reduce the number of duplicate writes and minimize the need for expensive `exist` checks.\n* **Cost Optimization:** Reducing duplicate writes directly translates to cost savings on storage and API calls. \n* **Asynchronous Cleanup:** Addressing potential hash collisions and ensuring eventual consistency in the background minimizes the impact on critical write operations. \n\n**Consequences:**\n\n* **Slight possibility of remaining duplicates:** Hash collisions might lead to rare cases of duplicates not being detected. However, the asynchronous cleanup process will eventually address these.\n* **Increased complexity:** Implementing the hashing, read-optimized store, and background cleanup process adds complexity to the system.\n* **Additional storage costs:** Utilizing a read-optimized store introduces minor storage overhead. \n\nThis approach offers a pragmatic solution that optimizes for performance and cost-effectiveness while acknowledging the limitations of ObjectStorage's `exist` method. \n","GenTime":"2024-08-19 15:57:20","text":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n\n\n##Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"Prediction":"## Decision\n\nThe components of the PlanQK platform **will be split** into individual repositories. \n\n","GenTime":"2024-08-19 15:57:22","text":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n\n\n##Decision\nStart with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n"}
{"File Name":"react-template\/0014-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":3328,"Prediction":"## Decision\n\nWe will use Dependabot to automatically keep dependencies up-to-date.  Dependabot will be configured to open Pull Requests (PRs) against the main development branch whenever new versions of dependencies are released. This will allow us to review the changes, assess compatibility, and merge updates in a controlled and timely manner. \n","GenTime":"2024-08-19 15:57:24","text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n"}
{"File Name":"titania-os\/0005-networkmanager-for-networking.md","Context":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n","Decision":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","tokens":69,"id":2574,"Prediction":"## Decision\n\nWe will use **NetworkManager** to manage network connections.\n\n## Justification\n\n* **Feature Richness:** NetworkManager provides a comprehensive set of features, exceeding those offered by SystemD's native tools. These features are important for the flexibility and control we require over our network configuration. \n* **DBus Interface:** The DBus interface offered by NetworkManager enables easy integration with our application, allowing us to dynamically control and query the network state. This is crucial for our use case.\n* **Industry Standard:** NetworkManager is widely adopted and considered an industry standard, particularly in Linux environments. This widespread use ensures good documentation, community support, and long-term stability.\n\nWhile SystemD's native tools provide a basic level of functionality, they lack the advanced features offered by NetworkManager. Wicd, while being another alternative, sees limited use in embedded systems, making it a less attractive option. \n\n## Consequences\n\n* **Increased Complexity:** Utilizing NetworkManager introduces an additional component and its dependencies into our system, potentially increasing the overall complexity.\n* **Resource Consumption:** As a dedicated service, NetworkManager has a resource footprint (CPU, memory). This impact should be minimal on modern systems but needs to be considered.\n\nDespite these potential drawbacks, the benefits offered by NetworkManager in terms of functionality, integration, and industry adoption outweigh the associated costs for our application.\n","GenTime":"2024-08-19 15:57:30","text":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n\n\n##Decision\nWe will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n"}
{"File Name":"super-eks\/0007-backup.md","Context":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n","Decision":"### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","tokens":41,"id":2034,"Prediction":"## Decision\n\n**Implement a comprehensive backup solution for the Super EKS cluster to ensure data protection and disaster recovery capabilities for resources not managed by Infrastructure as Code (IaC).** \n","GenTime":"2024-08-19 15:57:31","text":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n\n\n##Decision\n### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n"}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"Prediction":"## Decision\n\n**Option: Same database with a discriminator column**\n\n**Reasoning:**\n\nGiven the constraints and requirements, using a single database with a discriminator column offers the best balance of simplicity, security, and maintainability. \n\n* **Data Isolation:** Adding an \"organization_id\" column to every table ensures strict data separation at the database level.  This effectively prevents accidental data leakage between organizations, addressing constraint #1 and concern #1 directly.\n* **Metadata Management:** A single database simplifies metadata management (concern #2). Common metadata can be stored once and overridden by individual organizations as needed, reducing duplication and maintenance overhead. \n* **Performance & Scalability:** Since data per organization is expected to be low (concern #3), a single database is likely sufficient for performance and can be scaled vertically if needed.  \n* **Reduced Complexity:** Compared to managing multiple databases or schemas, a discriminator column approach is simpler to implement and maintain.\n\n**Consequences:**\n\n* **Database Design:** Requires careful database design to include the discriminator column in all relevant tables and enforce its use in queries. \n* **Query Complexity:** Queries might become slightly more complex due to the need to include the discriminator column. However, this can be mitigated with well-defined data access layers and ORMs.\n* **Future Scalability:** While sufficient for the current needs, horizontal scaling with very high data volumes and demanding performance requirements might necessitate revisiting this decision in the future.\n\n**Alternatives Considered:**\n\n* **Multiple databases:** Provides strong isolation but introduces significant complexity in management and data aggregation.\n* **Multiple schemas:** Offers a middle ground but still adds complexity compared to a single database with a discriminator column.\n\n**Monitoring:**\n\n* Regularly review database performance and query efficiency to identify any potential bottlenecks caused by the discriminator column. \n* Implement robust security audits and code reviews to ensure continued data isolation between organizations. \n","GenTime":"2024-08-19 15:57:39","text":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n\n\n##Decision\n- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n"}
{"File Name":"slingshot\/0003-use-one-geoserver-instance-with-multiple-workspaces.md","Context":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n","Decision":"Run a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","tokens":200,"id":2578,"Prediction":"## Decision\n\nWe will implement a proxy service that sits in front of a single GeoServer instance. This proxy will handle authentication and authorization for GeoServer requests. \n\nHere's how it will work:\n\n* **Publicly accessible layers:** Requests for these layers will pass through the proxy without any authentication required.\n* **Protected layers:**  Requests for protected layers will require the user to be authenticated with the proxy. \n* **Authentication:**  The proxy will use an authentication mechanism that integrates well with our existing user management system and supports API token-based authentication (e.g., JWT).  The Rails application will handle user login and provide valid tokens.\n* **Authorization:** The proxy will be configured with rules to determine which users or roles have access to specific protected layers.\n\n**Justification:**\n\n* **Single GeoServer instance:** This simplifies deployment, management, and data publication.\n* **Centralized authentication and authorization:** This provides a single point of control for all GeoServer access.\n* **Flexibility:** Using a proxy allows us to easily switch authentication mechanisms or integrate with other systems in the future.\n* **Improved Security:** We can leverage more robust authentication mechanisms than what is offered by GeoServer directly.\n\n**Alternatives Considered:**\n\n* **GeoServer's built-in authentication:** This was deemed too complex and inflexible for our needs.\n* **Developing a custom authentication plugin for GeoServer:** This would be a significant development effort and introduce a dependency on a specific GeoServer version.\n\n**Consequences:**\n\n* **Development effort:**  We will need to develop and maintain the proxy service.\n* **Single point of failure:** The proxy service becomes a critical component for accessing GeoServer. We will need to implement appropriate redundancy and monitoring. \n","GenTime":"2024-08-19 15:57:47","text":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n\n\n##Decision\nRun a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n"}
{"File Name":"adr-viewer\/0005-distinguish-amendments-to-records-with-colour.md","Context":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n","Decision":"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","tokens":33,"id":3813,"Prediction":"## Decision\n\nWe will allow Architecture Decision Records (ADRs) to be amended instead of superseded under the following circumstances:\n\n* **Minor Changes:** Only a small, non-critical part of the decision is being modified, such as updating a detail, adding a clarification, or reflecting a minor technology change.\n* **Continuity of Context:** The original decision's context and reasoning remain largely relevant and valuable for understanding the evolution of the architecture.\n* **Clarity and Traceability:** Amending the ADR, instead of creating a new one, provides clearer traceability and understanding of the decision's history and rationale over time. \n\nWhen amending an ADR, the following actions **must** be taken:\n\n* **Clearly mark the changes:**  Use a consistent method like  versioning, highlighting, or change logs to clearly indicate the specific sections that were amended.\n* **Document the reason for amendment:** Briefly explain the reason behind the modification and its impact on the original decision.\n* **Maintain a single source of truth:** Ensure the amended ADR supersedes any previous versions and remains the single source of truth for the architectural decision.\n\nIf a change significantly alters the original decision, its context, or rationale, creating a new, superseding ADR is preferred to maintain clarity and avoid confusion.\n","GenTime":"2024-08-19 15:57:53","text":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n\n\n##Decision\nAmended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n"}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"Prediction":"## Decision\n\nWe will use Dependabot to automatically keep dependencies up-to-date. This will be configured to open pull requests for both security updates and regular dependency updates. \n\nWe will configure Dependabot with the following parameters:\n\n* **Schedule:** Daily checks for new versions.\n* **Target branches:** Pull requests will be opened against the development branch.\n* **Automatic approval:** Security updates will be approved and merged automatically if tests pass. Regular dependency updates will require manual review and approval before merging. \n\n## Consequences\n\n**Pros:**\n\n* **Reduced security risks:** Automatic updates for security vulnerabilities will help keep the application secure.\n* **Improved maintainability:** Staying up-to-date with dependencies makes it easier to adopt new features and benefit from performance improvements. \n* **Reduced manual effort:** Automated dependency management frees up developer time for other tasks.\n\n**Cons:**\n\n* **Potential for breaking changes:**  Updates can introduce breaking changes. While automated tests should catch most issues, manual review is still important. \n* **Increased workload in the short term:** Initial setup and handling potential conflicts in the beginning might require additional effort.\n\n**Overall:**\n\nThe benefits of using Dependabot for automated dependency management outweigh the potential drawbacks. This approach aligns with our goals of maintaining a secure, maintainable, and up-to-date codebase. \n","GenTime":"2024-08-19 15:57:59","text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n"}
{"File Name":"generator-latex-template\/0005-custom-example-environment.md","Context":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n","Decision":"* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","tokens":28,"id":2337,"Prediction":"## Decision\n\nWe will use a JavaScript library to render LaTeX formulas client-side. This offers several benefits:\n\n* **Dynamic Rendering:**  Users can see the rendered output immediately without needing a page reload.\n* **Interactivity:**  The library might provide features like zooming or copying the rendered formula. \n* **Reduced Server Load:** The server doesn't need to pre-render the LaTeX, reducing its workload.\n\nWe will explore and evaluate different JavaScript libraries (e.g., MathJax, KaTeX) based on the following criteria:\n\n* **Ease of Integration:**  How simple is it to integrate the library into our existing codebase?\n* **Rendering Quality:**  How well does the library render complex LaTeX formulas, and how closely does it match the intended output?\n* **Performance:** How fast is the rendering, especially for large or numerous formulas?\n* **Features:** Does the library offer any additional features beyond basic rendering (e.g., interactivity, accessibility)?\n* **Licensing:** Is the library's license compatible with our project?\n\nOnce evaluated, we will select the most suitable library based on our specific needs.\n","GenTime":"2024-08-19 15:58:05","text":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n\n\n##Decision\n* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n"}
{"File Name":"buy-for-your-school\/0006-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":1254,"Prediction":"## Decision\n\nWe will use the 'dotenv' gem to manage our environment variables. \n\n**Rationale:**\n\n* **Actively Maintained:** Unlike Figaro, 'dotenv' is actively maintained and widely used, ensuring access to bug fixes, security patches, and community support.\n* **Centralized Management:** 'dotenv' allows us to define environment variables in a dedicated '.env' file, providing a single source of truth and making it easy to manage and modify configurations.\n* **Fail-Fast Behavior:** 'dotenv' can be configured to raise an exception if a required environment variable is missing, adhering to our preference for failing fast and providing clear error messages.\n* **Testability:** 'dotenv' loads variables on initialization. By loading the '.env' file in specific environments (e.g., development, test), we can control which variables are loaded. This allows for easy stubbing in our test suite without affecting the actual system environment. We can also leverage libraries like 'simplecov' to further manage environment loading during testing.\n\n**Implementation Details:**\n\n1. **Installation:** Add 'dotenv' to your Gemfile and run `bundle install`.\n2. **Configuration:**\n    * Create a '.env' file in your project root to store environment variables.\n    * In your application's initialization code, load the '.env' file using `Dotenv.load`.\n3. **Usage:** Access environment variables using `ENV['VARIABLE_NAME']`.\n4. **Testing:** \n    * Use environment variables in your tests to control test-specific configurations.\n    * Consider using testing libraries like 'simplecov' to manage environment loading within your test suite and prevent test pollution. \n\n**Alternatives Considered:**\n\n* **Direct ENV Access:** While possible, it lacks the benefits of centralized management, fail-fast behavior, and easy stubbing provided by 'dotenv'.\n* **Roll-Our-Own Solution:**  Building a custom solution adds complexity and maintenance overhead without significant advantages over existing, well-maintained libraries like 'dotenv'.\n\n**Consequences:**\n\n* **Improved Error Handling:** Early detection of missing environment variables.\n* **Centralized Configuration:** Easier management and modification of environment settings. \n* **Enhanced Testability:** Cleaner separation between test and production environments. \n","GenTime":"2024-08-19 15:58:15","text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n"}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"Prediction":"## Decision\n\nWe will send the version in an encrypted SAML extension. \n\n## Reasoning\n\n* **Security:** Encrypting the version number mitigates the security concerns of exposing it in cleartext. This prevents potential attackers from easily determining the version and exploiting known vulnerabilities. \n* **Manageability:** Using a SAML extension provides a standardized and structured way to include the version information within existing communication flows. This makes it easy to track and manage versions across different relying parties without requiring changes to the core SAML protocol.\n* **Compatibility:** While custom solutions like user-agent strings or unencrypted extensions might seem simpler, they can lead to compatibility issues and might not be supported by all SAML implementations. \n\n## Consequences\n\n* **Implementation Complexity:** Encrypting and decrypting the version information will add some complexity to the implementation.\n* **Performance:** Encryption and decryption will introduce a minor performance overhead. \n\n**Mitigation:**\n\n* We will use a lightweight encryption algorithm to minimize the performance impact.\n* We will leverage existing libraries and tools for encryption and SAML extension handling to reduce implementation complexity. \n\nThis approach balances the need for version tracking and management with security and compatibility considerations. \n","GenTime":"2024-08-19 15:58:21","text":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n\n\n##Decision\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n"}
{"File Name":"lbh-adrs\/Microfrontend-Composition.md","Context":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n","Decision":"**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","tokens":242,"id":2317,"Prediction":"## Decision\n\nWe will use **Single SPA** as the framework for client-side micro-frontend composition. \n\n## Justification\n\nWhile writing our own boilerplate code might seem simpler initially, using a dedicated framework like Single SPA offers significant advantages:\n\n* **Structure and Maintainability:** Single SPA provides a well-defined structure and lifecycle for micro-frontends, leading to more maintainable and scalable codebase as the project grows. \n* **Routing and Navigation:** Single SPA handles routing and navigation between micro-frontends, simplifying development and ensuring a consistent user experience.\n* **Independent Deployment:** The framework allows each micro-frontend to be developed, deployed, and updated independently, promoting team autonomy and faster development cycles. \n* **Community and Support:** Single SPA has a large and active community, providing extensive documentation, resources, and support.\n\nDeveloping a custom solution would require significant effort to replicate these features, potentially introducing complexity and hindering future development.\n\n## Consequences\n\n* **Learning Curve:** Teams will need to invest time in learning Single SPA's concepts and API.\n* **Dependency on Framework:** The project becomes reliant on Single SPA for core functionality. \n* **Potential Overhead:** Using a framework might introduce a slight performance overhead compared to a minimal custom solution.\n\nHowever, we believe these consequences are outweighed by the benefits of using a mature and well-supported framework.\n","GenTime":"2024-08-19 15:58:27","text":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n\n\n##Decision\n**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n"}
{"File Name":"beis-report-official-development-assistance\/0031-replace-publicactivity-with-our-own-hand-rolled-way-of-recording-changes.md","Context":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n","Decision":"We have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","tokens":145,"id":2403,"Prediction":"## Decision\n\nWe will retire the `PublicActivity` gem and implement a custom solution for recording and displaying user activity. \n\n## Reasons\n\n* **Lack of Usage:**  The information captured by `PublicActivity` has not been utilized, indicating it doesn't align with our current needs.\n* **Complexity vs. Value:**  Adapting `PublicActivity` to meet our evolving requirements for user-facing activity logs is proving more complex than building a tailored solution. \n* **Control and Flexibility:** A custom approach provides greater control over the data structure, storage, and presentation of user activity, allowing for easier future modifications and enhancements. \n\n## Consequences\n\n* **Development Effort:** Migrating away from `PublicActivity` will require development time to implement a new solution. \n* **Data Migration:**  We may need to migrate existing activity data from `PublicActivity`'s format to our new system, if that data is deemed valuable.\n* **Maintenance:** We will be responsible for the ongoing maintenance and support of the custom solution.\n\n## Status\n\nProposed\n\n## Action Items\n\n1. **Design:**  Define the data model, API, and UI\/UX for the custom user activity logging system.\n2. **Implementation:** Develop and test the new system. \n3. **Migration (if necessary):** Migrate existing activity data from `PublicActivity`. \n4. **Deployment:**  Deploy the custom solution and decommission `PublicActivity`.\n5. **Monitoring & Feedback:**  Monitor the new system's performance and gather user feedback to inform future iterations. \n","GenTime":"2024-08-19 15:58:34","text":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n\n##Decision\nWe have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n"}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"Prediction":"## Decision\n\nWe will refactor driver interfaces to use namespaces instead of global structs. This means:\n\n1. **Moving interface definitions:** All type definitions, enums, and `using` statements currently residing within global structs like `tof` will be moved into dedicated namespaces within `embvm` (e.g., `embvm::Tof`).\n\n2. **Updating Interface Classes:** Interface classes will no longer inherit from these global structs. Instead, they will directly use the types and enums provided by the corresponding namespace.\n\n3. **Replacing inheritance with using:** Where applicable, we will replace inheritance chains with `using namespace` directives within the interface class definition to simplify access to namespace members.\n\nThis decision leverages the built-in namespacing mechanism of C++ to achieve the desired organization and avoid unnecessary inheritance. This will result in cleaner, more maintainable code with a reduced risk of name collisions.\n","GenTime":"2024-08-19 15:58:38","text":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n\n##Decision\nEach driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\\nWe need to choose main programming language for this project.\\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"Prediction":"## Decision\n\nWe will use **[Programming Language Name]** as the primary language for this project. \n\n**Reasoning:**\n\nWe considered the following factors when making this decision:\n\n* **[Reason 1]:**  (e.g., Expertise of the team, existing codebase)\n* **[Reason 2]:** (e.g., Performance requirements, scalability)\n* **[Reason 3]:** (e.g., Ecosystem and libraries, community support)\n\n**Alternatives Considered:**\n\n* **[Alternative Language 1]:**  While [briefly mention advantages], it was ultimately ruled out due to [briefly mention disadvantages].\n* **[Alternative Language 2]:** This option was considered but dismissed due to [briefly mention disadvantages]. \n\n**Consequences:**\n\n* **Positive:** [List positive outcomes of using chosen language]\n* **Negative:** [List potential downsides or challenges to address] \n\n**Note:** \n\nReplace the bracketed placeholders above with your specific project details. This structure provides a clear and concise ADR  outlining your decision-making process. \n","GenTime":"2024-08-19 15:58:43","text":"## Context\\nWe need to choose main programming language for this project.\\n\n\n##Decision\nI'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n"}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"Prediction":"## Decision\n\nTo address the issues of nested keys and default values in application configuration, we will implement a centralized configuration module with the following features:\n\n**1. Nested Key Access and Validation:**\n\n*   We will define a function, such as `Config.get\/2`, that allows accessing configuration values using a path-like syntax for nested keys. For example: `Config.get([:rig, :required_key, :required_subkey])`.\n*   This function will handle nested key traversal and raise a clear error if any key in the path is missing, similar to `Application.fetch_env!\/2`.\n\n**2. Centralized Default Values and Inspectability:**\n\n*   All default configuration values will be defined within this centralized module. \n*   We will use a structured format (e.g., a map or a dedicated configuration file) to store default values, making them easily inspectable. \n\n**3. Type Safety (Optional):**\n\n*   For improved type safety, we can leverage typespecs to define the expected data type for each configuration value. \n\n**Example Implementation (Illustrative):**\n\n```elixir\ndefmodule MyApp.Config do\n  @default_config %{\n    rig: %{\n      required_key: %{\n        required_subkey: \"default_value\"\n      }\n    }\n  }\n\n  def get(path) do\n    get_in(@default_config, path)\n  end\n\n  def get!(path) do\n    case get_in(@default_config, path) do\n      nil -> raise \"Configuration key not found: #{inspect(path)}\"\n      value -> value\n    end\n  end\nend\n```\n\n**Usage:**\n\n```elixir\n# Accessing nested keys with default\nConfig.get([:rig, :required_key, :required_subkey]) \n\n# Accessing nested keys with error on missing key\nConfig.get!([:rig, :required_key, :another_subkey]) \n```\n\n**Benefits:**\n\n*   **Consistency:**  Provides a unified and reliable way to access configuration values.\n*   **Maintainability:** Centralized defaults improve code organization and make updates easier.\n*   **Error Handling:** Clear error messages for missing keys help in debugging.\n*   **Inspectability:** Configuration structure and defaults are readily available for review.\n\nThis decision promotes a more robust and maintainable approach to handling application configuration in Elixir. \n","GenTime":"2024-08-19 15:58:54","text":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n\n\n##Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n"}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"Prediction":"## Decision\n\nWe will use AWS managed databases for persistent storage:\n\n* **User Credentials:**  Due to the sensitive nature of user credentials, we will leverage AWS Cognito for user management and authentication. Cognito provides secure storage, authentication features, and integrates well with other AWS services. This aligns with the MoJ strategy of using managed services and removes the burden of managing a secure authentication system in-house.\n* **LPA Ownership and Granted Access:** We will use a managed relational database service like AWS RDS (Relational Database Service) for storing LPA ownership and granted access data. This data has relational properties and requires ACID properties for consistency. RDS offers managed PostgreSQL and MySQL instances, both suitable for this use case. This aligns with the MoJ's managed services strategy and offers scalability and reliability.\n\n## Consequences\n\n* **Pros:**\n    * Reduced operational overhead due to managed services.\n    * Improved security for sensitive data with Cognito.\n    * Scalability and reliability of AWS infrastructure.\n    * Potential cost savings compared to managing our own infrastructure.\n* **Cons:**\n    * Potential vendor lock-in with AWS.\n    * Learning curve associated with AWS services.\n    * Possible cost implications depending on usage patterns.\n\n## Alternatives Considered\n\n* **Self-hosted databases:** This option would provide more control but require significant development and maintenance effort. It contradicts the MoJ's managed services strategy and poses higher security risks.\n* **Other cloud providers:** While other cloud providers like Google Cloud Platform (GCP) or Azure offer similar services, AWS is the MoJ's preferred cloud provider, offering potential cost benefits and existing expertise within the organization. \n\nThis decision leans toward leveraging AWS managed services for persistent storage, aligning with the MoJ's strategy and prioritizing security, scalability, and maintainability.  \n","GenTime":"2024-08-19 15:59:02","text":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n\n\n##Decision\nUse DynamoDB for persistent storage\\n"}
{"File Name":"skunkworks\/state-management.md","Context":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n","Decision":"- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","tokens":208,"id":2298,"Prediction":"## Decision\n\nWe will use **React Context for local and shared state management** and **@tanstack\/react-query for managing server state\/remote data fetching.**\n\n## Reasoning\n\nWhile Redux offers a robust solution for state management, its boilerplate can be cumbersome for this project. React Context provides a lighter-weight solution suitable for managing both local and shared state within a reasonable scope.\n\nFor remote state management, @tanstack\/react-query stands out for its ease of use, caching capabilities, and automatic request management. It effectively abstracts away the complexities of fetching, caching, and updating server-side data, leading to cleaner and more maintainable code.\n\nThis decision promotes:\n\n* **Simplicity and Maintainability:** React Context reduces boilerplate for local and shared state, while @tanstack\/react-query streamlines server state management.\n* **Performance:** Both React Context and @tanstack\/react-query offer mechanisms for optimization, reducing unnecessary re-renders and improving application responsiveness.\n* **Scalability:**  While we leverage React Context's simplicity, we acknowledge its limitations for larger applications. Should our state management needs grow significantly, we will re-evaluate and consider a more robust solution.\n\n## Consequences\n\n* **Potential for \"Provider Hell\":** We need to be mindful of deeply nested context providers and employ best practices (e.g., combining related contexts) to mitigate this risk.\n* **Increased Responsibility for Data Fetching:** Utilizing React Context for shared state necessitates careful management of data fetching and updates within the relevant components. \n* **Learning Curve for @tanstack\/react-query:**  Adopting @tanstack\/react-query requires familiarizing the team with its API and best practices.\n\nThis approach strikes a balance between simplicity, performance, and scalability for our current project needs.  We will continue to monitor state management requirements and adapt our approach as the application evolves. \n","GenTime":"2024-08-19 15:59:10","text":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n\n\n##Decision\n- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n"}
{"File Name":"buddy-auth.jwt-oidc\/adr-001.md","Context":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","Decision":"Tokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","tokens":229,"id":3872,"Prediction":"## Decision ##\n\nWe will modify the library to specifically reject ID tokens signed with symmetric keys. The library will no longer attempt to build a public key from the issuer's data when a symmetric signing algorithm is detected. Instead, it will immediately mark the token as invalid.\n","GenTime":"2024-08-19 15:59:12","text":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n\n\n##Decision\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n"}
{"File Name":"GSoC-Braitenberg-Vehicles\/use_javafx_for_animation.md","Context":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","Decision":"--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","tokens":101,"id":1400,"Prediction":"## Decision\n\n**Continue using JavaFX\/TornadoFX for rendering and animation.**\n\n## Rationalization\n\nWhile acknowledging the existence of potentially more performant frameworks and engines like Unity and OpenGL, the decision prioritizes development time and learning curve. \n\n- **Time Constraints:**  Migrating to a new framework like Unity or OpenGL would require significant time investment for configuration and learning. This is deemed unfeasible given current project constraints. \n- **JavaFX Familiarity:** JavaFX presents a lower barrier to entry and allows for quicker integration and development due to existing familiarity.\n\nThis decision prioritizes pragmatic development over potential performance gains, recognizing that introducing a new technology comes with its own set of challenges and delays. \n","GenTime":"2024-08-19 15:59:16","text":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n\n\n##Decision\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"Prediction":"## Decision\n\nWe will store the state of asynchronous operations, like restore from snapshot, in a dedicated database table. \n\n## Reasons\n\nWhile the \"pure\" stateless approach has its advantages, the complexity introduced by the restore from snapshot feature, which necessitates sequential asynchronous operations, outweighs the benefits of staying completely stateless. \n\nHere's a breakdown of why we choose a database table over other options:\n\n* **Database Table (Chosen):**\n    * **Pros:** Relatively simple to implement, providing a persistent and structured way to track operation progress, even across broker restarts. It integrates well with existing infrastructure if a database is already in use.\n    * **Cons:** Introduces statefulness, albeit in a controlled manner. Requires schema management and potential scaling considerations depending on the database solution chosen.\n\n* **Background Housekeeping Routine:**\n    * **Pros:** Potentially lightweight if kept simple.\n    * **Cons:**  High complexity in ensuring resilience, handling restarts, and managing concurrency across multiple broker instances. Difficult to scale and debug.\n\n* **SNS and SQS:**\n    * **Pros:** Leverages AWS services for event-driven architecture.\n    * **Cons:**  Significant development overhead for integration. Adds complexity in managing message queues and potential delays in event processing. Overkill for the specific requirement of tracking sequential operations within the broker.\n\n## Consequences\n\n* **Increased Complexity:** The RDS broker will no longer be entirely stateless, introducing new complexities in state management and data consistency.\n* **Database Dependency:**  The broker's operation will now depend on the availability and performance of the chosen database. \n* **Enhanced Functionality:** The broker will be able to support the restore from snapshot feature with its sequential operations.\n* **Improved User Experience:**  The Cloud Controller can provide more accurate and timely updates on the progress of long-running operations.\n\n## Action Items\n\n1. **Database Design:** Define the schema for the new database table to store the state of asynchronous operations. This should include fields to track the operation type, status, relevant identifiers (like instance ID), timestamps, and any potential error information. \n2. **Broker Implementation:**  Modify the RDS broker to:\n    * On initiating a multi-step operation, create a new record in the database table.\n    * After each step, update the corresponding record with the new status and relevant information.\n    * Implement the `LastOperation` endpoint to query this table and provide accurate status updates.\n    * Handle potential error states and implement retry mechanisms where appropriate.\n3. **Testing:** Thoroughly test the new state management logic, ensuring data consistency, proper error handling, and correct interaction with the Cloud Controller.\n4. **Monitoring & Alerting:** Set up monitoring and alerting for the database and the relevant broker components to ensure the reliability and performance of the new implementation. \n","GenTime":"2024-08-19 15:59:28","text":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n\n\n##Decision\nWe decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n"}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"Prediction":"## Decision\n\n**Adopt pac4j for Azure AD authentication and authorization.**\n\n**Reasoning:**\n\n* **Reduced Development Effort:** Pac4j offers out-of-the-box support for Azure AD, including handling Microsoft's non-standard tenantId inclusion in the Issuer URL. This eliminates the need for custom filter and token validation logic, saving significant development and maintenance effort.\n* **Simplified Configuration:**  Using pac4j allows us to configure Azure AD integration by simply providing the discovery URL, simplifying our authentication setup and reducing potential error points. \n* **Potential for Future Expansion:** Pac4j supports a wide range of authentication mechanisms beyond Azure AD. This provides flexibility for future expansion if needed, such as integrating with other identity providers or social logins. \n\n**Consequences:**\n\n* **Dependency on External Library:** Introducing pac4j adds an external dependency to the project. We need to carefully evaluate its maturity, security posture, and long-term maintenance to ensure it aligns with our project's needs and constraints.\n* **Learning Curve:**  While pac4j simplifies authentication, the team will need to invest time in understanding its API and configuration options.\n\n**Next Steps:**\n\n1. **Thoroughly evaluate pac4j:** Conduct a deeper analysis of pac4j's documentation, community support, and potential security implications. \n2. **Create a Proof of Concept:** Develop a small-scale Proof of Concept (PoC) to validate pac4j's integration with our application and Azure AD environment. This PoC should test authentication flows, token validation, and authorization capabilities.\n3. **Develop Migration Plan:**  If the PoC is successful, outline a migration plan for transitioning from our custom solution to pac4j. This plan should address code changes, configuration updates, and potential testing requirements. \n","GenTime":"2024-08-19 15:59:36","text":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n\n\n##Decision\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"Prediction":"## Decision\n\nTo support a seamless migration from `Corvus.Tenancy` v2 to v3, we will introduce a compatibility layer in v3 that handles the creation of missing storage containers during the transition period. This layer will:\n\n1. **Detect Missing Containers:** Upon a tenant-specific operation, v3 will check for the existence of all required storage containers based on configured conventions or metadata.\n2. **Create Missing Containers (if absent):** If any container is missing, v3 will automatically create it using the defined conventions and security policies.\n3. **Logging and Monitoring:**  The creation of any missing containers will be logged for observability and potential troubleshooting. Metrics will track the number of missing containers created, indicating progress in the migration.\n\nThis compatibility layer will be enabled by default in `Corvus.Tenancy` v3. Once the application has been deemed fully migrated to v3, and all tenants have their required containers, the compatibility layer can be disabled through configuration. \n\nThis approach ensures:\n\n- **Backward Compatibility:** Existing deployments of applications using v2 will continue to function correctly.\n- **Smooth Migration:** Applications can be gradually upgraded to v3 without requiring immediate changes to their container management logic. \n- **Flexibility:**  Applications have the flexibility to adopt the new container creation responsibility in v3 at their own pace.\n- **Transparency:**  Logging and monitoring provide insights into the migration progress and potential issues. \n","GenTime":"2024-08-19 15:59:42","text":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n\n\n##Decision\nUpgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n"}
{"File Name":"registraties\/002-use-styled-components.md","Context":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n","Decision":"SASS will not be used in the Registraties project. Instead, styled components will be used.\\n","tokens":174,"id":1125,"Prediction":"## Decision\n\nWe will use **styled-components** for styling in this project instead of globally scoped SASS. \n\nThis decision is based on the following reasons:\n\n* **Avoid Global Namespace Collisions:** Styled-components encapsulate styles within components, eliminating the risk of global scope conflicts and unexpected side-effects.\n* **Improved Maintainability and Reusability:**  Component-specific styling makes code easier to understand, maintain, and reuse. Developers can focus on styling within a component's context without worrying about impacting other parts of the application.\n* **Enhanced Collaboration:** With styled-components, developers can work on components independently without the need for complex naming conventions or constant cross-team communication about global styles.\n* **Alignment with Amsterdam Design System:**  Adopting styled-components aligns with the existing usage in the Amsterdam reusable component library, ensuring consistency and interoperability across projects.\n* **Improved Performance:** Styled-components generate only the necessary styles for rendered components, potentially leading to performance improvements compared to globally scoped CSS.\n\nWhile migrating existing SASS code to styled-components might require an initial effort, the long-term benefits of maintainability, scalability, and alignment with the broader Amsterdam ecosystem outweigh the transition costs. \n","GenTime":"2024-08-19 15:59:48","text":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n\n\n##Decision\nSASS will not be used in the Registraties project. Instead, styled components will be used.\\n"}
{"File Name":"taxonomy-manager\/001-graph-database.md","Context":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n","Decision":"* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","tokens":95,"id":1510,"Prediction":"## Decision\n\n**We will use a graph database for storing and querying SKOS taxonomies.**\n\nThis decision leverages the native capabilities of graph databases to handle linked data structures like RDF, simplifying the implementation and reducing the need for custom serialization\/deserialization. While potentially introducing a steeper learning curve compared to familiar RDBMS systems, the benefits of direct RDF support outweigh the potential drawbacks in this context. \n","GenTime":"2024-08-19 15:59:50","text":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n\n##Decision\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n"}
{"File Name":"inner-source_kotlin_spark_template\/20180525-gradle-kotlin.md","Context":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n","Decision":"We will adopt Kotlin as a Gradle build's definition language.\\n","tokens":32,"id":1463,"Prediction":"## Decision\n\nWe will use Kotlin as the language for writing Gradle build scripts. \n","GenTime":"2024-08-19 15:59:51","text":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n\n\n##Decision\nWe will adopt Kotlin as a Gradle build's definition language.\\n"}
{"File Name":"fare-platform\/0002-permission-crud-operations-on-records-config.md","Context":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n","Decision":"To avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","tokens":30,"id":2215,"Prediction":"## Decision\n\nTo ensure that records have actual content and are useful, we will **require a file attachment when creating a new record**. This means that users will no longer be able to create records with only metadata; they must also upload at least one file associated with the record. \n","GenTime":"2024-08-19 15:59:53","text":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n\n\n##Decision\nTo avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n"}
{"File Name":"community\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3466,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions along with their context and consequences. This will help us to:\n\n* **Improve architectural understanding:** Provide a historical record of why certain decisions were made. \n* **Facilitate communication:**  Serve as a central point of reference for team members (current and future) to understand the rationale behind the architecture.\n* **Enable better decision-making:** Encourage deliberate decision-making by considering different options and trade-offs. \n* **Increase transparency:** Make the decision-making process visible and traceable.\n\nWe will use Markdown as the standard format for ADRs and store them in a dedicated \"\/docs\/adrs\" directory within the project repository. Each ADR will follow a consistent template including:\n\n* **Title:** A concise summary of the decision.\n* **Status:**  Proposed, Accepted, Rejected, Superseded, Deprecated, etc.\n* **Context:**  The background and circumstances leading to the decision. \n* **Decision:** The chosen solution and its description. \n* **Consequences:**  The positive and negative implications of the decision.\n\nWe will leverage a tool like [ADR Tools](https:\/\/github.com\/npryce\/adr-tools) to simplify the creation and management of ADRs. \n","GenTime":"2024-08-19 15:59:59","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"iampeterbanjo.com\/0012-parallel-database-tests-mongodb-server.md","Context":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n","Decision":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","tokens":80,"id":1589,"Prediction":"## Decision\n\nWe will **randomly create databases for each test** using a dedicated test database server and a unique database name generated for each test suite. \n\n## Reasoning\n\nWhile defining databases in tests offers some control, randomly creating databases provides better isolation and reduces the chance of unforeseen inter-test dependencies. This approach offers the following benefits:\n\n* **Enhanced Isolation:**  Each test suite operates in complete isolation, eliminating potential race conditions or data pollution from concurrently running tests.\n* **Reduced Test Interdependencies:** Tests are less likely to depend on the specific state of the database left by previous tests, leading to more reliable and independent test suites.\n* **Simplified Setup\/Teardown:**  Random database creation automates the process of setting up and tearing down the test environment, reducing boilerplate code in test files.\n\n## Tradeoffs\n\n* **Slightly Increased Complexity:** Implementing random database creation requires a mechanism for generating unique database names and managing connections. \n* **Potential Performance Overhead:** Creating and deleting databases for each test suite might introduce a minor performance overhead. However, this can be mitigated by using a fast database server dedicated for testing.\n\n## Consequences\n\n* Tests will be more reliable and independent.\n* The test suite will be easier to maintain due to reduced inter-test dependencies.\n* There might be a slight performance overhead during test execution.\n\n## Alternatives Considered\n\n* **Defining Databases in Tests:** This approach offers more control but increases the risk of inter-test dependencies and requires more manual setup.\n\n## References\n\n* [defined-test-database]: [Link to documentation or example for defining databases in tests]\n* [random-test-database]: [Link to documentation or example for randomly creating databases for each test] \n","GenTime":"2024-08-19 16:00:07","text":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n\n\n##Decision\nIn the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n"}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"Prediction":"## Decision\n\nWe will use a **NodePort Service** to expose the Ingress Controller to external traffic. \n\n## Decision Rationale\n\n* **Principle of Least Astonishment:** NodePort is a standard Kubernetes concept for exposing services externally, making it a familiar approach for users.\n* **Hybrid Cloud Support:** NodePort functions independently of cloud-specific load balancers, making it suitable for bare-metal, on-premise, and hybrid deployments.\n* **Flexibility:** NodePort allows direct access to the Ingress Controller without relying on cloud-specific integrations or BYO VM restrictions.\n* **Simplicity:**  NodePort is straightforward to configure and requires minimal overhead compared to more complex solutions. \n\n## Considered Alternatives\n\n* **LoadBalancer Service:** While convenient for cloud environments, it violates our requirement for hybrid flexibility and BYO VM compatibility. \n* **External IPs (managed by Kubernetes):**  This approach can be complex and depends on specific cloud provider support. \n\n## Implications\n\n* **Port Management:**  We will need to manage port assignments on the nodes to avoid conflicts and ensure the chosen port is accessible.\n* **External Load Balancing:** For high availability and distribution, an external load balancer will be required in front of the nodes. This load balancer will be outside the scope of Kubernetes.\n\nThis decision prioritizes flexibility and simplicity by utilizing a Kubernetes-native mechanism suitable for diverse deployment environments. \n","GenTime":"2024-08-19 16:00:14","text":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n\n\n##Decision\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n"}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"Prediction":"## Decision\n\nWe will implement a feature to estimate the time of rough cuts, including the time taken for voice-overs, using the following approach:\n\n**Method:**  Allow users to input an estimated time for each voice-over segment. This time will be added to the rough cut duration calculation. \n\n**Justification:**\n\n* **Ease of implementation:**  This method is straightforward to implement, requiring a simple input field for estimated voice-over duration and an update to the calculation logic. \n* **Accuracy of time estimated:** While not perfectly accurate, allowing for user-provided estimations acknowledges the variability in voice-over recording and editing time. Users are likely to have a reasonable understanding of the expected duration based on script length and complexity. \n* **Extensibility:** This approach can be extended in the future to incorporate more sophisticated estimation techniques, such as analyzing script content or integrating with voice-over recording software for more precise data. \n\n**Alternatives Considered:**\n\n* **Automatic estimation based on script analysis:** This approach, while potentially more accurate, would require complex natural language processing algorithms and might not be accurate for all scenarios. \n* **Integration with voice-over recording software:**  This would provide the most accurate data but introduces significant complexity in terms of development and potential compatibility issues.\n\n**Consequences:**\n\n* This approach relies on user input accuracy, so it's important to communicate the importance of realistic estimations. \n* The system might require further updates to incorporate more accurate time estimations in the future. \n","GenTime":"2024-08-19 16:00:20","text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n"}
{"File Name":"CrossyToad\/adr-0010-support-linear-and-discrete-input.md","Context":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n","Decision":"We will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","tokens":270,"id":2482,"Prediction":"## Decision\n\nWe will implement continuous input by transitioning our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events from `SDL_PollEvent`. \n\n## Reasons\n\n* **Maintainability and Control:** Managing our own input state provides greater control over how key combinations are handled and represented. Using the type system allows us to enforce constraints and prevent invalid combinations, leading to more robust and maintainable code.\n* **Flexibility for Future Features:**  Abstracting input state management within our own logic allows for easier implementation of potential future features like input buffering, macros, or remapping.\n* **Reduced Dependency on SDL:** While `SDL_GetKeyboardState` offers a simple solution, managing input state ourselves reduces reliance on SDL-specific functionality for core game logic. This increases portability and adaptability should the need to switch away from SDL arise in the future.\n\n## Consequences\n\n* **Increased Complexity:**  Implementing our own input state management adds complexity compared to directly using `SDL_GetKeyboardState`. We will need to carefully design and implement the state machine and its interactions with the rest of the game logic.\n* **Potential for Bugs:** Manually managing state introduces the possibility of introducing bugs if not implemented carefully. Thorough testing will be essential to ensure the correctness of the input handling system.\n\n## Status\n\nAccepted. We will proceed with implementing our own \"InputState\" mechanism based on the reasons outlined above.\n\n## Future Considerations\n\n* **Performance:**  While not an immediate concern, we should profile the performance impact of managing our own input state as the codebase grows. If necessary, we can explore optimizations or alternative approaches.\n* **Input Abstraction Layer:** In the future, consider creating a dedicated input abstraction layer to further decouple input handling from SDL and potentially support other input methods (e.g., gamepads, touchscreens). \n","GenTime":"2024-08-19 16:00:28","text":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n\n\n##Decision\nWe will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n"}
{"File Name":"iampeterbanjo.com\/0002-blog-engine.md","Context":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n","Decision":"In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","tokens":300,"id":1599,"Prediction":"## Decision\n\nWe will use **11ty** as the static site generator for the website blog. \n\nWhile Gatsby offered a more feature-rich JAMstack approach, its client-side rendering and reliance on service workers introduced complexity and unpredictability that we want to avoid for a blog focused on content delivery. \n\nHugo, though performant, necessitates introducing a new language (Go) into the stack, deviating from our preference for a fullstack JavaScript solution. \n\n11ty provides the best balance:\n\n* **Simplicity and Predictability:** Its straightforward static file output and relatively small codebase align with our desire for a maintainable and understandable system.\n* **Flexibility:** Support for various template engines gives us freedom in choosing our templating approach.\n* **Addressable Challenges:** The issues encountered with the frontmatter parser and template error reporting can be addressed through custom solutions or exploring alternative configurations within 11ty. \n\nWe will prioritize addressing the identified 11ty shortcomings (frontmatter parsing, template error reporting, and production readiness) to establish a robust foundation for our blog. \n","GenTime":"2024-08-19 16:00:33","text":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n\n\n##Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n"}
{"File Name":"cljdoc\/0008-use-circleci-as-analysis-sandbox.md","Context":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n","Decision":"For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","tokens":160,"id":2601,"Prediction":"## Decision\n\nWe will use a continuous integration (CI) service, specifically GitHub Actions, to analyze untrusted Clojure code. This decision is based on the following factors:\n\n* **Security:** While not perfect, leveraging GitHub's existing infrastructure and security measures for running Actions provides a more secure baseline compared to managing our own Docker execution environment. \n* **Resource Management:** Offloading the execution to GitHub Actions eliminates the need for us to manage resources and monitor for potential abuse like Bitcoin mining.\n* **Cost-Effectiveness:** GitHub Actions offers a generous free tier, making it a cost-effective solution for our needs. \n* **Integration:** As our project is already hosted on GitHub, integrating with GitHub Actions will be seamless.\n* **Flexibility:** GitHub Actions allows for customization and fine-tuning of the execution environment, giving us control over resource limits and security settings.\n\nWe acknowledge that using a CI service like GitHub Actions is not a foolproof solution.  \"Hacking\" the intended use of the CI service introduces potential challenges in terms of  accurately replicating the desired analysis environment and navigating any limitations imposed by the CI platform. However, we believe that the benefits in terms of security, resource management, and cost outweigh these risks. \n\nWe will carefully configure the GitHub Actions workflow to minimize the attack surface and mitigate potential risks:\n\n* **Strict resource limits:** We will enforce strict resource limits on the execution environment to prevent excessive resource consumption.\n* **Network restrictions:** We will restrict network access from the execution environment to prevent unwanted communication and potential data exfiltration.\n* **Ephemeral environments:** Each analysis job will run in a clean and isolated environment to prevent persistence and potential exploitation of vulnerabilities.\n* **Monitoring and Alerting:** We will implement monitoring and alerting mechanisms to detect any suspicious activity or attempts to circumvent the security measures.\n\nWe will continuously evaluate and improve our security posture as needed.\n","GenTime":"2024-08-19 16:00:42","text":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n\n\n##Decision\nFor a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n"}
{"File Name":"stamper\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1244,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for capturing important architectural decisions along with their context and consequences. Each ADR will be a separate markdown file stored in the '\/docs\/adr' directory of the repository. \n","GenTime":"2024-08-19 16:00:43","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"rotc\/0002-use-aws-as-example-cloud-platform.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3546,"Prediction":"## Decision\n\n**(Add a concise and descriptive title for your decision here)**\n\nThis section should clearly state the chosen option to address the context described above. Be specific and avoid ambiguity.\n\n**Example:**\n\n> **Adopt GraphQL for client-server communication.** \n","GenTime":"2024-08-19 16:00:45","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"Prediction":"## Context \n\n(Add a concise description of the forces at play. In particular, describe the business problem you\u2019re facing and the technical problem you need to solve.  You can optionally number your points for better readability.)\n","GenTime":"2024-08-19 16:00:47","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n"}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"Prediction":"## Decision\n\nWe will standardize the directory structure for all new GitHub repositories based on the following guidelines:\n\n**Top-level directories:**\n\n* **docs:** Documentation for the project, including READMEs, API documentation, and user guides.\n* **src:** Source code for the project.\n* **tests:** Unit tests, integration tests, and end-to-end tests.\n* **scripts:**  Helper scripts for development, deployment, or other tasks. \n* **examples:** Example code demonstrating how to use the project.\n* **tools:** Tools used in the development process, such as linters or code generators.\n\n**Further subdirectories within \"src\" will be determined on a project-by-project basis, depending on the programming language and framework being used.** \n\n**Rationale for standardization:**\n\n* **Consistency:** A standardized structure makes it easier for developers to navigate and understand different repositories.\n* **Maintainability:**  A consistent structure simplifies automation of tasks like testing and deployment. \n* **Onboarding:**  New developers can quickly get up to speed on the project structure.\n* **Best Practices:** This structure aligns with common industry best practices.\n\n**Implementation:**\n\n* We will create a template repository that embodies this structure. \n* New repositories should be initialized using this template. \n* Documentation will be created outlining the directory structure and its rationale.\n* We will encourage but not mandate refactoring existing repositories to conform to this standard when feasible. \n\nThis decision will be reviewed periodically to ensure it continues to meet our needs as our development practices evolve. \n","GenTime":"2024-08-19 16:00:54","text":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n\n\n##Decision\n### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n"}
{"File Name":"buy-for-your-school\/0009-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":1258,"Prediction":"## Decision\n\nWe will implement a comprehensive automated testing strategy that encompasses the following aspects:\n\n* **Unit Tests:** Focus on testing individual components (functions, classes, modules) in isolation to ensure their correct functionality.\n* **Integration Tests:** Verify the interaction between different units or components within the system.\n* **End-to-End (E2E) Tests:** Simulate real user scenarios and test the entire application flow, from the user interface to the database and back.\n* **Continuous Integration (CI):** Integrate automated tests into the development pipeline, triggering them on every code change. This allows for early detection of bugs and ensures that the application remains in a releasable state.\n* **Test Coverage Monitoring:** Track test coverage metrics (e.g., line coverage, branch coverage) to identify areas of the codebase that lack sufficient testing and prioritize test creation.\n\n**Tools and Technologies:**\n\n* We will use a combination of testing frameworks and libraries suitable for our chosen programming language and technology stack (e.g., JUnit, pytest, Selenium, Cypress).\n* A CI\/CD platform will be used to automate the execution of tests and provide feedback to developers (e.g., Jenkins, GitLab CI, CircleCI).\n\n**Benefits:**\n\n* **Increased Test Coverage:** Automated tests provide broader test coverage compared to manual testing, reducing the risk of undetected bugs.\n* **Faster Feedback:** Automated tests provide immediate feedback on code changes, enabling quicker identification and resolution of issues.\n* **Reduced Manual Effort:** Automation frees up developers and testers from repetitive manual testing tasks, allowing them to focus on more critical activities.\n* **Improved Software Quality:** Regular and comprehensive automated testing leads to a higher level of software quality and reliability. \n","GenTime":"2024-08-19 16:01:02","text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n"}
{"File Name":"adrflow\/5-Utility_Context_Lazily_Initialized.md","Context":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n","Decision":"The ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","tokens":227,"id":3635,"Prediction":"## Decision\nInstead of eagerly initializing the context in the constructor, we will lazily initialize it on the first method call that requires it. \n\nThis will be achieved by:\n\n1. **Modifying the constructor**: The constructor of the `synchronous utils` object will no longer initialize the context. \n2. **Introducing a private method**: A private method, such as `_ensureContext`, will be added to the object. This method will check if the context is initialized, and if not, it will attempt to load it.\n3. **Updating existing methods**: All existing methods that rely on the context will be updated to call `_ensureContext` at the beginning of their execution. \n\nThis approach will solve the immediate issue with the `init` command by preventing the context loading before the command execution. Furthermore, it will improve performance by avoiding unnecessary context loading for commands that do not actually require it. \n","GenTime":"2024-08-19 16:01:06","text":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n\n\n##Decision\nThe ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n"}
{"File Name":"testy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1049,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions along with their context and consequences. Each ADR will be a single Markdown file stored in a dedicated \"\/docs\/adr\" directory at the root level of the code repository. \n","GenTime":"2024-08-19 16:01:08","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"Prediction":"## Decision\n\nWe will create a Python library to encapsulate the functionality of submitting, monitoring, and retrieving results from GridLabD jobs. This library will be designed to be agnostic of specific deployment environments and storage solutions. \n\n**Rationale:**\n\n* **Reusability:** A library offers the most flexibility for reuse across different projects, whether they choose to integrate it directly or deploy it as a standalone service.\n* **Decoupling:**  Building a library avoids tight coupling to specific infrastructure choices like Flask or S3, allowing projects to use their preferred tools and configurations.\n* **Testability:** Libraries are inherently more testable than tightly integrated components, leading to more robust and maintainable code. \n\n**Implementation Details:**\n\n* The library will provide a high-level API for interacting with GridLabD jobs, abstracting away the complexities of job submission, monitoring, and result retrieval.\n* It will use a pluggable architecture for storage, allowing projects to implement custom storage handlers for different backends like S3, local filesystems, or databases. \n* The library will be thoroughly documented and accompanied by examples demonstrating its usage in different scenarios. \n\n**Consequences:**\n\n* **Increased Development Effort (Initially):** Designing a well-structured and flexible library requires more upfront effort than a project-specific solution.\n* **Potential Learning Curve:** Projects utilizing the library will need to invest time in understanding its API and integration points.\n* **Long-term Benefits:** The investment in a reusable library will pay off through reduced development time, improved maintainability, and increased consistency across projects. \n","GenTime":"2024-08-19 16:01:15","text":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n\n\n##Decision\nCreate a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n"}
{"File Name":"claim-additional-payments-for-teaching\/0009-capture-teacher-reference-number.md","Context":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n","Decision":"To aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","tokens":42,"id":2098,"Prediction":"## Decision\n\nWe will implement a qualification validation microservice. This service will be responsible for:\n\n* **Storing qualification data:** This includes qualification types, issuing bodies, and validation rules.\n* **Receiving claimant qualification data:**  This data will be sent from other parts of the system, likely during the application process.\n* **Validating qualifications:** The service will compare claimant qualifications against the eligibility criteria, using the stored validation rules.\n* **Returning validation results:** The service will return a clear pass\/fail result along with details of any discrepancies found.\n\nThis approach offers several advantages:\n\n* **Decoupling:** Separating qualification validation logic into a dedicated service improves modularity and maintainability.\n* **Scalability:**  The microservice architecture allows for independent scaling of the validation service as needed.\n* **Flexibility:**  Storing validation rules separately enables easier updates to qualification criteria without requiring code changes. \n\nWe will use a rules engine within the microservice to manage the validation logic. This will allow for flexibility in defining and updating validation rules without requiring code deployments. \n\nWe will use asynchronous communication for sending qualification data and receiving validation results to prevent blocking the main application flow.\n","GenTime":"2024-08-19 16:01:21","text":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n\n\n##Decision\nTo aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n"}
{"File Name":"ftd-scratch3-offline\/0009-use-jackson-to-read-json-files.md","Context":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n","Decision":"We will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","tokens":104,"id":2625,"Prediction":"## Decision\n\nWe will use **Jackson** for deserializing the `project.json` file from Scratch save files. \n\n## Reasons\n\n* **Familiarity:** The team has prior experience with Jackson, which reduces the learning curve and speeds up development.\n* **Feature Richness:** Jackson offers a comprehensive set of features including object mapping, which simplifies the process of converting JSON data into Java objects and vice-versa.  This is essential for working with the structure of a Scratch project.\n* **Comparable to GSON:** While GSON also offers many features, Jackson is generally considered more performant and offers more advanced customization options if needed in the future.\n* **Avoid org.json:**  org.json requires manual object creation, adding complexity and potential for errors. Jackson's object mapping capability eliminates this overhead. \n","GenTime":"2024-08-19 16:01:25","text":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n\n\n##Decision\nWe will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n"}
{"File Name":"rfcs\/0000-template.md","Context":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":60,"id":1879,"Prediction":"## Decision\n\n[decision]: #decision\n\n> This section describes our response to the forces described in the [Context](#context). This is where we record our decision.","GenTime":"2024-08-19 16:01:26","text":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n\n\n##Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n"}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"Prediction":"## Decision\n\nWe will establish a clear and consistent naming convention for GitHub organizations based on the following criteria:\n\n**1. Product or Service:** \n\n* Organizations will primarily align with the product or service the codebase belongs to. \n* For example, code for \"Product A\" would reside in the \"product-a\" organization.\n\n**2. Department or Team:**\n\n* In cases where code repositories are primarily maintained by a specific department or team and don't neatly fall under a single product, we will use a department\/team-based organization.\n* For example, infrastructure-related repositories could reside in a \"platform-engineering\" organization.\n\n**3. Open Source vs. Internal:** \n\n* Separate organizations will be used for open-source projects (\"company-name\") and internal projects (\"company-name-internal\").\n\n**4. Common Libraries and Shared Components:**\n\n* A dedicated organization (\"company-name-common\") will house libraries and components shared across multiple products or teams. \n\n**Decision Table:**\n\n| Criteria                    | Organization Name Structure                | Example                      |\n|-----------------------------|-------------------------------------------|-------------------------------|\n| Product\/Service              | `product-{product-name}`                  | `product-awesome-app`        |\n| Department\/Team            | `team-{team-name}`                        | `team-platform-engineering` |\n| Open Source                 | `company-name`                           | `acme-corp`                 |\n| Internal                    | `company-name-internal`                  | `acme-corp-internal`         |\n| Shared Libraries\/Components | `company-name-common`                    | `acme-corp-common`           | \n\n**Additional Notes:**\n\n* Hyphens will be used to separate words in organization names for better readability.\n* Organization names should be concise and descriptive.\n* We will create a document outlining these rules and examples to ensure clarity and consistency across the engineering team.\n* A process will be established to request and approve the creation of new organizations when needed. \n","GenTime":"2024-08-19 16:01:36","text":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n\n\n##Decision\nWe identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n"}
{"File Name":"architecture\/0003-monitor-condition-and-data-selectors.md","Context":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n","Decision":"Integrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","tokens":90,"id":1417,"Prediction":"## Decision\n\nWe will move away from using config options like `CONF_MONITORED_CONDITIONS` for selecting data points exposed by Home Assistant integrations. \n\nInstead, integrations will expose all reasonable data points as entities by default. Users can then choose which entities to utilize within the Lovelace UI, providing a single point of configuration and eliminating the need for pre-emptive selection of potentially unfamiliar data points. \n","GenTime":"2024-08-19 16:01:38","text":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n\n\n##Decision\nIntegrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n"}
{"File Name":"celestia-core\/adr-004-historical-validators.md","Context":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n","Decision":"For every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","tokens":56,"id":4101,"Prediction":"## Decision\n\nWe will implement a validator set history API to address the need for historical validator information. This API will allow querying the validator set for any given historical height. This will enable light clients and other applications to verify past states and reconstruct the validator set after being offline. \n","GenTime":"2024-08-19 16:01:40","text":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n\n\n##Decision\nFor every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n"}
{"File Name":"ksch-workflows\/0002-using-spring-boot.md","Context":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n","Decision":"Spring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","tokens":46,"id":1758,"Prediction":"## Decision\n\nWe will use a combination of a Dependency Injection (DI) framework and an Object-Relational Mapping (ORM) library to achieve both dependency injection and database access in a decoupled manner.\n\n**DI Framework:** We will choose a DI framework that aligns with the project's language and ecosystem. Some popular options include:\n\n* **Java:** Spring Framework, Google Guice\n* **Python:** Django Dependency Injection, Flask-Injector\n* **.NET:** .NET Core's built-in DI container\n* **JavaScript:** InversifyJS, Angular Dependency Injection\n\n**ORM Library:**  We will choose an ORM library that supports our chosen database technology and offers a clean API for data access. Popular options include:\n\n* **Java:** Hibernate, MyBatis\n* **Python:** Django ORM, SQLAlchemy\n* **.NET:** Entity Framework Core\n* **JavaScript:** TypeORM, Sequelize\n\n**Implementation Details:**\n\n* Define interfaces for database interactions (e.g., `UserRepository`, `ProductRepository`).\n* Implement concrete repository classes using the chosen ORM library.\n* Configure the DI framework to inject repository instances into dependent classes.\n\nThis approach will allow us to:\n\n* Write loosely coupled code by injecting dependencies instead of hardcoding them.\n* Easily swap out different implementations of repositories (e.g., for testing purposes).\n* Abstract away the underlying database technology, making it easier to switch databases in the future. \n","GenTime":"2024-08-19 16:01:47","text":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n\n\n##Decision\nSpring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n"}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"Prediction":"## Decision\n\nWe will replace the existing FlatBuffers-based serialization format for SameDiff models with a new format based on Protocol Buffers (protobuf). \n\n## Reasons\n\n* **Addressing Existing Issues:** Protobuf directly addresses many of the limitations identified in the current FlatBuffers implementation (as outlined in issue #8312). This includes:\n    * **Versioning and Schema Evolution:** Protobuf offers robust support for schema evolution, ensuring backward and forward compatibility as the SameDiff model format evolves.\n    * **Language Support:** Protobuf provides native libraries for a wide range of programming languages, including Java, C++, and Python, facilitating easier interoperability compared to FlatBuffers.\n    * **Debugging and Tooling:**  Protobuf boasts excellent tooling and human-readable text formats, simplifying debugging and inspection of serialized models.\n\n* **Widely Adopted and Mature:** Protobuf is a widely adopted serialization format within the machine learning community and beyond. This translates to well-tested implementations, active community support, and readily available documentation.\n\n* **Performance:** While not the primary driver, Protobuf generally offers comparable or better performance than FlatBuffers in many scenarios.\n\n## Consequences\n\n* **Migration Effort:** Transitioning from the existing FlatBuffers format to Protobuf will require a significant refactoring effort. This includes updating serialization\/deserialization logic, adapting existing model files, and potential adjustments to the SameDiff internal representation. \n* **Learning Curve:** Developers unfamiliar with Protobuf will need to invest time in learning the technology and its associated tools.\n* **Potential Performance Impacts:** While Protobuf is generally performant, rigorous benchmarking will be necessary to ensure minimal impact on existing workflows. \n\n## Status\n\nAccepted\n\n## Notes\n\n* A phased migration strategy should be considered to minimize disruption to existing users and workflows. \n* Thorough documentation and examples should be provided to guide developers through the transition. \n* Performance benchmarking should be conducted at various stages of the implementation to identify and mitigate potential bottlenecks. \n","GenTime":"2024-08-19 16:01:55","text":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n\n\n##Decision\nWe will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n"}
{"File Name":"google-cloud-cpp\/2018-06-13-storage-always-retries.md","Context":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":2466,"Prediction":"## Decision\n\nAdopt an optimistic retry strategy where the library automatically retries all requests, including non-idempotent ones, by default. This approach prioritizes developer experience and simplifies common usage patterns. \n\nProvide a mechanism for application developers to override the default retry behavior. This allows for fine-grained control and customization for specific operations where automatic retries are undesirable or could lead to unintended consequences. \n","GenTime":"2024-08-19 16:01:58","text":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n\n##Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n"}
{"File Name":"ToPS\/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md","Context":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n","Decision":"Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","tokens":38,"id":2080,"Prediction":"## Decision\n\nTo enable the execution of tests containing Prolog queries with SWI Prolog on CircleCI, we will implement one of the following solutions:\n\n**Option 1: Docker Image**\n\n* Create a custom Docker image that includes both the project's dependencies and a SWI Prolog installation.\n* Configure CircleCI to use this custom image for running tests.\n\n**Pros:**\n\n* Provides a consistent and reproducible environment for testing.\n* Isolates the SWI Prolog dependency from the host system.\n* Allows for version control of the SWI Prolog installation.\n\n**Cons:**\n\n* Requires building and maintaining a custom Docker image.\n* May increase build times depending on the image size and network speed.\n\n**Option 2: Remote SWI Prolog Server**\n\n* Set up a dedicated server that runs a SWI Prolog instance accessible over the network.\n* Configure tests to connect to this remote server for executing Prolog queries.\n\n**Pros:**\n\n* Avoids the need for a custom Docker image.\n* Centralizes SWI Prolog installation and maintenance.\n\n**Cons:**\n\n* Introduces an external dependency on the remote server.\n* Requires managing network connectivity and potential security concerns.\n* Might impact test performance due to network latency.\n\n**Option 3: Cloud-Based Prolog Service**\n\n* Utilize a cloud-based Prolog service like SWISH (SWI Prolog for Sharing) or a similar offering.\n* Adapt tests to interact with the chosen cloud service's API for executing Prolog queries.\n\n**Pros:**\n\n* No need to manage local SWI Prolog installations or servers.\n* Potential for scalability and easier collaboration.\n\n**Cons:**\n\n* Introduces dependency on a third-party service and potential costs.\n* Might require significant code modifications depending on the chosen service's API.\n\n\nThe final decision will be based on factors such as:\n\n* Project complexity and requirements\n* Team's familiarity with Docker, server administration, or cloud services\n* Cost considerations for cloud-based solutions\n* Desired level of control over the SWI Prolog environment\n* Security implications of different options\n","GenTime":"2024-08-19 16:02:07","text":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n\n\n##Decision\nChosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n"}
{"File Name":"digital-paper-edit-client\/2019-05-01-security.md","Context":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":4166,"Prediction":"## Decision\n\nWe will implement a two-factor authentication system using a combination of:\n\n1. **SSO (Single Sign-On):** Leveraging existing company credentials for initial login, streamlining the authentication process for journalists familiar with the company systems. \n2. **Time-based OTP (One-Time Password):**  Generated by a mobile app or hardware token for an added layer of security. \n\n**Access Levels:**\n\n* **Public:** Accessible without authentication. Only applies to non-sensitive, publicly shareable information.\n* **Protect:** Requires successful two-factor authentication. Grants access to internal data and tools.\n* **Restrict:** Requires Protect level access plus specific role-based permissions granted by administrators. Reserved for highly sensitive data and critical system functionalities.\n\n**Data Retention:**\n\n* User data associated with inactive accounts (no login for 12 months) will be anonymized and archived according to GDPR guidelines.\n* Administrators can define data retention policies for specific data categories within the application. \n\nThis solution aims to balance security and usability:\n\n* **Security:**  Two-factor authentication mitigates risks associated with compromised passwords. Granular access control safeguards sensitive information. \n* **Usability:** SSO simplifies the login process. OTPs, while adding a step, are familiar to many users and can be seamlessly integrated into existing workflows. \n\nWe acknowledge the increased complexity compared to a single-factor approach. However, we believe the enhanced security and compliance benefits outweigh the added implementation and maintenance overhead.  \n","GenTime":"2024-08-19 16:02:14","text":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n\n\n##Decision\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n"}
{"File Name":"log4brains\/20200925-multi-packages-architecture-in-a-monorepo-with-yarn-and-lerna.md","Context":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n","Decision":"Chosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","tokens":110,"id":1067,"Prediction":"## Decision\n\nWe will adopt a modular architecture with a clear separation of concerns to accommodate current and future requirements. The architecture will consist of the following core components:\n\n**1. Core Logic:**\n- This component will house the business logic for managing ADRs, including parsing, validation, storage, and transformation (e.g., for static site generation). \n- It will be designed as a library, independent of any specific UI or interface.\n- We will use a plain text file format (like Markdown) for ADR storage to ensure simplicity, portability, and compatibility with various tools.\n\n**2. CLI:**\n- A command-line interface built on top of the Core Logic, providing functionalities like:\n    - Creating new ADRs with a predefined template.\n    - Listing existing ADRs.\n    - Validating ADR structure. \n\n**3. Static Site Generator:**\n- A module utilizing the Core Logic to generate a static website from ADR files. \n- The website will enable browsing and navigating the ADRs in a user-friendly manner.\n\n**4. Local Web UI:**\n- A web application, potentially a Single Page Application (SPA), will interact with the Core Logic via an API.\n- Initially, it will provide a read-only preview of the ADRs, similar to the static website.\n-  The architecture will allow for future expansion to support editing and creating ADRs directly within the UI. \n\n**Future Extensions:**\n\n- **VSCode Extension:**  We will develop a separate VSCode extension leveraging the Core Logic via its API. This will allow for seamless ADR management within the IDE.\n- **ADR Aggregation:** A dedicated module can be developed to fetch and aggregate ADRs from multiple repositories, again utilizing the Core Logic for processing and standardization. \n\n**Rationale:**\n\n- **Modularity:**  Separating the system into well-defined components promotes reusability, maintainability, and testability. \n- **Technology Agnosticism:** The Core Logic's independence from specific technologies allows flexibility in choosing the best tools for UI, storage, and future extensions.\n- **Scalability:** The architecture allows for incremental development and integration of new features without significant refactoring.\n\n**Consequences:**\n\n- Requires careful API design for the Core Logic to ensure efficient communication between components.\n- Potential overhead in managing dependencies between modules. \n- Increased initial development time to establish the modular structure. \n","GenTime":"2024-08-19 16:02:24","text":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n\n\n##Decision\nChosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n"}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"Prediction":"## Decision\n\n**We will not implement feature flags directly within Storefront SCSS.**\n\n## Reasons\n\n* **Technical limitations:** Feature flags cannot be evaluated within the SCSS context. Implementing workarounds like conditional classes in templates adds unnecessary complexity and negatively impacts template readability.\n* **Bloated CSS:** Including unused styles from deactivated features increases the compiled CSS file size, impacting page load times and user experience.\n* **Backward compatibility challenges:** Maintaining backward compatibility within SCSS functions, mixins, and variables becomes significantly more complex and error-prone when using feature flags.\n\n## Implications\n\n* **Feature toggling will be handled exclusively through backend and templating logic (e.g., Twig).**\n* **Alternative solutions, such as CSS modules or atomic design principles, should be considered to improve CSS organization and maintainability, mitigating the need for feature flags within SCSS.**\n* **Thorough testing is essential to ensure that UI elements associated with feature-flagged functionality are correctly hidden or displayed based on the flag status.**\n\n## Status\n\nAccepted\n\n## Notes\n\nThis decision may be revisited if a future update to the SCSS pre-processing pipeline or Storefront architecture allows for a more robust and efficient integration of feature flags. \n","GenTime":"2024-08-19 16:02:30","text":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n\n\n##Decision\n* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n"}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"Prediction":"## Decision\n\nWe will use ESLint to lint all JavaScript and TypeScript code in the project. \n\nWe will use a configuration based on a widely adopted style guide (such as Airbnb or StandardJS) and extend it with project-specific rules. This approach provides a strong foundation for code consistency and best practices while allowing us to tailor the rules to our specific needs.\n\nFor TypeScript, we will utilize the officially supported ESLint parser and plugin to leverage ESLint's capabilities for type checking and analysis. \n\nThis decision will be implemented by:\n\n* Adding ESLint as a dev dependency to our project.\n* Configuring ESLint using a configuration file (e.g., `.eslintrc.js`).\n* Setting up appropriate npm scripts to run ESLint during development and in our CI\/CD pipeline. \n\nThis decision will be reviewed if:\n\n* We encounter significant performance issues or limitations with ESLint.\n* A better alternative for linting JavaScript and TypeScript code emerges. \n","GenTime":"2024-08-19 16:02:35","text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n"}
{"File Name":"tracking-consent-frontend\/0002-use-data-attribute-for-gtm-container.md","Context":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n","Decision":"* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","tokens":78,"id":3529,"Prediction":"## Decision\n\n**Option:** Configure the GTM container used by tracking consent via data attributes on the tracking consent component itself, rather than separate Javascript bundles. \n\n**Reason:** This approach prioritizes simplicity and ease of use for service developers. By consolidating configuration to the component level, we eliminate the need for managing separate bundles, streamlining the integration process and reducing potential points of failure. This also improves operability by providing a centralized and transparent configuration point. \n","GenTime":"2024-08-19 16:02:37","text":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n\n##Decision\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n"}
{"File Name":"CrossyToad\/adr-0007-SUPERSEDED-mtl-style-for-affects.md","Context":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n","Decision":"We're using MTL style.\\n","tokens":114,"id":2473,"Prediction":"## Decision\n\nWe will use MTL style for managing side effects. \n\n## Reasons\n\n* **Learning Experience:** The author is interested in gaining practical experience with MTL.\n* **Existing Inspiration:**  The `dino-rush` project, a key source of inspiration, utilizes MTL successfully. This suggests it's a viable approach for similar game development scenarios.\n* **Flexibility and Structure:** MTL provides a balance between structure and flexibility. It allows defining different monad stacks for various concerns (rendering, state updates, etc.) while maintaining composability.\n\n## Consequences\n\n* **Learning Curve:** There might be an initial learning curve associated with understanding and effectively using MTL.\n* **Abstraction Overhead:** MTL introduces a level of abstraction that might feel excessive for very small projects.\n* **Potential for Complexity:**  Improper usage of MTL can lead to complex monad transformer stacks, impacting code readability and maintainability. \n\n## Action Items\n\n*  **Thorough Research:**  Conduct in-depth research and exploration of MTL best practices and patterns.\n*  **Start Small:** Begin by implementing side effects for a small, isolated part of the game to gain familiarity with MTL.\n* **Regularly Review:** Continuously assess the complexity of the monad transformer stack and refactor if necessary to maintain code clarity. \n","GenTime":"2024-08-19 16:02:44","text":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n\n\n##Decision\nWe're using MTL style.\\n"}
{"File Name":"volley-management\/0006-use-azure-table-storage-for-persistence.md","Context":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n","Decision":"* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","tokens":68,"id":2520,"Prediction":"## Decision\n\nWe will adopt **Azure Cosmos DB with the Core (SQL) API** as our persistence mechanism.\n\n## Decision Outcome\n\n* **Chosen option:** Azure Cosmos DB with Core (SQL) API\n* **Rejected option:** Azure SQL Database \n\n## Reasoning\n\nWhile Azure SQL Database provides a robust relational model and features, Azure Cosmos DB with the Core (SQL) API offers a compelling alternative for this situation:\n\n* **Cost-Effectiveness:** Cosmos DB's flexible pricing model, particularly with its consumption-based options, can significantly reduce costs compared to Azure SQL Database, especially for systems without heavy relational requirements.\n* **NoSQL Flexibility:**  The Core (SQL) API allows us to leverage a familiar SQL-like syntax while still benefiting from the schema flexibility and scalability of a NoSQL database. This aligns well with the system's described lack of high complexity. \n* **Built-in Backup & Restore:** Cosmos DB provides automatic backups and point-in-time restore capabilities, addressing our backup concerns without complex configuration.\n\n**Trade-offs:**\n\n* We will forgo some advanced relational database features (e.g., complex joins, stored procedures) available in Azure SQL Database. However, the described simplicity of the system suggests these features won't be critical.\n*  A slight learning curve might exist when transitioning from a purely relational model to the Core (SQL) API.\n\n## Next Steps\n\n1. **Data Modeling:** Design the data model for Cosmos DB, considering its schema flexibility and the application's data access patterns.\n2. **Performance Testing:** Conduct benchmark tests to optimize query performance and ensure responsiveness.\n3. **Security Configuration:**  Implement appropriate access controls and security measures within the Cosmos DB environment. \n","GenTime":"2024-08-19 16:02:51","text":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n\n\n##Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n"}
{"File Name":"devops-challenge\/0009-doc-about-monorepo.md","Context":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n","Decision":"To use a monorepo to keep all code together and deploy it at the same time.\\n","tokens":210,"id":3483,"Prediction":"## Decision\n\nWe will implement a **monorepo approach** for our microservices architecture. This means all microservice codebases will reside in a single repository, organized into separate folders by service. \n\n## Justification\n\nWhile we acknowledge the benefits of polyglot microservices, the risk of incompatibility and the overhead of managing those incompatibilities outweighs the benefits in our current context. A monorepo approach provides several advantages:\n\n* **Simplified Dependency Management:** By keeping all code in one repository, we can more easily track dependencies and ensure compatibility across services. This reduces the risk of unexpected breakages due to incompatible library versions or API changes.\n* **Streamlined Testing and Deployment:** With all code in one place, we can implement a single, unified CI\/CD pipeline. This simplifies testing and deployment processes and ensures that all services are tested and deployed in a consistent manner.\n* **Improved Code Reusability and Collaboration:** A monorepo encourages code sharing and reuse across services, reducing code duplication and promoting consistency. It also makes it easier for developers to work on multiple services simultaneously, fostering better collaboration and knowledge sharing.\n\n## Consequences\n\n* **Increased Repository Size:** The repository size will increase, potentially leading to longer clone times and increased storage requirements. However, with modern version control systems and appropriate tooling, this can be mitigated.\n* **Potential for Merge Conflicts:** With multiple teams working on the same repository, merge conflicts might become more frequent. Implementing clear branching strategies and communication protocols can help minimize this risk.\n* **Loss of Language Flexibility:**  While we lose the flexibility to use different languages for each microservice, this trade-off is acceptable for our current context where minimizing complexity and ensuring system stability are paramount. We can re-evaluate this decision in the future if our needs change. \n\n## Next Steps\n\n*  Define a clear folder structure within the monorepo to organize the code for each microservice.\n*  Establish a comprehensive CI\/CD pipeline for building, testing, and deploying all microservices from the monorepo.\n*  Implement a robust branching strategy and communication protocols to manage contributions from multiple teams and minimize merge conflicts.\n*  Investigate and implement appropriate tooling to manage the larger repository size and improve developer experience. \n","GenTime":"2024-08-19 16:03:01","text":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n\n\n##Decision\nTo use a monorepo to keep all code together and deploy it at the same time.\\n"}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n","Decision":"None yet\\n","tokens":59,"id":4382,"Prediction":"## Decision\n\nWe will upgrade the CloudEvents Java SDK to a version that addresses issue 196 (https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196), eliminating the unnecessary work when converting between `CloudEvent` and `Document` formats. \n\n## Consequences\n\n* **Positive:** Improves performance by reducing unnecessary data serialization and deserialization.\n* **Positive:** Reduces resource consumption (CPU and Memory) during CloudEvent processing.\n* **Negative:**  Potential for breaking changes introduced by the SDK upgrade. This necessitates thorough testing to ensure compatibility. \n\n## Status\n\nAccepted\n\n## Action Items\n\n1. **Investigate:** Identify the latest CloudEvents Java SDK version that resolves issue 196.\n2. **Evaluate:** Assess the impact of the upgrade on the existing codebase, including potential breaking changes.\n3. **Upgrade:**  Upgrade the CloudEvents Java SDK dependency to the chosen version.\n4. **Test:** Conduct thorough testing to ensure the upgrade doesn't negatively impact existing functionality.\n5. **Monitor:** Monitor the application after deployment to detect and address any unforeseen issues related to the upgrade.\n","GenTime":"2024-08-19 16:03:06","text":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n\n\n##Decision\nNone yet\\n"}
{"File Name":"ReportMI-service-manual\/0014-use-gov-uk-notify-for-sending-notifications.md","Context":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n","Decision":"We will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","tokens":188,"id":2050,"Prediction":"## Decision\n\nThe Data Submission Service will use GOV.UK Notify to send notifications to suppliers.\n\n## Reasons\n\n*   **Reduced development effort:** Using GOV.UK Notify eliminates the need to build and maintain our own notification infrastructure.\n*   **Flexibility for future requirements:** GOV.UK Notify supports both email and SMS, allowing us to easily add SMS notifications in the future if required.\n*   **Cost-effectiveness:** Sending email notifications through GOV.UK Notify is free.\n*   **Proven solution:** GOV.UK Notify is a trusted service already in use within CCS.\n\n## Considered Options\n\n*   **Building a custom notification system:** This option was deemed unnecessary and costly, as GOV.UK Notify provides all the functionality needed.\n*   **Using a third-party notification service:** While other services exist, GOV.UK Notify is a government-backed solution already in use within CCS, making it the preferred choice.\n\n## Consequences\n\n*   The Data Submission Service team will need to familiarize themselves with the GOV.UK Notify API and best practices.\n*   The service will depend on the availability and performance of GOV.UK Notify.\n*   We will be bound by the features and limitations of GOV.UK Notify.\n","GenTime":"2024-08-19 16:03:12","text":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n\n\n##Decision\nWe will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n"}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"Prediction":"## Decision\n\nWe will be employing a Function-as-a-Service (FaaS) approach to implement our serverless architecture. This will allow us to break down our application into individual, independently deployable functions that are triggered by specific events. By leveraging FaaS, we can fully realize the benefits of serverless computing, including:\n\n* **Reduced Operational Overhead:** No need to manage servers or infrastructure, allowing our team to focus solely on developing and deploying code.\n* **Enhanced Scalability:** The platform will automatically scale individual functions up or down based on demand, ensuring optimal performance and cost-efficiency.\n* **Increased Agility:**  The modular nature of FaaS simplifies development, testing, and deployment, enabling faster iteration cycles and quicker time-to-market.\n\nWhile FaaS offers significant advantages, we acknowledge potential challenges:\n\n* **Vendor Lock-in:** Selecting a specific cloud provider for FaaS may create dependency and limit future flexibility. \n* **Cold Starts:** Initial function invocation may experience latency due to the time required to allocate resources. \n* **State Management:** Maintaining application state requires careful consideration due to the stateless nature of individual functions.\n\nTo mitigate these challenges, we will:\n\n* **Employ Infrastructure-as-Code:** Define and manage our infrastructure through code to ensure reproducibility and portability across different environments.\n* **Optimize Function Design:** Minimize cold starts by leveraging techniques like function warming and optimizing function dependencies. \n* **Utilize Appropriate State Management Solutions:** Leverage cloud provider-specific services or external tools for managing application state effectively. \n\nThis decision aligns with our commitment to software-defined infrastructure and empowers us to build a scalable, resilient, and cost-effective cloud-native application.\n","GenTime":"2024-08-19 16:03:20","text":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n\n\n##Decision\nWe will build cloud-native solutions.\\n"}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"Prediction":"## Decision\n\nWe will use Markdown Architectural Decision Records (ADRs) to document significant architectural decisions made during the development of Ockam. These ADRs will be version-controlled alongside the Ockam source code in a dedicated \"docs\/adr\" directory.\n","GenTime":"2024-08-19 16:03:21","text":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n"}
{"File Name":"aspan-server\/0003-replacing-ramda-with-lodash.md","Context":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n","Decision":"Decision here...\\n","tokens":22,"id":1890,"Prediction":"## Decision\n\nWe will adopt the use of arrow functions as the preferred syntax for defining functions in JavaScript, except where the specific context necessitates the use of traditional function expressions or declarations. \n","GenTime":"2024-08-19 16:03:23","text":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n\n##Decision\nDecision here...\\n"}
{"File Name":"beis-report-official-development-assistance\/0011-use-govuk-design-system-form-builder.md","Context":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n","Decision":"We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","tokens":83,"id":2386,"Prediction":"## Decision\n\nWe will utilize a Ruby gem specifically designed for integrating the GOV.UK Design System into Rails applications. This gem will provide:\n\n* **View helpers:** To simplify the generation of compliant HTML structures, class names, and ARIA attributes for all form elements and error displays.\n* **Automatic validation error formatting:** Ensuring errors adhere to GOV.UK Design System accessibility guidelines, replacing Rails' default error presentation. \n\nThis approach minimizes manual HTML markup, reduces the risk of errors, and enforces consistency with the design system. \n","GenTime":"2024-08-19 16:03:26","text":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n\n\n##Decision\nWe will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n"}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"Prediction":"## Decision\n\nWe will standardize on using NetworkManager for all new network requests going forward. For existing code using the Fetchable protocol, we will perform a gradual migration to NetworkManager instead of a complete replacement.\n\n## Reasons\n\n* **Consistency:** Utilizing NetworkManager across the codebase promotes consistency and maintainability.\n* **Gradual Migration:**  Direct replacement of Fetchable protocol might be excessively disruptive to the development process. Gradual migration allows us to reap the benefits of NetworkManager while minimizing risks and managing technical debt.\n\n## Consequences\n\n* **Short-term effort:**  Gradual migration requires an initial effort to adapt and integrate NetworkManager alongside existing Fetchable protocol implementations.\n* **Long-term benefits:** Consistency improvements will reduce cognitive load for developers and simplify future development and maintenance. \n* **Potential for code churn:** The migration process might introduce temporary code churn until the transition is complete.\n\n## Action Items:\n\n1. **Define a migration strategy:** Determine the order and prioritization of migrating code sections from Fetchable to NetworkManager. Consider factors like code complexity, usage frequency, and business impact.\n2. **Create documentation and examples:** Provide clear guidelines and examples for using NetworkManager to aid developers in the transition.\n3. **Implement feature parity (if necessary):** Ensure NetworkManager offers all the functionalities currently provided by the Fetchable protocol.\n4. **Monitor and review:** Regularly track the migration progress, gather feedback from developers, and adjust the strategy as needed. \n","GenTime":"2024-08-19 16:03:32","text":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n\n\n##Decision\nEvery new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n"}
{"File Name":"aspan_micro_front_ends\/0004-info-graphql-import.md","Context":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","Decision":"Decision here...\\n","tokens":20,"id":3194,"Prediction":"## Decision\n\nWe will **not** be using a separate mechanism for import and export definitions within our GraphQL SDL files (e.g., GraphQL Modules). \n\n**Reasoning:**\n\n* **Increased Complexity:** Introducing a new system for managing imports and exports adds complexity to our codebase, both in terms of developer understanding and tooling. \n* **Tooling Immaturity:** The tooling ecosystem around GraphQL Modules and similar solutions is still relatively immature compared to standard GraphQL SDL. This could lead to difficulties in development, debugging, and integration with other tools.\n* **Limited Benefits for Current Scope:**  Given the current size and scope of our project, the benefits of modularizing our GraphQL schema using imports and exports are minimal. We anticipate our schema remaining manageable within a single file or a small number of files easily managed without additional tooling.\n\n**Instead, we will:**\n\n* **Structure our schema logically:** We will maintain a well-organized schema within a single file or across a few files for readability.\n* **Leverage code splitting (if applicable):** If our schema grows significantly, we can explore code-splitting techniques supported by our chosen GraphQL server implementation to break down the schema into smaller, more manageable chunks at build time.\n* **Re-evaluate as the project grows:**  We acknowledge that the need for a more robust modularization solution may arise as our project grows in size and complexity. We will revisit this decision if we encounter significant challenges in managing our schema without imports and exports.\n\nThis decision prioritizes simplicity and maintainability in the short term, while acknowledging the potential need for reevaluation as the project evolves. \n","GenTime":"2024-08-19 16:03:40","text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"Prediction":"## Decision\n\nAfter careful consideration of the tradeoffs, we will adopt **GraphQL and Apollo** for the Settings Redesign project. This decision is driven by the following factors:\n\n* **Improved Performance:** GraphQL's ability to fetch only the required data through precise queries can significantly reduce data transfer overhead compared to REST, potentially leading to faster loading times and improved performance, especially over high-latency networks.\n\n* **Enhanced Developer Experience:**\n    * **Simplified Data Fetching:** Apollo Client provides an intuitive and declarative way to fetch data in React components, reducing boilerplate code and improving code readability.\n    * **Strong Typing and Tooling:** GraphQL's type system enables powerful developer tooling like code generation, autocompletion, and schema validation, leading to fewer runtime errors and faster development cycles. \n    * **Centralized Data Management:**  Apollo Server acts as a unified data layer, abstracting the complexities of communicating with various backend services, simplifying the frontend architecture.\n\n* **Future-Proofing:** Adopting GraphQL allows for more flexibility in evolving the API without breaking changes, as new fields can be added to the schema without affecting existing queries.\n\nWhile acknowledging the initial setup time required for implementing GraphQL and Apollo, we believe the long-term benefits in terms of performance, developer productivity, and maintainability outweigh the upfront costs. This decision aligns with the project goals of delivering a fast, responsive, and scalable Settings experience.\n","GenTime":"2024-08-19 16:03:46","text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n\n\n##Decision\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n"}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"Prediction":"## Decision\n\nWe will revert back to the original ODE solver for the following reasons:\n\n* **Stability and Correctness:** The silent failures of the adjoint ODE solver introduce a significant risk of generating incorrect results without any indication of an issue. This lack of transparency can be detrimental to the reliability of the model.\n* **Maintainability and Debugging:**  Silent failures are notoriously difficult to debug and fix. Reverting to a known stable solution reduces complexity and simplifies future maintenance.\n\nWhile the adjoint solver promised faster gradient evaluations, its instability outweighs this benefit in the context of generating reliable results. We prioritize correctness and stability over potential speed improvements.\n\nWe will investigate alternative approaches to improve gradient evaluation performance, such as:\n\n* **Alternative Adjoint Solver Implementations:** Explore different libraries or implementations of the adjoint solver that may offer better stability.\n* **Preconditioning and Regularization Techniques:** Investigate methods for improving the numerical stability of the adjoint solver.\n* **Hybrid Approaches:**  Potentially utilize the adjoint solver for specific parts of the model where stability is not a concern, while using the original solver for sensitive areas. \n\nWe will carefully evaluate any alternative solution for stability and performance before implementation. \n","GenTime":"2024-08-19 16:03:51","text":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n\n\n##Decision\nReverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n"}
{"File Name":"beis-report-official-development-assistance\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2383,"Prediction":"## Decision\n\nWe will implement a two-pronged approach to mitigate the risk of inefficient database queries impacting application performance:\n\n**1. Automated Query Performance Analysis:**\n\n* Integrate a tool into our CI\/CD pipeline to analyze the performance of database queries executed by new code changes. \n* This tool will flag queries exceeding a predefined performance threshold (e.g., execution time, resource consumption).\n* Flagged queries will trigger build warnings, prompting developers to optimize them before merging code. \n\n**2. Database Query Review Guidelines and Training:**\n\n* Establish clear guidelines for writing efficient database queries, including indexing strategies, optimal query structure, and common pitfalls to avoid.\n* Conduct training sessions for developers to familiarize them with the guidelines and best practices for database interaction. \n* Encourage peer code review with a specific focus on database queries to leverage collective knowledge and experience.\n\nThis combined approach aims to proactively identify and prevent inefficient database queries early in the development lifecycle while also fostering a culture of performance awareness among developers. \n","GenTime":"2024-08-19 16:03:56","text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n"}
{"File Name":"raster-foundry\/adr-0013-spark-execution-environment.md","Context":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n","Decision":"Given that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","tokens":162,"id":1681,"Prediction":"## Decision\n\nWe will implement a hybrid approach for our Spark cluster using Amazon EMR. This approach will combine:\n\n* **Long-lived core nodes:** A small number of on-demand or reserved EC2 instances will form the core of our Spark cluster. This core will always be active, minimizing bootstrapping time for new ingests. \n* **Auto-scaling with spot instances:** To handle spikes in ingest requests, we will configure EMR to automatically scale the cluster using spot instances. This will allow us to leverage the cost savings of spot instances while ensuring sufficient resources are available when needed.\n* **Cluster downscaling:**  When not processing ingests, the cluster will automatically downscale to the core nodes, minimizing costs. \n\nThis hybrid approach allows us to:\n\n* **Minimize ingest latency:** By keeping core components active, we significantly reduce the time required to spin up a new cluster for each ingest.\n* **Optimize cost:** Utilizing spot instances for scaling allows us to take advantage of significant cost savings compared to on-demand instances.\n* **Enable efficient resource utilization:** Auto-scaling ensures that we only provision the compute resources necessary for the current workload.\n\nWe will monitor cluster performance and cost closely and fine-tune the configuration of core nodes, auto-scaling policies, and spot instance bidding strategies to achieve the optimal balance between performance and cost. \n","GenTime":"2024-08-19 16:04:02","text":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n\n\n##Decision\nGiven that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n"}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"Prediction":"## Decision\n\nInstead of fetching all the values for a given key, we will implement a new mechanism in CassandraKeyValueService that fetches only the keys and timestamps within a specified time range. This will allow the sweep operation to identify cells eligible for deletion without loading the potentially large cell values into memory. \n\nThis approach has the following advantages:\n\n* **Reduced memory footprint:** By avoiding the retrieval of large cell values, we significantly reduce the memory required by Cassandra during sweep.\n* **Improved performance:** Fetching only keys and timestamps is significantly faster than retrieving all values. \n* **Targeted deletion:** This approach enables us to precisely target and delete only the outdated values, preserving the latest value and potentially improving Cassandra's read performance.\n\nWe will implement this new mechanism as an alternative to getPageWithRangeCreator and use it specifically for the sweep operation. This ensures backward compatibility with existing functionality while addressing the memory issue.\n","GenTime":"2024-08-19 16:04:07","text":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n\n\n##Decision\nWe are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n"}
{"File Name":"opg-data\/0009-api-domain-structure.md","Context":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n","Decision":"We will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","tokens":28,"id":2183,"Prediction":"## Decision\n\nWe will adopt a domain structure centered around core business capabilities rather than specific products. This structure will be organized into bounded contexts, each representing a specific area of responsibility and maintaining its own ubiquitous language and data model.\n\n## Reasoning\n\n* **Product Agnosticity:**  Focusing on business capabilities instead of products creates a flexible and adaptable structure. This allows us to accommodate future changes in product offerings without requiring significant rework of the domain structure. \n* **Consistency:** By applying this structure across the entire opg-data service, we ensure a unified understanding of the domain and facilitate better communication and collaboration between teams.  \n* **Scalability:** Bounded contexts promote maintainability and scalability by allowing independent development and evolution of different parts of the system. \n\n## Consequences\n\n* **Increased Abstraction:** This approach might initially require more upfront analysis and design effort to identify and define the core business capabilities. \n* **Learning Curve:**  Teams will need to familiarize themselves with the concept of bounded contexts and the chosen ubiquitous language within each context. \n\n## Examples of Bounded Contexts\n\nWhile the specific contexts depend on the opg-data service's functionality, some examples could be:\n\n* **User Management:**  Handles user registration, authentication, authorization, and profiles.\n* **Data Ingestion:**  Manages the intake, validation, and transformation of data from various sources. \n* **Data Analysis:** Provides capabilities for processing, analyzing, and generating insights from the data.\n\nThis decision promotes a robust and adaptable domain structure that aligns with the requirement for product agnosticism and consistency throughout the opg-data service. \n","GenTime":"2024-08-19 16:04:14","text":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n\n\n##Decision\nWe will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n"}
{"File Name":"cfo\/0002-use-yarn-install-instead-of-npm-install-for-web.md","Context":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n","Decision":"Sticking to `yarn` for `web` for now.\\n","tokens":23,"id":1320,"Prediction":"## Decision\n\nWe will standardize on using **npm** with **package-lock.json** and remove **yarn.lock** from the repository. \n\n## Reasons\n\n* **Consistency and Reduced Confusion:** Having both lock files introduces potential conflicts and confusion for developers who might not be familiar with both package managers. \n* **npm's Maturity and Ecosystem:** npm is the default package manager for Node.js and enjoys wider community support and tooling integration.  \n* **Avoiding Unnecessary Overhead:** Using both package managers adds complexity without significant benefit in most scenarios.\n\n## Consequences\n\n* **Potential for Minor Dependency Discrepancies:** Switching solely to npm might lead to slight variations in dependency versions compared to a pure yarn setup. We will carefully review the installed packages after switching.\n* **Migration Effort:**  Removing yarn.lock and ensuring consistent usage of npm requires a small migration effort.\n\n## Action Items:\n\n* **[Responsible Person\/Team]** Delete `yarn.lock` from the repository. \n* **[Responsible Person\/Team]** Update any documentation or scripts that reference yarn to use npm.\n* **[Responsible Person\/Team]** Run `npm install` to generate a clean `package-lock.json` based on the `package.json` file.\n* **[Responsible Person\/Team]** Thoroughly test the application after the migration to ensure all dependencies are working as expected. \n","GenTime":"2024-08-19 16:04:20","text":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n\n\n##Decision\nSticking to `yarn` for `web` for now.\\n"}
{"File Name":"karma-sabarivka-reporter\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4002,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences. Each ADR will be a separate Markdown file stored in the  `\/docs\/adr` directory of the repository. \n","GenTime":"2024-08-19 16:04:22","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"Data-Platform-Playbook\/010-using-pytest-for-verifying-pyspark-transformations.md","Context":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n","Decision":"We will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","tokens":128,"id":1012,"Prediction":"## Decision\n\nWe will introduce a testing framework for PySpark scripts within the Data Platform. This framework will focus on unit testing individual functions and transformations using small, controlled datasets. We will use Pytest as the testing framework and leverage the `spark-testing-base` library for creating Spark sessions and asserting dataframes within our test environment. \n\nThis decision prioritizes fast feedback loops for developers and emphasizes testing the core logic of our PySpark transformations. \n","GenTime":"2024-08-19 16:04:24","text":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n\n\n##Decision\nWe will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n"}
{"File Name":"pottery\/0003-pottery-shards-are-markdown-text.md","Context":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n","Decision":"The content of each shard will be in Markdown format.\\n","tokens":65,"id":2510,"Prediction":"## Decision\n\n**Shard Format:** We will use Markdown as the format for shards. \n\n**Rationale:**\n\n* **Ubiquity:** Markdown is a widely understood and used plain text formatting syntax. \n* **Human Readability:** Markdown is designed for optimal readability and ease of writing, both in its raw form and when rendered. This is essential for use in the terminal and text editors.\n* **Tooling:** Numerous command-line tools and text editors offer excellent Markdown support for generation, manipulation, and preview. \n* **Flexibility:** Markdown allows for basic formatting, links, and can be extended to accommodate future needs like code blocks or simple tables.\n\n**Consequences:**\n\n* **Complexity:** While Markdown is relatively simple, it does introduce a slight layer of complexity compared to completely raw text.\n* **Rendering:** We will need a mechanism to render Markdown in the terminal for optimal viewing. Tools like `glow` or `grip` can be leveraged for this purpose.\n\n**Alternatives Considered:**\n\n* **Plain text:**  While simple, it lacks formatting capabilities.\n* **ReStructuredText:**  More powerful than Markdown but steeper learning curve.\n* **HTML:**  Overkill for our use case and less readable in raw form. \n","GenTime":"2024-08-19 16:04:30","text":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n\n\n##Decision\nThe content of each shard will be in Markdown format.\\n"}
{"File Name":"axiomatic\/0001-enabling-configuration-repo-access.md","Context":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n","Decision":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","tokens":161,"id":3595,"Prediction":"## Decision\n\n**Option 2: Use an SSH key pair specifically for dir2consul.** \n\nThis decision is made for the following reasons:\n\n* **Security:** Using a dedicated SSH key pair for dir2consul limits the potential blast radius in case of a security breach. \n* **Cost-Effectiveness:** While it requires an initial setup, this approach avoids recurring costs associated with Service User accounts.\n* **Maintainability:**  Key rotation, though infrequent, can be managed through established processes and potentially automated.\n\nThis approach strikes a balance between security, cost, and maintainability.\n","GenTime":"2024-08-19 16:04:33","text":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n\n\n##Decision\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n"}
{"File Name":"arch\/0036-ha-for-mysql.md","Context":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n","Decision":"1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","tokens":112,"id":2459,"Prediction":"## Decision\n\n\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6570\u636e\u5e93\u67b6\u6784\u4e2d\u5b58\u5728\u7684\u6027\u80fd\u74f6\u9888\u3001\u5b89\u5168\u9690\u60a3\u548c\u9ad8\u53ef\u7528\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u6211\u4eec\u51b3\u5b9a\u91c7\u53d6\u4ee5\u4e0b\u884c\u52a8\uff1a\n\n**1. \u6570\u636e\u5e93\u5347\u7ea7\u548c\u8fc1\u79fb\uff1a**\n\n* \u5c06\u6570\u636e\u5e93\u7248\u672c\u4ece 5.1 \u5347\u7ea7\u5230\u6700\u65b0\u7a33\u5b9a\u7248\u672c\uff0c\u4f8b\u5982 8.0\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u529f\u80fd\u3002\n* \u5c06\u6570\u636e\u5e93\u4ece ECS \u8fc1\u79fb\u5230\u4e91\u6570\u636e\u5e93 RDS\uff0c\u5229\u7528\u4e91\u6570\u636e\u5e93\u63d0\u4f9b\u7684\u81ea\u52a8\u5907\u4efd\u3001\u76d1\u63a7\u3001\u6027\u80fd\u4f18\u5316\u548c\u5b89\u5168\u9632\u62a4\u7b49\u529f\u80fd\u3002\n\n**2. \u5b58\u50a8\u4f18\u5316\uff1a**\n\n* \u5c06\u6570\u636e\u5e93\u78c1\u76d8\u4ece\u666e\u901a\u4e91\u76d8\u66f4\u6362\u4e3a\u9ad8\u6027\u80fd\u4e91\u76d8\uff0c\u4f8b\u5982 ESSD\uff0c\u4ee5\u63d0\u5347 IOPS \u5e76\u89e3\u51b3\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff08\u6700\u9ad8\u4f18\u5148\u7ea7\uff09\u3002\n\n**3. \u9ad8\u53ef\u7528\u67b6\u6784:**\n\n*  \u542f\u7528 RDS \u5b9e\u4f8b\u7684\u9ad8\u53ef\u7528\u67b6\u6784\uff0c\u4f8b\u5982\uff1a\u591a\u53ef\u7528\u533a\u57df\u90e8\u7f72\uff0c\u5b9e\u73b0\u81ea\u52a8\u6545\u969c\u8f6c\u79fb\uff0c\u4fdd\u8bc1\u6570\u636e\u5e93\u7684\u9ad8\u53ef\u7528\u6027\u3002\n\n**4. \u8fde\u63a5\u65b9\u5f0f\u4f18\u5316\uff1a**\n\n*  \u4f7f\u7528\u6570\u636e\u5e93\u8fde\u63a5\u5730\u5740\u66ff\u4ee3 IP \u5730\u5740\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u65b9\u4fbf\u540e\u7eed\u8fdb\u884c\u6570\u636e\u5e93\u7ef4\u62a4\u548c\u8fc1\u79fb\u64cd\u4f5c\u3002\n\n**\u7406\u7531\uff1a**\n\n* \u6570\u636e\u5e93\u7248\u672c\u5347\u7ea7\u548c\u8fc1\u79fb\u5230\u4e91\u6570\u636e\u5e93\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u6027\u80fd\u3001\u5b89\u5168\u548c\u7ef4\u62a4\u65b9\u9762\u7684\u95ee\u9898\u3002\n* \u4f7f\u7528\u9ad8\u6027\u80fd\u4e91\u76d8\u80fd\u663e\u8457\u63d0\u5347 IOPS\uff0c\u89e3\u51b3\u5f53\u524d\u6700\u7d27\u8feb\u7684\u6027\u80fd\u74f6\u9888\u3002\n* \u542f\u7528 RDS \u9ad8\u53ef\u7528\u67b6\u6784\u53ef\u4ee5\u4fdd\u969c\u6570\u636e\u5e93\u7684\u7a33\u5b9a\u6027\u548c\u4e1a\u52a1\u8fde\u7eed\u6027\u3002\n* \u4f7f\u7528\u6570\u636e\u5e93\u8fde\u63a5\u5730\u5740\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u5e93\u8fde\u63a5\u7684\u7075\u6d3b\u6027\u3002\n\n**\u540e\u7eed\u6b65\u9aa4\uff1a**\n\n1. \u5236\u5b9a\u8be6\u7ec6\u7684\u6570\u636e\u5e93\u5347\u7ea7\u548c\u8fc1\u79fb\u65b9\u6848\uff0c\u5305\u62ec\u6570\u636e\u8fc1\u79fb\u3001\u517c\u5bb9\u6027\u6d4b\u8bd5\u3001\u56de\u6eda\u8ba1\u5212\u7b49\u3002\n2.  \u8bc4\u4f30\u4e0d\u540c\u4e91\u76d8\u7c7b\u578b\u7684\u6027\u80fd\u548c\u6210\u672c\uff0c\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6848\u3002\n3.  \u914d\u7f6e RDS \u9ad8\u53ef\u7528\u67b6\u6784\uff0c\u5e76\u8fdb\u884c\u6545\u969c\u5207\u6362\u6f14\u7ec3\uff0c\u9a8c\u8bc1\u65b9\u6848\u7684\u6709\u6548\u6027\u3002\n4.  \u4fee\u6539\u4e1a\u52a1\u7aef\u6570\u636e\u5e93\u8fde\u63a5\u65b9\u5f0f\uff0c\u4f7f\u7528\u6570\u636e\u5e93\u8fde\u63a5\u5730\u5740\u8fde\u63a5\u6570\u636e\u5e93\u3002\n\n\n**\u5907\u9009\u65b9\u6848\uff1a**\n\n*  \u5982\u679c\u4e1a\u52a1\u5bf9\u6210\u672c\u654f\u611f\uff0c\u53ef\u4ee5\u8003\u8651\u81ea\u5efa\u6570\u636e\u5e93\u9ad8\u53ef\u7528\u67b6\u6784\uff0c\u4f8b\u5982\uff1a\u57fa\u4e8e MySQL \u7684 MHA \u67b6\u6784\u3002\n\n**\u51b3\u7b56\u7684\u5f71\u54cd\uff1a**\n\n* \u63d0\u5347\u6570\u636e\u5e93\u6027\u80fd\uff0c\u89e3\u51b3 IOPS \u74f6\u9888\u95ee\u9898\u3002\n* \u589e\u5f3a\u6570\u636e\u5e93\u5b89\u5168\u6027\uff0c\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u3002\n* \u5b9e\u73b0\u6570\u636e\u5e93\u9ad8\u53ef\u7528\uff0c\u4fdd\u969c\u4e1a\u52a1\u8fde\u7eed\u6027\u3002\n* \u63d0\u9ad8\u6570\u636e\u5e93\u8fd0\u7ef4\u6548\u7387\uff0c\u964d\u4f4e\u8fd0\u7ef4\u6210\u672c\u3002 \n","GenTime":"2024-08-19 16:04:43","text":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n\n\n##Decision\n1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n"}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"Prediction":"## Decision\n\nWe will use **Emotion** as our CSS-in-JS library. \n\n## Rationale\n\nWhile Styletron offers the smallest possible CSS output, its limitations in supporting common CSS features like nesting and sibling selectors would hinder our development process and potentially limit design possibilities. \n\nEmotion provides a good balance between functionality and performance:\n\n* **Feature-rich:** It supports advanced CSS features like nesting and sibling selectors, offering greater flexibility in styling.\n* **Familiar API:**  Its API, similar to Styled Components,  is easy to learn and use, particularly for developers familiar with styled-components.\n* **Lightweight:** It is significantly smaller than Styled Components, which translates to better performance. \n* **Flexibility:** The ability to add styles directly to the `css` prop on React components offers additional convenience for simpler styling scenarios.\n\nThis combination makes Emotion a robust and practical choice for our project. \n","GenTime":"2024-08-19 16:04:47","text":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n\n\n##Decision\nWe will use Emotion as our CSS-in-JS library.\\n"}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"Prediction":"## Decision\n\nWe will use **Gunicorn** as our production-ready WSGI server for serving the Flask API. \n\n## Decision Rationale\n\n* **Production Ready:** Gunicorn is a pre-fork worker model based server that is specifically designed for robust and high-performance deployment of Python web applications, including Flask. \n* **Reverse Proxy Compatibility:** Gunicorn works seamlessly behind popular reverse proxies such as Nginx and Apache. This architecture allows for efficient request handling, load balancing, and security enhancements.\n* **Simplicity with Flask:** Gunicorn is straightforward to configure and use with Flask applications. Its minimalist approach ensures easy integration and minimal overhead.\n* **Performance:** Gunicorn's pre-fork model, where multiple worker processes handle requests concurrently, ensures efficient utilization of system resources and responsiveness even under high traffic loads. \n\n## Considered Alternatives\n\n* **Waitress:** While Waitress is a capable and pure-Python WSGI server, Gunicorn is generally favored for its performance advantages, especially in production deployments. \n* **uWSGI:** uWSGI is another powerful option; however, its broader feature set might introduce unnecessary complexity for our Flask API deployment. Gunicorn provides a more focused and streamlined approach for our specific requirements.\n\n## Consequences\n\n* **Positive:** Improved performance, scalability, and reliability in a production environment.\n* **Positive:** Ease of deployment and configuration with both Flask and common reverse proxy servers.\n* **Neutral:** Requires familiarization with Gunicorn's configuration options. \n\nThis decision aligns with our goals of using a production-ready server that seamlessly integrates with our technology stack while prioritizing simplicity and performance. \n","GenTime":"2024-08-19 16:04:55","text":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n\n\n##Decision\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n"}
{"File Name":"terraform-aws-dynamodb\/0004-terraform-requirements-in-module.md","Context":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n","Decision":"The Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","tokens":46,"id":2536,"Prediction":"## Decision\n\nWe will implement Terraform version constraints and minimum provider version constraints in all new LIC modules.  Existing modules will be reviewed and updated on a case-by-case basis, prioritizing widely used and critical modules. \n","GenTime":"2024-08-19 16:04:56","text":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n\n\n##Decision\nThe Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n"}
{"File Name":"DunkMe\/0003-relational-database.md","Context":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","Decision":"Chosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","tokens":78,"id":1661,"Prediction":"## Decision\n\n**Option Selected:** Microsoft SQL\n\n**Reasoning:**\n\nWhile all listed options are capable relational databases, consistency and integration with the existing Trade Me technical ecosystem are paramount.  Choosing Microsoft SQL leverages existing:\n\n* **Expertise:** Trade Me already possesses significant in-house knowledge and experience with Microsoft SQL. This reduces the learning curve for new developers and streamlines maintenance.\n* **Infrastructure:** It is highly likely Trade Me already has infrastructure and tooling in place to support Microsoft SQL deployments, backups, and monitoring. \n* **Potential Synergies:** Using the same database technology across projects may enable code reuse, simplified data sharing, and streamlined integration with other Trade Me systems.\n\nThis approach minimizes friction and maximizes efficiency during development and ongoing operations.\n","GenTime":"2024-08-19 16:05:00","text":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n\n##Decision\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"Prediction":"## Decision\n\nWe will model non-CRUD operations as resources themselves, utilizing a **\"Command Pattern\"** approach within our RESTful API. \n\nThis means:\n\n1. **Dedicated Endpoints for Actions:** Each unique operation, including state transitions and command executions, will be represented by a dedicated endpoint. \n    *  Instead of shoehorning actions into verbs on existing resources, we'll use nouns to represent them. For example, instead of `POST \/objects\/{id}\/transition`, we'd have `POST \/objects\/{id}\/transitions\/{transitionName}`.\n2. **Request Bodies as Commands:** The request body sent to these endpoints will represent the \"command\" to be executed. \n    * This allows for flexibility in parameters, clearly defining the input required for the operation.\n3. **Response Bodies as Command Results:** The response will encapsulate the outcome of the command, including success\/failure status, potential error messages, and any relevant data generated. \n    * We can leverage HATEOAS links to indicate the next possible actions or state transitions based on the result.\n\n## Consequences\n\n**Advantages:**\n\n* **Clear Semantics:** Provides a more descriptive and understandable API structure, clearly communicating the available actions.\n* **Flexibility:**  Allows for operations with the same name but different logic to coexist peacefully.\n* **HATEOAS Integration:** Simplifies the inclusion of links to guide clients through possible workflows.\n* **Extensibility:** Adding new fields to request\/response bodies for future operations is straightforward.\n\n**Disadvantages:**\n\n* **Increased Endpoint Count:**  Potentially leads to a larger API surface, requiring careful design and documentation. \n* **Potential for Verboseness:** May require more endpoints than a verb-centric approach for simpler actions.\n\n**Mitigation:**\n\n* **Careful Endpoint Design:** Employ consistent naming conventions and logical resource hierarchy to maintain API clarity.\n* **Comprehensive Documentation:**  Provide detailed documentation with examples to guide API consumers. \n","GenTime":"2024-08-19 16:05:09","text":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n\n##Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n"}
{"File Name":"island.is\/0001-use-nx.md","Context":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n","Decision":"- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":1117,"Prediction":"## Decision\n\nWe will adopt **Nx** as our monorepo management tool. \n\n## Justification\n\n* **Low complexity and overhead:** Nx is known for its developer experience, offering a low learning curve and minimal boilerplate. Its integration with popular frameworks reduces configuration overhead. \n* **Fit for our stack:** Nx has robust support for [mention your tech stack, e.g., TypeScript, React, Node.js]. This ensures smooth integration with our existing codebase and workflows.\n* **Optimized CI\/CD:** Nx's powerful dependency graph analysis and built-in caching mechanisms will significantly speed up our CI\/CD pipelines, especially for larger projects with shared codebases. This directly addresses our scaling concerns.\n* **Flexibility:** Nx allows for incremental adoption and doesn't enforce strict architectural patterns, making it adaptable to our future needs and evolution.\n\n## Considered Options\n\n* **Other monorepo tools:** [List and briefly explain why you didn't choose the other options, e.g., Lerna, Yarn workspaces, Bazel. This could include factors like community support, learning curve, specific features, or limitations.] \n\n## Consequences\n\n* **Positive:** Improved developer experience, faster CI\/CD, better code sharing and maintainability.\n* **Negative:**  Potential learning curve for developers unfamiliar with Nx. This will be mitigated with dedicated onboarding resources and documentation.\n\n## Documentation\n\n* **Nx Documentation:** [Link to Nx Documentation]\n* **Onboarding Guide:** [Link to internal onboarding resources if applicable]\n","GenTime":"2024-08-19 16:05:15","text":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n\n\n##Decision\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"Prediction":"## Decision\n\nWe will implement a checkpointing and recovery mechanism for the ETL process. This will allow the process to resume from the last successful checkpoint instead of restarting from the beginning in case of an interruption. \n\nThis decision prioritizes fault tolerance and efficiency over the increased complexity of implementing and managing checkpoints. \n","GenTime":"2024-08-19 16:05:17","text":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n\n\n##Decision\nThe ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n"}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"Prediction":"## Decision\n\nWe will not directly support images\/sprites in the ftduino extension for Scratch. This limitation stems from the ftduino hardware's inability to display graphical content. \n\nWe will investigate alternative approaches to represent visual elements, such as:\n\n* **Sound effects:** Mapping image\/sprite appearances to distinct sound cues.\n* **LED manipulation:** Utilizing the ftduino's LEDs to provide basic visual feedback correlated to image\/sprite properties (e.g., color, position).\n* **Abstraction:**  Allowing users to define custom behaviors in their Scratch programs that react to image\/sprite events, effectively abstracting the visual element into other ftduino-compatible actions. \n\nWe will prioritize these alternative approaches based on user feedback and feasibility.\n","GenTime":"2024-08-19 16:05:21","text":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n\n\n##Decision\nScratch images\/sprites and image\/sprite related blocks are not supported.\\n"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions. Each ADR will capture the context, decision, consequences, and other relevant information in a consistent and searchable format. We will utilize a lightweight Markdown-based template for each ADR and store them in a dedicated \"docs\/adrs\" directory within the project repository.\n","GenTime":"2024-08-19 16:05:23","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"Prediction":"## Decision \n","GenTime":"2024-08-19 16:05:24","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n"}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"Prediction":"## Decision\n\nWe will implement a mechanism to call Python functions from Matlab without spawning a new Python interpreter. This will be achieved by directly interfacing with the Python C API from within the Matlab MEX environment. This approach avoids the overhead and conflicts associated with multiple Python interpreters while maintaining performance.\n","GenTime":"2024-08-19 16:05:26","text":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n\n\n##Decision\nAt a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n"}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"Prediction":"## Decision\n\nWe will implement a custom `Result` type as part of the `ockam_core` crate to represent operations that may fail. This approach offers several benefits:\n\n* **Flexibility:** The custom `Result` type will allow us to define specific error types for different parts of the Ockam library and third-party crates. This granularity enables fine-grained error handling and reporting.\n* **Portability:** By defining our own error types, we decouple our error handling from platform-specific implementations. This makes the library more portable and consistent across different environments. \n* **Developer Experience:** The `Result` type will be designed with convenience in mind, providing helper functions for common error-handling tasks. We will strive for an ergonomic API that simplifies both error propagation and handling for both library developers and users.\n\n**Implementation Details:**\n\n* A new `Result<T, OckamError>` type will be introduced in `ockam_core` crate.\n* `OckamError` will be an enum that encapsulates various error categories and their corresponding underlying causes. \n* Each module within the Ockam library and third-party crates can define their own specific error types, which will be variants of the `OckamError` enum.\n* Helper functions and traits will be provided to streamline error creation, propagation, conversion to external error types (e.g., `std::error::Error`), and user-friendly display.\n\n**Example:**\n\n```rust\n\/\/ ockam_core\/src\/error.rs\npub type Result<T> = std::result::Result<T, OckamError>;\n\n#[derive(Debug)]\npub enum OckamError {\n    \/\/ ... other error types\n    TransportError(TransportError), \n}\n\n\/\/ ockam_transport_tcp\/src\/error.rs\n#[derive(Debug)]\npub enum TransportError {\n    ConnectionRefused,\n    \/\/ ... other transport-specific errors\n}\n```\n\nThis custom error handling approach addresses the key requirements:\n\n* **Agile:**  Specific error types within the `OckamError` enum provide context and granularity.\n* **Portable:** Abstracting error handling logic into a custom type promotes environment independence.\n* **Convenient:** Helper functions and a well-defined structure simplify error management for both library developers and users. \n","GenTime":"2024-08-19 16:05:36","text":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n\n\n##Decision\nIn search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n"}
{"File Name":"educational-platform\/0010-use-axon-framework.md","Context":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n","Decision":"Axon Framework will be used as DDD library.\\n","tokens":32,"id":2498,"Prediction":"## Decision\n\nWe will adopt a DDD library for implementing Domain-Driven Design building blocks instead of relying solely on custom classes. \n\n## Reasoning\n\n* **Reduced Boilerplate:** DDD libraries provide pre-built implementations of common DDD building blocks (e.g., Entities, Value Objects, Aggregates, Repositories), reducing the amount of boilerplate code we need to write and maintain.\n* **Improved Consistency:** Using a library enforces a consistent structure and approach to DDD implementation across the codebase, improving readability and maintainability.\n* **Focus on Business Logic:** Offloading the implementation of infrastructure concerns to a library allows developers to focus on the core business logic and domain modeling.\n* **Community Support:** Established DDD libraries typically have active communities and good documentation, providing valuable resources and support.\n\n## Consequences\n\n* **Learning Curve:** Introducing a new library will require developers to invest time in learning its APIs and conventions.\n* **Library Dependency:**  The project will become dependent on the chosen DDD library, which could pose challenges if we decide to switch libraries in the future.\n* **Potential Overkill:** For smaller projects with less complex domains, a full-blown DDD library might be overkill and introduce unnecessary complexity.\n\n## Next Steps\n\n1. **Research and Selection:**  Evaluate different DDD libraries based on factors such as maturity, community support, features, and compatibility with our tech stack.\n2. **Pilot Project:** Implement a small, non-critical feature or module using the chosen library to gain practical experience and assess its suitability.\n3. **Gradual Adoption:** If the pilot is successful, gradually migrate existing code to use the DDD library, prioritizing areas that would benefit most from its features.\n4. **Documentation and Training:** Document the library's usage and best practices within the project and provide training to team members. \n","GenTime":"2024-08-19 16:05:44","text":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n\n\n##Decision\nAxon Framework will be used as DDD library.\\n"}
{"File Name":"sdbmss\/0008-rdf-sparql-support.md","Context":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n","Decision":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","tokens":94,"id":1839,"Prediction":"## Decision\n\nGiven the lack of a unified data model, the SDBM will export its data in RDF using a custom ontology based on its existing relational database schema. This approach allows for the immediate creation of an RDF dataset while retaining the flexibility to map to a shared ontology in the future. \n","GenTime":"2024-08-19 16:05:46","text":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n\n\n##Decision\nThe project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n"}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"Prediction":"## Decision\n\nWe will adopt a multi-account strategy within the Modernisation Platform, separating application environments into distinct AWS accounts. Each environment (e.g., development, staging, production) will reside in a dedicated account. \n","GenTime":"2024-08-19 16:05:48","text":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n\n\n##Decision\nWe've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n"}
{"File Name":"cljdoc\/0014-add-support-for-examples.md","Context":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n","Decision":"Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","tokens":28,"id":2616,"Prediction":"## Decision\n\nWe will implement a system for incorporating user-submitted examples into cljdoc. This system will prioritize discoverability, ease of contribution, and tight integration with the existing documentation structure. \n\nWe will achieve this by:\n\n* **Storing examples alongside documentation:**  Examples will be directly associated with specific namespaces, functions, or macros within a library's documentation on cljdoc. This ensures users encounter relevant examples while browsing the API documentation.\n* **Facilitating community contributions:**  We will provide a clear and straightforward mechanism for users to submit, edit, and curate examples. This could involve a dedicated section on the cljdoc website or integration with existing platforms like GitHub.\n* **Prioritizing runnable examples:** Whenever possible, examples will be executable and verifiable. This could involve leveraging tools like ClojureScript or other REPL-based environments to allow users to experiment with code snippets directly within cljdoc.\n* **Implementing a moderation system:** To maintain example quality and relevance, a moderation system will be put in place. This could involve community moderation, automated checks, or a combination of both.\n\nThis decision aims to enhance the learning and exploration experience for cljdoc users by leveraging community knowledge and providing practical code examples alongside API documentation. \n","GenTime":"2024-08-19 16:05:53","text":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n\n\n##Decision\nAdd support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n"}
{"File Name":"alfresco-anaxes-shipyard\/0002-docker-registry-for-internal-and-protected-images.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n","Decision":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","tokens":130,"id":3598,"Prediction":"## Decision\n\nWe will standardize on **Amazon Elastic Container Registry (ECR)** as our Docker Image Registry for both internal and protected tiers.\n\n## Reasons\n\n* **Meets Requirements:** ECR fulfills our core requirements for access control, security scanning, scalability, global performance, and usability. It offers fine-grained IAM integration for access control, built-in vulnerability scanning, and is a fully managed service with high availability and scalability.\n* **Minimizes Operational Overhead:** As a managed service, ECR significantly reduces the burden on our already stretched IT resources. We avoid deploying, configuring, and maintaining yet another system.\n* **Cost-Effective:** ECR offers a tiered pricing model, making it cost-effective for both our internal usage and potential external customer\/partner access.\n* **Integration with Existing Infrastructure:**  Leveraging ECR aligns with our existing AWS infrastructure, simplifying management and potentially reducing costs through integrated tooling and services.\n\n## Considered Options\n\n* **Nexus3 Upgrade:** While upgrading our current Nexus3 deployment was considered, it doesn't address our concerns about scalability, global performance, or the significant maintenance overhead it would require from our IT team.\n* **Self-Hosted Open-Source Solution:**  Self-hosting an alternative registry solution like Harbor was evaluated but ultimately rejected due to the increased operational burden it would place on our IT resources. \n* **Other Cloud-Based Registries:**  Other cloud-based registries like Google Container Registry (GCR) and Azure Container Registry (ACR) were considered. However, ECR's tighter integration with our existing AWS infrastructure and potentially lower cost made it the more attractive option.\n\n## Consequences\n\n* **Migration Effort:** Transitioning from Nexus3 to ECR will require a migration effort for existing images and pipelines.\n* **Vendor Lock-in:** Adopting ECR increases our reliance on AWS services.\n\n## Mitigation\n\n* **Migration Plan:**  We will develop and execute a phased migration plan to minimize disruption and risk during the transition from Nexus3 to ECR.\n* **Vendor Lock-in Awareness:** While we acknowledge the increased vendor lock-in, the benefits of ECR outweigh the risks in this particular context. We will continue to evaluate multi-cloud strategies in the future as our needs evolve. \n","GenTime":"2024-08-19 16:06:03","text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n\n##Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n"}
{"File Name":"corona-hackathon\/0010-branching-strategy.md","Context":"## Context\\nWe have to make a decision on the branching strategy for development.\\n","Decision":"[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","tokens":17,"id":3653,"Prediction":"## Decision\n\nWe will adopt a **trunk-based development** branching strategy with the following details:\n\n* **Main Branch:** All development happens directly on the `main` branch or short-lived feature branches that are merged back into `main` frequently (at least once a day).\n* **Feature Branches:**  Short-lived branches used for implementing specific features or bug fixes. They should be kept small and focused to minimize merge conflicts.\n* **Release Branches:** Created from the `main` branch when preparing for a release. Only bug fixes related to the release should be committed to these branches. \n* **Hotfix Branches:** Created from the relevant release branch to fix critical bugs in production. Hotfixes are then merged back into both the release branch and `main`.\n\n## Rationale\n\nWe chose trunk-based development for the following reasons:\n\n* **Faster delivery:**  Frequent integration reduces merge conflicts and helps deliver value to users faster.\n* **Improved collaboration:** Encourages developers to collaborate more closely and share their work more frequently.\n* **Reduced risk:** Continuous integration helps to identify and address integration problems early on.\n* **Simplified workflow:**  Fewer branches make the workflow simpler and easier to manage.\n\n## Consequences\n\n* **Increased need for communication:** Developers need to communicate frequently to avoid conflicting changes.\n* **Potential for instability on the main branch:**  Frequent integration can introduce temporary instability. This can be mitigated by using practices like feature toggles and comprehensive testing. \n\nThis strategy will be reviewed periodically to ensure it continues to meet the needs of the project and team. \n","GenTime":"2024-08-19 16:06:10","text":"## Context\\nWe have to make a decision on the branching strategy for development.\\n\n\n##Decision\n[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n"}
{"File Name":"ReportMI-service-manual\/0011-use-auth0-for-temporary-user-authentication.md","Context":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n","Decision":"We will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","tokens":179,"id":2041,"Prediction":"## Decision\n\n**For the initial phase of development, we will use a simple, off-the-shelf authentication service (e.g., Auth0, Firebase Authentication) that offers easy integration and rapid prototyping.** \n\nThis decision prioritizes speed and agility in the short term, allowing us to focus on developing and iterating on core application features. \n\n**We will design the authentication layer with a clear separation of concerns, abstracting the specific authentication provider behind a well-defined interface.** This will enable us to easily migrate to SRS\/CCS G-Suite integration in the future without significant refactoring of the application codebase. \n\n**We will document a migration plan outlining the steps, timeline, and resources required to transition to SRS\/CCS G-Suite authentication.** This plan will ensure a smooth and timely integration with the long-term authentication solution. \n","GenTime":"2024-08-19 16:06:15","text":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n\n##Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n"}
{"File Name":"libelektra\/lookup_every_key.md","Context":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n","Decision":"Every key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","tokens":51,"id":1310,"Prediction":"## Decision\n\nWe will enforce a consistent approach for key lookup in data structures like maps and arrays. \n\n**For maps:** All key lookups will be performed using the dedicated `containsKey` or equivalent methods provided by the map implementation. Direct iteration through the key set should be avoided for lookup purposes.\n\n**For arrays:**  Key lookups are not natively supported as arrays are indexed by sequential integers.  If an associative lookup is needed, we will either:\n\n* **Use a map instead:**  If the data naturally fits a key-value structure, switching to a map is preferred.\n* **Linear search:** For small arrays where performance impact is negligible, a linear search can be used.  This should be clearly documented.\n\nThis decision promotes consistency and avoids potential performance pitfalls of iterating through key sets when dedicated lookup methods are available. \n","GenTime":"2024-08-19 16:06:19","text":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n\n##Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n"}
{"File Name":"where-away\/0009-use-xml-for-bookmarks-format.md","Context":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n","Decision":"Use XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","tokens":180,"id":2274,"Prediction":"## Decision\n\nWe will use **XML** to store the bookmarks file. While not natively supported by Node.js, the advantages outweigh the drawbacks:\n\n* **Clear Tree Representation:** XML excels at representing hierarchical data like bookmarks. \n* **Less Error-Prone Editing:** Compared to YAML, the lack of significant whitespace makes XML more robust for frequent manual editing.\n* **Lightweight Parsing Solution:**  We'll utilize the `xmldom` library due to its lack of dependencies, minimizing project overhead.\n* **Simplified Schema Validation:** The bookmark schema is simple enough to allow for a hand-rolled validator, avoiding the need for additional libraries.\n\nAlthough this choice introduces the need for a parsing library, `xmldom`'s lightweight nature and the benefits of XML for our specific use-case make this the most suitable option. \n","GenTime":"2024-08-19 16:06:23","text":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n\n\n##Decision\nUse XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n"}
{"File Name":"ODT_OnlineBank\/ARD-0001.md","Context":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n","Decision":"*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","tokens":220,"id":1665,"Prediction":"## Decision\n\nWe will develop the initial version of the Online Bank sample application using a Java Web stack.  This decision is based on the following factors: \n\n* **Familiarity:**  The ODT team has significant experience with Java and related web technologies. This will allow for faster development and easier maintenance of the initial application. \n* **Community Support:**  Java has a large and active community, providing access to a wealth of resources, libraries, and support. This will be beneficial for both the development team and the students using the application for learning. \n* **Future Flexibility:** While we are initially focusing on Java, the architecture of the application will be designed with modularity and flexibility in mind.  This will allow for easier adaptation to a Microsoft stack in the future, should the need arise. \n\nWe acknowledge the potential need for a Microsoft stack example in the future.  By focusing on a well-structured and modular architecture, we aim to minimize the effort required for a potential port to the Microsoft stack. \n","GenTime":"2024-08-19 16:06:28","text":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n\n##Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n"}
{"File Name":"texas-holdem-code-challenge\/0009-use-typescript-3-4-5.md","Context":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n","Decision":"Use TypeScript 3.4.5\\n","tokens":158,"id":2296,"Prediction":"## Decision\n\nWe will use TypeScript version 3.4.5 for this project.\n\n## Reasons\n\n* Using an unsupported TypeScript version with the ESLint plugin may lead to unexpected behavior and make debugging difficult.\n* We do not require the new features introduced in TypeScript 3.5.1 for this project.\n* TypeScript 3.4.5 is a recent stable release and should not present any compatibility issues.\n\n## Consequences\n\n* We will not be able to leverage the latest TypeScript features in 3.5.1.\n* We may need to upgrade to a newer, supported TypeScript version when the ESLint plugin adds support. \n","GenTime":"2024-08-19 16:06:31","text":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n\n\n##Decision\nUse TypeScript 3.4.5\\n"}
{"File Name":"govuk-aws\/0009-environment-bootstrapping-process.md","Context":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n","Decision":"The chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","tokens":80,"id":4025,"Prediction":"## Decision\n\nWe will implement an infrastructure-as-code approach using Terraform to automate the bootstrapping of a new Amazon environment. \n\nThis decision is driven by the following factors:\n\n* **Automation:** Terraform enables us to define and provision infrastructure components (VPC, subnets, security groups, EC2 instances, databases, etc.) in a declarative and repeatable manner.\n* **Version Control:** Terraform configurations are plain text files that can be versioned, reviewed, and collaborated on, ensuring consistency and traceability.\n* **Modularity and Reusability:** Terraform allows us to break down the infrastructure into reusable modules, simplifying management and promoting consistency across environments.\n* **State Management:** Terraform tracks the state of the deployed infrastructure, allowing for efficient updates and preventing configuration drift.\n* **Integration:** Terraform integrates seamlessly with other DevOps tools and services, including CI\/CD pipelines and configuration management systems.\n\n### Bootstrapping Process\n\nThe bootstrapping process will consist of the following high-level steps:\n\n1. **Environment Preparation:** An operator will execute a script that sets up the required Terraform workspace, defines variables specific to the environment (e.g., environment name, region), and retrieves any secrets from secure storage.\n2. **Infrastructure Provisioning:**  Terraform will be invoked to create the base infrastructure components defined in the configuration files. This will include elements like VPC, subnets, security groups, IAM roles, and S3 buckets.\n3. **Operator Checkpoint:**  At this stage, the operator will have the opportunity to review the provisioned infrastructure and verify its correctness using the AWS console or CLI. This ensures that the basic foundation of the environment meets the requirements.\n4. **Application Deployment:** Once the operator approves the infrastructure, Terraform will provision the necessary resources for the application, such as databases, load balancers, and application servers. \n5. **Data Restoration:**  The bootstrapping process will include a step to restore data from backups to the newly provisioned environment.\n6. **Validation and Testing:**  After the environment is fully set up, automated tests will be run to validate the functionality and performance of the application.\n\n### Monitoring and Logging\n\nThroughout the bootstrapping process, detailed logs and metrics will be collected and stored in a centralized logging system for monitoring, troubleshooting, and auditing purposes.\n\nThis approach addresses the requirements outlined in the context by providing a heavily automated yet controllable process for bootstrapping a new Amazon environment.\n","GenTime":"2024-08-19 16:06:42","text":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n\n\n##Decision\nThe chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n"}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"Prediction":"## Decision\n\nWe will implement support for multiple, remote TOSCA repositories in addition to the existing local repository. This will be achieved by allowing users to define a set of URLs, each representing a remote repository. These URLs will be configurable and stored within the Winery Repository.  Upon access, the Winery Repository will aggregate data from these remote repositories alongside its own, presenting a unified view to developers. \n\nThis approach balances the need for developer convenience with the flexibility to leverage existing infrastructure and avoid unnecessary data duplication. \n","GenTime":"2024-08-19 16:06:45","text":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n\n\n##Decision\nOption 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n"}
{"File Name":"simple-android\/001-screen-controllers.md","Context":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n","Decision":"Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","tokens":137,"id":1133,"Prediction":"## Decision\n\nWe will implement a unidirectional data flow architecture following the Model-View-Intent (MVI) pattern for our Android application. This pattern clearly separates business logic, UI state management, and user interactions, making our codebase more testable, maintainable, and scalable.\n","GenTime":"2024-08-19 16:06:47","text":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n\n\n##Decision\nEvery screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n"}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"Prediction":"## Decision\n\nTo ensure compatibility between the current and next-generation SDKs while minimizing maintenance overhead, we will adopt a **phased approach with a compatibility layer:**\n\n**Phase 1: Compatibility Layer and Parallel Development (Short-term)**\n\n* Introduce a compatibility layer that translates calls from the current `DecentralandInterface` to the next-gen SDK equivalents.\n* Develop the next-gen SDK as a separate codebase, focusing on new features, performance improvements, and a cleaner architecture.\n* This allows existing scenes to continue functioning seamlessly using the current SDK and `DecentralandInterface` while the next-gen SDK is being developed.\n\n**Phase 2: Migration and Deprecation (Mid-term)**\n\n* Provide clear migration guides and tools to help developers transition from the current SDK to the next-gen SDK.\n* Offer incentives for early adoption of the next-gen SDK.\n* Mark the current SDK and `DecentralandInterface` as deprecated but continue to maintain the compatibility layer for a defined period.\n\n**Phase 3: Sunset Current SDK (Long-term)**\n\n* After a reasonable deprecation period and ensuring most scenes have migrated, sunset the current SDK and the compatibility layer. \n* Focus development efforts solely on maintaining and enhancing the next-gen SDK.\n\n**Justification:**\n\nThis approach balances the following factors:\n\n* **Backwards Compatibility:** Ensures all currently deployed scenes continue to function without requiring immediate changes.\n* **Maintainability:** Separating the next-gen SDK development allows for cleaner code and reduces the complexity of maintaining two interconnected systems indefinitely.\n* **Future-Proofing:**  The next-gen SDK allows for innovation and improvements that might be difficult to implement within the constraints of the current SDK. \n\n**Consequences:**\n\n* **Initial Development Overhead:** Developing and maintaining the compatibility layer will require additional resources in the short term.\n* **Migration Effort:** Developers will need to invest time in migrating their scenes to the next-gen SDK. However, providing comprehensive migration resources can significantly reduce this effort.\n\nThis phased approach provides a clear roadmap for ensuring a smooth transition to the next-gen SDK while minimizing disruption for existing users and developers. \n","GenTime":"2024-08-19 16:06:56","text":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n\n\n##Decision\nWe decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n"}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"Prediction":"## Decision\nWe will implement a contract wallet detection mechanism for meta-transactions. When a meta-transaction is received, the system will first check if the signing address corresponds to a known contract wallet. If it does, the system will use a specialized method to retrieve the actual owner address of the contract wallet. This owner address will then be used as the message sender for the meta-transaction, ensuring that the transaction executes correctly. \n","GenTime":"2024-08-19 16:06:58","text":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n\n\n##Decision\nWe choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n"}
{"File Name":"libelektra\/cryptograhic_key_handling.md","Context":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n","Decision":"### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","tokens":73,"id":1270,"Prediction":"## Decision\n\n**Key Retrieval and Derivation:**\n\n* **Master Key:**  A single master key will be generated and managed securely. The method of generation and storage will depend on the specific security requirements of the deployment environment and may involve:\n    * Hardware Security Modules (HSM)\n    * Key Management Systems (KMS)\n    * Secure enclaves within trusted execution environments (TEEs)\n    * Encrypted storage with strict access controls.\n\n* **Key Derivation:**  Instead of storing individual keys for each KeySet or cryptographic operation, we will derive them from the master key.  We will use a Key Derivation Function (KDF) like HKDF or PBKDF2.  \n    * The KDF will take the master key, a unique identifier for the KeySet\/operation, and a salt as input. This ensures unique keys are generated even if the same master key is used for different KeySets.\n\n**Initialization Vectors (IVs):**\n\n* IVs will be generated using a cryptographically secure random number generator (CSPRNG) at the time of encryption.\n* We will **not** derive IVs from the master key.\n* IVs do not need to be secret and can be stored alongside the encrypted data.\n\n**Passing Keys to Crypto Libraries:**\n\n* Keys will only be passed to the underlying crypto libraries (OpenSSL\/libgcrypt) within the secure context of the crypto plugin.\n* Sensitive keys will never be exposed outside of this plugin.\n* We will utilize the native key handling mechanisms provided by each crypto library (e.g., OpenSSL's EVP_PKEY structure) to manage keys securely within their respective contexts.\n\n**Additional Considerations:**\n\n* **Key Rotation:**  A mechanism for rotating the master key will be implemented to maintain long-term security. \n* **API Design:** The API will be designed such that users never directly interact with raw key material.\n* **Auditing:** All key operations (generation, derivation, usage) will be logged for auditing purposes.\n\nThis approach prioritizes security by:\n\n* Minimizing the exposure of sensitive key material.\n* Using robust cryptographic primitives for key derivation and IV generation.\n* Leveraging the secure key management capabilities of established crypto libraries. \n","GenTime":"2024-08-19 16:07:08","text":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n\n\n##Decision\n### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n"}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"Prediction":"## Decision\n\nWe will use a **relational database management system (RDBMS)**  to store data internally. \n\n## Considered Options\n\n* **Relational Database Management System (RDBMS)**\n* **NoSQL database**\n* **Flat files (CSV, XML, JSON)**\n* **In-memory data structures (arrays, lists, etc.)** \n\n## Reasoning\n\nWhile different options offer specific advantages,  RDBMS provides the best balance considering our decision drivers:\n\n* **Easy to integrate:**  RDBMS are widely supported, with mature drivers and ORMs available for most programming languages.\n* **Easy to use:** SQL, the standard for interacting with RDBMS, is relatively simple to learn and use for common operations.\n* **Common technology:** RDBMS are a staple in software development, making it easier to find developers familiar with the technology. \n\n**Other options were dismissed for the following reasons:**\n\n* **NoSQL databases:** While offering scalability and flexibility, they introduce complexity for integration and data modeling that is unnecessary for our current needs.\n* **Flat files:** Lack the structure and query capabilities necessary for efficient data retrieval and manipulation as our data grows.\n* **In-memory data structures:**  Not suitable for persistent storage and introduce limitations in terms of data size and concurrency.\n\n## Consequences\n\n* **Positive:** Improved data organization, efficient querying, simplified data management. \n* **Negative:**  Potential for increased complexity compared to simpler storage mechanisms, especially if the data schema is complex or frequently changing. \n\n## Further Actions\n\n1. **Choose a specific RDBMS** based on factors like cost, scalability requirements, and existing infrastructure.\n2. **Design the database schema** carefully to ensure data integrity and efficiency. \n3. **Implement appropriate access control** to secure sensitive information. \n","GenTime":"2024-08-19 16:07:16","text":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n\n##Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n"}
{"File Name":"digitalrig-metal-aws\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":1740,"Prediction":"## Decision\n\nBookit will use AWS Aurora MySQL as its persistence mechanism.\n\n**Reasons:**\n\n* **SQL\/RDBMS Fit:** The team consensus is that a SQL\/RDBMS approach best suits the current understanding of the application's needs.\n* **Managed Solution:**  A managed, PaaS offering like AWS RDS is preferred to minimize operational overhead.\n* **Open Source and Cost:** Open source database solutions like MySQL and PostgreSQL are preferred for their cost-effectiveness. \n* **Performance and Availability:**  AWS Aurora MySQL offers compelling performance advantages and high availability through its automated backups and failover mechanisms. This outweighs the slight cost increase over standard RDS MySQL.\n* **PostgreSQL Aurora  Status:** While PostgreSQL is a viable option, the Aurora PostgreSQL offering is currently in preview and introduces uncertainty. \n\n**Consequences:**\n\n* **Vendor Lock-in:** Choosing Aurora MySQL increases the dependence on AWS services.\n* **Cost Implications:** While Aurora MySQL provides cost advantages over comparable solutions, it is still a cost consideration compared to self-managed open-source databases.\n* **Potential Overkill:**  The high-performance features of Aurora might be overkill for the application's current needs. This should be monitored. \n","GenTime":"2024-08-19 16:07:22","text":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n\n##Decision\nUse AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n"}
{"File Name":"TANF-app\/011-buildpacks.md","Context":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n","Decision":"Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","tokens":99,"id":3793,"Prediction":"## Decision\n\nWe will migrate our application deployment from DockerHub to a FedRAMP-authorized container registry with robust security documentation and support for buildpacks. This decision prioritizes security compliance (ATO attainment) and leverages the existing benefits of our buildpack-based deployment process. \n\nThis approach offers several advantages:\n\n* **Simplified ATO Process:**  Utilizing a FedRAMP-authorized platform with comprehensive security documentation significantly streamlines the ATO process.\n* **Enhanced Security:** FedRAMP-compliant registries offer higher security standards and practices, mitigating risks associated with DockerHub's security posture.\n* **Buildpack Compatibility:** Retaining our existing buildpack workflow ensures continuity and avoids significant code or infrastructure changes. \n\nWe will evaluate and select a suitable FedRAMP-authorized container registry (e.g., Iron Bank, Platform One) based on factors such as cost, integration capabilities, and specific security requirements.\n","GenTime":"2024-08-19 16:07:26","text":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n\n\n##Decision\nOur recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n"}
{"File Name":"decode-demo\/0003-remove-spectre-css-and-replace-with-bootstrap.md","Context":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n","Decision":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","tokens":98,"id":3999,"Prediction":"## Decision\n\nWe will use a mobile-first CSS framework that prioritizes responsive design and provides a user-friendly experience across various screen sizes, including mobile devices. This framework will replace Spectre CSS for the demo site's front-end development. \n\nThis decision prioritizes user experience for pilot participants by ensuring the demo site mimics the anticipated functionality and usability of the final wallet application on mobile devices. \n","GenTime":"2024-08-19 16:07:28","text":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n\n\n##Decision\nWe will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"Prediction":"## Decision\n\nAfter evaluating several CSS architecture options, including:\n\n* **ITCSS:** A layered architecture organizing CSS by specificity.\n* **BEM:** A component-based naming convention for maintainable CSS.\n* **Atomic CSS:** A methodology using single-purpose utility classes.\n* **Styled Components:** A library for writing CSS-in-JS using tagged template literals.\n* **CSS Modules:** A native browser feature for scoping CSS to individual components.\n\nWe have decided to adopt a hybrid approach that combines **BEM for component styling** and **CSS Modules for component-level encapsulation**. \n\n**Rationale:**\n\n* **BEM** provides a clear and consistent naming convention, improving code readability and maintainability. This directly addresses the \"Reusability\" and \"Longevity\" decision drivers by making the code easier to understand and modify over time. \n* **CSS Modules** address the \"Reusability\" and \"Longevity\" drivers by preventing style collisions and promoting component reusability. This aligns well with the project's goal of creating reusable components for the FxA ecosystem.\n* While some team members have experience with **ITCSS** and **Styled Components**, the learning curve for **BEM** and **CSS Modules** is considered minimal. This aligns with the \"Developer Experience\" and \"Ease of Use\" decision drivers. \n* **Atomic CSS**, while promoting reusability, can lead to verbose markup. We believe the combination of BEM and CSS Modules offers a better balance between reusability and maintainability. \n\nThis approach balances structure and flexibility, allowing us to write maintainable and scalable CSS that aligns with the Settings Redesign project's goals. It also provides a good foundation for future development within the FxA ecosystem.\n\n**Implementation Details:**\n\n* We will use a standard set of BEM modifiers and elements for consistency. \n* CSS Modules will be implemented using a build tool like Webpack or Parcel.\n* We will leverage CSS variables (as discussed in ADR 0015) within our CSS Modules to maintain a consistent design system and easily manage design changes.\n\nThis approach best addresses the project's needs for a maintainable, scalable, and reusable CSS architecture while considering the team's existing skills and the learning curve of new technologies.\n","GenTime":"2024-08-19 16:07:38","text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\n\n##Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n"}
{"File Name":"libmemory\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3590,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file living within the `docs\/adr` directory of our repository, following a consistent template.\n","GenTime":"2024-08-19 16:07:40","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"meadow\/0018-preservation-storage-object-naming-scheme.md","Context":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n","Decision":"Use a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","tokens":35,"id":3831,"Prediction":"## Decision\n\nWe will use a hierarchical storage structure within the preservation buckets to accommodate duplicate file names across different FileSets associated with a Work.\n\n**Specifically**:\n\n* **Work Level**: Each Work object will be assigned a unique directory within the bucket. \n* **FileSet Level**: Each FileSet associated with a Work will have its own subdirectory within the Work's directory.\n* **File Level**:  Files within a FileSet will be stored in the FileSet's directory. Duplicate filenames within a FileSet are disallowed and will require renaming or version control at the application level. \n\n**Example**:\n\nLet's say we have a Work with ID \"work123\" and two FileSets \"images\" and \"documents\".  The bucket structure would be:\n\n```\nwork123\/\n    images\/\n        image1.jpg\n        image2.png\n    documents\/\n        report.pdf\n        image1.jpg  \/\/ Allowed, as it's a different FileSet\n```\n\n**Rationale**:\n\n* **Logical Organization:** This structure mirrors the relationship between Works and FileSets, making it easy to understand and navigate.\n* **Duplicate Filename Handling**:  By separating files at the FileSet level, we avoid filename collisions while still allowing the same filename to exist within different FileSets of the same Work.\n* **Scalability**: This approach scales well for a large number of Works and FileSets.\n\n**Consequences**:\n\n* **Retrieval Complexity**: Retrieving a specific file requires knowing both the Work ID and the FileSet it belongs to.\n* **Application Logic**: The application needs to be aware of this structure and handle file path generation during upload and retrieval. \n","GenTime":"2024-08-19 16:07:47","text":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n\n\n##Decision\nUse a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n"}
{"File Name":"cygnus-infra\/0002-use-debian-as-a-host-os.md","Context":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n","Decision":"Use Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","tokens":145,"id":3868,"Prediction":"## Decision\n\nWe will use Debian as the base operating system for the software. \n","GenTime":"2024-08-19 16:07:48","text":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n\n\n##Decision\nUse Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n"}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"Prediction":"## Decision\n\nWe will implement a **hybrid approach** that combines the advantages of both cluster-wide and namespaced operators while mitigating their drawbacks. This approach will involve two components:\n\n**1. A Central Operator:**\n\n- This operator will be deployed in a dedicated namespace and will have cluster-wide read access. \n- It will be responsible for global tasks such as:\n    - Monitoring the cluster for new namespaces requiring management.\n    - Managing global resources like enterprise license pools.\n    - Handling cross-namespace features like cross-cluster search and replication.\n    - Providing a unified API endpoint for external integrations. \n- The central operator will **not** directly manage individual clusters.\n\n**2. Namespaced Agents:**\n\n- Lightweight agents will be deployed in each namespace containing clusters to be managed.\n- These agents will have admin permissions **only** within their respective namespaces. \n- They will communicate with the central operator to receive instructions and report back their status. \n- Agents will be responsible for tasks specific to their assigned cluster, such as:\n    - Deploying and managing cluster resources within the namespace.\n    - Scaling the cluster up or down.\n    - Monitoring the health and performance of the cluster.\n\nThis approach provides several benefits:\n\n* **Scalability:** The central operator handles global tasks while agents manage individual clusters, allowing the system to scale both up and down efficiently. \n* **Security:** By limiting the agents to namespace-level permissions and the central operator to read access where possible, we adhere to the principle of least privilege.\n* **Isolation:** Each agent operates independently, ensuring that one cluster's performance doesn't negatively affect others.\n* **Simplified Management:** The central operator provides a single point of control for managing multiple clusters across different namespaces.\n\nThis hybrid approach strikes a balance between centralized control and decentralized execution, allowing for a scalable, secure, and manageable solution for large Kubernetes installations. \n","GenTime":"2024-08-19 16:07:56","text":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n\n\n##Decision\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n"}
{"File Name":"experimenter\/0009-nimbus-web-sdk-architecture.md","Context":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n","Decision":"* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","tokens":307,"id":1819,"Prediction":"## Decision\n\nWe will create a new service, codenamed \"Cirrus,\" that exposes a RESTful API for interacting with Nimbus experiments and features. This service will house the stateless logic of the Nimbus SDK, effectively becoming a backend for both web applications and a revised, lightweight client SDK. \n\nHere's how this addresses the decision drivers:\n\n* **Architecture decisions for Nimbus on the web:** This approach directly aligns with the requirement for a server-side component to handle experimentation logic in web applications.\n* **Reusing existing SDK logic:** The core logic of the existing Rust SDK will be extracted and refactored into a reusable library that powers the Cirrus service. This ensures consistency and avoids code duplication.\n* **Python support via UniFFI:** The Cirrus service, through its API, inherently provides language-agnostic access to Nimbus functionality. Additionally, the shared Rust library can be exposed to Python using UniFFI, enabling a lightweight Python client SDK.\n* **Stateless SDK:** By moving the experiment evaluation and feature storage logic to the Cirrus service, the client SDK (both for web and revised native clients) becomes stateless. It will primarily handle API communication and data serialization.\n\nThis approach offers several benefits:\n\n* **Clear separation of concerns:** The client-side logic focuses on UI and user interaction, while the server-side handles experimentation and feature management.\n* **Scalability and Performance:**  Moving computationally intensive tasks to the server-side allows for horizontal scaling and potentially improved performance for clients.\n* **Language and Platform Agnosticism:** The RESTful API allows seamless integration with any language or platform capable of making HTTP requests.\n\nThis decision does introduce a dependency on a network connection for both web and revised native clients. However, this trade-off is deemed acceptable considering the benefits gained in terms of maintainability, scalability, and language support. \n","GenTime":"2024-08-19 16:08:05","text":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n\n##Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n"}
{"File Name":"cf-k8s-networking\/0008-implement-workarounds-for-capi-and-log-cache-to-unblock-global-strict-mtls.md","Context":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n","Decision":"We have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","tokens":153,"id":2249,"Prediction":"## Decision\n\nWe will adopt a phased approach to enabling STRICT mTLS for all components on the mesh:\n\n**Phase 1: Enable STRICT mTLS for compatible components**\n\n* Immediately enable STRICT mTLS for all components that are already compatible. This will enhance security for the majority of the mesh.\n* Document the known incompatible components (CAPI and Log-cache) and the reasons for their incompatibility.\n\n**Phase 2: Address CAPI incompatibility**\n\n* Explore solutions for the CAPI init container issue, such as:\n    * **Modifying the CAPI deployment:**  Investigate if the migrations can be performed by the main CAPI container instead of an init container. This would allow the sidecar to establish the mTLS connection before the migrations are run.\n    * **Delaying database migrations:** Explore options to delay the database migrations until after the sidecar is running and can facilitate the mTLS connection.\n    * **Allowlisting:**  Temporarily allowlist CAPI's init container from STRICT mTLS enforcement until a permanent solution is implemented. This will allow CAPI to function while we work on a fix but weaken the security posture.\n\n**Phase 3: Address Log-cache incompatibility**\n\n* Investigate solutions for Log-cache's custom TLS configuration, such as:\n    * **Reconfiguration:**  Determine if Log-cache's custom TLS setup can be adapted to coexist with the sidecar's mTLS or if it can be reconfigured to utilize the sidecar's mTLS for its connections.\n    * **Proxy traffic through sidecar:** Explore routing Log-cache's traffic through the sidecar to leverage its mTLS capabilities. \n\n**Timeline:**\n\n* Phase 1: Immediate\n* Phase 2 & 3: To be determined based on the complexity of chosen solutions and available resources.\n\n**Monitoring and Rollback:**\n\n* Closely monitor the mesh after each phase for any unexpected issues.\n* Establish a rollback plan to quickly revert to the previous state if any critical issues arise.\n\nThis phased approach allows us to improve security incrementally while minimizing disruption to the system and addressing incompatibilities methodically. \n","GenTime":"2024-08-19 16:08:14","text":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n\n\n##Decision\nWe have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n"}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a self-contained Markdown file, following a consistent template. \n","GenTime":"2024-08-19 16:08:16","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences. Each ADR will be a separate file living in the `\/docs\/architecture\/decisions` directory of our repository, following a consistent template. \n","GenTime":"2024-08-19 16:08:17","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"Prediction":"## Decision\n\nWe will use a semantic versioning scheme for Firefox TV releases, based on the following principles:\n\n* **Major versions (e.g., 3.x)**: Represent significant milestones with potentially breaking changes, major feature additions, or user interface overhauls. \n* **Minor versions (e.g., 3.9.x)**:  Indicate incremental updates, including new features, enhancements, and bug fixes, that are backward compatible with the previous minor version.\n* **Patch versions (e.g., 3.9.1)**:  Address critical bugs, security vulnerabilities, and other small fixes within the same minor version, maintaining backward compatibility.\n\n**Specific Versioning Rules:**\n\n* We will increment the **Major** version number for releases containing significant architectural changes, major feature additions, or user interface redesigns that break backward compatibility.\n* We will increment the **Minor** version number for releases containing new features, enhancements, or bug fixes that are backward compatible with the previous minor version.\n* We will increment the **Patch** version number for releases containing critical bug fixes, security patches, or other minor updates that maintain backward compatibility within the same minor version.\n\n**Release Candidate and Beta Versions:**\n\n* **Release Candidate (RC)** builds will be identified by appending `-rc.X` to the version number (e.g., `3.9.0-rc.1`).\n* **Beta** versions will be released under a separate track and will not follow the semantic versioning scheme described above.\n\n**Rollback Strategy:**\n\n* In the event of a critical issue with a release, we will prioritize releasing a patch version (e.g., from `3.9.0` to `3.9.1`) that addresses the issue while maintaining backward compatibility.\n* Rollbacks to previous minor versions (e.g., from `3.9` to `3.8`) will only be considered as a last resort if a critical issue cannot be resolved with a patch release and poses a significant risk to users.\n\n**Documentation and Communication:**\n\n* Release notes for all Firefox TV versions, including beta versions and release candidates, will be published on the GitHub releases page [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here).\n* Release notes will clearly outline the changes introduced in each version, including new features, enhancements, bug fixes, and any known issues.\n\nThis versioning strategy aims to provide clarity and transparency around Firefox TV releases, enabling users and developers to understand the significance of each update and make informed decisions about upgrading or rolling back.\n","GenTime":"2024-08-19 16:08:28","text":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n\n\n##Decision\nFirefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n"}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"Prediction":"## Decision\n\nThe decision is to pivot from the initial MVP approach of a standalone RTK browser within Anki. We will instead focus on building a streamlined integration between Anki and external RTK resources.\n\nThis revised MVP will prioritize:\n\n1. **Seamless Card Generation:** Develop a mechanism for users to effortlessly generate RTK cards from external sources, eliminating the need for manual input within Anki. This could involve browser extensions, API integrations, or dedicated import functionality. \n\n2. **Efficient Workflow Integration:** Ensure the card generation process integrates seamlessly into typical Anki workflows. This might involve direct deck import options or tools for quickly adding generated cards to existing decks.\n\n3. **Flexibility and Customization:**  While simplifying the process, the MVP should still allow for user customization of card content, formatting, and integration with their preferred RTK resources. \n\nThis shift in approach addresses the identified issues:\n\n* **Avoids redundancy:** By leveraging external RTK resources, we eliminate the need to recreate existing functionalities within Anki.\n* **Simplifies development:** Focusing on integration rather than building a complete browser streamlines development efforts. \n* **Prioritizes user needs:** The revised MVP prioritizes ease of use and efficiency for users seeking to incorporate RTK into their Anki workflow. \n\nThis decision allows for a more focused and achievable MVP that addresses the core problem of integrating RTK with Anki effectively.  \n","GenTime":"2024-08-19 16:08:35","text":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n\n\n##Decision\nAfter some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n"}
{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"Prediction":"## Decision\n\nWe will implement a combination of the following approaches to address the user alignment issue when running Macondo commands:\n\n**1. User ID and Group ID Mapping:**\n\n- Macondo will provide a command-line flag (e.g., `--user-align`) that, when enabled:\n    - Retrieves the user ID (UID) and group ID (GID) of the user running Macondo on the host machine.\n    - Configures the Docker container to run with the retrieved UID and GID, ensuring file ownership aligns with the host user.\n\n**2. Selective Volume Mounting:**\n\n- Instead of mounting the entire home directory, Macondo will prioritize mounting only the necessary directories and files required for the specific command execution. \n- This approach minimizes potential conflicts and improves security by limiting the container's access to the host filesystem. \n- Users can still utilize the existing functionality to mount additional directories using specific options if needed.\n\n**3. Working Directory Flexibility:**\n\n- Macondo will maintain its current functionality of allowing users to specify a working directory within the container using options like `-w` or `--workdir`. \n- This ensures users can easily work with files in specific locations on the host without relying solely on home directory mapping.\n\n**Rationale:**\n\n- This approach balances ease of use with security and flexibility. \n- User ID\/Group ID mapping ensures proper file ownership without requiring users to manually manage permissions.\n- Selective volume mounting reduces the potential attack surface and minimizes conflicts. \n- Working directory flexibility empowers users to tailor their workflow without imposing rigid directory structures.\n\n**Consequences:**\n\n- **Positive:**\n    - Improved user experience: Commands can be run seamlessly without worrying about file permissions.\n    - Enhanced security: Reduced risk of unintended file modifications or access violations.\n- **Negative:**\n    - Slightly increased complexity for users who require access to their entire home directory within the container (though this can be achieved with options).\n\nThis decision prioritizes user experience and security, making Macondo commands easier and safer to use for a wider range of tasks. \n","GenTime":"2024-08-19 16:08:44","text":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n\n\n##Decision\nAdd a mechanism to align the user\/group of the host system with that of the docker container.\\n"}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"Prediction":"## Decision\n\nWe will standardize our front-end dependency management by choosing either **npm** or **gem** as the single source of truth for all front-end packages, including Bootstrap and jQuery.  This will prevent version conflicts and allow us to update all front-end components consistently.\n","GenTime":"2024-08-19 16:08:45","text":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n\n\n##Decision\nWe removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n"}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"Prediction":"## Decision\n\nWe will adopt the use of URIs as the standard mechanism for identifying and locating all resources within our software ecosystem. This encompasses internal resources, such as database records and API endpoints, as well as external resources like third-party services and web content. \n","GenTime":"2024-08-19 16:08:47","text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n\n\n##Decision\nWe will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n"}
{"File Name":"architecture\/0013-home-assistant-container.md","Context":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n","Decision":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","tokens":41,"id":1430,"Prediction":"## Decision\n\nWe will support the following installation methods, as defined in [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md):\n\n- **Home Assistant Operating System:** This is the recommended and most integrated way to run our software. It provides a streamlined experience with automatic updates and pre-configured dependencies.\n- **Docker:** For users who prefer containerization or need to run our software on systems where direct installation is not possible, we will provide official Docker images.\n\nWe will **not** officially support the following installation methods:\n\n- **Python Virtual Environments:** While technically possible, supporting virtual environments introduces complexity in dependency management and update procedures. We believe the benefits of OS-level isolation provided by Docker outweigh the flexibility of virtual environments.\n- **Manual Installation from Source:** This method is highly discouraged for all users except developers. It requires advanced technical knowledge and is prone to errors. We will not provide official support for manual installations.\n\n## Rationale\n\n- **Simplicity and User Experience:** Focusing on Home Assistant Operating System and Docker simplifies the installation process for the majority of users. \n- **Maintenance Overhead:** Supporting multiple installation methods increases the maintenance burden and complexity of our development and support processes. \n- **Security and Stability:**  Home Assistant Operating System and Docker provide a more secure and stable environment compared to manual installations or virtual environments.\n\n## Consequences\n\n- Users who prefer virtual environments or manual installations will need to rely on community support or develop their own solutions.\n- We may need to provide clear documentation and guidance to help users migrate from unsupported installation methods.\n\nThis decision aligns with our goal of providing a stable, secure, and easy-to-use software experience for the majority of our users.\n","GenTime":"2024-08-19 16:08:55","text":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n\n\n##Decision\nThis is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n"}
{"File Name":"tech-events-calendar\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3753,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file in Markdown format, following a standardized template.  We will store and maintain these ADRs in a dedicated \"docs\/adr\" directory within our version control system. \n","GenTime":"2024-08-19 16:08:57","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"digitalrig-metal-aws\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":1741,"Prediction":"## Decision\n\nMigrate from Travis CI to a more flexible CI\/CD platform that better supports our desired pipeline stages and workflows.  We will evaluate and select a platform based on the following criteria:\n\n* **Flexibility and Customization:**  The platform should offer a high degree of flexibility in defining pipeline stages, job execution order, and conditional logic. This is crucial to overcome the limitations encountered with Travis CI.\n* **Hosted\/PaaS Solution:**  We want to minimize the operational overhead of managing our own CI\/CD infrastructure. The chosen platform should be cloud-based and offer a managed solution. \n* **Integration Ecosystem:**  The platform should seamlessly integrate with our existing technology stack, including our version control system (e.g., GitHub, GitLab), deployment targets, and other development tools.\n* **Cost-Effectiveness:**  The pricing model should align with our budget and usage patterns. We will compare the costs of different platforms and consider factors like build minutes, concurrent builds, and storage.\n* **Security and Compliance:**  The platform should prioritize security and comply with any relevant industry regulations.\n\nWe will create a Proof of Concept (PoC) with a shortlist of potential replacements (e.g., GitHub Actions, GitLab CI\/CD, CircleCI) to evaluate their suitability based on the above criteria. \n\nThis decision allows us to adopt a more robust and flexible CI\/CD solution, enabling us to build more complex and efficient pipelines that meet our evolving development needs. \n","GenTime":"2024-08-19 16:09:04","text":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n"}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"Prediction":"## Decision\n\nWe will change the relationship inversion process to update the update date of the sample that is the target of the relationship. \n\n## Reason\n\nMaintaining accurate update dates for samples is crucial for data integrity and tracking changes within the system. When a relationship is created or inverted, it inherently modifies the information associated with the target sample, even if its direct attributes remain unchanged. Therefore, updating the update date of the target sample upon relationship inversion ensures consistency and reflects the actual state of the data. \n\n## Consequences\n\n* **Improved Data Accuracy:**  Update dates will accurately reflect the latest modifications related to samples, including changes arising from relationship inversions.\n* **Enhanced Tracking:** Easier identification of samples impacted by relationship changes, facilitating change management and auditing.\n* **Potential Performance Impact:**  The update operation might introduce a slight performance overhead due to additional database writes. This impact should be minimal and will be monitored. \n\nThis decision addresses the issue highlighted in BSD-1088 and prioritizes data accuracy and consistency over potential minor performance implications. \n","GenTime":"2024-08-19 16:09:09","text":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n\n\n##Decision\nWe decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n"}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"Prediction":"## Decision\n\nOctagon components will be stateless, pure view layer components.  State management will be delegated to the consuming application. Component APIs will prioritize declarative configuration via props, aligning with React best practices and enabling predictable rendering based on application state. \n","GenTime":"2024-08-19 16:09:10","text":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n\n\n##Decision\nMake things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n"}
{"File Name":"arch\/0019-server-request-and-upgrade-capacity-evaluation.md","Context":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n","Decision":"1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","tokens":208,"id":2440,"Prediction":"## Decision\n\n\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u8d44\u6e90\u4f7f\u7528\u4e0d\u5747\u8861\u548c\u8d44\u6e90\u7533\u8bf7\u6d41\u7a0b\u4e0d\u89c4\u8303\u7684\u95ee\u9898\uff0c\u6211\u4eec\u51b3\u5b9a\u91c7\u53d6\u4ee5\u4e0b\u63aa\u65bd\uff1a\n\n**1. \u8d44\u6e90\u4f18\u5316\u4e0e\u518d\u5206\u914d:**\n\n* \u5bf9\u73b0\u6709\u673a\u5668\u8fdb\u884c\u76d1\u63a7\u548c\u5206\u6790\uff0c\u8bc6\u522b\u8d44\u6e90\u4f7f\u7528\u7387\u8fc7\u9ad8\u548c\u8fc7\u4f4e\u7684\u673a\u5668\u3002\n* \u5bf9\u4e8e\u8d44\u6e90\u4f7f\u7528\u7387\u8fc7\u4f4e\u7684\u673a\u5668\uff0c\u5c06\u5176\u4e0a\u7684\u670d\u52a1\u8fc1\u79fb\u81f3\u5176\u4ed6\u673a\u5668\uff0c\u5e76\u91ca\u653e\u7a7a\u95f2\u673a\u5668\u6216\u8c03\u6574\u5176\u89c4\u683c\u3002\n* \u5bf9\u4e8e\u8d44\u6e90\u4f7f\u7528\u7387\u8fc7\u9ad8\u7684\u673a\u5668\uff0c\u5206\u6790\u5176\u74f6\u9888\u8d44\u6e90\uff0c\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u7684\u4f18\u5316\uff0c\u4f8b\u5982\uff1a\n    * **CPU \u4f7f\u7528\u7387\u8fc7\u9ad8:**  \u5206\u6790\u4ee3\u7801\uff0c\u8fdb\u884c\u6027\u80fd\u4f18\u5316\uff0c\u6216\u8005\u589e\u52a0 CPU \u6838\u6570\u3002\n    * **\u5185\u5b58\u4f7f\u7528\u7387\u8fc7\u9ad8:** \u5206\u6790\u5185\u5b58\u5360\u7528\uff0c\u4f18\u5316\u4ee3\u7801\uff0c\u6216\u8005\u589e\u52a0\u5185\u5b58\u5bb9\u91cf\u3002\n    * **\u786c\u76d8\u4f7f\u7528\u7387\u8fc7\u9ad8:**  \u6e05\u7406\u65e0\u7528\u6570\u636e\uff0c\u6216\u8005\u66f4\u6362\u66f4\u5927\u5bb9\u91cf\u7684\u786c\u76d8\u3002\n\n**2. \u5236\u5b9a\u5408\u7406\u7684\u8d44\u6e90\u7533\u8bf7\u6d41\u7a0b\uff1a**\n\n* \u5efa\u7acb\u8d44\u6e90\u7533\u8bf7\u5e73\u53f0\uff0c\u6240\u6709\u8d44\u6e90\u7533\u8bf7\u5fc5\u987b\u901a\u8fc7\u5e73\u53f0\u8fdb\u884c\u3002\n* \u7533\u8bf7\u65f6\u9700\u63d0\u4f9b\u8be6\u7ec6\u7684\u8d44\u6e90\u9700\u6c42\u8bf4\u660e\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a\n    * **\u670d\u52a1\u7c7b\u578b**:  \u4f8b\u5982 Web \u670d\u52a1\u3001\u6570\u636e\u5e93\u670d\u52a1\u7b49\u3002\n    * **\u9884\u8ba1\u8d1f\u8f7d**:  \u4f8b\u5982\u5e76\u53d1\u7528\u6237\u6570\u3001\u6570\u636e\u91cf\u7b49\u3002\n    * **\u6027\u80fd\u9700\u6c42**:  \u4f8b\u5982\u54cd\u5e94\u65f6\u95f4\u3001\u541e\u5410\u91cf\u7b49\u3002\n* \u5efa\u7acb\u8d44\u6e90\u7533\u8bf7\u8bc4\u5ba1\u673a\u5236\uff0c\u5bf9\u7533\u8bf7\u7684\u8d44\u6e90\u8fdb\u884c\u5408\u7406\u6027\u8bc4\u4f30\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u3002\n\n**3.  \u6839\u636e\u670d\u52a1\u7279\u6027\u9009\u62e9\u5408\u9002\u7684\u5b58\u50a8\u65b9\u6848\uff1a**\n\n* \u5bf9\u4e8e\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f8b\u5982\u6570\u636e\u5e93\u3001SVN \u7b49\uff0c\u5efa\u8bae\u4f7f\u7528\u9ad8\u6027\u80fd\u4e91\u76d8\uff0c\u4f8b\u5982 SSD \u4e91\u76d8\u7b49\uff0c\u4ee5\u63d0\u5347\u670d\u52a1\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002\n* \u5bf9\u4e8e\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u4e0d\u9ad8\u7684\u670d\u52a1\uff0c\u4f8b\u5982\u9759\u6001\u6587\u4ef6\u5b58\u50a8\u7b49\uff0c\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u4e91\u76d8\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u3002\n\n**4.  \u5efa\u7acb\u8d44\u6e90\u4f7f\u7528\u76d1\u63a7\u548c\u9884\u8b66\u673a\u5236\uff1a**\n\n* \u5bf9\u6240\u6709\u673a\u5668\u7684\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u8fdb\u884c\u5b9e\u65f6\u76d1\u63a7\uff0c\u5e76\u8bbe\u7f6e\u5408\u7406\u7684\u9884\u8b66\u9608\u503c\u3002\n* \u5f53\u8d44\u6e90\u4f7f\u7528\u7387\u8d85\u8fc7\u9884\u8b66\u9608\u503c\u65f6\uff0c\u53ca\u65f6\u901a\u77e5\u76f8\u5173\u4eba\u5458\u8fdb\u884c\u5904\u7406\uff0c\u907f\u514d\u56e0\u8d44\u6e90\u4e0d\u8db3\u5bfc\u81f4\u670d\u52a1\u4e0d\u53ef\u7528\u3002\n\n**5.  \u5b9a\u671f\u8fdb\u884c\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u7684\u56de\u987e\u548c\u5206\u6790\uff1a**\n\n* \u5b9a\u671f\u5bf9\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u8fdb\u884c\u56de\u987e\u548c\u5206\u6790\uff0c\u8bc6\u522b\u8d44\u6e90\u4f7f\u7528\u4e0d\u5408\u7406\u7684\u60c5\u51b5\uff0c\u5e76\u8fdb\u884c\u76f8\u5e94\u7684\u8c03\u6574\u3002\n* \u6839\u636e\u670d\u52a1\u7684\u5b9e\u9645\u53d1\u5c55\u60c5\u51b5\uff0c\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u914d\u7f6e\uff0c\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u6216\u4e0d\u8db3\u3002\n\n\n**\u5b9e\u65bd\u8fd9\u4e9b\u63aa\u65bd\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u63d0\u5347\u670d\u52a1\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002** \n","GenTime":"2024-08-19 16:09:21","text":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n\n\n##Decision\n1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n"}
{"File Name":"celestia-core\/adr-024-sign-bytes.md","Context":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n","Decision":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","tokens":249,"id":4097,"Prediction":"## Decision\n\nWe will define a new binary protocol for communication between Tendermint and remote signers. This protocol will address the following:\n\n**1. Encoding:**\n\n* Replace JSON encoding with Amino encoding for messages exchanged between Tendermint and remote signers. This provides a more compact and efficient representation suitable for both hardware wallets and Ethereum smart contracts. \n\n**2. Message Structure:**\n\n* Define distinct request and reply message types for each operation (e.g., `SignVoteRequest`, `SignVoteResponse`). \n* Include an error code and optional error message field in all reply messages to allow remote signers to communicate errors encountered during processing.\n\n**3. Implementation:**\n\n* Implement the new binary protocol in a backwards compatible manner, allowing existing JSON-based implementations to continue functioning. \n* Update `privval\/socket.go` to support both the new binary protocol and the existing JSON-based protocol. \n* Deprecate the JSON-based protocol in the documentation and provide clear migration instructions for users.\n\n**4. Future Considerations:**\n\n* Investigate the use of Protocol Buffers as a potential alternative to Amino encoding in the future.\n* Explore the possibility of leveraging this new protocol for other use cases beyond remote signing. \n","GenTime":"2024-08-19 16:09:27","text":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n\n\n##Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n"}
{"File Name":"cukeness\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3316,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions for the project. Each ADR will be a separate markdown file stored in a dedicated `\/docs\/adrs` directory in the project repository. \n","GenTime":"2024-08-19 16:09:29","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"tdr-dev-documentation\/0017-govuk-notify-keycloak-message.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n","Decision":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","tokens":33,"id":1786,"Prediction":"## Decision\n\nWe will NOT use GovUK Notify as the email provider for Keycloak. Instead, we will configure Keycloak to use its built-in email mechanism and rely on the system's underlying SMTP configuration.\n\n## Reasons\n\n* **Simplicity and maintainability:** Using Keycloak's built-in email mechanism reduces the complexity of our infrastructure. We avoid introducing an additional integration point with GovUK Notify, simplifying maintenance and troubleshooting.\n* **Configuration efficiency:** Keycloak's built-in email settings are straightforward to configure within the Keycloak administration console. This approach avoids the need for managing API keys and potentially complex authentication flows with GovUK Notify.\n* **Existing infrastructure leverage:** This decision leverages our existing infrastructure for email delivery. If the system is already configured to send emails through a reliable SMTP server, we can capitalize on that existing configuration.\n\n## Consequences\n\n* **Limited email tracking:** We lose the advanced tracking and reporting features offered by GovUK Notify. \n* **Potential for increased operational effort:** Depending on the scale and complexity of email communication, managing email deliverability and potential issues might require more direct intervention with the underlying SMTP server.\n\n## Status\n\nAccepted\n\n## Notes\n\nThis decision can be revisited if:\n\n* Email volume and tracking requirements significantly increase.\n* We encounter challenges with email deliverability and require more sophisticated control over the email infrastructure.\n\nIt's crucial to ensure that the chosen SMTP server is properly configured for sending emails, including aspects like SPF and DKIM settings, to avoid email deliverability issues. \n","GenTime":"2024-08-19 16:09:36","text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n\n\n##Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n"}
{"File Name":"apply-for-teacher-training\/0015-carrying-over-applications.md","Context":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n","Decision":"- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","tokens":467,"id":4010,"Prediction":"## Decision\n\nWe will implement the \"carry over\" functionality for applications by adapting the existing \"Apply again\" approach, with the following modifications:\n\n* **Eligibility:** Only applications in the eligible states (Withdrawn, Unsubmitted, Ready to send to provider, Conditions not met, Offer withdrawn, Offer declined, Application cancelled, Rejected) at the end of the recruitment cycle will be eligible for carry-over. Applications in progress with the provider (Awaiting provider decision) or with an active offer (Offer, Meeting conditions, Recruited) will not be eligible.\n* **Application Type:** All carried-over applications will be treated as new \"Apply 1\" applications in the new cycle, regardless of whether they originated as \"Apply 1\" or \"Apply again\" in the previous cycle. \n* **References:**\n    * Carried-over applications will retain their existing reference status and data. \n    * If an application had complete references, they will be copied over. \n    * If an application had incomplete references, the candidate will be prompted to complete them in the new cycle.\n* **Course Choices:** The carried-over application will be associated with the new recruitment cycle, and candidates will be required to select courses from the available options in the new cycle. \n* **Cycle Transition:** The carry-over process will be automated and executed once the current recruitment cycle closes and the new cycle begins. This ensures that all carried-over applications access course choices from the new cycle. \n\nThis approach leverages existing functionality, minimizes development effort, and provides a seamless experience for candidates wishing to re-apply in the next recruitment cycle. \n","GenTime":"2024-08-19 16:09:43","text":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n\n\n##Decision\n- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n"}
{"File Name":"holochain-rust\/0003-redux-architecture-pattern.md","Context":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n","Decision":"The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","tokens":92,"id":1495,"Prediction":"## Decision\n\nWe will adopt an actor-based architecture using the Actix framework for the Rust rewrite. \n\n## Reasoning\n\n* **Managing Complexity**: The actor model promotes modularity and encapsulation by treating components as independent entities (actors) that communicate via messages. This directly addresses the implicit dependencies and complexity observed in the Go codebase. \n* **Concurrency and State**: Actors inherently manage their own state and execute concurrently. This aligns well with the distributed nature of Holochain and the desire for a single global state within each agent. Actix, being a mature and performant actor framework in Rust, provides the necessary primitives for managing concurrency and message passing efficiently.\n* **Clean Architecture**: Actix encourages a clear separation of concerns, leading to a more testable and maintainable codebase. This aligns with the goal of a clean and fitting architecture for the Rust rebuild.\n\n## Consequences\n\n* **Learning Curve**: The team will need to invest time in learning the Actix framework and actor-based design principles.\n* **Performance Overhead**: While Actix is designed for performance, any concurrency framework introduces some overhead. This will need to be carefully monitored and mitigated if it becomes a bottleneck.\n* **Debugging**: Debugging concurrent systems can be challenging. However, the structured concurrency model provided by Actix, coupled with its tooling, should aid in identifying and resolving issues.\n\nThis decision provides a strong foundation for a clean and maintainable architecture that addresses the complexities inherent in Holochain while aligning well with its distributed nature and concurrency requirements.\n","GenTime":"2024-08-19 16:09:50","text":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n\n##Decision\nThe new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n"}
{"File Name":"libelektra\/store_name.md","Context":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n","Decision":"Only store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","tokens":26,"id":1291,"Prediction":"## Decision\n\n**Option 1: Introduce Key Name Interning**\n\n* Implement a string interning mechanism for key names within Elektra. This means storing each unique key name only once in memory and using references (e.g., pointers) to this single instance wherever the key name is needed. \n\n**Option 2: Utilize Data Structures with Reduced Overhead**\n\n* Investigate replacing existing data structures (e.g., linked lists) with more memory-efficient alternatives like tries, radix trees, or hash tables specifically designed for storing strings and their associated data. This can reduce memory consumption by optimizing storage and access patterns for key names.\n\n**Option 3: Lazy Loading of Key Names**\n\n* Explore loading key names into memory only when explicitly accessed or required for operations. This approach keeps the most frequently used key names in memory, while less frequently used ones reside on disk (or other persistent storage). \n\n**Option 4: Key Name Compression**\n\n* Investigate compression algorithms suitable for key names. Evaluate the trade-off between memory savings from compression and the computational overhead of compression and decompression operations.  \n\n**Decision Outcome:**\n\n* **Option 1 (Key Name Interning)** is chosen as the initial approach.  This option offers a balance between implementation complexity and potential memory savings. \n\n**Reasoning:**\n\n* **Effectiveness:** Key name interning directly addresses the problem of duplicate key names in memory.\n* **Implementation:** It is a well-understood technique with established implementations in various languages and frameworks. \n* **Performance:** While introducing some overhead for managing the intern pool, it can lead to significant memory savings, especially if key names are frequently repeated.\n* **Maintainability:** The implementation can be relatively self-contained and should not introduce significant complexity to the Elektra codebase.\n\n**Consequences:**\n\n* **Benefits:**  Reduced memory consumption, potentially improving performance for operations involving key names.\n* **Drawbacks:** Additional overhead for managing the string intern pool. Careful design and implementation are needed to avoid performance bottlenecks.\n* **Other considerations:** Performance benchmarking is crucial to measure the effectiveness of interning and to identify potential bottlenecks. \n","GenTime":"2024-08-19 16:09:59","text":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n\n\n##Decision\nOnly store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n"}
{"File Name":"TruBudget\/0003-project-data-model.md","Context":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n","Decision":"Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","tokens":438,"id":1225,"Prediction":"## Decision\n\n**Option 3: Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream.**\n\n## Justification\n\nOption 3 provides the best balance of efficiency, scalability, and maintainability:\n\n* **Efficient Querying:** Utilizing multiple key-items within the project stream allows for targeted queries of specific resources and relationships without loading unnecessary data. This minimizes database calls and latency, especially when fetching deeply nested resources.\n* **Low Storage\/Memory Footprint:**  Unlike Option 2, only the modified resource is updated within the project stream, reducing storage requirements and memory consumption. This approach is more efficient for handling frequent updates and large datasets. \n* **Simplified Data Management:**  Having all related data within a single project stream simplifies data consistency and management. It eliminates the need for complex joins or multiple queries across different streams, reducing code complexity and potential errors.\n\nWhile Option 1 offers granular data fetching, the potential for numerous queries with large projects introduces significant performance bottlenecks. Option 2, on the other hand, suffers from storage bloat and increased processing overhead due to full tree copies for every change.\n\nTherefore, Option 3 presents the most practical and efficient solution for managing project, subproject, and workflow data.\n","GenTime":"2024-08-19 16:10:05","text":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n\n\n##Decision\nSince we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n"}
{"File Name":"tdr-dev-documentation\/0015-govuk-notify-staging-configuration.md","Context":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n","Decision":"The \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","tokens":126,"id":1784,"Prediction":"## Decision\n\nA separate GovUK Notify service will be created for each TDR environment **and** for each GovUK Notify mode (trial and live).\n\nThis will result in the following GovUK Notify services:\n\n*   **TDR Integration Environment - Trial Mode**\n*   **TDR Integration Environment - Live Mode**\n*   **TDR Staging Environment - Trial Mode**\n*   **TDR Staging Environment - Live Mode**\n*   **TDR Production Environment - Trial Mode**\n*   **TDR Production Environment - Live Mode**\n\n## Reasoning\n\nThis approach provides the following benefits:\n\n*   **Clear separation of environments:** Prevents accidental emails being sent from one environment to another. For example, a test user in the Integration environment accidentally receiving a password reset email intended for a user in the Production environment. \n*   **Flexibility for testing:** Allows for realistic testing of email functionality in the trial mode without impacting live email sending limits. \n*   **Compliance with GovUK Notify Usage:**  Adheres to GovUK guidelines on service separation and responsible usage of trial mode.\n\n## Consequences\n\n*   Increased administrative overhead of managing multiple GovUK Notify services.\n*   Potential for confusion if the purpose of each service is not clearly documented.\n\nThis decision mitigates the increased administrative overhead by using infrastructure-as-code to automate the creation and management of the GovUK Notify services. Additionally, this ADR will serve as documentation to clarify the purpose of each service. \n","GenTime":"2024-08-19 16:10:11","text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n\n##Decision\nThe \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n"}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"Prediction":"## Decision\n\nWhile a USE\/RED approach offers a structured methodology for visualizing system performance, the time constraint makes it impractical for immediate implementation. We will utilize the existing Grafana dashboard as a starting point for monitoring the etcd cluster. This provides immediate visibility into key metrics. \n\nHowever, we acknowledge the limitations of adopting a pre-built solution without customization for USE\/RED. This decision will be revisited in the future to evaluate the possibility of implementing a more tailored and insightful monitoring system when time allows. \n","GenTime":"2024-08-19 16:10:14","text":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n\n\n##Decision\nAs I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a mechanism to capture significant architectural decisions made during the development of Opinionated Digital Center.  Each ADR will be a concise text document that includes:\n\n* **Title:** A short, descriptive title for the decision.\n* **Status:** Proposed, Accepted, Deprecated, Superseded, etc.\n* **Context:**  A description of the forces at play, including technological, political, social, and project local. Essentially, a description of the situation that requires a decision.\n* **Decision:** The architectural decision made.\n* **Consequences:** The resulting context and implications of making this decision. This may include both positive and negative consequences, and should consider short-term and long-term impacts.\n* **Alternatives Considered:** A brief discussion of other options considered, and why they were rejected.\n* **Links:** Links to related ADRs, documentation, or other relevant resources.\n\nWe will use a lightweight, Markdown-based format for ADRs, and store them in a dedicated `\/docs\/adrs` directory within the project repository. \n\nThis approach allows for:\n\n* **Transparency and Traceability:**  Provides a clear and accessible history of why architectural decisions were made.\n* **Improved Communication:**  Facilitates communication and collaboration among team members, both present and future.\n* **Reduced Risk:** Helps to identify and mitigate potential architectural risks early on. \n* **Knowledge Sharing:**  Serves as valuable documentation for onboarding new team members and understanding the rationale behind the system's design. \n","GenTime":"2024-08-19 16:10:21","text":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"LogLady\/0003-implement-redux.md","Context":"## Context\\nState handling in React becomes very complicated when you have too many states\\n","Decision":"We implement Redux to handle state\\n","tokens":17,"id":1854,"Prediction":"## Decision\n\n**Adopt a centralized state management solution, specifically Redux, to address the increasing complexity of state management in our React application.**\n\n**Reasoning:**\n\n* **Scalability and Maintainability:** As the application grows, Redux provides a structured and centralized way to manage and update the application's state. This improves code organization, readability, and maintainability.\n* **Single Source of Truth:** Redux ensures that the application's state is stored in a single, immutable store. This eliminates state inconsistencies and makes debugging easier. \n* **Testability:** Redux's predictable state management makes it easier to write unit tests for components and reducers.\n* **Performance Optimization:** Redux, in conjunction with its connect function and `useSelector` hook (for modern React), allows components to re-render only when the specific state they depend on changes, optimizing performance for large applications.\n\n**Alternatives Considered:**\n\n* **Context API with `useReducer` hook:**  A viable option for simpler state management needs, but might not scale as effectively as Redux for large applications with complex state interactions.\n* **Other state management libraries (MobX, Zustand):** While offering different approaches, these libraries might not be as widely adopted or offer the same ecosystem maturity as Redux. \n\n**Consequences:**\n\n* **Increased Complexity Initially:** Introducing Redux adds a learning curve for developers unfamiliar with its concepts and patterns.\n* **Potential Boilerplate Code:** Redux might introduce some boilerplate code, although modern tools and libraries can mitigate this.\n\n**Tradeoffs:**\n\nWe acknowledge the initial learning curve and potential for boilerplate code. However, we believe that the benefits of improved scalability, maintainability, and performance outweigh these drawbacks in the long run, especially as our application continues to grow in size and complexity.\n","GenTime":"2024-08-19 16:10:29","text":"## Context\\nState handling in React becomes very complicated when you have too many states\\n\n\n##Decision\nWe implement Redux to handle state\\n"}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"Prediction":"## Decision\n\n**Use an asynchronous logging framework with a distributed tracing system.**\n\nThis will involve:\n\n1. **Selecting an asynchronous logging library:** This will allow logs to be written without blocking the main application thread, improving performance. Popular choices include:\n    - Logback (Java)\n    - Log4j2 (Java)\n    - Serilog (.NET)\n    - Winston (Node.js)\n\n2. **Implementing a distributed tracing system:** This will enable us to track requests across multiple services or microservices. Popular options include:\n    - Jaeger\n    - Zipkin\n    - OpenTelemetry\n\n3. **Adding unique request IDs to log messages:** This will allow us to correlate all log messages associated with a specific request.\n\n4. **Logging relevant information at each application layer:**  This may include:\n    - Incoming request details (e.g., endpoint, headers, parameters)\n    - Outgoing request details (e.g., service called, URL, response time)\n    - Errors and exceptions\n    - User IDs and session IDs\n\n5. **Setting up a centralized log management system:** This could be a dedicated log management tool or a cloud-based service. It will provide a central location for storing, searching, and analyzing logs.\n\n## Rationale\n\n* **Asynchronous logging ensures minimal performance impact:**  Log writing happens in the background, preventing delays in processing user requests.\n* **Distributed tracing enables end-to-end request tracking:**  It provides visibility into how requests flow through the application, even across multiple services.\n* **Unique request IDs facilitate log correlation:**  This makes it easy to identify all log entries related to a specific user request, simplifying debugging and troubleshooting.\n* **Centralized logging simplifies management and analysis:**  A centralized system provides a unified view of application logs, enabling efficient monitoring and analysis.\n\nThis approach balances performance, traceability, and maintainability, providing a robust and scalable logging infrastructure for tracing user requests throughout the application.\n","GenTime":"2024-08-19 16:10:37","text":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n\n\n##Decision\nThe use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n"}
{"File Name":"figgy\/0002-queues.md","Context":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"Prediction":"## Decision \n\n**Implement a multi-queue system in Sidekiq to categorize and prioritize background jobs.**\n\nThis decision is driven by the need to:\n\n* **Isolate critical jobs:** Ensure high-priority tasks like user requests and cleanup after user actions are processed quickly and reliably, preventing potential bottlenecks from lower-priority jobs.\n* **Optimize resource allocation:**  Allocate dedicated workers and resources to different job types based on their specific requirements.  For example, bulk ingests might require more memory and processing power than routine cleanup tasks.\n* **Improve monitoring and debugging:** Easily track the progress and performance of different job categories.  Identify and address bottlenecks within specific workflows more effectively. \n\n**Proposed Queue Structure:**\n\nWe propose the following queue structure, open to adjustments based on further analysis:\n\n* **high_priority:**\n    * Processing user requests (e.g., ingesting files, (re)generating derivatives)\n    * Cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting their parent resource)\n* **metadata:**\n    * Refreshing metadata from Voyager\/PULFA\n* **ingest:**\n    * Bulk-ingesting content \n* **maintenance:**\n    * Routine cleanup (cleaning out guest users)\n    * Fixity checking\n\n**Implementation Details:**\n\n* Each background job will be assigned to a specific queue based on its type.\n* Sidekiq workers will be configured to process jobs from specific queues.\n* The number of workers allocated to each queue will be determined based on the expected workload and desired performance.\n\nThis approach allows for greater flexibility and control over background job processing, ensuring that critical tasks are prioritized while optimizing resource utilization. \n","GenTime":"2024-08-19 16:10:44","text":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n\n\n##Decision\nWe will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"Prediction":"## Decision\n\nWe will utilize an Application Load Balancer (ALB) situated within a screened subnet to expose necessary services externally. This approach offers numerous advantages:\n\n* **Minimized Attack Surface:** The ALB acts as a single point of entry, effectively concealing individual service instances from direct external access and significantly reducing the attack surface.\n* **Enhanced Security Posture:**  By residing in a screened subnet, the ALB benefits from additional security measures like Network Access Control Lists (NACLs) and Security Groups, further bolstering our security posture. \n* **Flexibility and Scalability:** ALBs provide advanced traffic routing capabilities, allowing for flexible configuration and seamless scaling of services as needed.\n* **Health Checks and Failover:** Integrated health checks within the ALB can automatically detect and reroute traffic away from unhealthy instances, ensuring high availability for our exposed services. \n\nThis approach aligns with our commitment to minimizing external exposure while maintaining accessibility for critical services. \n","GenTime":"2024-08-19 16:10:49","text":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n\n\n##Decision\nWe have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n"}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"Prediction":"## Decision\n\nTo mitigate the risks associated with tight coupling between the Horace and Herbert projects, we will implement the following:\n\n1. **Define a clear API between Horace and Herbert**: This API will specify the exact functionalities and data structures that Horace expects from Herbert. This will create a stable interface and minimize the impact of changes in Herbert on Horace.\n2. **Implement Versioning for Herbert's API**: Each breaking change to the API will result in a new major version. Horace can then explicitly specify the version of Herbert's API it depends on. This allows for independent evolution of both projects while maintaining compatibility.\n3. **Introduce Integration Tests**: Automated integration tests will be created to validate the interaction between Horace and Herbert. These tests will run automatically on every build or on a regular schedule to detect breaking changes early on.\n4. **Establish Communication Channels**: Improve communication between the development teams of Horace and Herbert. This can include regular meetings, shared documentation, or dedicated communication channels to discuss upcoming changes and potential impacts.\n\nThis approach allows for a more modular and maintainable architecture, reduces the risk of regressions, and fosters better collaboration between the teams responsible for Horace and Herbert.\n","GenTime":"2024-08-19 16:10:54","text":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n\n\n##Decision\nTo make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n"}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"Prediction":"## Decision\n\nWe will use Kubernetes as our container orchestration platform. This decision is based on the following factors:\n\n* **Industry Standard:** Kubernetes has become the de facto standard for container orchestration, with a large and active community. This ensures widespread support, ample resources, and a rich ecosystem of tools and integrations.\n* **Scalability and Resilience:** Kubernetes is designed for high availability and scalability, allowing us to easily scale our applications up or down based on demand. Its self-healing capabilities automate container management and ensure application uptime.\n* **Flexibility and Portability:** Kubernetes is cloud-agnostic, offering deployment flexibility across various cloud providers or on-premise environments. This portability aligns with our goal of avoiding vendor lock-in and maintaining deployment flexibility.\n* **Developer Experience:** Kubernetes provides a declarative approach to infrastructure management, simplifying deployment and configuration through YAML files. This empowers developers to focus on application development rather than infrastructure management.\n\nThis decision aligns with the overall context and container platform considerations outlined in ADR 0002.\n","GenTime":"2024-08-19 16:10:59","text":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n\n\n##Decision\nWe use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n"}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"Prediction":"## Decision\n\nPACE will implement versioned documentation using the following approach:\n\n1. **Directory-based versioning:** Each PACE release will have its documentation built and deployed to a dedicated subdirectory named after its version within a central documentation repository. For example:\n    - `https:\/\/pace-docs.com\/latest\/` points to the latest stable release.\n    - `https:\/\/pace-docs.com\/v1.2.3\/` points to the documentation for version 1.2.3. \n2. **Semantic Versioning:** Documentation versions will strictly follow the PACE project's semantic versioning. \n3. **In-Application Version Redirection:** The PACE application UI will include a prominent link to the documentation. This link will always point to the documentation corresponding to the specific version of PACE being used.\n4. **Clear Versioning in Documentation:** Each documentation version will prominently display its corresponding PACE version. \n\n## Rationale\n\n- **Simplicity and Maintainability:**  This approach avoids complex tooling and integrates smoothly with existing build processes.\n- **Clear Version Association:** Users can easily locate and verify they are referencing the correct documentation for their PACE version.\n- **Flexibility:** This approach accommodates future needs like hosting documentation for development branches. \n\n## Consequences\n\n- **Increased Storage:**  Separate documentation builds will consume more storage space. However, this is not a significant concern given the expected size of the documentation. \n- **Maintenance Overhead:**  A process will be required to deploy documentation for new releases. However, this is a routine task that can be easily automated. \n","GenTime":"2024-08-19 16:11:06","text":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n\n\n##Decision\nProjects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n"}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"Prediction":"## Decision\n\n**Use GOV.UK Notify to send emails and SMS messages to users.**\n\nThis decision leverages the existing approval for Hackney to use GOV.UK Notify and benefits from the service's reliability and cost-effectiveness. Integration with Notify will be implemented, and documentation will be updated in the API Playbook to reflect this new standard for email and SMS communication. \n","GenTime":"2024-08-19 16:11:08","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n\n\n##Decision\nUse Notify over a third party service such as SendGrid for the sending of emails and SMS\\n"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"Prediction":"## Decision\n\nWe will introduce a new endpoint on the existing `session-frontend` microservice to provide the time remaining on a user's session. This endpoint will satisfy the following requirements:\n\n* **Unauthenticated:** The endpoint will not require authentication or authorisation, avoiding modification of the session itself.\n* **Stateless:**  The endpoint will operate solely on the session cookie provided in the request header, ensuring no server-side state management.\n* **Lightweight:**  The endpoint will be designed for minimal processing overhead, returning only the remaining session time in a simple format (e.g., JSON).\n\nThis approach provides several benefits:\n\n* **Minimises impact on service teams:** Service teams only need to upgrade the `hmrcTimeoutDialog` library, with no additional routing, configuration, or code changes required.\n* **Avoids knowledge duplication:** Session management logic remains centralised within `session-frontend`, adhering to the Single Responsibility Principle.\n* **Maintains loose coupling:** The endpoint provides a well-defined interface, minimizing dependencies between frontend microservices. \n* **Scalable and resilient:** Leveraging the existing `session-frontend` service ensures scalability and avoids introducing a single point of failure.\n\n**Implementation Details:**\n\n1. **New Endpoint:** `\/session-frontend\/session-remaining-time`\n2. **Request:** The request will include the user's session cookie.\n3. **Response:** A JSON payload will be returned, containing:\n    * `remainingTime`: The remaining session time in milliseconds.\n    * `timeoutWarning`: (Optional) A boolean flag indicating if the user has already been warned about imminent timeout. This flag can be used for more sophisticated timeout handling in the `hmrcTimeoutDialog` component.\n\nThis approach balances the need for a robust solution with the constraints of minimizing impact on service teams and maintaining a loosely coupled architecture. \n","GenTime":"2024-08-19 16:11:17","text":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n\n##Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n"}
{"File Name":"celestia-core\/adr-005-consensus-params.md","Context":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n","Decision":"### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","tokens":111,"id":4099,"Prediction":"## Decision\n\nWe will implement a system for managing consensus-critical parameters that provides flexibility, network specificity, and future-proofing:\n\n1. **Genesis File Initialization:**  All consensus-critical parameters will be defined in the blockchain's genesis file. This ensures that:\n    *  Every node starts with the same initial parameters.\n    *  Network variations are inherently supported by different genesis files.\n\n2. **ABCI Exposure:** All parameters will be queryable through the ABCI (Application Blockchain Interface). This enables:\n    *  External tools and dashboards to access and display these parameters.\n    *  Potential for future mechanisms to update these parameters based on network consensus.\n\n3. **Parameter Registry:**  Instead of hardcoding specific parameters, we will create a generic parameter registry within the consensus engine.  This registry will:\n    * Store parameters by name (e.g., \"max_block_size\", \"evidence_validity_period\").\n    *  Provide type safety and validation upon initialization and potential future updates. \n    *  Allow for easy extensibility as new consensus-critical parameters are introduced.\n\n4. **Initial Parameter Set:**  We will start by migrating existing parameters like `max_block_size` and `max_tx_size` to the genesis file and parameter registry. We will also define an initial set of anticipated future parameters (e.g., \"evidence_validity_period\", \"checkpoint_frequency\") with placeholder or default values in the genesis file to avoid breaking changes later.\n\n## Consequences\n\n* **Increased Flexibility:** The system can adapt to future needs as new parameters are introduced without requiring hard forks.\n* **Enhanced Network Governance:**  Provides a foundation for potential on-chain governance mechanisms to update parameters based on network consensus.\n* **Improved Transparency:**  Makes consensus parameters readily available to all stakeholders, improving understanding and trust.\n* **Added Complexity:**  Introduces a new parameter registry component and requires modifications to the ABCI and consensus engine. \n","GenTime":"2024-08-19 16:11:25","text":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n\n\n##Decision\n### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"Prediction":"## Decision\n\nGiven the context and decision drivers, we will pursue the following data strategy:\n\n* **Primary Data Source:**  Utilize **Platform 2: Koshvani web** (http:\/\/koshvani.up.nic.in\/) as the primary data source. This decision is based on the platform's explicit purpose: \"Scrape, analyze and publish open budget data for public consumption.\" Accessing data directly from the source designed for this purpose offers advantages in terms of data structure, consistency, and potential use of APIs. \n\n* **Secondary Data Source (Conditional):** Leverage **Platform 1: Budget - Government of Uttar Pradesh** (http:\/\/budget.up.nic.in\/) as a supplementary data source. This platform, while containing valuable budget documents, might require more extensive processing and analysis to extract structured information. It can provide contextual information and potentially historical data not available in the primary source. \n\n## Reasoning\n\n* **Direct Access to Open Data:** Koshvani web explicitly aims to provide open budget data, indicating a higher likelihood of well-structured, machine-readable formats (e.g., APIs, CSV downloads) compared to extracting data from budget documents.\n* **Reduced Processing Overhead:**  Scraping and structuring data from PDFs and website text (likely required for Platform 1) demands significant processing compared to readily available open data on Platform 2. This minimizes development effort.\n* **Complementary Information:** Platform 1, while not ideal as the primary source, can offer valuable contextual information, historical data, or specific details absent from the more structured open data platform. \n\n## Consequences\n\n* **Potential Data Limitations:** Relying primarily on Koshvani web may mean accepting limitations in data granularity or specific data points not included in their open data initiative.\n* **Increased Development Time (Conditional):** Utilizing Platform 1 as a secondary source introduces additional complexity and development time for data extraction and alignment with the primary source.\n* **Enhanced Understanding of UP Fiscal Data:** Combining data from both sources has the potential to create a more comprehensive and insightful view of Uttar Pradesh's fiscal landscape.\n\nThis approach aims to balance efficiency in accessing readily available open data with the potential to gain a more holistic understanding by incorporating data from the budget documents. \n","GenTime":"2024-08-19 16:11:35","text":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n\n\n##Decision\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n"}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"Prediction":"## Decision\n\nWe will adopt the React Context API for managing global state and avoiding prop drilling in the dotcom project. This decision is based on the following:\n\n* **Simplicity:** The Context API is built into React, eliminating the need for external dependencies. This aligns with our goal of keeping the codebase maintainable.\n* **Performance:**  While the spike ([PR #801](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801)) demonstrated potential performance improvements, we will need to closely monitor performance as the application scales and consider optimizations if necessary. \n* **Targeted Solution:** The initial focus will be on managing the `edition` property globally. This allows us to address the specific prop drilling issue identified without over-engineering a solution for all state management needs.\n\n**Implementation Details:**\n\n* We will start by migrating the `edition` property to the Context API.\n* A new context will be created specifically for global application state.\n* Clear documentation and guidelines will be provided on when and how to utilize the Context API.\n\nThis decision will be revisited if:\n\n* Performance issues arise due to extensive use of the Context API.\n* The complexity of state management grows beyond the capabilities of the Context API. \n* A more robust state management solution becomes necessary. \n","GenTime":"2024-08-19 16:11:41","text":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n\n\n##Decision\n-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n"}
{"File Name":"princeton_ansible\/0004-key-scope","Context":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n","Decision":"Keys should be minted on a per-feature, per-application basis.\\n","tokens":61,"id":2256,"Prediction":"## Decision\n\n**Implement short-lived, scoped access tokens with granular permissions.**\n\n## Rationale\n\nThis approach minimizes the impact of a compromised key or token by limiting its potential for damage:\n\n* **Short-lived:** Tokens expire after a short period, rendering them useless after that time, even if compromised.\n* **Scoped:** Tokens grant access only to the specific resources and actions required for the task at hand, reducing the potential attack surface.\n* **Granular Permissions:**  Fine-grained permission control allows us to further limit the scope of a token based on the principle of least privilege.\n\n## Consequences\n\n* **Increased complexity:** Managing short-lived tokens introduces additional complexity compared to long-lived keys. We will need to implement mechanisms for token generation, distribution, validation, and revocation.\n* **Potential performance impact:** Frequent token generation and validation can introduce performance overhead. This should be mitigated by using efficient algorithms and caching strategies.\n* **User experience considerations:**  Short token lifespans might necessitate frequent re-authentication, potentially impacting user experience. This can be addressed through mechanisms like refresh tokens or single sign-on. \n\n## Considered Alternatives\n\n* **Long-lived keys with strong isolation:** This approach relies on isolating keys and limiting their accessibility. While simpler to implement, it leaves us more vulnerable in case of a successful attack.\n* **Hardware Security Modules (HSMs):**  While offering the highest level of security, HSMs come with significant cost and complexity implications, potentially making them unsuitable for all scenarios.\n\n## Action Items\n\n* Define token expiration times and scopes based on specific use cases.\n* Implement a secure token generation and validation mechanism.\n* Integrate token management into the application's authentication and authorization framework.\n* Implement monitoring and logging for token-related activities to detect suspicious behavior. \n","GenTime":"2024-08-19 16:11:49","text":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n\n\n##Decision\nKeys should be minted on a per-feature, per-application basis.\\n"}
{"File Name":"island.is\/0012-chart-library.md","Context":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n","Decision":"- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","tokens":79,"id":1119,"Prediction":"## Decision\n\nAfter evaluating several charting libraries based on the identified decision drivers, we have decided to use **[Library Name]** for all charting needs across the Island.is repository. \n\n**Rationale:**\n\n* **[Library Name]** meets all the technical requirements for displaying charts and graphs, including the desired customization options for a unified look and feel.\n* **[Library Name]** offers a well-documented API with strong community support, simplifying development and troubleshooting.\n* **[Library Name]** fits within our budget constraints with [Pricing model details - e.g. free, tiered, open-source].\n* **[Library Name]** has a [Small\/Medium\/Large] bundle size which is deemed acceptable considering its features and performance.\n* **[Library Name]** provides excellent Typescript support, aligning with our existing codebase and improving developer experience.\n\n** Considered Options:**\n\n* **[Alternative Library 1]:** Briefly explain why this option was not chosen, highlighting its shortcomings based on the decision drivers.\n* **[Alternative Library 2]:**  Briefly explain why this option was not chosen, highlighting its shortcomings based on the decision drivers.\n\n**Consequences:**\n\n* **Positive:**\n    * Unified look and feel across all Island.is projects.\n    * Improved development efficiency and consistency.\n    * Easier maintenance and updates due to a single charting library. \n* **Negative:**\n    * Potential learning curve for developers unfamiliar with **[Library Name]**.\n    * Possible limitations in the future if **[Library Name]** does not evolve to meet new requirements. \n\n**Action Items:**\n\n* Update documentation and guidelines to reflect the use of **[Library Name]** as the standard charting library.\n* Create internal resources and examples to facilitate the adoption of **[Library Name]**. \n* Migrate existing charts to **[Library Name]** as needed. \n","GenTime":"2024-08-19 16:11:57","text":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n\n\n##Decision\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n"}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences.  Each ADR will be a separate file living in the `\/docs\/adrs` directory of our repository, following a standardized template (e.g., MADR or similar). \n","GenTime":"2024-08-19 16:11:59","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n"}
{"File Name":"support-rota\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":3389,"Prediction":"## Decision\n\nWe will use the 'dotenv' gem to manage environment variables in our application. \n\n### Reasoning\n\n* **Actively Maintained:** 'dotenv' is an actively maintained and widely used gem, addressing the support concerns with the deprecated 'Figaro' gem.\n* **Centralized Management:** It provides a centralized location (`.env`) to define and manage environment variables, improving code readability and reducing the risk of missing variables.\n* **Fail-Fast Mechanism:**  'dotenv' can be configured to raise an exception if a required environment variable is missing, promoting early failure detection during startup.\n* **Testability:**  The gem allows loading environment variables from different sources, making it easy to stub them in test environments without affecting the actual system environment.  \n\n### Implementation Details\n\n1. **Installation:** Add `gem 'dotenv'` to your Gemfile and run `bundle install`.\n2. **Configuration:**\n    * Create a `.env` file in the root of your project to store environment variables.\n    * In your application's entry point (e.g., `config\/application.rb` or similar), load the environment variables using:\n    ```ruby\n    require 'dotenv\/load'\n    Dotenv.load # Load .env \n    ``` \n3. **Access Variables:** Access environment variables using `ENV['VARIABLE_NAME']`.\n4. **Testing:**\n   *  Use `Dotenv.load('path\/to\/.env.test')` to load test-specific variables. \n   * Utilize testing frameworks like RSpec to manage different environment configurations for your tests.\n\n### Consequences\n\n* **Standardized Approach:** Introduces a consistent and maintainable way to handle environment variables throughout the codebase.\n* **Improved Security:**  Encourages storing sensitive information outside of version control (using `.env.example` for a template).\n* **Simplified Testing:**  Provides a clean mechanism for stubbing environment variables during testing.\n\n### Alternatives Considered\n\n* **Rails Credentials:** While suitable for secret management, Rails Credentials may not be ideal for feature flags or non-sensitive environment variables due to encryption overhead. \n* **Custom Wrapper:** Building a custom wrapper adds complexity and maintenance overhead compared to utilizing a well-maintained gem like 'dotenv'.\n\nThis approach balances simplicity, maintainability, and testability, aligning with the project's need for robust environment variable management.\n","GenTime":"2024-08-19 16:12:09","text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n"}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"Prediction":"## Decision\n<!-- [Document the decision how to solve the problem. ] -->\nWe will use a migrations-based approach to manage the database schema evolution for both PostgreSQL and SQLite. This approach offers several advantages over managing three separate schema versions:\n\n* **Incremental changes:** Migrations allow us to define small, self-contained changes to the database schema over time. This makes it easy to track the history of the schema and understand how it has evolved.\n* **Version control:** Migrations can be easily version controlled alongside our application code. This ensures that the database schema is always in sync with the application codebase.\n* **Flexibility:** This approach provides flexibility to apply and rollback changes to the database, making it easy to switch between different project milestones for development and testing purposes.\n* **Tooling:**  Many mature migration tools exist for both PostgreSQL (e.g., Flyway, Liquibase) and SQLite (e.g., SQLite migration library). These tools automate the process of applying and managing migrations.\n\nWe will choose a specific migration tool based on our project requirements and evaluate its compatibility with both PostgreSQL and SQLite.  \n\n**Schema Design Considerations:**\n\n* We will design the schema with future milestones in mind, aiming for a structure that can accommodate anticipated features without requiring major overhauls.\n* To ensure compatibility between PostgreSQL and SQLite, we will adhere to the common subset of SQL features supported by both databases. This may involve careful consideration of data types, constraints, and indexing strategies.\n\nBy adopting a migrations-based approach and adhering to these design considerations, we can ensure a robust, maintainable, and scalable database schema for both the web and desktop versions of our application.\n","GenTime":"2024-08-19 16:12:17","text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"Prediction":"## Decision\n\n**Decision:** Adopt a microservice architecture for the software system.\n\n**Reasoning:** \n\nThe inherent characteristics of microservice architecture directly address the critical requirements of scalability, reliability, and performance:\n\n* **Scalability:**  Microservices allow for independent scaling of specific components based on demand. This targeted approach optimizes resource utilization and ensures responsiveness even during peak loads. \n* **Reliability:** Microservices enhance fault isolation. If one service encounters an issue, it won't necessarily cascade and impact the entire system. This isolation allows for quicker recovery and minimizes downtime.\n* **Performance:**  Microservices promote the use of specialized technologies best suited for each component's function, potentially leading to performance gains compared to a monolithic architecture. Furthermore, independent deployment cycles enable continuous optimization and faster iteration on performance improvements. \n\n**Consequences:**\n\n* **Increased Complexity:** Managing a distributed system of microservices introduces architectural, operational, and deployment complexities. \n* **Inter-service Communication Overhead:**  Communication between microservices can introduce latency.  Careful design and implementation of communication protocols are crucial.\n* **Data Consistency Challenges:** Maintaining data consistency across multiple services requires careful planning and potentially the use of techniques like eventual consistency.\n\n**Next Steps:**\n\n* **Define Service Boundaries:** Conduct a thorough domain analysis to determine appropriate service boundaries and minimize inter-service dependencies.\n* **Technology Selection:** Evaluate and select technologies best suited for each microservice's functionality, considering factors like programming languages, databases, and communication protocols.\n* **Establish Monitoring and Logging:** Implement robust monitoring and logging mechanisms to gain visibility into the distributed system's health, performance, and potential issues. \n* **Deployment Strategy:** Choose a deployment strategy (e.g., containers, serverless) that aligns with the scalability and management needs of the microservice architecture. \n","GenTime":"2024-08-19 16:12:25","text":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n\n\n##Decision\nAll agreed with this decision\\n"}
{"File Name":"saas-platform-frontend\/0003-use-javascript-over-typescript.md","Context":"## Context\\nI need to decide in which language I implement the frontend.\\n","Decision":"I use Javascript.\\n","tokens":16,"id":3715,"Prediction":"## Decision\n\nWe will implement the frontend using **[Language Choice]** (e.g., React with TypeScript).\n\n## Reasons\n\n* **[Reason 1]:**  (e.g.,  The team has strong existing expertise in React and TypeScript, enabling faster development and easier maintenance).\n* **[Reason 2]:**  (e.g., React's component-based architecture aligns well with the modular design of our application, promoting reusability and maintainability). \n* **[Reason 3]:** (e.g., TypeScript's static typing will improve code quality, reduce bugs, and enhance long-term maintainability).\n* **[Reason 4 (Optional)]:** (e.g.,  The strong community support and extensive ecosystem around React and TypeScript provide access to a wide range of libraries and tools). \n\n## Alternatives Considered\n\n* **[Alternative Language 1]** (e.g., Vue.js with JavaScript): While Vue.js offers a gentler learning curve and potentially faster initial development, the team's stronger React expertise and the benefits of TypeScript outweigh these advantages in the long run.\n* **[Alternative Language 2]** (e.g., Angular):  Although Angular provides a comprehensive framework, its steeper learning curve and opinionated structure might not be the most efficient choice for this specific project. \n\n## Consequences\n\n* **Positive:**\n    * Faster development due to team expertise.\n    * Improved code quality and maintainability through TypeScript.\n    * Access to a vast ecosystem of libraries and tools.\n* **Negative:**\n    * Potential learning curve for new team members unfamiliar with React or TypeScript. \n    * (Add any potential drawbacks specific to your chosen language). \n","GenTime":"2024-08-19 16:12:32","text":"## Context\\nI need to decide in which language I implement the frontend.\\n\n\n##Decision\nI use Javascript.\\n"}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"Prediction":"## Decision \n\n(Add a clear and concise decision, based on the context provided below. This should be a concrete action or direction, not a restatement of the problem.) \n","GenTime":"2024-08-19 16:12:34","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n"}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for significant architectural decisions. Each ADR will be a single Markdown file stored in a dedicated \"adr\" directory at the root of the project repository. We will follow the MADR template, which includes the following sections:\n\n* **Title**: Short, descriptive title summarizing the decision.\n* **Status**: Proposed, Accepted, Rejected, Superseded, Deprecated, etc.\n* **Context**: Background information and forces at play leading to the decision.\n* **Decision**: The architectural decision being made.\n* **Consequences**: The intended positive and negative consequences of the decision.\n* **Pros and Cons**: A more detailed list of pros and cons (optional).\n\nNew ADRs will be created for each significant architectural decision and updated as necessary throughout the project lifecycle. \n\nThis approach will provide a clear and concise history of the architectural decisions made on the project, making it easier for the team to understand the rationale behind the current architecture and make informed decisions in the future. \n","GenTime":"2024-08-19 16:12:39","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"Prediction":"## Decision\n\n**(Replace this with a concise description of the decision that was made.  Be specific about what was chosen and any important parameters.)** \n\nFor example: \n\n> We will use PostgreSQL as our primary database.  We will use the JSONB datatype to store flexible schema data.  We will host the database on Amazon RDS for ease of management and scaling. \n","GenTime":"2024-08-19 16:12:41","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"Prediction":"## Decision \n\n**(This section is intentionally left blank as no Context was provided. Please provide the context so I can help you formulate a corresponding decision.)** \n","GenTime":"2024-08-19 16:12:42","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"File Name":"mlmc\/0002-use-mongodb-for-operation.md","Context":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n","Decision":"* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","tokens":48,"id":1574,"Prediction":"## Decision\n\nWe will use a document-based NoSQL database for the Operation API's data storage. \n\n## Decision Rationale\n\n* **Easy to Use:** NoSQL databases are renowned for their ease of use, especially for developers accustomed to working with JSON or similar data formats. They generally require less setup and maintenance compared to relational databases.\n* **Fast Storage and Retrieval:** NoSQL databases excel at handling read and write operations quickly, making them suitable for applications requiring rapid data access, like a missile operation API where response time is critical.\n* **Dynamic and Schemaless:** The schemaless nature of document databases provides flexibility to evolve the data structure over time without complex migrations, accommodating potential future changes in missile attributes or operational parameters.\n\n## Considered Options\n\n* **Relational Database (SQL):** While reliable and widely used, relational databases require a pre-defined schema and can be less performant for the dynamic data structures and high-speed requirements of the API. \n* **Key-value Store:**  A potential option for simple data models. However, the missile operation API might require storing more complex data structures for each missile, making a key-value store less suitable.\n\n## Implications\n\n* **Data Consistency:**  Consider eventual consistency models inherent to some NoSQL solutions and implement appropriate measures to ensure data integrity for critical missile operations.\n* **Security:** Implement robust security measures to safeguard the sensitive nature of the data stored within the NoSQL database. \n* **Operational Monitoring:**  Establish comprehensive monitoring and alerting systems to ensure the database's performance and availability align with the critical nature of the missile operation API. \n","GenTime":"2024-08-19 16:12:49","text":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n\n\n##Decision\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n"}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"Prediction":"## Decision\n\n**Migrate away from the existing distributed MailQueue implementation based on RabbitMQ, Cassandra, and ObjectStorage to a centralized, scalable solution based on Apache Kafka.**\n\n## Reasons\n\n* **Simplified Administration:** A centralized solution will provide a single point of administration for all mail queue operations, regardless of the number of James servers in the distributed setup. This addresses the current pain point of needing to interact with each James server individually.\n* **Rich Feature Set:** Apache Kafka supports all required functionalities, including browsing, size retrieval, arbitrary mail removal, delaying retries, and throttling. This eliminates the limitations of the current implementation with RabbitMQ and Cassandra. \n* **Scalability and Performance:** Kafka is known for its high throughput and scalability, making it suitable for handling large volumes of email traffic.\n* **Eliminate Tombstone Issues:** Using Kafka as the primary queue removes the reliance on Cassandra and the associated tombstone anti-pattern, improving data management and performance.\n\n## Consequences\n\n* **Migration Effort:** Migrating from the existing distributed system to a Kafka-based solution will require development effort and thorough testing.\n* **Operational Expertise:** The team will need to acquire expertise in deploying, managing, and monitoring Apache Kafka.\n* **Potential Vendor Lock-in:** Choosing a specific technology like Kafka might lead to potential vendor lock-in. However, the benefits of a unified and feature-rich solution outweigh this concern.\n\n## Alternatives Considered\n\n* **Enhancing the existing solution:** This was deemed impractical due to the inherent limitations of RabbitMQ and Cassandra for the required operations.\n* **Evaluating other message brokers:** While other message brokers exist, Kafka's combination of features, performance, and ecosystem make it the most suitable choice for this use case. \n\n## Status\n\nProposed\n\n## Open Issues\n\n*  Evaluate different Kafka deployment options (self-hosted vs. managed).\n* Define a detailed migration plan with minimal downtime. \n* Determine the optimal Kafka configuration for the expected email volume and usage patterns.\n","GenTime":"2024-08-19 16:12:58","text":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n\n##Decision\nDistributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n"}
{"File Name":"support-rota\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":3388,"Prediction":"## Decision\n\n**Maintain the use of Rollbar as our primary error tracking tool for existing applications, but standardize on Sentry for all new projects.**\n\n**Justification:**\n\n* **Existing Investment:** We have a significant number of applications already integrated with Rollbar and existing experience with the tool. Migrating everything would be time-consuming and disruptive.\n* **Standardization:** Having multiple error tracking tools introduces unnecessary complexity and overhead. Consolidating on a single tool for new projects simplifies workflows and reduces cognitive load for developers. \n* **Technical Tooling Alignment:** Adopting Sentry for new projects aligns with our strategy of supporting the technical tooling preferences of incoming teams, easing the transition to business as usual.\n* **Future Flexibility:** This decision allows us to evaluate Sentry in real-world scenarios and potentially revisit a complete migration to Sentry in the future, if justified.\n\n**Action Items:**\n\n* **Rollbar Environment Standardization:** Develop and implement a standardized approach for aligning Rollbar environment names with Rails environments to improve clarity and maintainability. Update documentation to reflect this change.\n* **Sentry Onboarding:** Define an onboarding process and documentation for new projects integrating with Sentry.\n* **Evaluation Framework:** Establish criteria and a timeline for evaluating the success of Sentry adoption and the potential for future migration of existing Rollbar projects.\n\n**Consequences:**\n\n* Short-term, we maintain a degree of tooling complexity by supporting both Rollbar and Sentry.\n* Long-term, this strategy allows us to streamline our error tracking and potentially consolidate on a single tool while minimizing disruption to existing projects. \n","GenTime":"2024-08-19 16:13:04","text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n"}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"Prediction":"## Decision\n\nWe will adopt the **Conventional Commits** specification for structuring our git commit messages. \n\n### Reasons\n\n* **Improved readability and understandability:** Conventional Commits provide a clear and consistent format, making it easier for developers to understand the purpose and scope of each commit.\n* **Automated changelog generation:** The structured format allows for automated changelog generation, reducing manual effort and ensuring accuracy. \n* **Semantic versioning compatibility:**  Conventional Commits align well with semantic versioning, enabling automatic version bumps based on commit message analysis.\n* **Enhanced collaboration and code review:** A standardized format facilitates communication within the team and streamlines the code review process.\n* **Widely adopted standard:**  Conventional Commits is a widely used and recognized standard, making it easier for new team members to adapt and for tools to integrate with.\n\n### Consequences\n\n* **Learning curve:**  Team members unfamiliar with Conventional Commits will require a brief onboarding period.\n* **Stricter commit discipline:**  Developers need to adhere to the specified format, which might require adjustments to their workflow. \n\nWe believe the benefits of improved maintainability, collaboration, and automation outweigh the potential drawbacks of a small learning curve and stricter discipline. \n","GenTime":"2024-08-19 16:13:10","text":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n\n##Decision\nWe agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n"}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"Prediction":"## Decision\n\nWe will adopt JSON Web Tokens (JWTs) as the mechanism for authentication and authorization within FlowKit, replacing the existing rudimentary system based on Postgres usernames and passwords. \n\nThis decision is driven by the following factors:\n\n* **Granular Access Control:** JWTs allow for fine-grained control over API access, enabling us to specify permissions at the individual resource level. This is crucial for supporting diverse use cases, such as sharing specific results via dashboards without granting broader data access.\n* **Centralized Management:** JWTs can be centrally managed, simplifying administration and enhancing security. This is particularly beneficial as it decouples authentication from individual FlowKit instances.\n* **Enhanced Security:** The cryptographic signing of JWTs ensures their integrity and authenticity, mitigating risks associated with tampering. Their ephemeral nature further limits the impact of potential token theft.\n* **Low Overhead and Broad Support:**  JWTs impose minimal overhead on both development and user experience. Extensive language support ensures seamless integration with FlowKit's technology stack. \n\nWhile alternative authentication mechanisms like traditional login systems, integration with Docker host authentication, or public key authentication were considered, they were deemed less suitable due to factors like increased administrative burden, limited granularity, and potential usability drawbacks. \n","GenTime":"2024-08-19 16:13:17","text":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n\n\n##Decision\nAPI authentication and access control will use JWTs.\\n"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"Prediction":"## Decision\n\nWe will use **Poetry** for dependency management and virtual environment creation for all new Python projects. \n\n**Reasoning:**\n\n* **Modern Approach:** Poetry leverages the latest `pyproject.toml` standard, promoting better integration with the evolving Python ecosystem and reducing boilerplate configuration. \n* **Streamlined Workflow:** Poetry aims to be a one-stop shop for dependency management, virtual environments, packaging, and even publishing. This simplifies the developer workflow compared to using separate tools like `venv` and `pip`.\n* **Active Development and Community:** While relatively new, Poetry is actively maintained, addresses many shortcomings of Pipenv, and is gaining traction in the Python community.\n\n**Consequences:**\n\n* **Learning Curve:**  Developers unfamiliar with Poetry will need to invest time in learning its commands and features.\n* **Tooling Integration:**  Ensure existing IDEs and CI\/CD pipelines seamlessly integrate with Poetry.  \n* **Project Migration:** Existing projects using different dependency management tools might require adjustments to adopt Poetry.\n\n**Alternatives Considered:**\n\n* **Pipenv:** While recommended by PyPA, it suffers from performance issues, inconsistencies, and stalled development.\n* **venv + pip:** This traditional approach offers flexibility but lacks the streamlined experience and advanced features of Poetry.\n\n**Monitoring:**\n\n* We will actively monitor Poetry's development and community feedback to assess its long-term viability and address potential issues. \n* Evaluate developer satisfaction and productivity gains after Poetry adoption.  \n","GenTime":"2024-08-19 16:13:24","text":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n\n\n##Decision\nWe will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n"}
{"File Name":"TruBudget\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1226,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will follow a standardized template (e.g., MADR or Nygard) and be stored in a dedicated version-controlled repository (e.g., `\/docs\/adr` within the project's codebase) for easy access and traceability.\n","GenTime":"2024-08-19 16:13:26","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"Prediction":"## Decision\n\nWe will implement a mechanism for eager shape computation in Nd4j's model import framework. This will involve:\n\n* **Defining a ShapeInference context:** This context will hold information about input shapes, operations, and any relevant configuration parameters needed for shape inference.\n* **Developing a shape inference function per operation:** Each Nd4j operation used in model import will have a corresponding function capable of determining the output shape(s) given the input shapes and operation parameters within the ShapeInference context.\n* **Integrating eager shape computation into the model import pipeline:**  When encountering an operation during model import, the framework will invoke the corresponding shape inference function to determine the output shapes. These shapes will be used to construct the Nd4j graph.\n\nThis approach prioritizes eager shape computation for model import, acknowledging the potential for future expansion to a fully eager computation mode.\n","GenTime":"2024-08-19 16:13:30","text":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n\n\n##Decision\nIn order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n"}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"Prediction":"## Decision\n\n**We will use AWS Secrets Manager as the single source of truth for storing secrets within the Modernisation Platform.**\n\n## Reasons\n\n* **Consistency and Maintainability:** Using a single, centralized service promotes consistency across the Modernisation Platform, reducing complexity and making it easier to manage and audit secrets.\n* **Integration with AWS Ecosystem:** As the Modernisation Platform heavily utilizes AWS services, Secrets Manager offers seamless integration and eliminates the need for additional third-party dependencies.\n* **Robust Security Features:** Secrets Manager provides industry-standard security features such as encryption at rest and in transit, granular access control, and automatic rotation, ensuring the confidentiality and integrity of sensitive information.\n* **Auditing and Compliance:** Secrets Manager provides detailed audit logs of all secret access and management operations, simplifying compliance with regulatory requirements. \n\n## Alternatives Considered\n\n* **AWS Parameter Store:** While viable for smaller applications, it lacks the advanced security features and fine-grained access control provided by Secrets Manager. \n* **HashiCorp Vault:** A powerful solution but introduces additional infrastructure complexity and potential vendor lock-in, which is not ideal for the current scope of the Modernisation Platform.\n* **LastPass & Git-Crypt:** Deemed unsuitable for production environments due to limited scalability, security concerns, and incompatibility with the Modernisation Platform's existing infrastructure.\n\n## Consequences\n\n* **Migration Effort:** Existing secrets stored in disparate systems will need to be migrated to Secrets Manager.\n* **Learning Curve:** Teams unfamiliar with Secrets Manager will require training and support to adopt the new solution effectively. \n\nThis decision will be reviewed periodically or as significant changes occur within the Modernisation Platform's security requirements or the technology landscape. \n","GenTime":"2024-08-19 16:13:38","text":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n\n\n##Decision\nWe've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"Prediction":"## Decision\n\nWe will use a single Terraform configuration with multiple workspaces to manage different environments (staging, prod, etc.). This decision is based on the following:\n\n* **DRY (Don't Repeat Yourself):**  Using workspaces eliminates code duplication across environments, leading to a more maintainable and less error-prone codebase.\n* **Terraform Best Practice:** Utilizing workspaces for environment management aligns with Terraform's own recommendations, suggesting potential advantages in terms of tool support and future compatibility.\n* **Simplified Management:**  Switching between environments becomes as simple as changing the active workspace, streamlining the development and deployment process.\n\nWhile we acknowledge the explicitness provided by per-environment directories, the benefits of a DRY approach and alignment with Terraform best practices outweigh this advantage in our context. \n","GenTime":"2024-08-19 16:13:41","text":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\n\n##Decision\nUse workspaces to manage multiple environments.\\n"}
{"File Name":"heptaconnect-docs\/2021-10-30-route-capabilities.md","Context":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n","Decision":"All limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","tokens":106,"id":3210,"Prediction":"## Decision\n\nIntroduce a **Route Policy** mechanism to the core application. This mechanism allows defining reusable policies that modify or restrict data flow behavior within a route without requiring changes to the route implementation itself. \n\n**Route Policies will offer the following capabilities:**\n\n* **Configurability:** Defined via a simple configuration structure (e.g., JSON, YAML) directly within the route definition or externally.\n* **Reusability:**  Policies can be applied to multiple routes as needed, promoting consistency and reducing redundancy.\n* **Extensibility:**  Support for custom policy implementations to cater to project-specific requirements.\n* **Enforcement:** Core logic will enforce the defined policies, ensuring consistent behavior across integrations.\n\n**Examples of Route Policies:**\n\n* **One-Time Transfer:** Restrict data flow to a single execution, preventing subsequent transfers.\n* **Equivalence Mapping:** Enable data transfer only if an equivalent entry is found on the target system. \n* **Data Transformation:**  Modify the data payload during the transfer based on predefined rules.\n* **Conditional Routing:** Dynamically route data based on content or metadata.\n\n**Implementation Details:**\n\n* Policies will be evaluated within the core routing logic before data transfer.\n* A chain-of-responsibility pattern can be employed to process multiple policies sequentially.\n* Integrators will not need to handle policy logic; they interact with the core routing mechanism as before. \n\nThis approach provides a centralized, configurable, and extensible way to manage route behavior, addressing the need for specific data flow limitations without burdening integrators. \n","GenTime":"2024-08-19 16:13:48","text":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n\n\n##Decision\nAll limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n"}
{"File Name":"tove\/adr-03.md","Context":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"Prediction":"## Decision\n\nWe will utilize **Azure Blob Storage** for storing and serving transcription files. \n\n### Justification\n\nWhile both Azure Blob Storage and File Services offer potential solutions, Blob Storage aligns better with our requirements for the following reasons:\n\n* **Simplified Sharing:** Blob Storage enables straightforward file sharing through Shared Access Signatures (SAS). SAS tokens provide time-limited, secure access to specific blobs or containers, allowing controlled downloads without exposing account credentials. This mechanism aligns directly with our need to provide secure, temporary download links to users.\n* **Optimized for Large Files:** Blob storage is designed to handle large files efficiently, making it suitable for storing and serving potentially sizable transcription files.\n* **Scalability and Performance:** Blob Storage offers high scalability and performance, crucial for handling varying user demands and large volumes of transcription data.\n* **Cost-Effectiveness:** For our use case of infrequent file access (primarily downloads), Blob Storage's tiered storage options (Hot, Cool, Archive) provide cost optimization by aligning storage costs with access patterns. \n\n**Process for Sharing Files with Users:**\n\n1.  **Generate SAS Token:** Upon user request for a download, generate a SAS token with appropriate permissions (read) and expiry time.\n2.  **Construct Download URL:** Create a download URL by appending the SAS token to the Blob URL.\n3.  **Serve to User:** Provide the user with the generated download URL. Depending on the size of the files and estimated zipping time:\n    * **Direct Download:** If zipping is quick, trigger a direct download using the URL.\n    * **Download Link:** If zipping takes longer, present the URL as a download link, allowing the user to download the file when ready.\n\n### Alternatives Considered\n\n* **Azure File Services:** This service provides a more traditional file system interface. While suitable for scenarios requiring file share access, it lacks the granular access control and optimized large file handling that Blob Storage offers.\n\n### Consequences\n\n* **Learning Curve:**  Teams will need to familiarize themselves with Azure Blob Storage concepts and APIs.\n* **Potential Vendor Lock-in:** Adopting Azure Blob Storage may lead to some vendor lock-in. \n\nThis decision favors ease of use, scalability, and cost-effectiveness, aligning with the core requirements of our transcription storage and delivery system. We will continually re-evaluate this decision as the application evolves and usage patterns change. \n","GenTime":"2024-08-19 16:13:58","text":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n\n\n##Decision\nWe don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n"}
{"File Name":"cloud_controller_ng\/0003-switching-to-request-specs-for-controllers.md","Context":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","Decision":"--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","tokens":403,"id":1829,"Prediction":"## Decision\nWe will no longer write controller specs and instead rely solely on request specs for testing controllers. \n","GenTime":"2024-08-19 16:14:00","text":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n\n\n##Decision\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n"}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"Prediction":"## Decision\n\nWe will enforce a structured commit message format for all commits in the `klokwrk-project` system. This format will be based on the Conventional Commits specification (https:\/\/www.conventionalcommits.org\/) to ensure human and machine readability. \n\n**Rationale:**\n\n* **Improved maintainability:** Structured commit messages provide context and clarity, making it easier to understand the changes made in each commit. This simplifies debugging, code review, and future development.\n* **Automated tooling:** Adhering to a standard format like Conventional Commits enables the use of tools for automated changelog generation, release notes creation, and other commit history analysis. This automation saves time and improves the quality of project documentation.\n* **Enhanced collaboration:** Consistent commit messages facilitate better communication within the development team. Everyone understands the intent and impact of changes, leading to more effective collaboration and fewer misunderstandings. \n\n**Consequences:**\n\n* **Increased upfront effort:** Developers will need to invest a small amount of additional effort to learn and adhere to the structured format.\n* **Potential for conflicts:** Enforcing the format might lead to minor conflicts during git merges, requiring manual resolution.\n\n**Mitigation:**\n\n* **Provide clear guidelines and examples:** Documentation and training materials will be provided to help developers understand and correctly use the chosen format.\n* **Integrate with tooling:** Utilize Git hooks and other tools to automate format validation and provide immediate feedback to developers. This will ensure adherence to the chosen standard and minimize the potential for errors. \n","GenTime":"2024-08-19 16:14:06","text":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n"}
{"File Name":"gsp\/ADR041-service-operated-policies.md","Context":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n","Decision":"* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","tokens":335,"id":3907,"Prediction":"## Decision\n\nWe will implement a two-pronged approach to manage IAM permissions for provisioned resources:\n\n**1. Service Operator IAM Role:**\n\n* The service operator will utilize its own dedicated IAM role with permissions to manage all aspects of the provisioned resources. This allows the operator to perform actions like:\n    * Creating, updating, and deleting resources based on the declarative manifests.\n    * Configuring bucket-level settings such as Lifecycle Rules, Versioning, etc.\n    * Applying necessary tags and policies to resources.\n\n**2. User-Specific IAM Policies:**\n\n* For each provisioned resource, the service operator will generate and manage user-specific IAM policies. These policies will grant users the necessary permissions to interact with the resources while respecting the boundaries set by the declarative configuration.\n* **Principle of Least Privilege:** The user-specific policies will adhere to the principle of least privilege, granting only the necessary permissions for users to read configuration, read, write, and manage objects within the bucket.\n* **Exclusion of Conflicting Actions:**  The generated policies will explicitly exclude permissions that could conflict with the service operator's management. For instance, in the provided example, the policy would not include permissions to modify Lifecycle Rules, ensuring the operator retains control over that aspect.\n\n**Policy Generation Strategy:**\n\n* The service operator will utilize a templating mechanism or a policy generator to dynamically create user-specific policies based on the resource type and the declarative configuration defined in the manifest. \n* This approach ensures that the generated policies remain consistent with the desired state and adapt to any changes in the resource configuration.\n\n**Benefits:**\n\n* **Clear Separation of Concerns:** This approach provides a clean separation between the service operator's management responsibilities and the permissions granted to users.\n* **Enhanced Security:** The principle of least privilege and the exclusion of conflicting actions contribute to a more secure environment by minimizing the potential for unintended or malicious modifications.\n* **Flexibility and Control:**  The declarative nature of the service operator, combined with the dynamic policy generation, allows for flexibility in defining and managing user access while retaining control over the provisioned resources. \n","GenTime":"2024-08-19 16:14:15","text":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n\n##Decision\n* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n"}
{"File Name":"adr-tools\/0009-help-scripts.md","Context":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n","Decision":"Where necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","tokens":63,"id":3571,"Prediction":"## Decision\n\nWe will transition from embedding help text directly within command scripts to using a dedicated help system. This system will support:\n\n* **Separation of concerns:** Decoupling help text from the codebase will improve maintainability and readability.\n* **Dynamic content generation:** The new system will enable us to include calculated values, such as file locations, in the help text.\n* **Improved presentation:** Moving away from comment-based formatting will allow for richer formatting options and a more user-friendly presentation.\n\nWe will investigate the following options for implementation:\n\n1. **Markdown files:** This approach provides a good balance between ease of use and formatting flexibility.\n2. **Dedicated help library:** This option provides the most control and flexibility but requires a more significant initial investment.\n\nThe final decision on the specific implementation will be made after a thorough evaluation of the trade-offs and consideration of factors such as the complexity of the help content and the development resources available. \n","GenTime":"2024-08-19 16:14:20","text":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n\n\n##Decision\nWhere necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n"}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"Prediction":"## Decision\n\nWe will decouple the data models for different concerns to address the issues of validation and varying data requirements across the application. Specifically, we will have separate models for:\n\n1. **XML Serialization\/Deserialization:** This model will strictly adhere to the XSD schema, ensuring that any XML generated or parsed is valid. \n2. **Internal Backend Handling:** This model will be optimized for backend processing and may include additional fields or structures not present in the XML schema. It will be flexible enough to handle partially complete or invalid data coming from the UI, allowing for graceful degradation and user-friendly error reporting.\n3. **Algorithm Processing:** This model will be tailored to the specific needs of the algorithms used, potentially with data transformations or simplifications optimized for performance. \n4. **REST Service Communication:** This model will represent the data format used for communication with the REST API. This might involve a different structure or level of detail compared to the internal backend model, depending on the API design.\n\n## Consequences\n\n**Benefits:**\n\n* **Improved Data Integrity:**  Strict adherence to the XSD for XML handling ensures data validity.\n* **Increased Flexibility and Maintainability:**  Decoupling models allows for independent evolution and optimization for different concerns. \n* **Enhanced User Experience:** The backend can handle and process partially complete data, allowing for a more forgiving user interface and improved error handling.\n\n**Drawbacks:**\n\n* **Increased Complexity:** Managing multiple models introduces some complexity to the codebase.\n* **Data Mapping Overhead:**  Conversion between different models is required, potentially adding processing overhead.\n\n**Mitigation:**\n\n* Use a suitable design pattern like Data Mapper or Translator to manage the complexity of mapping between data models.\n* Carefully consider the performance implications of data mapping and optimize where necessary. \n\nThis decision allows for a more robust, maintainable and user-friendly system by separating concerns and handling data validation effectively. \n","GenTime":"2024-08-19 16:14:28","text":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n\n\n##Decision\nChosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n"}
{"File Name":"git-en-boite\/0016-handle-concurrent-git-operations","Context":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n","Decision":"Reduce worker concurrency to 1 by using a single container (for now).\\n","tokens":47,"id":3497,"Prediction":"## Decision\n\nImplement a distributed locking mechanism to prevent concurrent git operations from conflicting. This mechanism will ensure that only one worker can acquire a lock on a specific repository at a time, preventing race conditions and ensuring data integrity. \n\n**Possible Locking Mechanisms:**\n\n* **Redis:** Utilize Redis's atomic SETNX command to acquire and release locks. This provides a fast and reliable distributed locking solution.\n* **Database-based locking:** Implement a locking table within the existing database to manage repository locks. This solution might be simpler to integrate if the application already heavily relies on the database.\n* **ZooKeeper:**  Leverage ZooKeeper's ephemeral nodes to implement distributed locks. This is a robust solution but might introduce more complexity compared to Redis or database-based solutions.\n\n**Considerations:**\n\n* **Performance:** Choose a locking mechanism that minimizes latency and overhead.\n* **Complexity:**  Select a solution that aligns with the team's expertise and the existing infrastructure.\n* **Scalability:** Ensure the chosen mechanism can handle the expected workload and potential future growth.\n\n**Further Actions:**\n\n1. **Evaluate and benchmark** different locking mechanisms based on the specific requirements and constraints of the system.\n2. **Design and implement** the chosen locking mechanism, ensuring proper lock acquisition, release, and handling of potential failures. \n3. **Thoroughly test** the implementation to ensure its effectiveness in preventing concurrent git operation conflicts.\n4. **Monitor the system** after deployment to identify any performance bottlenecks or potential issues with the locking mechanism. \n","GenTime":"2024-08-19 16:14:35","text":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n\n\n##Decision\nReduce worker concurrency to 1 by using a single container (for now).\\n"}
{"File Name":"opg-lpa\/0006-modernise-the-code-base.md","Context":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n","Decision":"We have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","tokens":306,"id":1652,"Prediction":"## Decision\n\n**Incrementally migrate the codebase from PHP to Python.**\n\n**Reasoning:**\n\n* **Mitigate Developer Risk:** Transitioning to a more popular language like Python will make it easier to attract and retain developers in the future. This reduces the risk of being stuck with an aging codebase and a dwindling pool of specialists.\n* **Modernization:** Python's ecosystem offers a wider selection of modern frameworks and libraries, allowing us to address technical debt incrementally and adopt best practices more effectively.\n* **Improved Developer Experience:** Python's cleaner syntax and thriving community contribute to a better developer experience, increasing productivity and code maintainability.\n* **Phased Approach:** A complete rewrite is risky and costly.  An incremental migration allows us to gradually replace parts of the system,  minimizing disruption and managing risk.  \n\n**Consequences:**\n\n* **Migration Cost:**  There will be a cost associated with migrating the codebase, both in terms of development time and potential performance impacts during the transition.\n* **Knowledge Gap:**  The team will need time to learn Python and related technologies. Training and knowledge sharing will be crucial.\n* **Dual Maintenance:** There will be a period where we need to maintain both the PHP and Python codebases. This requires careful planning and coordination.\n\n**Next Steps:**\n\n1. **Pilot Project:** Identify a non-critical module for a pilot migration to Python. This will provide valuable experience and inform the overall migration strategy.\n2. **Skill Development:** Invest in Python training and resources for the development team.\n3. **Migration Roadmap:**  Develop a phased migration plan, prioritizing modules based on business value and technical complexity. \n4. **Performance Testing:**  Establish benchmarks and monitor performance closely during and after migration phases to identify and address any issues.\n","GenTime":"2024-08-19 16:14:43","text":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n\n##Decision\nWe have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n"}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"Prediction":"## Decision\n\nWe will proceed with **one package per component** for the following reasons:\n\n* **Dependency Management:**  While more complex to set up, granular control over dependencies is crucial for the long-term health and maintainability of projects consuming these components. Minimizing bloat and unintended side effects from updates outweighs the initial setup cost.\n* **Independent Versioning & Releases:**  Separate packages allow us to release updates to individual components without forcing consumers to update unrelated components. This provides greater flexibility, stability, and allows for faster iteration on individual components.\n* **Scalability & Clarity:** As the number of components grows, a monorepo structure with individual packages will provide better organization, clearer dependency trees, and easier onboarding for new developers.\n\nWe acknowledge the challenges associated with this approach:\n\n* **Development Workflow:**  We will invest in tooling and automation (e.g., Lerna, Yarn Workspaces) to streamline the development workflow, manage inter-component dependencies, and simplify testing within the monorepo structure.\n* **Publishing & Versioning:**  We will implement a robust publishing pipeline using tools like Lerna and semantic-release to manage versioning, changelogs, and automate the publishing of individual packages.\n* **Style Guide Integration:**  We will treat the style guide as a separate entity that consumes the component packages. This will allow us to showcase different versions of components and demonstrate their usage in isolation.\n\nThis approach prioritizes long-term benefits over short-term convenience. We believe that the increased maintainability, flexibility, and independent versioning will ultimately result in a more robust and scalable component library. \n","GenTime":"2024-08-19 16:14:50","text":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n\n\n##Decision\nThe complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n"}
{"File Name":"sdbmss\/0002-build-crud-and-search.md","Context":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n","Decision":"The following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","tokens":72,"id":1837,"Prediction":"## Decision\n\n**Migrate the Schoenberg Database of Manuscripts (SDBM) from its existing ColdFusion implementation to a new technology stack that enables the following:**\n\n* **Flexible data model:**  Address the limitations of the current data model to support the complexity and evolving nature of manuscript data. \n* **User contributions:** Implement features for user authentication, authorization, and a streamlined contribution workflow (e.g., data entry forms, validation, moderation).\n* **Complex search capabilities:** Integrate a robust search engine that allows for advanced filtering, faceting, and full-text search across various manuscript data fields.\n* **Modern technology stack:** Choose a contemporary technology stack for the backend, frontend, and database that prioritizes maintainability, scalability, security, and developer productivity. This might include technologies like:\n    * **Backend:** Python (Django\/Flask), Node.js (Express), Ruby on Rails, etc. \n    * **Frontend:** React, Vue.js, Angular, etc.\n    * **Database:** PostgreSQL, MySQL, MongoDB, etc.\n\n**Justification:**\n\nThe existing ColdFusion-based SDBM can no longer meet the needs of the Schoenberg Institute due to its outdated technology, inflexible data model, and lack of user contribution features. Migrating to a modern technology stack with a focus on flexibility, user engagement, and advanced search capabilities will provide a more sustainable and feature-rich platform for managing and researching manuscript data. \n","GenTime":"2024-08-19 16:14:56","text":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n\n\n##Decision\nThe following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n"}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"Prediction":"## Decision\n\nWe will replace dynamic mappings with a predefined schema for mail headers to address the issues caused by the flexibility of dynamic mapping. \n\nThis decision is driven by the following factors:\n\n- **Predictability and Stability:** A predefined schema provides a stable and predictable structure for indexing and accessing mail header data. This eliminates the risks associated with dynamic type guessing and ensures consistent data representation.\n- **Performance Optimization:**  Removing dynamic mapping eliminates the need to pause indexing during schema changes, significantly improving indexing performance and reducing latency.\n- **Controlled Field Count:** A predefined schema allows us to explicitly define and control the maximum number of fields, preventing issues related to exceeding field count limits.\n\n**Implementation Details:**\n\n1. **Schema Definition:**  We will analyze existing mail headers and define a comprehensive schema that accommodates common headers with appropriate data types.\n2. **Data Migration:** Existing data will be migrated to the new schema. This may require data transformation and re-indexing.\n3. **Handling Unknown Headers:** A strategy will be implemented for handling unknown headers encountered after the schema definition. This could involve logging, storing them in a separate field, or employing a more flexible approach for specific scenarios. \n\n**Consequences:**\n\n* **Reduced Flexibility:**  While we gain predictability and performance, we lose some flexibility in handling new, unforeseen headers.\n* **Development Effort:** Defining the schema and migrating existing data will require development effort.\n\n**Trade-offs:**\n\nWe are trading flexibility for improved performance, stability, and data consistency. This trade-off is justified by the functional issues encountered with dynamic mappings and the benefits of a well-defined schema. \n","GenTime":"2024-08-19 16:15:03","text":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n\n\n##Decision\nRely on nested objects to represent mail headers within a mapping\\n"}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"Prediction":"## Decision\n\nWe will implement the \"Privacy Policy\" and \"About Me\" pages as standalone pages, outside the main application map and spiral navigation.  These pages will be accessible through dedicated links, potentially in the footer of each page, ensuring user privacy and preventing forced exposure during regular map navigation. \n","GenTime":"2024-08-19 16:15:05","text":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n\n##Decision\nI'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n"}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"Prediction":"## Decision\n\n**IITD officially adopts OpenJDK 11 as the supported Java Development Kit (JDK) for all new and existing IITD hosted Java applications.**\n\n## Rationale\n\nThis decision aligns with the following factors:\n\n* **Oracle's Licensing Change:**  Oracle's shift to a subscription-based model for Oracle JDK necessitates exploring cost-effective alternatives.\n* **OpenJDK Compatibility:** Oracle has stated that OpenJDK offers comparable functionality to Oracle JDK, making it a viable replacement for most use cases.\n* **Security:** Upgrading to JDK 11 (or later) significantly enhances security by incorporating the latest security patches and features. JDK 8 is no longer supported by Oracle and poses security risks.\n* **Longevity and Support:** OpenJDK, as an open-source project, benefits from continuous community support and long-term maintenance.\n* **Industry Trend:** OpenJDK has gained widespread adoption within the software development community, solidifying its position as a reliable and robust JDK.\n\n## Implications\n\n* **Migration Plan:** A phased migration plan will be developed to facilitate the transition of existing applications from Oracle JDK to OpenJDK 11.\n* **Support and Maintenance:** IITD Infrastructure team will provide support and maintenance for OpenJDK 11, including security updates and bug fixes.\n* **Developer Training:**  Resources and training materials will be made available to IITD developers to ensure a smooth transition to OpenJDK 11 and familiarize them with any differences.\n* **New Projects:** All new Java-based projects initiated within IITD must utilize OpenJDK 11 as the standard JDK.\n\nThis decision ensures cost-effectiveness, enhanced security, and alignment with industry best practices while minimizing disruption to existing IITD applications. \n","GenTime":"2024-08-19 16:15:13","text":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n\n##Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n"}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"Prediction":"## Decision\n\nTwo search methods will be implemented to support researchers in finding specific whale observations or sets of observations:\n\n**1. Basic Search:**\n\n*   **Functionality:** Allows users to search by a single search term. This term will be matched against key fields in the Whale Observation data model, such as species, location (general area or specific coordinates), date, and observer name. \n*   **Implementation:** This search will be implemented using standard database queries (e.g., SQL LIKE operator) for simple string matching.\n*   **Target Users:**  Researchers needing a quick and easy way to find observations based on a single known attribute.\n\n**2. Advanced Search:**\n\n*   **Functionality:** Provides a more powerful search mechanism allowing users to combine multiple criteria. This will include:\n    *   **Field-specific search:** Users can specify which fields to search within (e.g., searching for \"Humpback\" only within the \"Species\" field).\n    *   **Logical operators:** Support for AND\/OR operators to refine results (e.g., \"Humpback\" AND \"Alaska\" within a specified date range).\n    *   **Range searches:** Ability to search for observations within a specific numerical range (e.g., sighting dates between 2020-2023, or latitude\/longitude bounding boxes).\n*   **Implementation:**  This search will be implemented using a combination of database indexing, query optimization techniques, and potentially a dedicated search engine (e.g., Elasticsearch) if the data volume and complexity warrant it.\n*   **Target Users:** Researchers needing to perform more complex queries, combining multiple search criteria for targeted results.\n\nThis two-tiered approach aims to balance ease of use for simple searches with the power and flexibility required for more in-depth research. \n","GenTime":"2024-08-19 16:15:21","text":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n\n\n##Decision\nCreate `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"Prediction":"## Decision\n\n**Move all documentation related to specific development frameworks or tools to their respective GitHub repositories.  Maintain high-level documentation, FAQs, whitepapers, and general information about the Cosmos Network on the main website.**\n\nThis means:\n\n* **SDK Documentation:**  Migrate all SDK-specific documentation to the Cosmos SDK GitHub repository. \n* **Hub Documentation:** Migrate all Hub-specific documentation to the Cosmos Hub GitHub repository.\n* **Framework\/Tool-Specific Documentation:** Follow the same pattern for all other frameworks and tools, like Lotion, each with its dedicated documentation within its repository. \n* **Website Focus:** Maintain the main website as the central hub for:\n    * High-level introductions to the Cosmos Network.\n    * FAQs and common user questions.\n    * The official whitepaper and related technical specifications.\n    * General news and announcements.\n\n**Rationale:**\n\n* **Improved Maintainability:** Decentralizing documentation empowers the developers actively working on each project to maintain the documentation alongside the codebase. This leads to more accurate, up-to-date, and relevant documentation.\n* **Enhanced Discoverability:** Users looking for specific information about a framework or tool can find it directly within the relevant repository, making navigation more intuitive.\n* **Clear Separation of Concerns:** This approach creates a clear separation between general Cosmos Network information and specific technical documentation, improving the overall user experience.\n\n**Consequences:**\n\n* **Potential Duplication:** There's a risk of some content duplication across repositories, but this can be minimized with careful planning and cross-linking.\n* **Migration Effort:** Migrating existing documentation will require a one-time effort, but the long-term benefits outweigh this initial investment.\n\n**Next Steps:**\n\n1. **Develop a Detailed Migration Plan:** Outline the specific steps involved in moving documentation to the appropriate repositories, including timelines and responsibilities.\n2. **Implement Redirects:** Ensure that existing links to documentation on the website redirect seamlessly to the new locations within the repositories.\n3. **Communicate Changes:** Clearly communicate the documentation restructuring to the community, providing guidance on where to find specific information. \n","GenTime":"2024-08-19 16:15:30","text":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n\n\n##Decision\nRe-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n"}
{"File Name":"tendermint\/adr-012-peer-transport.md","Context":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n","Decision":"Transport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","tokens":179,"id":1956,"Prediction":"## Decision\n\nWe will decouple the `Switch` from physical connection handling by introducing a new component, tentatively named `Transport`. The `Transport` will be responsible for managing physical connections, while the `Switch` will focus on higher-level peer management and message routing. This separation of concerns will result in a cleaner architecture and improved maintainability.\n\n**Responsibilities of `Transport`:**\n\n* Establish and manage physical connections (e.g., TCP, WebSockets).\n* Handle connection-level events (e.g., connect, disconnect, errors).\n* Provide a unified interface for sending and receiving raw data to\/from peers.\n* Potentially handle encryption and decryption of data.\n\n**Responsibilities of `Switch` (updated):**\n\n* Manage peer discovery and connection establishment (delegated to `Transport`).\n* Handle peer handshakes and authentication.\n* Maintain a peer registry and track peer status.\n* Route messages between peers based on configured rules.\n* Handle disconnections and peer failures.\n\n**Migration Strategy:**\n\n* Introduce the `Transport` interface and an initial implementation.\n* Refactor the `Switch` to use the `Transport` for physical connection handling.\n* Update the `config.P2PConfig` to configure the `Transport` instead of the `Switch`.\n* Migrate existing tests and ensure backward compatibility during the transition.\n\n**Benefits:**\n\n* Improved separation of concerns and modularity.\n* Increased testability of individual components.\n* Easier to introduce new transport mechanisms in the future (e.g., QUIC).\n* Reduced complexity of the `Switch` component.\n\n**Drawbacks:**\n\n* Increased complexity due to the introduction of a new component.\n* Potential performance overhead due to the additional layer of abstraction.\n\n**Overall, the benefits of improved architecture and maintainability outweigh the potential drawbacks. This change will pave the way for a more scalable and flexible p2p package in the future.** \n","GenTime":"2024-08-19 16:15:38","text":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n\n\n##Decision\nTransport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n"}
{"File Name":"connecting-to-services\/0008-add-basic-authentication-to-test-environments.md","Context":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n","Decision":"We have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","tokens":107,"id":2207,"Prediction":"## Decision\n\nTo address the lack of access control and potential for misinterpretation, we will implement the following:\n\n**For Humans:**\n\n* **Basic Authentication:** Implement HTTP Basic Authentication on all pre-production environments (development, review, staging).  This will present a simple username\/password challenge that is easily understood and built into all major browsers.\n* **Clear Visual Indicators:**  All pre-production environments will prominently display clear visual indicators (banners, watermarks, specific color schemes) to distinguish them from the production environment.\n\n**For Bots:**\n\n* **Robots.txt:** Utilize a `robots.txt` file to instruct web crawlers and bots not to index or access content in pre-production environments. This file will disallow all access to these environments.\n\n**Justification:**\n\n* **Simplicity and Effectiveness:** Basic Authentication provides a straightforward and effective way to deter casual access while remaining easy to implement.\n* **Clear Communication:** Visual indicators combined with the authentication challenge clearly communicate the non-production status of the environments to human users.\n* **Industry Standard:** Using `robots.txt` is a widely adopted and respected method for managing bot access, ensuring pre-production environments are not mistakenly indexed.\n\n**Alternatives Considered:**\n\n* **More robust authentication:** While options like OAuth or SAML were considered, their complexity is unnecessary for the level of security needed in this context. \n* **IP whitelisting:**  This option was dismissed due to potential administrative overhead and limitations for distributed teams or dynamic IP addresses.\n\n**Consequences:**\n\n* **Slightly increased friction for developers:** Accessing pre-production environments will require entering credentials.\n* **Reduced risk of misinterpretation:**  Clearer distinction between environments minimizes the risk of users or search engines mistakenly interacting with pre-production versions.\n\nThis approach balances security with ease of implementation and maintenance, striking a suitable balance for the given context.\n","GenTime":"2024-08-19 16:15:46","text":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n\n\n##Decision\nWe have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n"}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) to document significant architectural decisions for this project. Each ADR will follow a lightweight Markdown template, including sections for context, decision, status, consequences, and links to relevant discussions or documentation. These ADRs will be stored in a dedicated \"docs\/adr\" directory within the project's version control system. \n","GenTime":"2024-08-19 16:15:48","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"Prediction":"## Decision\n\nWe will use a key management system (KMS) to handle encryption key generation, storage, and rotation. The specific KMS will be chosen based on the hosting platform and security requirements but should support the following features:\n\n* **Automatic Key Rotation:** The KMS should automatically rotate keys according to a defined schedule (e.g., every 30 days).\n* **Key Versioning:** The KMS should manage multiple versions of keys, allowing for decryption of data encrypted with older keys even after rotation.\n* **Envelope Encryption:** The session cookie will be encrypted using a data key. This data key will be encrypted with a master key managed by the KMS. This approach allows for data key rotation without re-encrypting the entire user session data.\n\n## Consequences\n\n* **Increased Security:** Using a dedicated KMS enhances security by providing a centralized, secure location for managing encryption keys.\n* **Simplified Key Management:** Automatic key rotation and versioning simplify key management and reduce the risk of using compromised keys.\n* **Seamless Key Rotation:** Envelope encryption allows for key rotation without impacting active user sessions, ensuring uninterrupted service.\n* **Potential Vendor Dependency:** Choosing a specific KMS might lead to a vendor lock-in, which needs to be evaluated against the benefits offered. \n\nThis approach allows us to maintain a secure session management system with a robust and transparent key management process, minimizing the impact on active users during key rotation. \n","GenTime":"2024-08-19 16:15:55","text":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n\n\n##Decision\n* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n"}
{"File Name":"dogma\/0015-routing-unrecognized-messages.md","Context":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n","Decision":"Reinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","tokens":138,"id":1612,"Prediction":"## Decision\n\nWe will introduce a new error type, `UnroutableMessage`, to be returned by `AggregateMessageHandler.RouteCommandToInstance()` and `ProcessMessageHandler.RouteEventToInstance()` when they encounter an unexpected message type.\n\nThis decision balances the need for flexibility introduced by ADR-14 with the requirement for clear error handling in routing methods. By introducing a specific error type, we allow calling code to differentiate between legitimate errors and situations where a message cannot be routed due to an unexpected type.\n\nThis approach offers several advantages:\n\n* **Explicit Error Reporting:**  It clearly signals when a message type cannot be routed to an appropriate handler.\n* **Improved Error Handling:**  Calling code can now implement specific logic for handling unroutable messages, instead of relying on generic error handling.\n* **Maintainability:**  The introduction of a dedicated error type improves code clarity and makes the system easier to maintain.\n\nWe considered alternatives, such as:\n\n* **Panicking:** This approach, while simple, lacks the nuance of specific error handling and can lead to unexpected application crashes.\n* **Logging and Ignoring:**  This approach, though silent, hides potential issues and can complicate debugging.\n\nUltimately, we believe that introducing a dedicated `UnroutableMessage` error type strikes the right balance between flexibility, clarity, and robustness. \n","GenTime":"2024-08-19 16:16:01","text":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n\n\n##Decision\nReinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n"}
{"File Name":"runner\/0280-command-input-echoing.md","Context":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n","Decision":"- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","tokens":124,"id":3741,"Prediction":"## Decision\n\nWe will implement an opt-in command echoing system for improved log clarity. \n\n* By default, command echoing will be disabled to reduce log clutter.\n* Users can explicitly enable echoing using the `::echo::on` command and disable it with `::echo::off`. \n* Echoing will be automatically enabled if the `ACTIONS_STEP_DEBUG` secret is enabled, providing a familiar debugging mode for users who require it.\n* Any command resulting in an error will have its output echoed regardless of the echo setting. This ensures users have sufficient information for troubleshooting failed commands. \n","GenTime":"2024-08-19 16:16:04","text":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n\n\n##Decision\n- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n"}
{"File Name":"alfresco-anaxes-shipyard\/0008-applying-extensions-to-docker-images.md","Context":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n","Decision":"We will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","tokens":351,"id":3604,"Prediction":"## Decision\n\n**Apply extensions at build time.** \n\nWhile this approach requires customers to build their own images, it outweighs the disadvantages of runtime application. \n\n**Reasoning:**\n\n* **Immutability:** This approach preserves the immutability of Docker images, ensuring consistency and reliability across environments. This outweighs the convenience of runtime application.\n* **Security:** Building extensions at build time eliminates the security risks associated with dynamically applying code at runtime. \n* **Stability:** Applying extensions at build time ensures they are integrated and tested as part of the image creation process, reducing the risk of runtime failures.\n* **Performance:** Extensions are readily available within the image, eliminating the performance overhead of fetching and applying them at runtime.\n\n**Consequences:**\n\n* **Increased Complexity for Customers:** Customers will need to manage their own image building process, incorporating the desired official and custom extensions.  \n* **Potential for Image Sprawl:**  Different extension combinations could lead to a proliferation of custom images.  We will mitigate this by providing clear documentation, tooling, and pre-built images for common configurations. \n\n**Mitigation:**\n\n* **Provide comprehensive documentation and examples for building custom images with extensions.**\n* **Develop tooling to simplify the process of building images with specific extension sets.**\n* **Offer pre-built images with commonly used extensions.** \n* **Investigate and implement solutions for managing image versions and dependencies effectively.**\n\nThis decision prioritizes the long-term benefits of immutability, security, and stability over the short-term convenience of runtime extension application. We believe that providing the necessary tools and support will enable customers to effectively manage the image building process. \n","GenTime":"2024-08-19 16:16:11","text":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n\n\n##Decision\nWe will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"Prediction":"## Decision \n\nThe application will be structured as a **microservices architecture**, with the following main components:\n\n* **API Gateway:**  Handles routing of requests, load balancing, and potentially authentication\/authorization for the different REST servers\/webapps.\n* **REST Server\/Web Apps:** Individual services responsible for specific functionalities, exposing REST APIs.  These could be further broken down based on domain or bounded contexts (e.g., User Management, Product Catalog, Order Processing).\n* **Document Processor Components:**  Separate services dedicated to handling different document-related tasks. Examples could include:\n    * **Document Conversion Service:** Responsible for converting between document formats (e.g., PDF to DOCX, HTML to PDF).\n    * **Document Parsing Service:**  Extracts data from documents based on predefined templates or rules.\n    * **Document Generation Service:** Creates new documents based on templates and data inputs.\n* **Message Queue:** Facilitates asynchronous communication between components, such as sending document processing requests from web apps to the document processor components and receiving notifications upon completion.\n* **Shared Database(s):**  Used to store data that needs to be accessed by multiple components.  Consider separate databases for different domains or functionalities to maintain loose coupling.\n\nThis structure allows for:\n\n* **Scalability:** Each component can be scaled independently based on its specific needs.\n* **Flexibility:** New functionalities can be added easily by creating new microservices.\n* **Maintainability:** Smaller codebases are easier to understand and maintain.\n* **Technology Diversity:** Different technologies can be used for different components based on their strengths. \n\n\nThis is a high-level overview. Further details about specific technologies, communication protocols, and data models will be decided upon during the detailed design phase. \n","GenTime":"2024-08-19 16:16:19","text":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n\n\n##Decision\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n"}
{"File Name":"architecture\/0007-use-static-code-analysis.md","Context":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n","Decision":"Use Static Code Analysis to find violations of standards.\\n","tokens":45,"id":1418,"Prediction":"## Decision\n\nWe will adhere to established coding standards and best practices for the chosen programming language and framework. This will include, but is not limited to:\n\n* **Style Guide:**  Adopting a specific style guide (e.g., PEP 8 for Python, Google Java Style Guide) and enforcing it using linters.\n* **Naming Conventions:**  Following clear and consistent naming conventions for variables, functions, classes, and files.\n* **Code Structure and Organization:**  Implementing established patterns for organizing code (e.g., MVC, MVVM) to ensure modularity and maintainability.\n* **Documentation:**  Enforcing clear and consistent documentation practices using inline comments, docstrings, and potentially external documentation tools.\n* **Testing:** Adopting a robust testing strategy including unit tests, integration tests, and end-to-end tests, with a focus on code coverage. \n\nWe will integrate automated tools (linters, formatters, testing frameworks) into our development pipeline (CI\/CD) to ensure continuous adherence to these standards. \n","GenTime":"2024-08-19 16:16:23","text":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n\n##Decision\nUse Static Code Analysis to find violations of standards.\\n"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"Prediction":"## Decision\n\nWe will use a serverless architecture with a Function-as-a-Service (FaaS) platform for the initial implementation of the Children's Social Care Placement service. \n\n## Reasons\n\n* **Rapid Prototyping and Iteration:** Serverless FaaS allows for quick development and deployment of individual functions, enabling faster feedback loops and quicker adaptation to changing requirements during the exploratory phase.\n* **Reduced Operational Overhead:**  Serverless platforms abstract away infrastructure management, allowing developers to focus on building and iterating on the core service logic. This is crucial in the Alpha stage where experimentation and learning are prioritized.\n* **Cost-Effectiveness for Exploration:** Serverless platforms typically operate on a pay-as-you-go model, making them cost-effective for exploratory projects where usage patterns are unpredictable and likely to be low.\n* **Flexibility for Future Evolution:** While we prioritize developer productivity in the Alpha, a serverless approach provides flexibility to adapt and scale specific components as the service matures and requirements solidify. \n\n## Consequences\n\n* **Potential Vendor Lock-in:** Choosing a specific FaaS platform may lead to some level of vendor lock-in down the line. However, we believe this risk is mitigated by the exploratory nature of the Alpha phase. \n* **Monitoring and Debugging Complexity:** Debugging and monitoring distributed serverless applications can be more challenging than traditional monolithic applications. We will need to utilize appropriate tools and strategies to manage this.\n* **Long-Term Performance Considerations:** While suitable for early exploration, the performance characteristics of serverless architectures may need to be reevaluated for high-volume production scenarios in later phases. \n\n## Status\n\nAccepted\n\n## Notes\n\nThis decision will be revisited at the end of the Alpha phase to determine if the chosen architecture remains the most suitable option for the Beta phase and beyond.  We will consider factors such as performance, scalability, security, and cost-effectiveness based on the insights gained during the Alpha. \n","GenTime":"2024-08-19 16:16:32","text":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n\n\n##Decision\nThe decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"Prediction":"## Decision\n\nWe will disable the \"Convert to Pure Component\" Quick Fix by default. This refactoring has limited applicability and incurs a significant performance penalty, as evidenced by the provided flame chart. \n\nWe acknowledge that some users may still benefit from this feature. Therefore, we will provide the following options:\n\n1. **Enable the Quick Fix through a configuration setting:** Users can explicitly enable the \"Convert to Pure Component\" Quick Fix if they deem it necessary for their workflow. This allows them to opt-in to the functionality while keeping it disabled by default for performance reasons.\n2. **Explore alternative solutions for providing the functionality:** In the long term, we can investigate alternative approaches to offer similar functionality without relying on the computationally expensive `react-codemod` implementation.  This could involve:\n    * **Developing a more performant refactoring implementation specifically for the extension.**\n    * **Providing guidance and snippets for manual refactoring.**\n    * **Exploring integration with other tools that offer this functionality more efficiently.**\n\nBy taking these steps, we can improve the overall performance of the extension while still providing users with options to access the functionality if needed. \n","GenTime":"2024-08-19 16:16:37","text":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n\n\n##Decision\nWe will stop proposing a Quick Fix for this refactoring.\\n"}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"Prediction":"## Decision\n\nOperations in FDML **will be defined using elements with attributes**. \n\n## Reasoning\n\nWhile representing operations solely as elements might appear simpler, using attributes offers several advantages concerning the decision drivers:\n\n* **Consistency:**  Attributes promote a unified and predictable structure for defining operation parameters. This consistency enhances the readability and understandability of FDML definitions.  By adopting a consistent approach, we minimize confusion and potential errors during development and maintenance. \n* **Expressiveness:** Attributes provide a structured way to represent operation parameters, improving the expressiveness of FDML. For instance, attributes can have specific data types or constraints, leading to more precise and robust operation definitions. \n\nThis decision aligns with the goal of creating a clear, concise, and expressive language for defining feature models. \n","GenTime":"2024-08-19 16:16:41","text":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n\n##Decision\n* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"Prediction":"## Decision\n\nWe will **not** provide a `getCursor` functionality directly within `FragmentFrame`. \n\n**Reasoning:**\n\n* **Referential Transparency and Performance:**  Introducing `getCursor` in `FragmentFrame` risks compromising referential transparency. Since `FragmentFrame` can be dynamically generated (as seen with the `cell` method), repeatedly calling `getCursor` might create numerous duplicate cursors, impacting performance and potentially causing unnecessary re-renders in React components.\n* **Centralized Cursor Management:** Maintaining a single source of truth for cursors within the \"root\" `DataFrame` simplifies our architecture and ensures consistency. It also aligns with the concept of the \"root\" `DataFrame` as the origin of all derivative frames.\n* **DataFrame as Data Source:** Passing the \"root\" `DataFrame` reference along the chain of derivative structures might seem cumbersome at first, but it clearly communicates the dependency on the root for cursor information. This approach promotes a clear separation of concerns, where the `DataFrame` is responsible for managing data and cursors, while `FragmentFrames` focus on representing data slices.\n\n**Implementation Details:**\n\n1. **DataFrame as Cursor Provider:** The \"root\" `DataFrame` remains the sole provider of cursors through its existing `getCursor` method. \n2. **Passing DataFrame Reference:** We will pass the reference of the \"root\" `DataFrame` down the chain to `PivotFrame` and `FragmentFrame` instances. This can be done efficiently during their creation.\n3. **Proxy\/Pass-Through Mechanism:** Instead of implementing a full `getCursor` method in `FragmentFrame`, we can introduce a lightweight proxy or pass-through mechanism.  This could be a simple function that directly calls the `getCursor` method of the referenced \"root\" `DataFrame`.\n\nThis approach maintains referential transparency, avoids cursor duplication, and keeps the cursor logic centralized within the \"root\" `DataFrame`. \n","GenTime":"2024-08-19 16:16:49","text":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n\n\n##Decision\nAt the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"Prediction":"## Decision\n\nWe will develop a custom C++ library leveraging the Armadillo API for data management within the project. This library will encapsulate common data handling operations specific to our project's needs, ensuring efficient and consistent data manipulation while abstracting the complexity of the Armadillo library from the rest of the codebase. \n","GenTime":"2024-08-19 16:16:51","text":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n\n\n##Decision\nWe will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n"}
{"File Name":"google-cloud-cpp\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","Decision":"will not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","tokens":272,"id":2462,"Prediction":"## Decision\n\nWe will **not** directly build, support, provide, or test any binary packages for google-cloud-cpp. We will also **not** host configuration files for building binary packages.\n\nThis means we will **not** endorse any specific binary package management system.\n\nHowever, we will strive to make our code easily packageable by others in various formats to enable community-driven binary distribution efforts. We will consider reasonable requests and code tweaks that facilitate easier packaging. \n","GenTime":"2024-08-19 16:16:54","text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n","Decision":"Pending\\n","tokens":36,"id":425,"Prediction":"## Decision\n\nWe will use the `bootstrap` gem to maintain consistent styles across the application. This decision prioritizes ease of use and rapid development, leveraging Bootstrap's pre-built components and grid system. \n","GenTime":"2024-08-19 16:16:55","text":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n\n\n##Decision\nPending\\n"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"Prediction":"## Decision\n\nWe will use a layered approach to securing the interaction between the client and server code, allowing for flexibility based on user deployment needs.\n\n**Layer 1: Transport Layer Security (TLS)**\n\n* **Mandatory** for all deployments.\n* Encrypts all communication between client and server, protecting against man-in-the-middle attacks.\n* Requires valid certificates for the server.\n\n**Layer 2: Authentication**\n\n* **Configurable** based on deployment needs. Options include:\n    * **API Keys:** Simple for internal or trusted environments.\n    * **OAuth 2.0:** Suitable for external access, leveraging existing user accounts.\n    * **Mutual TLS (mTLS):** Strongest security for highly sensitive environments, requiring client certificate authentication.\n\n**Layer 3: Authorization**\n\n* **Implemented** on the server side (Dropwizard) using role-based access control (RBAC).\n* Ensures clients are only authorized to perform specific actions based on their assigned roles.\n* Roles and permissions will be managed within the Dropwizard application.\n\n**Deployment Scenarios:**\n\n* **Internal\/Trusted Environments:**  TLS with API keys for simplicity.\n* **External Access:** TLS with OAuth 2.0 for secure delegated authorization.\n* **Highly Sensitive Data:** TLS with mTLS for maximum security.\n\n**Justification:**\n\n* Layered approach provides flexibility and scalability for various deployment scenarios.\n* TLS ensures basic communication security.\n* Configurable authentication caters to different trust levels.\n* RBAC provides granular control over client actions.\n\n**Consequences:**\n\n* Increased complexity compared to a single security solution.\n* Requires configuration and management of different security components.\n*  Enhanced security and control over client-server interactions. \n","GenTime":"2024-08-19 16:17:03","text":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n\n##Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n"}
{"File Name":"ansible-docker\/choose_database.md","Context":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"Prediction":"## Decision: \n\n**Based on familiarity, prior usage, and alignment with course materials, MySQL will be adopted as the database management system for this project.** \n","GenTime":"2024-08-19 16:17:04","text":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n\n\n##Decision\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n"}
{"File Name":"gp-finder\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3475,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for architecturally significant decisions. Each ADR will be a single Markdown file stored within a dedicated \"docs\/adr\" directory at the root level of the project repository. \n\nWe chose ADRs because they:\n\n* Provide a concise and consistent format for documenting decisions.\n* Are easy to maintain and track within a version control system.\n* Offer a clear audit trail of the decision-making process.\n* Encourage collaboration and communication amongst team members. \n","GenTime":"2024-08-19 16:17:07","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"PIMS\/geocoder.md","Context":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n","Decision":"Integrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","tokens":262,"id":3968,"Prediction":"## Decision\n\nWe will integrate the Data BC Geocoder API into the inventory system to improve the accuracy and efficiency of managing property location data. \n\n**This integration will allow us to:**\n\n* **Populate latitude and longitude automatically:** When a user enters a property address, the system will query the Data BC Geocoder API to retrieve the corresponding latitude and longitude coordinates. This eliminates manual entry and reduces the risk of human error.\n* **Verify manually entered addresses:**  The Geocoder API will be used to validate addresses entered manually by users. This helps ensure data quality and prevent inaccuracies in the inventory.\n* **Leverage a reliable data source:** By utilizing Data BC as the source for location data, we ensure access to accurate and up-to-date information.\n\n**Implementation Details:**\n\n* We will leverage the provided API Swagger documentation and example code to implement the integration.\n* API calls will be made to the `https:\/\/geocoder.api.gov.bc.ca` endpoint.\n* We will acquire the necessary Developer API keys through the process outlined in the [Developer API Keys documentation](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys) and handle them securely. \n*  Error handling will be implemented to manage cases where the Geocoder API is unavailable or returns unexpected results. \n*  The user interface will be updated to clearly indicate when addresses are being validated and coordinates are being retrieved from the Geocoder API.\n","GenTime":"2024-08-19 16:17:14","text":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n\n\n##Decision\nIntegrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n"}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will be a single Markdown file stored within a dedicated \"docs\/adr\" directory in the project repository.  We will follow a common ADR template like MADR or use a generator tool for consistency.\n","GenTime":"2024-08-19 16:17:16","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"talktofrank-beta\/0003-site-wide-cms-configuration.md","Context":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n","Decision":"A new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","tokens":59,"id":1894,"Prediction":"## Decision\n\nWe will implement a new \"Site Settings\" feature within the CMS. This feature will:\n\n* **Store global site settings:** This will include data such as the warning message content, and potentially other future site-wide configurations.\n* **Provide a dedicated CMS interface:**  Editors will have a clear and intuitive way to manage and update these settings. \n* **Expose settings data through the existing API:** The existing API will be extended to provide endpoints for retrieving the site settings, ensuring consistency and reusability.\n\nThe frontend application will be modified to:\n\n* **Fetch the site settings on every page load:** This ensures the warning message and any future global elements are always up-to-date.\n* **Dynamically display elements based on the fetched settings:**  The warning message content will be rendered based on the retrieved data from the API.\n\n## Consequences\n\n**Positive:**\n\n* **Centralized management of global site elements:** Easier and more efficient maintenance for editors.\n* **Improved user experience:** Consistent display of important information across the site.\n* **Flexibility for future additions:**  The new \"Site Settings\" feature allows for easy implementation of other global features without significant code changes.\n\n**Negative:**\n\n* **Increased development effort:**  Changes are required to both the CMS and frontend applications.\n* **Potential performance impact:** Fetching additional data on each page load might slightly increase loading times. This will need to be monitored and potentially mitigated through caching mechanisms. \n","GenTime":"2024-08-19 16:17:22","text":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n\n\n##Decision\nA new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n"}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"Prediction":"## Decision\n\nWe will **deprecate and remove the FileMailQueue implementation**. \n\n## Reasons\n\n* **Failing tests and instability:** FileMailQueue doesn't meet the defined test contract, leading to disabled tests and an unstable test suite. This makes it difficult to confidently develop and maintain James.\n* **Redundancy and maintainability:** Reimplementing a message queue within James duplicates existing solutions like ActiveMQ and RabbitMQ. This increases the maintenance burden and risk of bugs compared to leveraging a dedicated, well-tested library.\n* **Focus on core functionality:**  By removing FileMailQueue, the team can focus on improving the core features of James and supporting the more robust and scalable ActiveMQ and RabbitMQ implementations. \n\n## Consequences\n\n* **Migration required:**  Users relying on FileMailQueue will need to migrate to either the ActiveMQ or RabbitMQ implementation. \n* **Potential breaking change:** This is a breaking change, and proper versioning and communication will be crucial.\n\n## Action items\n\n* **Deprecate FileMailQueue in the next release:**  Clearly communicate the deprecation in the release notes and provide migration guidance to ActiveMQ or RabbitMQ.\n* **Remove FileMailQueue in a future release:**  Set a timeline for complete removal and ensure all documentation and examples are updated.\n* **Invest in ActiveMQ and RabbitMQ:**  Focus development efforts on enhancing the functionality, performance, and reliability of these implementations.\n","GenTime":"2024-08-19 16:17:28","text":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n\n\n##Decision\nDeprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"Prediction":"## Decision\n\nWe will use **wkhtmltopdf** to convert HTML documents to PDF. \n\n## Reasons\n\n* **Visual Fidelity:** While all considered options provide basic HTML to PDF conversion, wkhtmltopdf is known for its superior rendering capabilities, aiming to replicate the HTML layout and styling as accurately as possible. This is crucial to maintain visual consistency with the original HTML document.\n* **Free and Open Source:** wkhtmltopdf aligns with the requirement for free software solutions.\n* **Java Integration:** Although wkhtmltopdf is an external command-line tool, it can be easily integrated into a Java environment using libraries like Apache Commons Exec or process building mechanisms provided by the JDK itself.\n\n## Considered Options and Tradeoffs:\n\n* **pandoc:** Pandoc is a powerful document converter but might not prioritize visual fidelity for HTML to PDF conversion like wkhtmltopdf.\n* **Unoconv:**  Unoconv depends on LibreOffice and might introduce additional complexity for installation and management compared to a standalone tool like wkhtmltopdf.\n* **Native (firefox) view:** Saving directly from the browser offers high fidelity but lacks automation capabilities required for a software solution. \n\n## Consequences\n\n* **External Dependency:** Using wkhtmltopdf introduces an external dependency.  We will need to ensure it's available in the target environment (development, testing, and production).\n* **Maintenance:**  Keeping wkhtmltopdf updated might be necessary to benefit from bug fixes and potential rendering improvements.\n\nLet's proceed with integrating wkhtmltopdf and evaluate its performance in converting the specific HTML documents used in Docspell.  \n","GenTime":"2024-08-19 16:17:36","text":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n\n\n##Decision\nwkhtmltopdf.\\nIt shows the best results.\\n"}
{"File Name":"gsp\/ADR022-logging.md","Context":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n","Decision":"We will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","tokens":416,"id":3914,"Prediction":"## Decision\n\n**Option 3: Ship Pod\/Application logs to CloudWatch and utilize CloudWatch Insights.**\n\n## Reasoning\n\n* **Centralized Storage:** This option leverages CloudWatch as the single source of truth for all logs, eliminating data duplication across multiple platforms and simplifying log management.\n* **Reduced Maintenance Overhead:** By utilizing CloudWatch Insights, we avoid the operational burden of managing and scaling a separate logging infrastructure like ELK or Splunk.\n* **Cost-Effective:** Minimizes storage costs by avoiding log duplication. Additionally, CloudWatch Insights offers flexible pricing options based on usage. \n* **Integration with Existing Tooling:**  Many teams at GDS are already familiar with CloudWatch, making onboarding and adoption smoother.\n* **Security:** CloudWatch integrates seamlessly with existing AWS security tools and practices.\n\n## Consequences\n\n* **Vendor Lock-in:** Choosing CloudWatch Insights increases reliance on AWS services. \n* **Potential for Increased CloudWatch Costs:** Ingesting application logs into CloudWatch will increase costs. However, careful configuration and log filtering can mitigate this. \n* **Learning Curve for CloudWatch Insights:**  While many teams are familiar with CloudWatch, they may need to learn the specifics of CloudWatch Insights.\n\n## Status\n\nAccepted\n\n## Action Items\n\n* Implement a proof of concept to ingest Pod\/Application logs into CloudWatch using Fluentd.\n* Define a log retention policy for CloudWatch logs to manage storage costs.\n* Develop training materials and documentation for developers on using CloudWatch Insights effectively.\n* Investigate cost optimization strategies for CloudWatch log ingestion and storage. \n","GenTime":"2024-08-19 16:17:43","text":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n\n\n##Decision\nWe will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n"}
{"File Name":"origin\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1562,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) to document significant architectural decisions made throughout this project. Each ADR will follow a consistent template, including sections for context, decision, status, consequences, and other relevant information. \n","GenTime":"2024-08-19 16:17:45","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"connecting-to-services\/0015-add-info-page.md","Context":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n","Decision":"The application will provide an info page.\\n","tokens":55,"id":2198,"Prediction":"## Decision\n\nWe will implement a `\/status` endpoint within the application that returns a JSON payload containing the following information:\n\n* **Application Name:**  The name of the application.\n* **Version:**  The version number of the deployed application. This should be retrieved from build information or version control tags to ensure accuracy.\n* **Environment:** The deployment environment (e.g., Development, Staging, Production).\n* **UTC Timestamp:** The current UTC timestamp at the time of the request.\n\n**Justification:**\n\n* This approach provides a centralized and consistent way to access crucial debugging information. \n* Including the application version eliminates confusion when troubleshooting issues across different deployments.\n* Providing the UTC timestamp simplifies debugging time-sensitive logic and aids in diagnosing time zone-related problems. \n\n**Alternatives Considered:**\n\n* **Logging Version Information:** While logging this information is useful, it requires access to log files which may not be readily available in all debugging scenarios.\n* **Relying on System Tools:** Depending on the deployment environment, using system tools to retrieve version or time information might be inconsistent or require additional permissions. \n\n**Consequences:**\n\n* **Minor Development Overhead:** Implementing the `\/status` endpoint adds a small amount of development work. \n* **Improved Debugging Experience:** The easily accessible information will save developer time and effort when debugging deployment or time-related issues.\n","GenTime":"2024-08-19 16:17:51","text":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n\n##Decision\nThe application will provide an info page.\\n"}
{"File Name":"automate\/adr-2019-01-23.md","Context":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n","Decision":"Every route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","tokens":115,"id":1210,"Prediction":"## Decision\n\nWe will implement a standardized route naming convention and lifecycle management process for Automate UI. This process will include:\n\n* **Route Naming Convention:** Define a clear and consistent naming convention for all routes, ensuring they are descriptive, hierarchical, and reflect the UI structure.  This might include utilizing prefixes for specific sections (e.g., `\/settings`, `\/infrastructure`, `\/data`) and avoiding abbreviations.\n* **Route Mapping Documentation:**  Maintain comprehensive documentation of all routes, their corresponding UI components, and their location within the application hierarchy. This documentation will serve as a single source of truth for developers.\n* **Route Deprecation Policy:**  Establish a formal deprecation policy for routes to ensure backwards compatibility. When a route needs to be renamed or removed, we will follow a process that includes:\n    * Marking the old route as deprecated but maintaining its functionality.\n    * Redirecting users from the old route to the new route.\n    * Providing clear communication to users about the deprecation and the new route location.\n    * Allowing sufficient time for users to migrate before removing the old route entirely.\n* **Decoupling Navigation Highlighting:**  Refactor the navigation highlighting logic to rely on a data-driven approach instead of route prefixes. This could involve associating UI elements with specific tags or categories that drive the highlighting logic, providing more flexibility in organizing and restructuring UI elements without breaking navigation.\n\nBy implementing these measures, we aim to achieve a more maintainable, robust, and user-friendly routing system in Automate UI. This will minimize the risk of breaking backwards compatibility in future updates and provide a more consistent and predictable experience for users navigating the application. \n","GenTime":"2024-08-19 16:17:59","text":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n\n\n##Decision\nEvery route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n"}
{"File Name":"opg-data-lpa-codes\/0001-flask-in-aws-lambda-function.md","Context":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n","Decision":"* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","tokens":469,"id":1540,"Prediction":"## Decision\n\nWe will implement the API endpoints and logic as a single Flask application deployed within a single AWS Lambda function. \n\n**Reasoning:**\n\nWhile the \"function-per-endpoint\" approach is often favored in serverless architectures, the specific context of this project makes a single-function Flask app a pragmatic choice.  Here's why:\n\n* **Project Simplicity and Isolation:** The project's small scale, well-defined boundaries, and lack of external dependencies minimize the risks associated with monolithic deployments.\n* **Development Speed:** Leveraging existing Flask knowledge, tooling, and infrastructure from previous projects will significantly accelerate development and testing.\n* **Maintainability:** A single Lambda function simplifies deployment, monitoring, and ongoing maintenance compared to managing multiple functions, especially given the project's anticipated low complexity.\n* **AWS Chalice Precedent:** The existence of AWS Chalice, a similar framework by AWS, suggests that deploying a WSGI app within Lambda is not inherently an anti-pattern.\n* **Mitigating Flask-Lambda Concerns:** While Flask-Lambda has limited contributors, its simplicity and the project's straightforward nature minimize the potential impact of any issues.\n\n**Addressing Concerns:**\n\n* **Cold Starts:** While a Flask app might have slightly longer cold starts than a single function, we believe the overall development and maintenance benefits outweigh this potential drawback, particularly for a small project.\n* **Anti-Pattern Concerns:** The \"anti-pattern\" argument appears largely theoretical in this context. The project's specific characteristics make a single-function approach more practical and efficient.\n\n**Future Considerations:**\n\nIf the project scope expands significantly or if external dependencies are introduced, we will re-evaluate this decision. We might consider breaking down the application into smaller, more focused Lambda functions.\n\n**Monitoring:**\n\nWe will closely monitor the Lambda function's performance, including cold starts and resource utilization. If performance issues arise due to the chosen architecture, we will revisit this decision. \n","GenTime":"2024-08-19 16:18:07","text":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n\n\n##Decision\n* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"Prediction":"## Decision\n\nWe will implement a hierarchical tenancy model with three distinct levels:\n\n**1. Platform Tenant:** This is the top-level tenant representing the Marain platform itself. It exists for both managed and self-hosted deployments and houses platform-level configurations and resources.\n\n**2. Client Tenant:** Each client using Marain, either directly or through a partner, will have their own dedicated Client Tenant. \n    * For managed services, we will create and manage these tenants. \n    * For self-hosted deployments, clients will be responsible for creating and managing their own Client Tenants. \n    * Client Tenants can be associated with specific resource groups or cloud resources to isolate data storage as required.\n\n**3. Service Tenant:**  Each Marain service, like Workflow or Operations, will have a corresponding Service Tenant under the Client Tenant. This allows us to:\n    * Manage service-specific configurations and resources.\n    * Track service usage and dependencies within a client's context.\n    * Differentiate between direct and indirect service usage. For example, a client may not have direct access to the Operations Service Tenant but can still utilize its functionality through their access to the Workflow Service Tenant.\n\n**Implementation Details:**\n\n* The `Marain.Tenancy` service will be extended to support this hierarchical model. \n* Each tenant will be identifiable by a unique ID and will store relevant metadata, including its parent tenant (if applicable). \n* APIs will be updated to accept a tenant context, allowing for tenant-specific operations.\n* Authorization and access control mechanisms will be implemented to enforce tenant isolation and permissions.\n\n**Advantages:**\n\n* **Flexibility:**  Supports both managed and self-hosted deployments, as well as varying degrees of data isolation.\n* **Scalability:** Allows for a large number of clients and services to be managed effectively.\n* **Resource Management:** Facilitates efficient resource utilization and cost allocation.\n* **Security:** Enforces clear boundaries between tenants, enhancing data security and privacy.\n\n**Future Considerations:**\n\n* Explore the possibility of allowing nested Client Tenants to support complex partner or reseller scenarios.\n* Integrate with billing systems to enable tenant-based billing and usage reporting. \n","GenTime":"2024-08-19 16:18:17","text":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n\n\n##Decision\nTo support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n"}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"Prediction":"## Decision\n\nWe will use Heroku's free tier to host the project for testing purposes. \n\n## Reasoning\n\n* **Ease of Use:** Heroku offers a simple deployment process, often with one-click deployments from source control platforms like GitHub. This minimizes the setup time and effort required by the development team.\n* **Free Tier Availability:** Heroku provides a free tier suitable for testing and low-traffic applications. This satisfies the \"no cost\" requirement.\n* **Continuous Integration\/Continuous Deployment (CI\/CD) Support:** Heroku integrates seamlessly with popular CI\/CD tools, allowing for automated deployments upon successful builds and tests.\n\n## Consequences\n\n* **Resource Limitations:** Heroku's free tier comes with limited resources (CPU, memory, etc.). This may not be suitable for performance testing or simulating high user loads.\n* **Sleep Mode:** Applications on the free tier may enter \"sleep mode\" after a period of inactivity, resulting in slower initial response times. \n* **Vendor Lock-in:** While not a major concern for testing environments, relying solely on Heroku's free tier may lead to challenges when scaling or migrating the application in the future.\n\n## Alternatives Considered\n\n* **Other free cloud platforms (e.g., Netlify, Vercel):** These platforms offer similar advantages to Heroku, with slight variations in features and ease of use. We may consider them if specific requirements arise.\n* **Self-hosting:** While offering maximum control, self-hosting introduces significant overhead in terms of infrastructure management, security, and maintenance. This option is not feasible given the need for a simple and cost-effective solution.\n\n## Status\n\nAccepted\n\n## Notes\n\nWe will re-evaluate this decision if the project's resource requirements exceed the limitations of Heroku's free tier or if alternative free hosting solutions with better capabilities become available. \n","GenTime":"2024-08-19 16:18:25","text":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n\n\n##Decision\nI decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n"}
{"File Name":"gp-finder\/0006-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":3469,"Prediction":"## Decision\n\nWe will implement an **application metrics system** using a combination of the following:\n\n* **Instrumentation:** Embed lightweight code within the application to collect key metrics and events. These metrics can include request rate, response time, error rates, resource utilization, and custom business-specific metrics.\n* **Metrics Library:** Utilize a library like Micrometer or Dropwizard Metrics to simplify the instrumentation process, provide standardized metrics formats, and offer integrations with various monitoring backends.\n* **Monitoring Backend:** Choose a monitoring and visualization tool like Prometheus, Graphite, or Datadog to store, aggregate, visualize, and analyze the collected metrics.  \n* **Alerting System:** Configure alerts based on defined thresholds for critical metrics within the chosen monitoring backend. This will enable proactive identification and response to potential issues.\n\nThis approach offers a lightweight and efficient method for monitoring application health and KPIs compared to log scraping. It provides real-time insights, historical data analysis, and proactive alerting capabilities, ultimately contributing to a more reliable and performant application for our users.\n","GenTime":"2024-08-19 16:18:30","text":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n\n##Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n"}
{"File Name":"cdk\/002-component-constuctors.md","Context":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","tokens":95,"id":1193,"Prediction":"## Decision\n<!--- This is for the actual decision being made. -->\nTo improve the developer experience and intuitiveness of the library, we will implement a comprehensive set of examples demonstrating the usage of each class and common design patterns. These examples will serve as practical guides, showcasing how to effectively utilize the library's components in various scenarios.  \n","GenTime":"2024-08-19 16:18:32","text":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n\n\n##Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n"}
{"File Name":"register-trainee-teachers\/0004-non-integer-trainee-ids.md","Context":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n","Decision":"We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","tokens":57,"id":3802,"Prediction":"## Decision\n\nTransition from using sequential integer-based trainee IDs to UUIDs (Universally Unique Identifiers) in all system URLs and external APIs. \n\n## Reasons\n\n* **Enhanced Security:** UUIDs mitigate the risk of enumeration attacks by removing predictability from URLs, making it significantly harder for malicious actors to guess or iterate through valid trainee resource identifiers.\n* **Improved Interoperability:**  UUIDs provide a standardized, database-agnostic format for identifying trainees, fostering seamless integration with other systems without relying on internal database IDs. This promotes a more decoupled and flexible architecture.\n* **Future-Proofing:** Adopting UUIDs anticipates potential future needs, such as distributed databases or microservices, where globally unique identifiers are essential. \n\n## Consequences\n\n* **Implementation Effort:** Refactoring existing codebases and APIs to utilize UUIDs will require development time and resources.\n* **Data Migration:** A strategy for migrating existing data and URLs to the new UUID format will need to be defined and executed carefully. \n* **Potential Performance Impact (Minor):** While generally negligible, UUIDs have slightly larger storage and index sizes compared to integers, potentially leading to minor performance overhead in specific database operations.\n\n## Trade-offs\n\nWe acknowledge the implementation effort required for this transition. However, the benefits of increased security, improved interoperability, and future-proofing outweigh the temporary costs.\n\n## Action Items\n\n1. **Choose a UUID Version:**  Select an appropriate UUID version (e.g., v4 for random generation) based on the specific requirements and constraints of the system.\n2. **Update Data Models:** Modify database schemas to accommodate UUIDs for trainee identification.\n3. **Refactor Codebase:** Update all components (controllers, models, views, APIs) to utilize UUIDs for trainee references.\n4. **Implement Data Migration Strategy:** Plan and execute a seamless transition of existing data and URLs to the new UUID format.\n5. **Update Documentation and Training Materials:** Ensure all relevant documentation and training resources reflect the change to UUIDs. \n","GenTime":"2024-08-19 16:18:40","text":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n\n\n##Decision\nWe chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n"}
{"File Name":"linshare-mobile-android-app\/0011-upload-document.md","Context":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n","Decision":"- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","tokens":94,"id":1643,"Prediction":"## Decision\n\nTo address the issue of `UriPermission` revocation in Android 9 and above, we will transition from relying solely on `android.net.Uri` to utilizing `ContentResolver` and persistent URI permissions. \n\nSpecifically, instead of directly passing the `Uri` to the Worker, we will:\n\n1. **Request persistent URI permissions:** When selecting a file using an `ACTION_OPEN_DOCUMENT` intent, we will request persistent URI permissions using `takePersistableUriPermission(Uri, int)`. \n2. **Store granted permissions:** The granted persistent URI permissions will be stored securely and associated with the upload task.\n3. **Use `ContentResolver` in Worker:**  The Worker will then use the stored persistent URI and `ContentResolver` to open an `InputStream` for the file, ensuring access even after the Activity is destroyed. \n\nThis approach offers several benefits:\n\n* **Reliable file access:** Ensures the Worker can access the file even after the Activity is no longer running, preventing \"Permission Denied\" errors.\n* **Android compatibility:**  Provides a solution compatible across Android versions, addressing the specific challenges introduced in Android 9 and above. \n* **Security:** Leverages the Android framework's built-in security mechanisms for handling URI permissions, maintaining the security of user data.\n\nThis decision aims to improve the reliability and robustness of our file upload functionality, particularly in the context of background tasks and newer Android versions.\n","GenTime":"2024-08-19 16:18:47","text":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n\n\n##Decision\n- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n"}
{"File Name":"cloud-sdk-js\/0021-odata-url-builder.md","Context":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n","Decision":"- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","tokens":61,"id":3630,"Prediction":"## Decision\n\nWe will extend the existing request builder APIs to accommodate OData-specific features like querying navigation properties and retrieving raw values. This approach leverages our existing infrastructure while providing comprehensive support for OData capabilities. \n\nWe will introduce the following additions:\n\n*   **Navigation Property Accessors:**  Methods or properties will be added to generated entity classes, allowing developers to chain navigation properties directly within the request builder. For example: `api.people('scottketchum').friends.get()`. \n*   **Raw Value Retrieval:** A new method, such as `getRawValue()` or similar, will be integrated into the request builder to specifically target and retrieve the raw value of a property. For example: `api.people('scottketchum').age.getRawValue()`.\n\nThese additions will be implemented while adhering to the design principles of the current request builder APIs to maintain consistency and ease of use for developers. \n","GenTime":"2024-08-19 16:18:51","text":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n\n\n##Decision\n- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"Prediction":"## Decision\n\nWe will use **Spring Framework** to implement the application, leveraging its features for:\n\n* **Inversion of Control (IoC):**  To inject infrastructure service implementations into the core hexagon, adhering to the principles of Hexagonal Architecture.\n* **REST API Implementation:** Utilizing Spring MVC or Spring WebFlux to define incoming adapters for handling HTTP resources efficiently.\n* **Additional Benefits:** Taking advantage of Spring's AOP capabilities, security features, and other modules to streamline development and ensure a cohesive application design.\n\nThis decision is based on Spring's alignment with Hexagonal Architecture principles, its maturity and feature set, and the author's expertise in the framework. \n","GenTime":"2024-08-19 16:18:54","text":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n\n\n##Decision\nSpring framework will be the backbone for `menu-generation` application.\\n"}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"Prediction":"## Decision\n\nWe will not implement a derivative generation and management system at this time. \n\n## Justification\n\nWhile derivative creation was a common solution for handling large files and improving user experience in the past, the evolving capabilities of modern web browsers and the limitations of derivatives for certain file types lead us to this decision. \n\nThe following factors contributed to this decision:\n\n* **Improved Browser Capabilities:** Modern web browsers can now handle a wider range of file types and sizes natively, reducing the need for pre-generated derivatives. \n* **Limited Derivative Applicability:** Thumbnails, a common derivative type, are only relevant for image and video files. For other file types, they provide no benefit. Generic file type icons can convey information more effectively in these cases.\n* **Complexity and Overhead:** Developing and maintaining a derivative generation and management system introduces significant complexity and overhead. This includes handling various file formats, managing storage, and ensuring synchronization with original files. \n\n## Consequences\n\n* **Potential for Larger File Downloads:**  Users might need to download larger files to view them, potentially leading to longer loading times, especially for users with slower internet connections. \n* **Reliance on Browser Capabilities:** We depend on users having up-to-date browsers that can effectively handle a diverse range of file types. \n* **Alternative Icon System Needed:**  We will need to implement a robust system for displaying meaningful file type icons for non-image and non-video files. \n\n## Status\n\nAdopted\n\n## Review Date\n\nThis decision will be reviewed again on [Date] or if significant changes in user needs, browser capabilities, or file handling technologies occur. \n","GenTime":"2024-08-19 16:19:02","text":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n\n\n##Decision\nScholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n"}
{"File Name":"push-sdk-ios\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","tokens":16,"id":4196,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will be a separate markdown file stored in a dedicated \"adr\" directory at the root level of the repository. \n","GenTime":"2024-08-19 16:19:03","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\\nWe need a simple way to manage our package version.\\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"Prediction":"## Decision\n\nWe will use **semantic versioning (SemVer)** to manage our package versions. \n\n## Justification\n\n* **Widely Adopted:** SemVer is a widely adopted standard, making it easier for other developers to understand and work with our codebase.\n* **Clear Versioning:** It provides a clear and concise way to communicate changes in our package, indicating breaking changes, new features, and bug fixes. This helps avoid confusion and ensures compatibility between different versions.\n* **Automated Tooling:** SemVer is supported by various package managers and build tools, allowing for easier dependency management and automated releases. \n\n## Alternatives Considered\n\n* **Date-based versioning:** While simple, it lacks information about the nature of changes and makes it difficult to track dependencies.\n* **Internal build numbers:**  Offers limited information to external users and can lead to confusion when integrating with other systems.\n\n## Consequences\n\n* **Stricter Release Process:** Adhering to SemVer requires a more disciplined approach to versioning and release management.\n* **Learning Curve:** Developers unfamiliar with SemVer may need some time to understand its principles.\n\nHowever, the benefits of improved clarity, compatibility, and automation outweigh these consequences.\n","GenTime":"2024-08-19 16:19:09","text":"## Context\\nWe need a simple way to manage our package version.\\n\n\n##Decision\nWe use versioneer to do this for us.\\n"}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"Prediction":"## Decision\n\nWe will implement a continuous integration step that performs the following actions:\n\n1. **Build the application for production:**  Use the Google Closure Compiler in advanced compilation mode to generate the production bundle.\n2. **Deploy the production bundle to a staging environment:** This environment should mirror the production environment as closely as possible.\n3. **Run the full suite of tests against the staging environment:**  This ensures that the tests are executed against the exact same code that will be released to production.\n\n**Rationale:**\n\n* Running tests against the production bundle in a production-like environment eliminates the risk of false positives caused by differences between the development and production environments.\n* Advanced compilation mode can introduce subtle bugs that are not caught by testing against development sources. Testing the production bundle directly addresses this risk.\n* Automating this process as part of continuous integration ensures that these checks are performed consistently and frequently. \n\n**Consequences:**\n\n* **Increased CI\/CD pipeline complexity:** Implementing this step adds an additional stage to the CI\/CD pipeline.\n* **Longer feedback loop:** Building and deploying to a staging environment takes time, which will increase the time it takes to receive feedback on code changes.\n\n**Mitigation:**\n\n* **Optimize the CI\/CD pipeline:** Use techniques like parallelization and caching to minimize the impact on build times.\n* **Implement selective testing:**  Prioritize running critical tests against the staging environment, while less critical tests can continue to be run against the development sources.\n\nThis approach prioritizes reliability and minimizes the risk of deploying broken code to production, even at the cost of increased complexity and slightly longer feedback loops. \n","GenTime":"2024-08-19 16:19:16","text":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n\n\n##Decision\nTesting will be made against the production bundle to catch compilation errors before they reach our users.\\n"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"Prediction":"## Decision\n\nWe will use the OpenAPI Specification (OAS) to document our API. \n\n## Reasons\n\n* **Meets both driver requirements:** OAS provides both human-readable documentation (e.g., through Swagger UI) and a machine-readable format (JSON or YAML) for client generation.\n* **Widely adopted standard:**  This ensures compatibility with a large ecosystem of tools for documentation generation, client code generation, and API testing.\n* **Reduced development effort:** Automatic client generation significantly reduces the effort required to integrate with the API from various platforms and languages.\n\n## Considered Alternatives\n\n* Manually writing API documentation and client code. \n* Using an alternative API specification language (e.g., API Blueprint).\n\n## Rejected Alternatives\n\n* **Manual documentation and client code:** This is time-consuming, error-prone, and difficult to keep in sync with API changes.\n* **Other specification languages:** While some alternatives exist, OpenAPI has a larger community, broader tool support, and is more widely adopted. \n\n## Consequences\n\n* **Positive:** Improved developer experience, reduced development time, and better consistency across client implementations.\n* **Negative:**  Requires learning and adhering to the OpenAPI specification, potential limitations in customization for automatically generated client code. \n","GenTime":"2024-08-19 16:19:22","text":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n\n##Decision\n* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n"}
{"File Name":"terraform-aws-dynamodb\/0010-terraform-backend-in-module.md","Context":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n","Decision":"Restored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","tokens":57,"id":2539,"Prediction":"## Decision\n\nWe will adjust the module to optionally support backend configuration. \n\n**Rationale:**\n\n* **Flexibility:**  This change caters to both use cases: those requiring a simple, backend-less setup and LIC teams who need a backend for collaboration and state management. \n* **Ease of Use:**  By incorporating backend configuration within the module, we streamline the user experience for LIC teams, eliminating the need for manual Terraform configuration. \n\n**Implementation Details:**\n\n1.  Introduce a new variable to the module (e.g., `enable_backend`) that defaults to `false`.\n2.  When `enable_backend` is set to `true`, the module will:\n    *  Expose variables for necessary backend configuration options (e.g., backend type, bucket name, etc.).\n    *  Dynamically generate the `terraform { backend { ... } }` block within the module based on these variables.\n\n**Consequences:**\n\n* **Increased Complexity:** The module's logic will become slightly more complex to handle the optional backend configuration.\n* **Backward Compatibility:** The change should be implemented in a backward-compatible manner. Existing users who don't require a backend should not experience any changes in behavior.\n\n**Alternatives Considered:**\n\n* **Maintaining Status Quo:**  This would leave LIC teams to manage backend configuration themselves, creating inconsistency and potentially increasing the risk of errors. This option was deemed unsuitable due to its negative impact on usability.\n\n**Follow-up Actions:**\n\n* Update the module documentation to clearly explain the new backend configuration options and how to use them.\n* Develop and execute test cases to ensure the backend functionality works as expected in various scenarios.\n","GenTime":"2024-08-19 16:19:29","text":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n\n\n##Decision\nRestored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n"}
{"File Name":"once-ui\/0002-build-a-high-quality-ui-components-library.md","Context":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n","Decision":"- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","tokens":40,"id":1151,"Prediction":"## Decision\n\nWe will create a private, dedicated Angular library to house our shared components. This library will be published to a private package registry and included in our applications as a dependency.\n\n## Reasoning\n\n* **Reusability:** Creating a dedicated library promotes component reuse across multiple applications, ensuring consistency and reducing development time.\n* **Maintainability:** Centralizing components in a library simplifies maintenance. Updates and bug fixes only need to be made in one place, and then propagated to the applications via version updates.\n* **Consistency:** A shared library enforces a consistent design language and API across applications, providing a unified user experience.\n* **Modularity:**  The library can be structured in a modular way, allowing applications to only import the specific components they need.\n* **Private registry:** Using a private registry (like Azure Artifacts, GitHub Packages, or a self-hosted solution) ensures that our proprietary components remain secure and accessible only within our organization. \n\n## Consequences\n\n* **Initial Setup:**  Building and maintaining the library adds some overhead compared to simply copying and pasting components. \n* **Version Management:**  Care must be taken to manage library versions effectively and handle updates across multiple applications.\n* **Testing:**  A robust testing strategy is required to ensure the library's stability and compatibility with different applications.\n\n## Alternatives Considered\n\n* **Copy-pasting components:** This approach leads to code duplication, inconsistencies, and difficulty in maintenance. It is not scalable for a suite of applications.\n* **Using a public component library:** Public libraries may not perfectly match our design language and requirements. Contributing to an open-source library adds overhead and may not be suitable for our specific needs.\n\n## Status\n\nProposed\n\n## Open Issues\n\n* Choose a suitable private package registry.\n* Define a clear versioning strategy for the library.\n* Establish a thorough testing process for new components and library updates. \n* Determine the initial set of components to include in the library. \n","GenTime":"2024-08-19 16:19:37","text":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n\n\n##Decision\n- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n"}
{"File Name":"blueprint\/0019-org-management.md","Context":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n","Decision":"Improve transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","tokens":128,"id":3554,"Prediction":"## Decision\n\nWe will leverage infrastructure-as-code (IaC) principles and GitOps practices to manage our GitHub organization. This approach will involve:\n\n* **Defining organization structure, repository settings, team memberships, and other configurations in code using a declarative language.** We will evaluate and choose a suitable tool for this purpose, such as:\n    * **Terraform:** Offers a robust and widely adopted solution with providers for GitHub management. \n    * **Pulumi:** Provides a similar approach with support for multiple programming languages.\n    * **GitHub Actions:** Can be used to automate tasks and enforce configurations through scripting. \n\n* **Storing the configuration code in a dedicated Git repository.** This repository will act as the single source of truth for our GitHub organization's configuration.\n* **Implementing a GitOps workflow for applying changes.**  This workflow will involve:\n    * Contributors proposing changes to the organization's configuration via pull requests to the dedicated repository.\n    * Automated checks and tests running against every pull request to ensure validity and prevent unintended consequences.\n    * Reviews and approvals from designated individuals or teams before changes are merged.\n    * Automated application of the approved changes to our GitHub organization using a tool like Terraform Cloud, GitHub Actions, or a similar solution.\n\nThis approach offers several benefits:\n\n* **Improved Transparency:** All changes to the organization's configuration are tracked and auditable through Git history.\n* **Enhanced Collaboration:** Individual contributors can propose changes and participate in discussions via pull requests.\n* **Automated Governance:** Automated tests and approval workflows enforce policies and maintain consistency across the organization.\n* **Reduced Risk of Errors:**  Declarative configuration and automated application minimize manual errors and ensure predictable outcomes.\n\nThis approach aligns well with the Operate First movement by embracing automation, transparency, and collaboration in managing our infrastructure.\n","GenTime":"2024-08-19 16:19:45","text":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n\n\n##Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n"}
{"File Name":"arch\/0045-reuse-python-custom-libs.md","Context":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n","Decision":"1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","tokens":119,"id":2446,"Prediction":"## Decision\n\n\u4e3a\u4e86\u89e3\u51b3\u4ee3\u7801\u91cd\u590d\u548c\u5e93\u7248\u672c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u6211\u4eec\u51b3\u5b9a\u91c7\u7528\u4ee5\u4e0b\u65b9\u6848\uff1a\n\n1. **\u5efa\u7acb\u79c1\u6709 PyPI \u4ed3\u5e93\uff1a**  \u6211\u4eec\u4f1a\u642d\u5efa\u4e00\u4e2a\u79c1\u6709\u7684 PyPI \u4ed3\u5e93\uff0c\u7528\u4e8e\u5b58\u50a8\u548c\u7ba1\u7406\u6211\u4eec\u81ea\u5df1\u5f00\u53d1\u7684 Python \u5305\u3002\u8fd9\u5c06\u4f7f\u6240\u6709\u9879\u76ee\u90fd\u80fd\u8bbf\u95ee\u548c\u4f7f\u7528\u6700\u65b0\u7684\u3001\u7ecf\u8fc7\u6d4b\u8bd5\u7684\u5e93\u7248\u672c\u3002 \n\n2. **\u5c06\u73b0\u6709\u5e93\u91cd\u6784\u4e3a\u72ec\u7acb\u7684 Python \u5305\uff1a**  \u6211\u4eec\u4f1a\u5c06\u73b0\u6709\u9879\u76ee\u4e2d\u53ef\u590d\u7528\u7684\u4ee3\u7801\uff0c\u4f8b\u5982 price\u3001sms\u3001mail \u7b49\uff0c\u91cd\u6784\u4e3a\u72ec\u7acb\u7684 Python \u5305\uff0c\u5e76\u5c06\u5176\u53d1\u5e03\u5230\u79c1\u6709 PyPI \u4ed3\u5e93\u3002\n\n3. **\u9879\u76ee\u4f9d\u8d56\u7ba1\u7406\uff1a**  \u6240\u6709\u65b0\u9879\u76ee\u548c\u73b0\u6709\u9879\u76ee\u90fd\u5c06\u4f7f\u7528 `pip` \u4ece\u79c1\u6709 PyPI \u4ed3\u5e93\u5b89\u88c5\u6240\u9700\u7684\u5e93\u3002\u9879\u76ee\u5c06\u4f7f\u7528 `requirements.txt` \u6216 `setup.py` \u6587\u4ef6\u660e\u786e\u58f0\u660e\u5176\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u786e\u4fdd\u7248\u672c\u4e00\u81f4\u6027\u3002\n\n4. **\u6301\u7eed\u96c6\u6210\/\u6301\u7eed\u4ea4\u4ed8\uff08CI\/CD\uff09\uff1a**  \u6211\u4eec\u4f1a\u5efa\u7acb CI\/CD \u6d41\u7a0b\uff0c\u4ee5\u81ea\u52a8\u5316\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u53d1\u5e03 Python \u5305\u5230\u79c1\u6709 PyPI \u4ed3\u5e93\u3002\n\n##  \u597d\u5904\n\n* **\u4ee3\u7801\u590d\u7528:**  \u907f\u514d\u91cd\u590d\u9020\u8f6e\u5b50\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002\n* **\u7248\u672c\u4e00\u81f4\u6027:**  \u786e\u4fdd\u6240\u6709\u9879\u76ee\u4f7f\u7528\u76f8\u540c\u7248\u672c\u7684\u5e93\uff0c\u51cf\u5c11\u56e0\u7248\u672c\u4e0d\u517c\u5bb9\u5bfc\u81f4\u7684\u95ee\u9898\u3002\n* **\u6613\u4e8e\u7ef4\u62a4:**  \u901a\u8fc7\u96c6\u4e2d\u7ba1\u7406\u5e93\uff0c\u53ef\u4ee5\u66f4\u8f7b\u677e\u5730\u66f4\u65b0\u548c\u7ef4\u62a4\u4ee3\u7801\u3002\n* **\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf:**  \u901a\u8fc7 CI\/CD \u6d41\u7a0b\uff0c\u53ef\u4ee5\u81ea\u52a8\u8fdb\u884c\u4ee3\u7801\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u3002\n\n##  \u4e0d\u8db3\n\n* **\u642d\u5efa\u548c\u7ef4\u62a4\u79c1\u6709 PyPI \u4ed3\u5e93\u9700\u8981\u4e00\u5b9a\u7684\u6210\u672c\u3002**\n* **\u9700\u8981\u5bf9\u73b0\u6709\u4ee3\u7801\u8fdb\u884c\u91cd\u6784\uff0c\u6709\u4e00\u5b9a\u7684\u5de5\u4f5c\u91cf\u3002**\n* **\u56e2\u961f\u6210\u5458\u9700\u8981\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u79c1\u6709 PyPI \u4ed3\u5e93\u548c\u76f8\u5173\u7684\u5de5\u5177\u3002**\n\n##  \u540e\u7eed\u6b65\u9aa4\n\n1. **\u9009\u62e9\u5408\u9002\u7684\u79c1\u6709 PyPI \u4ed3\u5e93\u89e3\u51b3\u65b9\u6848\uff0c\u4f8b\u5982\uff1a**\n    * **JFrog Artifactory**\n    * **Sonatype Nexus Repository**\n    * **Azure Artifacts**\n    * **AWS CodeArtifact**\n    * **GitLab Package Registry**\n2. **\u5236\u5b9a\u4ee3\u7801\u91cd\u6784\u548c\u53d1\u5e03\u6d41\u7a0b\u89c4\u8303\u3002**\n3. **\u5bf9\u56e2\u961f\u6210\u5458\u8fdb\u884c\u76f8\u5173\u57f9\u8bad\u3002** \n","GenTime":"2024-08-19 16:19:55","text":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n\n\n##Decision\n1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n"}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"Prediction":"## Decision\n\nWe will implement a **dedicated feedback system** that allows users to leave reviews and ratings for individual meals they have ordered. This system will be distinct from any occasional surveys used to gather broader feedback on the app or service as a whole.\n\nThis decision is based on the following:\n\n* **Specificity:** Meal-specific feedback provides businesses with more actionable insights to improve their offerings.  \n* **User Experience:**  Allowing users to express their opinions directly on the relevant meal provides a more intuitive and satisfying experience.\n* **Motivation:** Users are more likely to leave feedback when prompted directly after experiencing the meal, rather than through a general survey.\n\nThe feedback system will include the following features:\n\n* **Star ratings:** Users can rate meals on a scale (e.g., 1-5 stars).\n* **Written reviews:** Users can provide detailed feedback on their experience. \n* **Meal association:** Reviews will be directly linked to the specific meal ordered.\n* **Business visibility:** Businesses will have access to view, analyze and respond to feedback on their meals.\n\nThis approach allows for both granular feedback on individual meals and broader feedback through separate surveys, providing a comprehensive solution for gathering user insights. \n","GenTime":"2024-08-19 16:20:00","text":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n\n\n##Decision\nWe'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n"}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"Prediction":"## Decision\n\nWe will expose libnd4j's compilation flexibility to end users through a combination of:\n\n1. **Pre-built Binaries:** Offer a set of pre-built Nd4j binaries targeting common platforms and optimized for different use cases (e.g., CPU-only, GPU-enabled with CUDA, inference-focused). This provides a streamlined experience for most users.\n2. **Build Profiles:** Introduce build profiles for users who need more control. These profiles, configurable via environment variables or build scripts, will expose key libnd4j compilation options like backend selection (CPU, CUDA, etc.), optimization flags, and dependency choices. \n3. **Advanced Customization:**  For users requiring even finer control, document the process of building both libnd4j and Nd4j from source. This documentation should cover dependency management, build system specifics, and available compilation options in detail. \n\nThis approach balances ease of use with flexibility. Most users will benefit from pre-built binaries, while those with specific requirements can leverage profiles or full custom builds. \n","GenTime":"2024-08-19 16:20:05","text":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n\n\n##Decision\nIn order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"Prediction":"## Decision\n\nWe will separate the alerting policies for CPU and memory utilization for production cells. While memory usage remains a good indicator of overall cell load and will continue to be used for scaling decisions, CPU utilization will be monitored with a distinct threshold to ensure system responsiveness and prevent performance degradation due to high CPU load. This independent monitoring approach acknowledges the non-linear relationship observed between CPU and memory usage in tenant applications, providing a more accurate reflection of cell health and resource requirements. \n","GenTime":"2024-08-19 16:20:08","text":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n\n\n##Decision\nIn order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"Prediction":"## Decision\n\n**Option 1: Use Apache Tika**\n\n**Rationale:** Apache Tika is a Java library known for its robust and extensive file type support. It excels at extracting text and metadata from various formats, aligning with our requirement to minimize external dependencies.\n\n**Option 2: Employ a dedicated library for each file type.**\n\n**Rationale:** While this approach offers potential performance gains for specific file types, it introduces complexity in maintenance and increases the project's reliance on external libraries. \n\n**Decision:** We will proceed with **Option 1: Apache Tika**. Tika's comprehensive file type support and active development community outweigh the potential performance benefits of dedicated libraries. This choice prioritizes maintainability and reduces external dependencies, aligning with our architectural priorities. \n","GenTime":"2024-08-19 16:20:12","text":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n\n\n##Decision\n- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n"}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"Prediction":"## Decision\n\nWe will not be implementing an identity merging feature within the EL query system itself. \n\nInstead, we will leverage the Monocle configuration file to allow users to define identity mappings. This approach keeps the EL query system simple and avoids adding complexity. Users can specify how contributor identities from different code review systems should be mapped to a unified identity within Monocle. \n","GenTime":"2024-08-19 16:20:14","text":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n\n\n##Decision\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n"}
{"File Name":"digitalrig-metal-aws\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1744,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as the mechanism for capturing architectural decisions on this project. Each significant architectural decision will be documented in its own ADR, following a lightweight template (e.g., Markdown file) that includes:\n\n- **Title:** A concise and descriptive title for the decision.\n- **Status:** The current status of the decision (e.g., Proposed, Accepted, Rejected, Superseded, Deprecated).\n- **Context:**  The background and circumstances that led to the decision.\n- **Decision:** The actual decision made.\n- **Consequences:** The positive and negative implications of the decision.\n- **Alternatives Considered:** Other options that were considered, along with their pros and cons.\n\nADRs will be stored in a dedicated directory within the project repository, making them easily accessible to all stakeholders. \n","GenTime":"2024-08-19 16:20:18","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"Prediction":"## Decision\n\nWe will use **Blazemeter's Taurus** as our load testing tool for benchmarking tests. \n\n## Reasons\n\n* **Simplicity:** Taurus offers a simple and intuitive configuration syntax, making it easy to define and execute complex test scenarios without extensive scripting.\n* **Flexibility:** Taurus supports various open-source testing tools like JMeter and Selenium, providing flexibility in choosing the right tool for different testing needs.\n* **Reporting and Analysis:** Taurus generates comprehensive reports that can be easily integrated with JMeter or other data analysis tools like Jupyter Notebook for in-depth performance analysis and visualization. \n\n## Alternatives Considered\n\n* **JMeter:** While powerful, JMeter can be complex to configure for large-scale tests and lacks the reporting flexibility of Taurus.\n* **Gatling:** Gatling is a powerful load testing tool, but its Scala-based DSL might require a steeper learning curve for our team.\n* **Other Tools:** Other tools were considered but deemed less suitable due to factors like complexity, limited reporting capabilities, or lack of community support.\n\n## Consequences\n\n* **Positive:** Taurus's simplicity and flexibility will enable us to quickly set up and execute complex benchmarking tests with minimal effort. The comprehensive reporting features will facilitate efficient performance analysis and identification of bottlenecks.\n* **Negative:**  Adopting a new tool requires some initial learning and setup. However, Taurus's user-friendly design and extensive documentation should minimize this impact. \n","GenTime":"2024-08-19 16:20:24","text":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n\n\n##Decision\nCreate an instance with an easy test (Just request the keys and the metrics).\\n"}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"Prediction":"## Decision\n\nWe will use an API documentation generator that integrates directly with our codebase. This generator will:\n\n* **Automatically generate documentation:** It will parse our code and extract relevant information, such as endpoints, parameters, request\/response formats, and code comments, to automatically generate API documentation. \n* **Support a standard documentation format:**  We will choose a generator that supports a widely adopted format like OpenAPI (Swagger) or API Blueprint. This ensures our documentation is easily readable, testable, and can be integrated with other tools.\n* **Allow for customization and extensions:** While automation is key, we need the ability to customize the generated output. This includes adding manual descriptions, examples, and organizing the documentation for clarity. \n* **Facilitate testing and interaction:** The chosen solution should allow developers and consumers to test API endpoints directly from the documentation, for example, using a built-in console or integration with testing tools.\n\nThis approach ensures our API documentation:\n\n* **Stays up-to-date:**  By integrating with the code, changes in code automatically reflect in the documentation.\n* **Is consistent and comprehensive:** The generator enforces a structured format and extracts information directly from the code, ensuring all endpoints and parameters are documented. \n* **Improves developer experience:** Easy-to-use and interactive documentation with testing capabilities makes it easier for developers to understand and utilize the API.\n\nThis decision prioritizes efficiency, maintainability, and a developer-friendly approach to API documentation. \n","GenTime":"2024-08-19 16:20:31","text":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n\n##Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n"}
{"File Name":"ibc-go\/adr-027-ibc-wasm.md","Context":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n","Decision":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","tokens":374,"id":1441,"Prediction":"## Decision\n\nWe will implement a Wasm-based light client module in ibc-go. This module will allow for the addition and upgrade of IBC light clients via governance-controlled Wasm bytecode deployment. \n\nThis approach addresses the identified problems by:\n\n* **Simplifying the process of adding and updating light clients:** Instead of requiring codebase modifications and coordinated upgrades across all connected chains, new light clients and updates can be deployed through a single governance proposal on the chain running the Wasm light client module. \n* **Decoupling chain upgrades from light client updates:** Chains can independently upgrade their consensus mechanisms without requiring other connected chains to immediately update their light clients. This reduces friction and downtime associated with upgrades.\n* **Enhancing flexibility and extensibility:**  The Wasm-based approach allows for a wider range of light client implementations, including those written in languages other than Go, as long as they can be compiled to Wasm.\n\nThis decision introduces a dependency on a Wasm VM within ibc-go and requires careful consideration regarding security and resource management for the execution of Wasm bytecode. These aspects will be addressed in subsequent ADRs and implementation details. \n","GenTime":"2024-08-19 16:20:37","text":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n\n\n##Decision\nWe decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n"}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"Prediction":"## Decision\n\nBased on the context of building a foundation for future growth and extensibility in the `v0.1` relayer library, the following decisions are made regarding its concurrency and architecture:\n\n**Concurrency:**\n\n* **Single-threaded execution model:** To prioritize simplicity and ease of development in the initial version, `v0.1` will adopt a single-threaded execution model. This simplifies the handling of shared state and avoids the complexities of concurrency primitives.\n* **Asynchronous API:** While internally single-threaded, `v0.1` will expose an asynchronous API using `async`\/`await` to allow for future integration with asynchronous frameworks and potential migration to multi-threaded execution in future versions. \n* **Event-driven architecture:**  `v0.1` will be built on an event-driven architecture, allowing components to communicate through events. This facilitates loose coupling and extensibility, enabling future addition of components without significant refactoring.\n\n**Architecture:**\n\n* **Modular design:** The relayer will be divided into distinct modules with clear responsibilities, such as chain handling, packet relaying, and event processing. This promotes code maintainability, testability, and independent evolution of components.\n* **Configuration-driven:**  `v0.1` will be highly configurable through a configuration file or environment variables. This allows for flexible deployment in different environments and enables experimentation with different configurations without code changes.\n* **Minimal dependencies:**  `v0.1` will strive to minimize external dependencies to reduce the maintenance burden and potential for conflicts. Essential dependencies for core functionality, such as networking and IBC client libraries, will be carefully selected.\n\n**Simplifying Assumptions for `v0.1`:**\n\n* **Single chain pair:**  `v0.1` will focus on relaying packets between a single pair of chains. Support for multiple chain pairs will be explored in subsequent versions.\n* **No packet filtering:**  `v0.1` will relay all packets observed on the source chain to the destination chain without any filtering or validation logic. Introduction of packet filtering and validation will be considered in future versions.\n* **Basic error handling:**  `v0.1` will implement basic error handling, primarily focusing on logging and reporting errors. Robust error recovery mechanisms and retry strategies will be addressed in later iterations.\n\nThese decisions prioritize simplicity and extensibility in the `v0.1` relayer, establishing a solid foundation for future feature additions and enhancements. By explicitly documenting these assumptions and architectural choices, we aim to ensure a clear understanding of the initial scope and facilitate informed decisions in future development iterations. \n","GenTime":"2024-08-19 16:20:48","text":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n\n\n##Decision\n### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n"}
{"File Name":"adr-demo\/0004-use-elastic-search-for-image-search.md","Context":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n","Decision":"Given our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","tokens":113,"id":2513,"Prediction":"## Decision\n\nWe will replace our current simple search form and database query mechanism with a more robust and feature-rich search engine solution. This decision is driven by the following factors:\n\n* **Scalability and Performance:**  A dedicated search engine is designed to handle large datasets and complex queries efficiently. This ensures optimal performance even as our user base and data volume grow.\n* **Advanced Search Capabilities:** Implementing the desired features (date range searches, \"and\/or\" queries, wildcard searches, synonyms) using database queries would be complex and difficult to maintain. Search engines natively support these features.\n* **Improved User Experience:** A dedicated search engine allows for faster and more relevant search results, greatly improving the user experience. \n\n**Options Considered:**\n\n* **Enhance Current System:** Enhancing the current database query-based system is not feasible due to the complexity of implementing advanced search features and the potential performance impact. \n* **Develop a Custom Search Solution:**  Building a custom search engine is time-consuming, resource-intensive, and requires specialized expertise. \n* **Integrate with a Third-Party Search Engine:**  This option provides a balance of functionality, performance, and development effort. \n\n**Decision:**\n\nWe will integrate our application with a third-party search engine. This approach offers several advantages: \n\n* **Reduced Development Time:**  Leveraging existing, well-tested solutions allows us to focus on core features and deliver value faster.\n* **Scalability and Performance:**  Third-party search engines are designed for high availability and can handle large amounts of data and traffic.\n* **Advanced Features:**  These services offer a wide range of built-in features such as relevancy ranking, faceting, spell correction, and more.\n\n**Next Steps:**\n\n1. **Research and select a suitable third-party search engine** based on factors like cost, scalability, features, and ease of integration.\n2. **Design and implement the integration** between our application and the chosen search engine. This includes indexing data, implementing the search interface, and handling search results.\n3. **Thoroughly test the integration** to ensure accurate and efficient search functionality. \n4. **Deploy the new search functionality** incrementally, monitoring performance and user feedback closely.\n","GenTime":"2024-08-19 16:20:57","text":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n\n##Decision\nGiven our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n"}
{"File Name":"DunkMe\/0002-data-access-api.md","Context":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","Decision":"Chosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","tokens":111,"id":1660,"Prediction":"## Decision\n\n**Option selected:** ASP.NET Web API (C#)\n\n**Reasoning:**\n\nASP.NET Web API offers a robust and well-supported framework for building APIs, aligning well with our existing expertise and technology stack. \n\nSpecifically:\n\n* **.NET Core 2 Compatibility:** Targeting .NET Core 2 ensures  performance, scalability, and access to a modern ecosystem.\n* **Dapper ORM Integration:** Leveraging Dapper as the ORM allows us to utilize existing skills within the team, streamlining development and reducing the learning curve. \n* **Testability:** ASP.NET Web API's architecture naturally lends itself to implementing a repository pattern, which in turn facilitates unit testing and ensures the maintainability of the codebase.\n\nThis combination positions us to develop a high-quality, performant, and testable data access API that integrates seamlessly with our current technological landscape. \n","GenTime":"2024-08-19 16:21:01","text":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n\n##Decision\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n"}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"Prediction":"## Decision\n\nWe will implement a single, generic graph transformation mechanism that can be used for both pattern refinement and pattern detection. \n\nThis mechanism will receive the following inputs:\n\n* **Graph:** The graph on which the transformation is applied.\n* **Matching Subgraph:** The subgraph that has been identified as a match.\n* **Replacement Structure:**  The structure to replace the matching subgraph with. This will be either a Refinement Structure (for pattern refinement) or a Detector (for pattern detection).\n\nBy providing the replacement structure as an input parameter, we can reuse the same mechanism for both processes. \n","GenTime":"2024-08-19 16:21:05","text":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n\n\n##Decision\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n"}
{"File Name":"lbh-adrs\/Validation.md","Context":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n","Decision":"**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","tokens":276,"id":2307,"Prediction":"## **Decision**\n\nWe will implement **Fluent Validation** for API request validation. \n\nWhile Data Annotations offer a simple approach, Fluent Validation provides superior flexibility, maintainability, and testability.  \n\nThis decision aligns with our goals to:\n\n* **Centralize validation logic:** Fluent Validation keeps validation rules separate from our data models, improving code readability and organization. \n* **Enhance testability:**  Fluent Validation simplifies unit testing of validation logic in isolation.\n* **Enable customization:** Externalizing error messages through dependency injection allows for greater control and adaptability, including localization.\n* **Support complex validation scenarios:**  Fluent Validation's expressive API and built-in validators accommodate intricate business rules and conditional validations more effectively. \n","GenTime":"2024-08-19 16:21:08","text":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n\n##Decision\n**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n"}
{"File Name":"sre-adrs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made for our team.\\n","Decision":"We will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","tokens":16,"id":2261,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions within our project. Each ADR will capture the context, decision, status, and consequences of a particular architectural choice. We will use a lightweight Markdown format and store these ADRs in a dedicated `\/adr` directory within our version control system.\n","GenTime":"2024-08-19 16:21:10","text":"## Context\\nWe need to record the architectural decisions made for our team.\\n\n\n##Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n"}
{"File Name":"corona-hackathon\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3663,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences. Each decision will be documented in a separate file within a dedicated \"adr\" directory at the root level of the project repository. \n\nWe will adopt the MADR (Markdown Architectural Decision Records) template for consistency and ease of maintenance. This lightweight format relies on Markdown and encourages concise and focused documentation.\n\nThis approach ensures that architectural decisions are:\n\n- **Explicitly documented:**  Preventing ambiguity and misunderstandings.\n- **Traceable:**  Providing a clear history of why a decision was made.\n- **Communicated:**  Making the rationale behind decisions accessible to all stakeholders.\n- **Maintainable:**  Allowing for easy updates and evolution of architectural decisions over time. \n","GenTime":"2024-08-19 16:21:14","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"Prediction":"## Decision\n\nWe will adopt the following definitions and guidelines to differentiate between decorators and presenters in our codebase:\n\n**Decorator:**\n\n* **Purpose:** Extends or modifies the functionality of an object **transparently**, without changing its interface.\n* **Implementation:** Wraps the original object and delegates calls to it, optionally adding behavior before, after, or around the original methods.\n* **Usage:** When we need to add responsibilities to individual objects dynamically, without affecting other instances of the same class.\n* **Example:** Implementing logging, caching, or authorization around existing methods.\n\n**Presenter:**\n\n* **Purpose:** Formats and prepares data for presentation by a specific view or user interface element.\n* **Implementation:** Accepts a data object and exposes methods to retrieve formatted data, tailored for the view.\n* **Usage:** When we need to decouple data processing and formatting logic from the view, improving testability and maintainability.\n* **Example:** Transforming a database record into a formatted string for display in a web page.\n\n**Distinguishing factors:**\n\n* **Transparency:** Decorators modify behavior transparently, while presenters explicitly handle data transformation for presentation.\n* **Focus:** Decorators focus on adding\/modifying functionality, while presenters focus on formatting and preparing data.\n* **Interface:** Decorators maintain the original object's interface, while presenters define their own interface tailored to the view.\n\nThese guidelines aim to provide a clear separation of concerns, ensuring that decorators are used for dynamic behavior modification and presenters are used for view-specific data preparation.\n","GenTime":"2024-08-19 16:21:21","text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n\n\n##Decision\nDecorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n"}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"Prediction":"## Decision\n\nThe documentation platform for all PACE projects (Brille, Euphonic, Horace\/Herbert) will be **GitHub Pages**. \n\n## Justification\n\nWhile both Read the Docs and GitHub Pages offer suitable platforms for hosting Sphinx-based documentation, the following factors led to choosing GitHub Pages:\n\n* **Brille API Documentation:** Brille's requirement for C code documentation poses a limitation with Read the Docs, which lacks direct support for this. GitHub Pages, with its flexible build process, allows integration of tools like Doxygen or Breathe to generate and incorporate API documentation from C source. \n* **Unified Workflow:** Using GitHub Pages keeps the documentation closely coupled with the code repositories, all within the GitHub ecosystem. This simplifies the workflow for developers, especially for version control and deployment.\n* **Cost-effectiveness:** Both options are free for open-source projects.\n\n## Consequences\n\n* **Increased Build Complexity (Minor):**  Integrating C documentation generation into the GitHub Pages build pipeline will require additional configuration and tooling.\n* **Learning Curve (Minor):**  Developers unfamiliar with GitHub Pages may require a brief familiarization period.\n\nThis decision favors a unified and streamlined developer experience while accommodating the specific needs of Brille's documentation. \n","GenTime":"2024-08-19 16:21:26","text":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n\n\n##Decision\nWe will use GitHub pages for all PACE project documentation.\\n"}
{"File Name":"DunkMe\/0001-front-end.md","Context":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","Decision":"Chosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","tokens":93,"id":1659,"Prediction":"## Decision\n\n**Option Selected:** Angular (with TypeScript) \n","GenTime":"2024-08-19 16:21:27","text":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n\n\n##Decision\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"Prediction":"## Decision\n\n**Option 3: Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event.**\n\nThis approach provides the best balance between performance and data consistency. While it introduces a configurable latency for policy updates and data freshness, it minimizes network overhead and deserialization efforts compared to option 2. \n\n## Rationale\n\n- **Performance:** Caching significantly reduces the number of remote calls required for enriching events, mitigating the primary downside of option 2. This leads to improved throughput and lower latency for event delivery.\n- **Scalability:** By reducing the load on the cluster with fewer remote calls, the system can handle a higher volume of events and connections, contributing to better scalability.\n- **Data Consistency:** While not entirely real-time, the configurable cache duration allows for a practical trade-off. A short cache duration (e.g., seconds) can ensure near real-time updates for policy and data, while still providing significant performance benefits. \n- **Flexibility:** The configurable cache duration allows tuning the system based on specific use cases and performance requirements. \n\n## Consequences\n\n- **Implementation Complexity:** Implementing a robust caching mechanism adds complexity to the system. Careful consideration needs to be given to cache invalidation strategies and potential data inconsistency issues.\n- **Cache Management:**  Monitoring and managing the cache will be crucial for optimal performance. This includes monitoring cache hit ratios, eviction policies, and potential cache staleness.\n- **Configuration:**  The cache duration will need to be configurable to allow for fine-tuning based on specific needs and acceptable latency for policy and data updates.\n\n**Mitigation:**\n\n- Utilize a mature caching library with built-in mechanisms for cache invalidation and consistency.\n- Implement comprehensive monitoring and alerting for cache performance and potential issues.\n- Provide clear documentation and guidelines for configuring the cache duration based on specific use cases and requirements.\n\nThis decision prioritizes performance and scalability while maintaining an acceptable level of data consistency through configurable caching. It minimizes the impact on existing event processing while enabling the desired signal enrichment functionality. \n","GenTime":"2024-08-19 16:21:36","text":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n\n\n##Decision\nWe provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n"}
{"File Name":"smarthub\/0002-use-lerna-for-package-versioning.md","Context":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n","Decision":"Migrate code base to monorepo structure and use `lerna` for versioning management.\\n","tokens":39,"id":3691,"Prediction":"## Decision\n\nWe will introduce a monorepo structure for the Smarthub project to manage the interdependencies between packages within the Smarthub SDK. \n","GenTime":"2024-08-19 16:21:38","text":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n\n##Decision\nMigrate code base to monorepo structure and use `lerna` for versioning management.\\n"}
{"File Name":"raster-foundry\/adr-0004-static-asset-pipeline.md","Context":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n","Decision":"Given the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","tokens":408,"id":1680,"Prediction":"## Decision\n\nWe will use **Webpack** in conjunction with **NPM** scripts for our static asset pipeline. \n\n**Reasons:**\n\n* **Developer Experience:** Webpack offers features like hot reloading and a built-in development server, leading to faster feedback cycles and improved developer productivity. This aligns with our requirement for a smooth development process.\n* **Performance:** Webpack's dependency graph and bundle splitting capabilities are expected to result in faster build times and improved application performance, addressing our need for speed.\n* **Modern Tooling:**  While some team members have experience with Gulp, the team generally views Grunt as outdated.  Webpack is a widely adopted and modern solution that aligns better with our technology direction.\n* **Familiarity and Support:**  Other teams at Azavea are successfully using Webpack, providing us with internal expertise and support.\n* **Ecosystem and Features:** Webpack's ecosystem provides access to a vast library of plugins and loaders, making it highly configurable and capable of integrating with our existing tools.\n* **Overcoming NPM Concerns:**  While some concerns exist about the NPM ecosystem, these concerns are not unique to Webpack and can be mitigated through strategies like a lazy NPM cache.\n\n**Alternatives Considered:**\n\n* **Grunt:** Rejected due to negative past experiences, particularly regarding slow compile times and configuration complexity. \n* **Gulp:** While offering potential speed advantages, Gulp lacks the wider adoption and ecosystem benefits of Webpack. The team's existing familiarity with NPM scripts makes a Gulp-specific solution less appealing.\n\nThis decision allows us to leverage a modern and performant static asset pipeline that prioritizes developer experience and aligns with our organizational knowledge base. \n","GenTime":"2024-08-19 16:21:45","text":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n\n\n##Decision\nGiven the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n"}
{"File Name":"fundraising-application\/020_PayPal_IPN_Queue.md","Context":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n","Decision":"* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","tokens":190,"id":1523,"Prediction":"## Decision\n\n**Implement a queuing system to store and process incoming PayPal IPNs.** This system will store all incoming IPN messages, allowing us to:\n\n* **Improve Debugging:** Access and analyze the exact payload received from PayPal during error scenarios.\n* **Enable Automated Retries:** Automatically retry processing failed IPN messages once the underlying issue is resolved. \n* **Increase System Resilience:** Decouple the processing of IPN messages from the initial request, making the system more resilient to temporary outages or errors.\n\nThis decision prioritizes system observability, resilience, and maintainability by providing a clear record of interactions with PayPal and a mechanism for automated recovery. \n","GenTime":"2024-08-19 16:21:48","text":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n\n##Decision\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n"}
{"File Name":"volley-management\/0002-use-sendgrid-as-mailing-system.md","Context":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n","Decision":"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","tokens":20,"id":2523,"Prediction":"## Decision\n\nWe will utilize a cloud-based email service provider (ESP) to manage and send transactional emails to users. \n","GenTime":"2024-08-19 16:21:50","text":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n\n\n##Decision\nChosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n"}
{"File Name":"saas-plaform-tenant-identity-provider\/0002-use-a-hosted-identity-solution-over-a-self-hosted-one.md","Context":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n","Decision":"We will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","tokens":142,"id":3421,"Prediction":"## Decision\n\n**Use a SaaS identity provider for the initial implementation of the platform.**\n\n## Reasoning\n\nWhile self-hosting offers more control and potentially lower costs at scale, the following factors favor a SaaS solution, especially in the early stages of development:\n\n* **Reduced Management Effort:**  SaaS providers handle infrastructure, maintenance, security updates, and compliance, freeing up resources to focus on core product development.\n* **Faster Time-to-Market:**  Implementing a pre-built SaaS solution is significantly faster than building and managing an in-house identity provider. \n* **Enhanced Security:** Established SaaS providers typically offer robust security features and expertise that would be expensive and time-consuming to replicate internally. \n* **Lower Initial Costs:** SaaS solutions often have lower upfront costs compared to the initial investment required for hardware, software, and expertise for a self-hosted solution.\n\nAlthough a SaaS solution may lead to vendor lock-in and potentially higher costs in the long run, the advantages during the initial development and deployment phases outweigh these concerns. \n\nThe analysis from the provided source confirms that identity management is not a core value driver and is unlikely to change drastically. This further supports the decision to utilize a SaaS solution, as it provides the necessary functionality without diverting resources from core development.\n\n## Consequences\n\n* **Vendor Lock-in:**  Switching providers later will require migration effort and potential service disruption.\n* **Limited Customization:** SaaS solutions may not offer the same level of customization as a self-hosted solution.\n* **Potential Cost Scaling:** Costs may become higher than a self-hosted solution as the user base and usage grow significantly.\n\n## Future Considerations\n\n* **Re-evaluate the decision to self-host once the platform reaches a significant scale** and the cost of a SaaS solution outweighs the benefits.\n* **Negotiate favorable contract terms with the chosen SaaS provider** to mitigate the risks of vendor lock-in and price hikes.\n* **Implement a clear exit strategy** in case a switch to a different identity provider becomes necessary in the future. \n","GenTime":"2024-08-19 16:21:58","text":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n\n##Decision\nWe will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"Prediction":"## Decision\n\nWe will implement custom metadata as key-value pairs directly within the document JSON structure. This approach offers flexibility, searchability, and avoids complex database schema changes.\n\n**Details:**\n\n* Each document will have a dedicated \"metadata\" field within its JSON representation.\n* This field will store user-defined key-value pairs, where keys are strings and values can be strings, numbers, or booleans.\n* Pagination data (e.g., page number) will be included as standard metadata with predefined keys.\n\n**Rationale:**\n\n* **Flexibility:**  Allows for arbitrary metadata without predefined fields, accommodating diverse user needs.\n* **Searchability:**  Key-value pairs can be easily indexed and searched, enabling efficient retrieval based on metadata.\n* **Simplicity:**  Avoids introducing a separate metadata database or table, minimizing complexity and maintenance overhead. \n\n**Consequences:**\n\n* **Schema Evolution:** Handling potential conflicts or changes in metadata structure over time will require careful consideration. \n* **Storage Space:**  Increased storage consumption due to storing metadata within each document. This impact should be minimal for most use cases. \n","GenTime":"2024-08-19 16:22:03","text":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n\n\n##Decision\n- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n"}
{"File Name":"opg-lpa\/0002-custom-save-handler-in-service-front.md","Context":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n","Decision":"Use a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","tokens":490,"id":1654,"Prediction":"## Decision\n\n**Implement a locking mechanism for session data access within the service-front component.**\n\nThis mechanism will ensure that only one request can modify the session data at any given time, preventing data races and inconsistencies.  \n\n**Specifics:**\n\n* **Locking Strategy:**  Utilize a distributed locking mechanism compatible with PHP and Redis, such as Redlock, to prevent concurrent session data modifications across multiple instances of the service-front component.\n* **Granularity:** Employ fine-grained locking at the session level, allowing concurrent read access to session data while ensuring exclusive access during write operations. \n* **Timeout:** Implement appropriate lock timeouts to avoid deadlocks in case of unexpected request termination.  \n* **Error Handling:**  Handle potential locking exceptions gracefully, providing informative error messages and potentially retrying the operation.\n\n**Consequences:**\n\n* **Improved Data Integrity:**  Eliminates the session data corruption issue described, ensuring data consistency and preventing CSRF vulnerabilities.\n* **Increased Latency:**  Introduces minor overhead due to lock acquisition and release, potentially increasing request processing time.  \n* **Implementation Complexity:**  Requires implementing and testing the chosen locking mechanism, adding complexity to the service-front component.\n\n**Alternatives Considered:**\n\n* **Optimistic Locking:**  This approach was deemed unsuitable for this scenario as it relies on detecting conflicts rather than preventing them, leading to potential data loss and inconsistencies.\n* **Session Data Partitioning:** While partitioning session data based on request types could minimize conflicts, it introduces significant complexity and may not completely eliminate the issue.\n\n**Justification:**\n\nThe chosen solution prioritizes data integrity and security over potential minor performance impacts. Implementing a robust locking mechanism effectively addresses the identified data race condition, ensuring consistent and reliable session data management. \n","GenTime":"2024-08-19 16:22:11","text":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n\n\n##Decision\nUse a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n"}
{"File Name":"pcmt\/adr-007.md","Context":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"Prediction":"## Decision\n\nPCMT will adopt a multi-part versioning scheme to address the listed goals. The scheme will consist of the following parts:\n\n**1. Suitability Level:**\n\n   * **Pre-v1:**  Indicates the software is under active development and not ready for production use. This level targets global stakeholders.\n   * **v1:** Indicates the software is production-ready for global stakeholders. \n   * **v2:** Indicates the software is production-ready for national stakeholders and likely includes additional features or modifications specific to those users.\n\n**2. Semantic Versioning:**\n\n   * Following the **SemVer 2.0.0** specification (MAJOR.MINOR.PATCH). This communicates the nature of changes between releases to developers and users familiar with semantic versioning. \n   * Allows for clear indication of backward compatibility through major, minor, and patch version increments.\n\n**3.  Akeneo Derivation:**\n\n   * A separate identifier, potentially appended to the version string (e.g., \"v1.2.3-Akeneo4.2\"), will denote the specific Akeneo version upon which the PCMT version is built.\n\n**Version String Format:**\n\nA typical PCMT version string will appear as follows:\n\n`[Suitability Level].[SemVer]-[Akeneo Derivation]`\n\n**Example:**\n\n* `Pre-v1.0.0-Akeneo4.2` - Under development, initial release built on Akeneo 4.2\n* `v1.0.0-Akeneo4.2` - Initial production release for global stakeholders, built on Akeneo 4.2\n* `v1.1.0-Akeneo4.2` - Minor feature update, backward compatible with v1.0.0, still built on Akeneo 4.2\n* `v2.0.0-Akeneo5.0` - Major release for national stakeholders with significant changes and potentially a new Akeneo base version \n\n**Rationale:**\n\n* This multi-part scheme allows PCMT to communicate information to different audiences effectively. \n*  Stakeholders unfamiliar with semantic versioning can quickly understand the software's suitability for their needs through the Suitability Level.\n*  Developers benefit from the clarity and compatibility information provided by semantic versioning. \n*  Tracking the Akeneo base version simplifies dependency management and ensures clarity about the underlying platform. \n","GenTime":"2024-08-19 16:22:21","text":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n\n\n##Decision\n1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n"}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"Prediction":"## Decision\n\n**Option 1: Use a GDS compliant framework\/library:**\n\n* **Pros:**  Fastest implementation, ensures adherence to GDS guidelines, likely offers theming capabilities.\n* **Cons:**  Might limit design flexibility outside the framework's scope, potential dependency on third-party updates.\n\n**Option 2:  Build custom components adhering to GDS:**\n\n* **Pros:** Full control over implementation and styling, easier to extend beyond GDS in the future if needed.\n* **Cons:**  Higher initial development time, requires manual GDS compliance maintenance.\n\n**Decision:**  We will opt for **Option 1** and select a well-maintained GDS-compliant framework\/library. This approach balances development speed with adherence to GDS standards while allowing for future theming through the framework's capabilities. \n\n**Justification:**  Using a framework minimizes initial development effort and ensures GDS compliance from the outset. While custom development offers flexibility, the theming requirement for food.gov branding suggests that leveraging a framework's theming engine will be more efficient than building a custom solution.\n\n**Consequences:** \n\n* We will need to evaluate and select a suitable GDS framework\/library.\n* Future design changes outside the framework's scope might be more complex.\n* We will be reliant on the chosen framework's updates and maintenance.\n","GenTime":"2024-08-19 16:22:27","text":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n\n\n##Decision\nWe will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n"}
{"File Name":"documents-api\/0002-use-base64-content-for-file-uploading.md","Context":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n","Decision":"We decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","tokens":113,"id":3987,"Prediction":"## Decision\n\nWe will use the **S3 Presigned URL** approach for file uploads. \n\n## Reasons\n\n* **Reduced Server Load:** Offloads file handling and processing to the client and directly to S3, minimizing server resource utilization (bandwidth, storage, processing).\n* **Improved Performance:** Clients upload directly to S3, potentially leading to faster upload speeds, especially for larger files.\n* **Enhanced Security:**  Clients interact with S3 directly using temporary, restricted credentials, limiting the API's attack surface and reducing the need for the API to manage long-term S3 credentials.\n* **Scalability:**  S3's architecture is inherently scalable, making this approach suitable for handling a large volume of uploads.\n\n## Consequences\n\n* **Increased Client-Side Complexity:** Clients need to handle the presigned URL generation and the upload process, potentially requiring additional logic and libraries. \n* **Limited Control and Validation:** The API has less direct control over the upload process, making it slightly more challenging to implement features like upload progress monitoring, validation, and transformations on the server-side.\n\n## Status\n\nAccepted\n\n## Notes\n\nWe acknowledge the increased client-side complexity. However, we believe the benefits of improved performance, security, and scalability outweigh this drawback. We will provide clear documentation and potentially client-side libraries to simplify the integration process. \n","GenTime":"2024-08-19 16:22:34","text":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n\n##Decision\nWe decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"Prediction":"## Decision\n\n**Replace the existing drawing tools with a new library-agnostic drawing tools framework.** This framework will address the identified issues by:\n\n* **Optimizing rendering performance:** The framework will minimize DOM updates by only re-rendering when necessary, such as after a drawing action is completed. This will be achieved by leveraging techniques like virtual DOM diffing and event debouncing.\n* **Enforcing clear separation of concerns:** The framework will implement a unidirectional data flow architecture.  A dedicated Drawing Tools Store will hold the state of the annotation and be solely responsible for updating it. Drawing tools will dispatch events to the store, which will update the annotation and trigger necessary re-renders. This approach ensures predictable data flow and simplifies debugging.\n* **Providing a simple and consistent API:** The framework will expose a clear and concise API for defining and interacting with drawing tools. This will eliminate the need for complex static methods and improve code readability.\n* **Establishing annotation standards:** The framework will define standardized data structures for representing drawing annotations. This will ensure consistency across different tools and simplify data processing. \n\n**This decision promotes:**\n\n* **Maintainability:** The standardized structure and clear separation of concerns will make the codebase easier to understand, debug, and extend.\n* **Performance:** Optimized rendering will lead to a smoother user experience, especially on lower-powered devices.\n* **Scalability:** The framework will provide a solid foundation for adding new drawing tools and features in the future.\n\n**Potential drawbacks:**\n\n* **Initial development time:** Building the new framework will require an upfront investment.\n* **Migration effort:** Adapting existing tools and workflows to the new framework will take time and effort.\n\n**However, we believe the long-term benefits of improved maintainability, performance, and scalability outweigh these drawbacks.**\n","GenTime":"2024-08-19 16:22:41","text":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n\n\n##Decision\nWhat we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n"}
{"File Name":"dos-capacity-status-api\/002-shared_database.md","Context":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n","Decision":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","tokens":54,"id":3937,"Prediction":"## Decision\n\nWe will implement the API as an internal API within the DoS ecosystem, leveraging existing authentication and authorization mechanisms. External access will not be provided. \n\n**Reasoning:**\n\n* **Security:** As the API modifies sensitive data related to service capacity, exposing it externally introduces significant security risks. \n* **Integration:**  Being part of the DoS ecosystem allows for tighter integration with existing DoS components, simplifying development and maintenance.\n* **Performance:** Internal APIs typically benefit from faster communication and lower latency compared to external APIs.\n\n**Consequences:**\n\n* **Limited accessibility:**  The functionality will only be available to authorized users and applications within the DoS ecosystem. \n* **Potentially increased complexity:** Integrating with existing authentication and authorization mechanisms might introduce development overhead. \n","GenTime":"2024-08-19 16:22:45","text":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n\n\n##Decision\nThe relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n"}
{"File Name":"aws_infrastructure\/0005-use-terraform-as-infrastructure-as-code.md","Context":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n","Decision":"Use Terraform\\n","tokens":21,"id":1464,"Prediction":"## Decision\n\n**Decision:** We will use Terraform as our Infrastructure as Code (IaC) tool to provision and manage resources within our AWS account. \n\n**Reason:**\n\n* **Tracking Resources:** Terraform's state management capabilities allow us to track all provisioned resources, providing a clear and consistent view of our infrastructure within AWS. This addresses the core need of understanding what's running in our AWS account.\n* **Code-based Infrastructure:** Defining infrastructure as code promotes transparency, reproducibility, and version control. It enables us to track changes, roll back to previous configurations, and easily replicate environments.\n* **Automation:**  Terraform automates the provisioning and modification of infrastructure, reducing manual errors and increasing efficiency when managing AWS resources.\n* **Widely Adopted:** Terraform is a widely adopted IaC tool with extensive documentation, a large community, and strong support for AWS.\n\n**Alternatives Considered:**\n\n* **AWS CloudFormation:** While a viable option within the AWS ecosystem, Terraform's broader cloud provider support offers more flexibility should we need to integrate with other platforms in the future.\n* **Pulumi:**  Pulumi's use of general-purpose programming languages is appealing, but Terraform's maturity, community, and established best practices make it a more suitable choice at this time.\n\n**Consequences:**\n\n* **Learning Curve:**  Adopting Terraform will require an initial investment in training and skill development for the team.\n* **State Management:**  Proper management of Terraform state files is crucial. We will need to establish secure storage and access controls.\n\n**Status:** Accepted\n\n**Next Steps:**\n1. **Tooling Setup:** Set up Terraform, configure AWS credentials, and establish version control for our infrastructure code.\n2. **Pilot Project:** Begin with a small, well-defined project to gain experience with Terraform and establish best practices within our team.\n3. **Documentation:** Develop internal documentation and guidelines for using Terraform effectively within our organization. \n","GenTime":"2024-08-19 16:22:54","text":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n\n##Decision\nUse Terraform\\n"}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"Prediction":"## Decision\n\nWe will be using **Java record classes** as the standard for defining data objects that can be created using the user interface. \n\n## Justification\n\n* **Simplicity and Clarity:** Java records provide a concise and clear syntax for defining immutable data objects. This aligns with our requirement for simple requirements in application creation. \n* **Lombok Compatibility:** Java records fulfill the same purpose as Lombok's `@Value` annotation, which is to reduce boilerplate code associated with data objects. Using records directly eliminates the need for Lombok in this context, ensuring compatibility while promoting a standard language feature.\n* **Immutability:** Immutability is generally a good practice for data objects, leading to more predictable and maintainable code. Java records enforce immutability by design. \n\n## Considered Options\n\n* **Lombok's `@Value` annotation:** While a valid option, it introduces an external dependency on Lombok. Using a standard Java feature like records is preferred for this core functionality.\n* **Traditional Java classes with getters and setters:** This approach is verbose and doesn't enforce immutability as effectively as records. \n\n## Consequences\n\n* **Adoption of a modern Java feature:** Developers working with the codebase will need to be familiar with Java records. \n* **Potential refactoring:** Existing code using Lombok's `@Value` or traditional data classes might need to be refactored to use records. \n\nThis decision promotes clean code, leverages Java language features, and avoids unnecessary external dependencies, ultimately contributing to a more maintainable and robust application. \n","GenTime":"2024-08-19 16:23:01","text":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n\n\n##Decision\n* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n"}
{"File Name":"james\/0005-distributed-task-termination-ackowledgement.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2121,"Prediction":"## Decision\n\nWe will implement a distributed pub-sub messaging system to handle task execution and termination signaling across the cluster. \n\n### Reasoning\n\n* **Scalability and Flexibility:** A pub-sub system allows us to efficiently distribute messages across a large number of nodes without direct coupling between senders and receivers. This supports horizontal scaling of our task manager. \n* **Loose Coupling:**  Nodes don't need to know about each other directly. They subscribe to relevant topics (e.g., task type, termination events) and react to published messages. This simplifies node management and enhances fault tolerance.\n* **Targeted Notifications:**  Specific topics for task types and termination events enable targeted message delivery. Only nodes subscribed to a relevant topic receive and process the message, optimizing resource usage.\n* **Asynchronous Communication:** Pub-sub promotes asynchronous communication, preventing blocking operations and improving system responsiveness.\n\n### Consequences\n\n* **Implementation Overhead:** Integrating and managing a distributed pub-sub system adds complexity to the project.\n* **Dependency on External System:** We might depend on an external message broker service, introducing a potential single point of failure if not properly managed. \n* **Message Ordering:**  While not strictly required for task execution and termination signals, ensuring message order (if needed) requires additional configuration and handling within the chosen pub-sub system.\n\n### Considered Alternatives\n* **Centralized Coordinator:** A central coordinator could manage task distribution and termination signaling. However, this approach introduces a single point of failure and scalability bottleneck.\n* **Point-to-Point Communication:** Direct communication between nodes for each task and event notification would be complex to manage and could lead to performance issues as the cluster grows.\n\n### Next Steps\n\n1. **Select a Pub-Sub System:** Evaluate and choose a suitable distributed pub-sub system (e.g., Kafka, RabbitMQ, Redis Pub\/Sub) based on project requirements, scalability needs, and existing infrastructure.\n2. **Design Message Structure:** Define the structure and content of messages for task execution requests and termination notifications.\n3. **Implement Publish and Subscribe Mechanisms:**  Implement logic for nodes to publish task execution requests and subscribe to relevant topics for receiving tasks and termination signals. \n4. **Handle Termination Notifications:**  Develop mechanisms for nodes to handle termination events and gracefully notify blocking clients.\n","GenTime":"2024-08-19 16:23:11","text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n\n##Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n"}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"Prediction":"## Decision\n\nWe will use **Clean Architecture** as our high-level component architecture pattern for microservices with non-trivial business logic within the `klokwrk-project` system. \n\n**Reasoning:**\n\n* **Alignment with DDD:** Clean Architecture strongly emphasizes separating business logic from infrastructure concerns, aligning perfectly with DDD's focus on the domain. This separation enables clear boundaries between Bounded Contexts, promoting the use of ubiquitous language within each.\n* **Hexagonal Architecture Compatibility:** Clean Architecture can be seen as a superset of Hexagonal Architecture, allowing for its principles like ports and adapters to be easily implemented. \n* **Tactical DDD Support:** Clean Architecture's inner layers naturally lend themselves to implementing tactical DDD patterns like aggregates, entities, value objects, and domain events.\n* **Asynchronous Communication:**  While not prescribing a specific approach, Clean Architecture allows for various asynchronous communication patterns (e.g., message queues, event buses) to be integrated seamlessly within its structure.\n* **Open-Source Frameworks:** Several mature open-source frameworks based on Clean Architecture are available (e.g., Spring Boot with appropriate modularization,  Lagom, Axon), providing a starting point for implementation.\n\n**Consequences:**\n\n* **Increased Complexity:** Clean Architecture introduces an initial learning curve and might seem overly complex for simple applications.\n* **Discipline Required:**  Maintaining the separation of concerns enforced by Clean Architecture requires discipline and consistent adherence to its principles.\n* **Potential Boilerplate:**  Depending on the chosen framework, some boilerplate code might be required to wire up the different layers and components.\n\n**Alternative Considered:**\n\n* **Layered Architecture:** While suitable for anemic CRUD applications, layered architecture often leads to tight coupling between layers, making it unsuitable for complex business logic and hindering the application of DDD principles.\n\n**Note:**  For simpler CRUD-based applications within the `klokwrk-project` system, we can employ a more traditional layered architecture approach. However, we need to carefully assess the complexity and potential for future evolution before making this decision on a per-application basis.\n","GenTime":"2024-08-19 16:23:20","text":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each record will be a separate Markdown file, stored in a dedicated \"docs\/adr\" directory in the project repository. \n","GenTime":"2024-08-19 16:23:22","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"hmpps-interventions-docs\/0005-for-release-prioritise-consistency-over-availability.md","Context":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n","Decision":"Due to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","tokens":201,"id":1799,"Prediction":"## Decision\n\nWe will **not implement a retry mechanism** for Delius integration failures before the launch on 7 June 2021. \n\n## Reasons\n\n* **Time constraints**:  The fixed deadline and imminent user onboarding leave insufficient time to develop, test, and deploy a robust retry mechanism.\n* **Administrative burden**: Introducing an admin task or script for retries would add significant overhead, especially considering the anticipated influx of new users. \n* **Unknown scope of the issue**: The uncertainty surrounding the reference data and potential failure points makes it difficult to design an effective and comprehensive retry solution at this stage.\n\n## Consequences\n\n* **Potential for integration failures**:  We acknowledge the risk of Delius integration failures without a retry mechanism. \n* **Manual intervention required**: In case of failures, manual intervention will be required to identify and address the root cause and re-trigger the integration process.\n\n## Action Items\n\n1. **Develop a monitoring and alerting system:** Implement robust monitoring for Delius integrations to promptly detect and alert on failures.\n2. **Prioritize manual recovery**:  Develop clear procedures for manual recovery in case of integration failures, ensuring minimal disruption to users and appointment booking.\n3. **Schedule retry mechanism development**:  Allocate dedicated time post-launch to develop, test, and implement a comprehensive retry mechanism for Delius integrations, addressing the identified risks and uncertainties. \n\nThis approach prioritizes a timely launch while acknowledging and mitigating the risks associated with potential integration failures. The focus will be on close monitoring, rapid response to failures, and a commitment to developing a robust retry mechanism as a high priority post-launch activity. \n","GenTime":"2024-08-19 16:23:29","text":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n\n##Decision\nDue to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n"}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"Prediction":"## Decision\n\nWe will update uniffi to **require that all components are `Send + Sync`**. This means:\n\n1. **Removing the `[Threadsafe]` attribute:** This attribute will no longer be necessary or supported in uniffi. \n2. **Enforcing `Send + Sync`:** The uniffi compiler will be updated to emit an error if a component is not `Send + Sync`. This ensures that all components are inherently thread-safe.\n\nThis decision prioritizes explicitness and predictability in thread-safety management. By requiring component authors to explicitly manage their own locking via `Send + Sync`, we eliminate the hidden mutex and its associated performance implications and potential for unexpected blocking. \n\nThis approach also aligns with Rust's philosophy of explicitness and control over concurrency. By leveraging Rust's type system and the `Send + Sync` traits, we ensure that thread safety is guaranteed at compile time. \n\nThis decision paves the way for future enhancements to uniffi, such as alternative method receivers and passing interface references over the FFI, which are not feasible with the current hidden mutex approach. \n","GenTime":"2024-08-19 16:23:34","text":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n\n\n##Decision\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n"}
{"File Name":"html-diff-service\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3250,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions. Each ADR will be a single Markdown file stored within a dedicated \"docs\/adr\" directory at the root of the project repository. We will follow the MADR (Markdown Architectural Decision Records) template as a standardized structure for our ADRs. \n","GenTime":"2024-08-19 16:23:36","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"saas-platform-frontend\/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md","Context":"## Context\\nWe need a workflow to build our appliation and components.\\n","Decision":"We use Storybook only for building new self contained components.\\n","tokens":16,"id":3722,"Prediction":"## Decision\n\nWe will use **[Workflow Management Tool]** to orchestrate the build process of our application and components. \n\n**Reasons:**\n\n* **[Reason 1 for choosing this specific tool]:**  e.g., Strong integration with our existing version control system (e.g., Git)\n* **[Reason 2 for choosing this specific tool]:** e.g., Provides a user-friendly interface for defining and managing complex workflows.\n* **[Reason 3 for choosing this specific tool]:** e.g., Offers scalability and parallelism for faster build times. \n\n**Alternatives Considered:**\n\n* **[Alternative Workflow Tool 1]:** e.g., Jenkins \n* **[Alternative Workflow Tool 2]:** e.g., GitLab CI\/CD\n\n**Why we didn't choose the alternatives:**\n\n* **[Alternative Workflow Tool 1]:** e.g.,  While powerful, it requires more manual configuration and maintenance.\n* **[Alternative Workflow Tool 2]:** e.g., Lacks some of the advanced features required for our complex build processes.\n\n**Consequences:**\n\n* **Positive:** \n    * Improved consistency and reliability of our builds.\n    * Reduced manual intervention and potential for errors.\n    * Enhanced visibility into the build process.\n* **Negative:**\n    * Initial learning curve for the chosen workflow management tool.\n    * Potential vendor lock-in.\n\n**Important Notes:**\n\n* We will use a combination of shell scripts and\/or configuration files within the chosen tool to define the specific build steps for each component.\n* We will implement a robust testing strategy within the build workflow to ensure code quality.\n\n\n**Replace the bracketed placeholders above with specific details relevant to your project and technology choices.** \n","GenTime":"2024-08-19 16:23:44","text":"## Context\\nWe need a workflow to build our appliation and components.\\n\n\n##Decision\nWe use Storybook only for building new self contained components.\\n"}
{"File Name":"branchout\/0002-language.md","Context":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n","Decision":"Shell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","tokens":36,"id":1472,"Prediction":"## Decision\n\nWe will use **Go** as the programming language for this project. \n\n## Justification\n\nWhile no language perfectly meets all the criteria of \"universal, simple, and easily testable,\" Go presents a strong balance:\n\n* **Universality:** Go is platform-agnostic, compiling to native code for various operating systems and architectures. This aligns with the requirement for a widely usable language.\n* **Simplicity:** Go's syntax is clean, concise, and easy to learn, especially for developers familiar with C-style languages. Its minimalist approach reduces cognitive overhead and promotes maintainability.\n* **Testability:**  Go has built-in testing frameworks and supports excellent tooling for automated testing. This makes it easier to write robust and reliable code. \n* **Few Dependencies:** Go's standard library is comprehensive, reducing the need for external dependencies. This aligns with the requirement for a self-contained codebase. \n\n## Considered Options\n\n* **Shell:** While simple, Shell scripts often lack structure for larger projects, hindering maintainability and testability. \n* **Java:** Java offers extensive libraries and robust tooling, but its verbosity and heavier runtime environment contradict the simplicity and minimal dependency requirements.\n* **JavaScript:** While versatile, JavaScript's dynamic typing and reliance on frameworks can impact code clarity and testability, especially in larger codebases. \n\n## Consequences\n\n* **Positive:** Go's simplicity, strong typing, and built-in testing support should lead to a more maintainable, reliable, and performant codebase.\n* **Negative:** The team might need to learn a new language (Go), which could require some initial ramp-up time. \n","GenTime":"2024-08-19 16:23:51","text":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n\n\n##Decision\nShell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n"}
{"File Name":"evidence-api\/0002-create-two-platform-apis-instead-of-a-single-api.md","Context":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n","Decision":"We decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","tokens":151,"id":1480,"Prediction":"## Decision\n\nWe will design the application as a RESTful API following the principles outlined in HackIT's API Playbook and Clean Architecture. \n\n**Justification:**\n\n* **API-first approach:**  Building an API allows for flexibility and scalability. It enables other systems and services within the council to easily interact with and leverage the evidence management capabilities.\n* **Standardization:** Adhering to the API Playbook ensures consistency with other HackIT projects, simplifying integration and maintenance.\n* **Maintainability and Testability:** Clean Architecture promotes separation of concerns, leading to more maintainable and testable code. This is crucial for a system handling sensitive information.\n* **Future-proofing:** An API-based design facilitates future expansion and integration with new features and services. \n\n**Consequences:**\n\n* **Increased initial development time:** Designing and implementing a robust and well-documented API requires more upfront effort compared to a monolithic approach.\n* **Need for dedicated API management:**  Consideration must be given to API versioning, security, documentation, and monitoring.\n\nThis decision aligns with the core requirements of secure document management, access control, and future extensibility. It prioritizes a maintainable and scalable solution that can serve as a central hub for evidence handling within the council's ecosystem. \n","GenTime":"2024-08-19 16:23:57","text":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n\n\n##Decision\nWe decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n"}
{"File Name":"gsp\/ADR025-ingress.md","Context":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n","Decision":"No. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","tokens":69,"id":3905,"Prediction":"## Decision\n\nWe will consolidate on Istio for ingress management and deprecate the use of nginx-ingress. \n\n## Reasoning\n\n* **Reduced Complexity:** Maintaining two ingress systems adds unnecessary complexity to our infrastructure. Consolidating on Istio simplifies our architecture and reduces the cognitive load on our team.\n* **Feature Richness:** Istio's Virtual Service offers a superset of features compared to basic Ingress resources. This includes advanced traffic routing, resilience patterns (e.g., retries, timeouts), and observability features.\n* **Alignment with Service Mesh Strategy:** Our adoption of Istio as a service mesh already provides us with powerful traffic management capabilities. Leveraging Istio for ingress aligns with our overall service mesh strategy.\n\n## Consequences\n\n* **Migration Effort:**  Migrating existing Ingress resources to Istio Virtual Services will require effort. We will need to plan and execute this migration carefully, considering potential downtime or disruptions.\n* **Learning Curve:** While Istio offers advanced features, there is a learning curve associated with its configuration and management. We need to ensure our team is adequately trained on Istio's ingress capabilities.\n\n## Action Items\n\n* **Migration Plan:** Develop a detailed migration plan outlining the steps, timeline, and resources required to move from nginx-ingress to Istio.\n* **Documentation:** Update our documentation to reflect the decision to use Istio for ingress and provide guidance on configuring Virtual Services.\n* **Training:**  Provide training or workshops for our team members on configuring and managing ingress with Istio.\n\n[Ingress]: https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress\/\n[ADR019]: .\/adr019.md \n[ADR005]: .\/adr005.md\n[Virtual Service]: https:\/\/istio.io\/latest\/docs\/reference\/config\/networking\/virtual-service\/\n","GenTime":"2024-08-19 16:24:06","text":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n\n\n##Decision\nNo. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n"}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"Prediction":"## Decision\n\nWe will adopt a consistent serialization approach using JSON (RFC 8259) for all response types moving forward. This decision is driven by the following factors:\n\n* **Consistency:** JSON provides a standardized and well-defined format for representing complex data structures, ensuring uniformity across all response types. \n* **Extensibility:** JSON allows for the seamless addition of new properties and nested objects without requiring changes to the existing parsing logic. This flexibility accommodates future growth and evolving response structures.\n* **Widely Supported:** JSON is a ubiquitous data format with extensive support in various programming languages and tools, simplifying development and integration efforts.\n\n**Specific Actions:**\n\n1. **Deprecate Pipe-Delimited Serialization:**  Gradually phase out the use of pipe-delimited serialization in existing responses, migrating them to the JSON format.\n2. **Implement JSON Serialization for New Responses:** Utilize JSON for serializing all new response types, including the `code` property for cascade and option types.\n3. **Documentation and Communication:**  Update documentation and inform stakeholders (developers, clients) about this change in serialization strategy.\n\n**Justification:**\n\nThis decision promotes maintainability, scalability, and interoperability by leveraging a standardized and widely adopted data format. While migrating existing responses might require some effort, the long-term benefits of consistency and extensibility outweigh the initial cost. \n","GenTime":"2024-08-19 16:24:12","text":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n\n\n##Decision\nAs response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n"}
{"File Name":"toc-poc\/1577574698834_architecture_choice_for_template_generation.md","Context":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n","Decision":"Use ports and adapters architecture style.\\n","tokens":94,"id":1883,"Prediction":"## Decision\n\nWe will implement a hierarchical data structure to represent the table of contents. This structure will support arbitrary depth and allow nesting of items. \n\nTo facilitate rendering and potential future dynamic behavior, we will implement this data structure as a **composite pattern**. This pattern allows treating individual objects and compositions of objects uniformly. Each node in the hierarchy will be an object with properties like:\n\n* **Title:** The text displayed in the table of contents.\n* **Slug:** A unique identifier for linking to the content section.\n* **Children:**  A list of child nodes, which can be empty for leaf nodes.\n* **(Optional) Behavior:** Specific methods for handling user interactions or dynamic updates.\n\nThis approach offers several advantages:\n\n* **Flexibility:**  Accommodates the multi-level requirement and potential future complexities.\n* **Maintainability:** Encapsulates rendering logic and behavior within the data structure.\n* **Extensibility:**  Easily add new features or modify existing behavior without affecting the entire codebase.\n\nWhile this approach adds complexity compared to a simple data structure, it provides the flexibility and maintainability necessary for the anticipated future development of the application.\n","GenTime":"2024-08-19 16:24:17","text":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n\n\n##Decision\nUse ports and adapters architecture style.\\n"}
{"File Name":"digitalrig-metal-aws\/0007-build-and-deploy-pipelines-separate.md","Context":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":1739,"Prediction":"## Decision\n\nWe will separate the build and deploy stages into distinct pipelines. This will allow configuration changes to be deployed rapidly, without requiring a full rebuild of the application. \n\n**Rationale:**\n\n- **Faster deployment cycles:** Decoupling build and deployment allows for significantly faster deployments of configuration changes, reducing lead time for bug fixes and new feature rollouts. \n- **Reduced resource consumption:** Eliminating unnecessary rebuilds conserves resources and reduces the overall cost of deployment. \n- **Improved developer experience:** Developers can quickly deploy configuration changes without waiting for lengthy build processes, leading to increased productivity and faster feedback loops. \n\n**Consequences:**\n\n- **Increased pipeline complexity:** Maintaining separate build and deploy pipelines introduces some additional complexity in pipeline management. \n- **Potential for configuration drift:** Separate deployments require careful management to ensure consistency between the built artifacts and deployed configuration. \n\n**Mitigation:**\n\n- We will utilize infrastructure-as-code tools and practices to manage the complexity of the separated pipelines. \n- Robust automated testing and monitoring will be implemented to detect and mitigate configuration drift. \n","GenTime":"2024-08-19 16:24:22","text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n"}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions. Each ADR will capture the context, decision, consequences, and status of a specific architectural choice. \n","GenTime":"2024-08-19 16:24:24","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"Prediction":"## Decision\n\nWe will implement a plugin system based on **well-defined extension points**. Extension developers will be able to:\n\n1. **Register custom components and views** to these predetermined extension points using a dedicated API (e.g., manifest file, programmatic registration). \n2. **Leverage a data-driven approach**, potentially using configuration objects, to control the rendering and behavior of their extensions within each extension point. \n\nThis approach offers a balance between flexibility and maintainability by providing:\n\n* **Structure and Guidance:** Clear extension points provide guidance for developers and ensure consistency within the administration interface.\n* **Flexibility:** Data-driven configuration allows for customization within the constraints of the defined extension points.\n* **Maintainability:**  A well-defined API reduces the risk of regressions and simplifies future updates to the core application. \n\nThis ADR will further detail the specifics of the chosen API, the structure of the extension points, and provide concrete examples of their implementation and usage. \n","GenTime":"2024-08-19 16:24:28","text":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n\n\n##Decision\n### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"Prediction":"## Decision\n\nMessages from Order Management, Rating Manager, and Recommendation Manager **will be asynchronous**. \n\n## Reasons\n\n* **Improved Performance and Responsiveness:** Asynchronous communication allows these services to operate independently and not block each other. This leads to faster response times for users, even during peak loads, as one service's delay won't cascade and affect others.\n* **Loose Coupling:** Decoupling these services via asynchronous messaging reduces dependencies between them. This makes it easier to modify or scale individual services independently without impacting the entire system. \n* **Fault Tolerance and Resilience:** If one service experiences downtime or errors, asynchronous communication prevents cascading failures. Other services can continue to operate and handle messages independently.\n* **Flexibility for Future Growth:** As the system evolves and new features are added, asynchronous communication provides the flexibility to integrate new services seamlessly without significant architectural changes.\n\n## Consequences\n\n* **Increased Complexity:** Implementing asynchronous messaging requires a messaging infrastructure and handling complexities like message ordering, retries, and idempotency.\n* **Data Consistency Considerations:** Asynchronous operations might lead to eventual consistency challenges. Strategies like event sourcing or compensating transactions might be needed to ensure data consistency across services.\n* **Debugging and Monitoring:** Debugging and monitoring asynchronous systems can be more challenging compared to synchronous ones. Proper logging and tracing mechanisms are crucial for identifying and resolving issues.\n\n## Status\n\nAccepted\n\n## Open Issues\n\n* Selection of specific messaging technology (e.g., message broker, queuing system).\n* Implementation details for handling message ordering, retries, and idempotency.\n* Defining monitoring and alerting strategies for the messaging infrastructure and asynchronous workflows. \n","GenTime":"2024-08-19 16:24:36","text":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n\n\n##Decision\nYes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"Prediction":"## Decision\n\nMigrate away from using Elasticsearch as a primary datastore for documents that do not leverage its full-text search capabilities. \n\n## Reasons\n\n* **Index Management Overhead:** The increasing number of document types not using Elasticsearch's core functionality, coupled with the \"one type per index\" limitation in Elasticsearch 6+, will lead to a significant increase in the number of indices. This will result in increased management overhead and potential performance degradation. \n* **Cost Inefficiency:** Utilizing Elasticsearch for data that doesn't benefit from its search and analytics features incurs unnecessary storage and operational costs. \n* **Simplified Architecture:** Migrating these documents to a more suitable datastore will simplify the overall architecture and reduce dependencies on Elasticsearch for non-core functionalities.\n\n## Alternatives Considered\n\n* **Sticking with Elasticsearch:**  While possible, this option would lead to the aforementioned drawbacks and is not a sustainable long-term solution. \n* **Upgrading to Elasticsearch 7+:**  Later versions of Elasticsearch remove the \"one type per index\" limitation. However, this does not address the core issue of using Elasticsearch for data that doesn't leverage its strengths and could still lead to unnecessary complexity and cost. \n\n## Consequences\n\n* **Migration Effort:** Migrating existing documents and adapting the application to utilize a new datastore will require development effort.\n* **Choosing a New Datastore:**  Careful consideration must be given to selecting a suitable replacement datastore that meets the specific requirements of the documents being migrated.\n\n## Action Items\n\n1. **Identify Document Types:** Clearly identify all document types that do not utilize Elasticsearch's full-text search capabilities.\n2. **Analyze Requirements:** Analyze the storage, access patterns, and other requirements for the identified document types.\n3. **Select Datastore:**  Evaluate and select a suitable replacement datastore based on the identified requirements (e.g., relational database, NoSQL database, object storage).\n4. **Plan Migration:**  Develop a comprehensive migration plan, including data migration strategy, application code changes, and testing procedures.\n5. **Execute Migration:**  Implement the migration plan in a phased and controlled manner, ensuring minimal disruption to the application. \n","GenTime":"2024-08-19 16:24:45","text":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n\n\n##Decision\nWe will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n"}
{"File Name":"james\/0013-precompute-jmap-preview.md","Context":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2134,"Prediction":"## Decision\n\nInstead of parsing the full message body on each preview request, we will compute the preview once upon message creation\/update and store it as a separate property in the JMAP message representation. \n\n## Consequences\n\n**Pros:**\n\n* **Significantly reduced response time** for message listing requests as the preview is readily available.\n* **Reduced CPU load** by avoiding redundant parsing of message bodies.\n\n**Cons:**\n\n* **Increased storage space required** to store the preview for each message.\n* **Slightly increased complexity** in message creation\/update logic to generate and store the preview.\n\n## Status\n\nProposed\n\n## More information\n\nWe will need to assess the storage impact and weigh it against the performance benefits. We can consider using compression for the stored preview if the storage overhead becomes significant. \n\nAdditionally, we should explore different algorithms for generating the preview to ensure it provides a meaningful representation of the message content.\n","GenTime":"2024-08-19 16:24:50","text":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n\n\n##Decision\nWe should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n"}
{"File Name":"gsp\/ADR004-tenant-isolation.md","Context":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n","Decision":"We will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","tokens":153,"id":3889,"Prediction":"## Decision\n\n**Deploy a dedicated Kubernetes cluster per Service Team per environment to their respective AWS accounts.**\n\n**Rationale:**\n\n* **Isolation:** This approach provides the strongest isolation between Service Teams and environments, addressing the unique network isolation requirements and mitigating the risk of resource contention.\n* **Security:** By leveraging separate AWS accounts, we enhance security by limiting blast radius and enforcing the principle of least privilege.\n* **Billing Clarity:** Deploying clusters to Service Team accounts provides clear separation of billing and simplifies cost allocation.\n* **Smooth Transition:**  Deploying to existing Service Team VPCs minimizes disruption during migration as they can maintain their existing network configurations.\n\n**Trade-offs:**\n\n* **Increased Management Overhead:** Managing multiple clusters introduces operational complexity compared to a single, shared cluster.\n* **Potential Resource Underutilization:**  Separate clusters might lead to underutilization of resources in some environments. \n\n**Mitigations:**\n\n* **Infrastructure as Code (IaC):** Implement IaC to automate cluster provisioning and management, reducing operational overhead.\n* **Centralized Monitoring and Logging:** Establish centralized monitoring and logging for all clusters to provide consolidated visibility and simplify troubleshooting.\n* **Resource Optimization Analysis:** Conduct regular resource utilization reviews to identify and address potential underutilization.\n\nThis approach prioritizes isolation, security, and billing clarity, aligning with the key concerns outlined in the context. While acknowledging the trade-offs, we believe the benefits outweigh the costs and the proposed mitigations will address potential challenges. \n","GenTime":"2024-08-19 16:24:56","text":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n\n\n##Decision\nWe will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n"}
{"File Name":"mediawiki-extensions-Kartographer\/webgl_renderer.md","Context":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n","Decision":"Accepted (02-16-2021)\\n","tokens":198,"id":3286,"Prediction":"## Decision\n\nWe will adopt the Mapbox GL Style Specification as the standard for vector tile styling and client-side rendering in our maps infrastructure. \n\nThis decision is based on the following factors:\n\n* **Reduced Maintenance:** Migrating to Tegola for vector tile generation and leveraging client-side rendering with Mapbox GL Style Specification allows us to replace Mapnik, minimizing maintenance overhead. \n* **Flexibility and Interoperability:** The Mapbox GL Style Specification is supported by various client-side mapping frameworks like MapLibre GL, Leaflet, and OpenLayers. This flexibility allows us to experiment with different rendering engines and choose the most suitable option for our needs.\n* **Future-Proofing:** Choosing a widely adopted and actively developed standard like the Mapbox GL Style Specification ensures our maps infrastructure remains adaptable to future advancements in mapping technology.\n\nThis decision lays the foundation for a more maintainable, flexible, and future-proof maps infrastructure. \n","GenTime":"2024-08-19 16:25:01","text":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n\n\n##Decision\nAccepted (02-16-2021)\\n"}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"Prediction":"## Decision\n\nWe will adopt a modular approach to structuring our Terraform code, adhering to the following principles:\n\n**Modularity:**\n* **Single Responsibility:** Each Terraform module will be responsible for defining and managing a single, well-defined component of our infrastructure. For example, a module could manage a VPC, a database cluster, or a specific application deployment. \n* **Self-Contained:** Each module will strive to be self-contained, containing all the necessary code (including resources, variables, outputs) to define and deploy its component.\n* **Reusability:** Modules will be designed to be reusable across different environments and projects whenever possible.\n\n**Structure and Naming:**\n* **Directory Structure:**  We will adopt a consistent directory structure that reflects the logical grouping of our infrastructure components. This structure will make it easy to locate and manage related modules. \n* **Descriptive Naming:** Modules, resources, variables, and outputs will have clear and descriptive names that reflect their purpose and function.\n\n**Documentation:**\n* **README:**  Each module will include a README file providing a clear and concise overview of:\n    * The purpose of the module.\n    * The infrastructure components it manages.\n    * Required input variables and their descriptions.\n    * Outputs provided by the module. \n* **In-Code Comments:**  Terraform code within modules will be well-commented to explain the reasoning behind specific configurations and decisions.\n\n**Tooling:**\n* **Terraform Modules:** We will leverage Terraform's built-in module functionality to encapsulate and reuse our code.\n* **Linting and Formatting:** We will enforce consistent code style and formatting using tools like `terraform fmt` and `tflint`. This will improve code readability and maintainability.\n\nThis approach will result in a more organized, understandable, and maintainable Terraform codebase, making it easier for team members to understand, modify, and reuse infrastructure code. \n","GenTime":"2024-08-19 16:25:09","text":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n\n\n##Decision\nCreate style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n"}
{"File Name":"clone_difitalcitizenship\/0007-choice-of-azure-region.md","Context":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n","Decision":"We decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","tokens":29,"id":1186,"Prediction":"## Decision\n\nTo ensure data sovereignty within the European Union, we will follow these principles:\n\n1. **Data Residency:** All data at rest will be stored exclusively within data centers physically located within the EU. This includes primary data storage, backups, and any cached data. We will leverage cloud providers offering data residency guarantees within the EU region.\n2. **Data Processing Location:** All data processing, including data transformation, analysis, and computations, will be performed within the EU. This will be achieved by selecting cloud services and resources operating within the EU region.\n3. **Data Transfer Restrictions:** \n    * Data egress from the EU will be strictly prohibited. This includes any data transfers initiated by users, applications, or background processes. \n    * We will implement technical controls and policies to enforce this restriction at the infrastructure and application levels.\n    * Third-party services and integrations requiring data transfer outside the EU will be carefully evaluated and, if necessary, replaced with EU-based alternatives.\n4. **Data Encryption:** All data will be encrypted at rest and in transit using strong encryption algorithms. Encryption keys will be managed within the EU to maintain full control over data access.\n5. **Compliance and Auditing:**  \n    * We will comply with relevant EU data protection regulations, including GDPR, and maintain documentation of our compliance efforts.\n    * We will implement robust logging and auditing mechanisms to track data access, usage, and transfers. Regular audits will be conducted to ensure compliance with our data sovereignty policies.\n\nThis decision may lead to increased costs or limited availability of certain services compared to leveraging global cloud infrastructure. However, ensuring data sovereignty within the EU is paramount for legal compliance and maintaining user trust. \n","GenTime":"2024-08-19 16:25:17","text":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n\n\n##Decision\nWe decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n"}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"Prediction":"## Decision\n\nWe will **migrate away from Algolia** and implement search functionality directly within our PostgreSQL database, leveraging its full-text search capabilities and PostGIS for geographical querying. \n\nThis decision prioritizes simplicity, maintainability, and cost-effectiveness over the advanced features offered by a dedicated search service like Algolia. \n","GenTime":"2024-08-19 16:25:19","text":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n\n\n##Decision\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n"}
{"File Name":"openfido-app-service\/0003-deployment.md","Context":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":2471,"Prediction":"## Decision\n\n**Decision:** We will use a single AWS account for both development\/staging and production environments, leveraging ECS for container orchestration. \n\n**Justification:**  While separate accounts offer better isolation and potentially finer-grained security, a single account for both environments provides the following advantages in our context:\n\n* **Simplicity & Speed:**  Easier to manage infrastructure and deployments with a single set of resources and permissions.\n* **Cost-Effectiveness:** Reduced overhead in managing multiple accounts, potentially lowering overall AWS costs, especially in the early stages of development. \n* **Shared Resources:** Allows development\/staging to leverage production-like resources (e.g., databases), leading to more realistic testing. \n\n**Caveats:**\n\n* **Security Risks:** Requires robust security measures within the account to isolate environments and protect production data.\n* **Resource Contention:** Development\/staging activities might impact production performance if resources are not appropriately scaled and managed.\n* **Operational Complexity:**  Clear separation of environments through naming conventions, tagging, and potentially dedicated VPCs is crucial to avoid confusion and potential errors.\n\n**Mitigation Strategies:**\n\n* **Implement strict IAM roles and policies** to limit access to production resources and data.\n* **Use different VPCs or subnets** for development\/staging and production environments.\n* **Implement infrastructure-as-code (IaC)** to manage environments consistently and securely. \n* **Use separate ECS clusters** for development\/staging and production to isolate workloads.\n* **Monitor resource utilization closely** and implement auto-scaling to prevent resource contention. \n* **Establish clear naming conventions and tagging policies** to easily distinguish resources across environments.\n\n**Reassessment:** We will re-evaluate this decision as the application and its user base grow. Factors like increased security concerns, significant performance impacts, or exceeding cost optimization thresholds will trigger a reevaluation towards a multi-account strategy. \n","GenTime":"2024-08-19 16:25:27","text":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n\n\n##Decision\nUse CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n"}
{"File Name":"content-data-api\/adr-009-track-metrics-by-basepath.md","Context":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n","Decision":"Track metrics at the base_path level\\n","tokens":46,"id":1866,"Prediction":"## Decision\n\nWe will implement tracking of performance and quality metrics at the `base_path` level for Guides and Travel Advice, in addition to the existing `content_id` level tracking. This will provide users with a more granular view of content performance within these formats, enabling more targeted analysis and optimization. \n","GenTime":"2024-08-19 16:25:29","text":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n\n\n##Decision\nTrack metrics at the base_path level\\n"}
{"File Name":"green_log\/0006-use-lock-free-io.md","Context":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n","Decision":"_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","tokens":37,"id":1927,"Prediction":"## Decision\n\nWe will implement a logging mechanism that utilizes a dedicated logging thread and a thread-safe queue. Application threads will push log entries into the queue, while the dedicated logging thread will consume and write those entries sequentially to the chosen output (file or STDOUT). \n\nThis approach addresses both concerns:\n\n* **Interleaving:** The single logging thread ensures that log entries are written sequentially, preventing interleaving from different threads.\n* **Performance:** Using a queue allows application threads to offload the logging task quickly, minimizing performance impact. The logging thread can then handle writing to disk or console asynchronously.\n\n## Consequences\n\n* **Implementation Complexity:** This solution introduces additional complexity compared to a simple synchronous logging approach.\n* **Queue Management:** Consideration is needed for queue size limitations and potential blocking scenarios if the queue becomes full. We could mitigate this by using a bounded queue with a configurable overflow strategy (e.g., discard oldest entries, block until space is available).\n* **Thread Management:**  We introduce the overhead of thread creation and management for the logging thread. \n\nOverall, the benefits of non-interleaved logs and minimal performance impact outweigh the added complexity, especially in scenarios where log integrity and readability are crucial. \n","GenTime":"2024-08-19 16:25:34","text":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n\n\n##Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"Prediction":"## Decision\n\nTo improve the performance and manageability of the archiving process, we will refactor it into a multi-threaded pipeline. Each stage of the archiving process will be implemented as a separate component of the pipeline, allowing for parallel execution where possible. This approach will provide the following benefits:\n\n- **Reduced Archiving Time:** Parallel execution of independent stages will significantly reduce the overall time required for archiving.\n- **Improved Resource Utilization:** By leveraging multi-threading, we can efficiently utilize available system resources.\n- **Enhanced Scalability:** The pipeline architecture allows for easy scaling by adding or removing stages as needed.\n- **Simplified Maintenance:** Modular design with well-defined components simplifies maintenance and future modifications. \n\nWe will use a queueing mechanism to manage the flow of data between different stages of the pipeline. This ensures smooth data transfer and prevents bottlenecks.\n","GenTime":"2024-08-19 16:25:38","text":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n\n\n##Decision\nFor the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n"}
{"File Name":"Head-Start-TTADP\/0007-use-plantuml-for-diagrams.md","Context":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n","Decision":"We will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","tokens":48,"id":1172,"Prediction":"## Decision\n\nWe will use [Service Name] as a single source of truth for architectural diagrams. All diagrams, including data models and boundary diagrams, will be created and maintained within this service. \n","GenTime":"2024-08-19 16:25:40","text":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n\n\n##Decision\nWe will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences. Each ADR will be a self-contained Markdown file stored in a dedicated \"docs\/adr\" repository folder. \n","GenTime":"2024-08-19 16:25:42","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"katas\/002-no-eslint.md","Context":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"Prediction":"## Decision\n\nWe will **not** be using a linter for this project.\n\n## Reasons\n\nWhile linters are generally a good practice for enforcing code style and consistency, the nature of this project, focusing on code formatting katas as described in ADR1, makes a linter counterproductive. \n\nThe katas are intended to be exercises in code formatting. Introducing a linter would directly solve the problem the katas are designed to address, thereby defeating their purpose. \n","GenTime":"2024-08-19 16:25:44","text":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n\n\n##Decision\nBut I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n"}
{"File Name":"ichatsdk-evolution\/0001-use-c-language.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4062,"Prediction":"## Context and Problem Statement\n\nOur application needs to securely store user passwords. How do we choose a password hashing algorithm that balances security with performance?\n\n## Decision Drivers \n\n* **Security:** The chosen algorithm must be resistant to common attacks such as brute-forcing and rainbow tables.\n* **Performance:** Hashing should be reasonably fast to avoid impacting user experience during login.\n* **Industry Standards:**  The solution should align with industry best practices and recommendations for password storage. \n","GenTime":"2024-08-19 16:25:47","text":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n"}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions throughout the project lifecycle. Each ADR will capture the context, decision, and consequences of a specific architectural choice.  We will utilize a lightweight Markdown format for each ADR and store them in a dedicated \"docs\/adr\" directory within the project repository. \n","GenTime":"2024-08-19 16:25:49","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"plant-shop\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3349,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions for this project. Each ADR will be a separate Markdown file stored in a dedicated \"docs\/adr\" directory at the root level of our repository. We will use a descriptive file naming convention such as `<ADR-Number>-<Short-Decision-Description>.md`.  For example, this decision would be recorded as `001-use-adrs-for-architecture-decisions.md`. \n","GenTime":"2024-08-19 16:25:52","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"Prediction":"## Decision\n\nWe will implement the IBC handlers as traits defined within the `ibc` crate. This approach offers several advantages:\n\n* **Modularity and Reusability:** Defining handlers as traits allows for greater modularity and reusability. Different modules can implement the same handler trait, tailoring the logic to their specific needs while adhering to a common interface.\n* **Testability:** Trait-based implementation facilitates unit testing. We can easily mock dependencies and test individual handler implementations in isolation.\n* **Extensibility:**  New handler types can be added without modifying existing code, promoting extensibility and future-proofing the IBC integration. \n\nThis approach aligns with Rust's idiomatic use of traits for abstraction and polymorphism. \n","GenTime":"2024-08-19 16:25:55","text":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n\n\n##Decision\nConcepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n"}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"Prediction":"## Decision\n\nWe will adopt a single, consistent naming convention for all files and directories within the monorepo. This convention will be strictly enforced for all new code and we will gradually refactor existing code to adhere to the new standard. \n\nWe will prioritize clarity and consistency with common tooling over strict adherence to any particular style guide.  Specific details of the naming convention (e.g., kebab-case, snake_case) will be determined in a follow-up document or ADR, with input from key stakeholders and a review of established best practices.\n","GenTime":"2024-08-19 16:25:58","text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n"}
{"File Name":"adr-demo\/0001-record-architecture-decisions.md","Context":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n","Decision":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":146,"id":2517,"Prediction":"## Decision\n\nWe will adopt the use of Architectural Decision Records (ADRs) to document significant architectural decisions made throughout the project's lifecycle. \n\nThis decision is driven by the need to:\n\n* **Maintain a clear history of why architectural decisions were made:** This ensures everyone understands the reasoning behind the current architecture, even as the team and codebase evolve.\n* **Reduce the risk of regressions and breaking changes:** ADRs provide context for future decisions, helping developers understand the potential impact of modifications.\n* **Facilitate informed decision-making:** ADRs provide a centralized repository of knowledge that can be consulted when making future architectural choices. \n\nWe will utilize a lightweight, plain text format (e.g., Markdown) for ADRs and store them within the project's version control system alongside the codebase. This ensures:\n\n* **Easy access and discoverability for all team members.**\n* **Version control and history tracking for each decision and its modifications.**\n* **Seamless integration with existing development workflows.** \n\nWe will establish a clear process for creating, reviewing, and updating ADRs, which will be integrated into our existing development workflow.\n","GenTime":"2024-08-19 16:26:03","text":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n\n\n##Decision\nWe will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n"}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"Prediction":"## Decision\n\nWe will implement JMAP Push notifications using WebSockets as the underlying transport mechanism. \n\n## Reasons\n\n* **Efficiency:** WebSockets offer a persistent, bidirectional communication channel between the client and server, reducing latency compared to polling mechanisms. This aligns with JMAP's goal of efficient updates.\n* **Industry Standard:** WebSockets are a widely adopted standard for real-time communication on the web, with broad browser and server-side language support. \n* **JMAP Compatibility:** The JMAP specification recommends WebSockets as a suitable transport for push notifications.\n* **Flexibility:** WebSockets allow us to push various types of data beyond just state changes, potentially enabling future enhancements like real-time collaboration features.\n\n## Alternatives Considered\n\n* **Long Polling:** Clients periodically poll the server for updates. This is less efficient than WebSockets and can lead to increased server load.\n* **Server-Sent Events (SSE):** SSE offers server-to-client push functionality but lacks the bi-directionality of WebSockets. This limits future expansion possibilities.\n\n## Consequences\n\n* **Increased Complexity:** Implementing and maintaining a WebSocket-based solution adds complexity compared to simpler alternatives.\n* **Scalability Considerations:**  We need to design the system to handle a large number of concurrent WebSocket connections efficiently. This might involve load balancing and horizontal scaling strategies.\n* **Potential for Errors:**  Network interruptions and other errors are inherent in real-time communication.  We need to implement robust error handling and reconnect logic. \n\n## Status\n\nAccepted\n\n## Notes\n\nWe will investigate and select a suitable WebSocket library\/framework to facilitate implementation and ensure scalability. Additionally, we will define clear error handling procedures and reconnect strategies to ensure a resilient and reliable push notification system. \n","GenTime":"2024-08-19 16:26:11","text":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n\n\n##Decision\nWe will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n"}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"Prediction":"## Decision\n\nWe will use Okta as our Identity and Access Management (IAM) solution for the following reasons:\n\n* **Cost-effective:** Okta's free tier generously accommodates our projected user base, offering a cost-effective solution for our current needs.\n* **Simplified Integration:**  Okta's seamless integration with Spring Boot and Spring Security minimizes development overhead and accelerates implementation.\n* **Standardized Authentication:** Leveraging Okta's Microsoft social OpenConnect ID provider ensures secure and standardized authentication through our existing Microsoft ecosystem.\n* **Enhanced Security:** Transitioning from custom client code to Okta's robust React client SDK bolsters security and reduces maintenance efforts.\n* **Scalability and Flexibility:** Okta's administrative dashboards streamline user management and simplify the potential addition of roles, groups, and future social login options.\n\nThe successful spike demonstrated at  https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x  further validates this decision.\n","GenTime":"2024-08-19 16:26:16","text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n"}
{"File Name":"rails-template\/0009-use-scripts-to-rule-them-all.md","Context":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n","Decision":"By default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","tokens":67,"id":4178,"Prediction":"## Decision\n\nWe will be adopting the \"Scripts to Rule Them All\" pattern as outlined in dxw's RFC-023 [1]. This decision is based on the RFC approval and aims to improve the maintainability, transparency, and consistency of our scripting infrastructure. \n\nWe will be documenting this decision and its implications within this repository to serve as a reference for future development and maintenance. \n\n[1] https:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md \n","GenTime":"2024-08-19 16:26:19","text":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n\n\n##Decision\nBy default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n"}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"Prediction":"## Decision\n\nTo prevent sensitive data from appearing in logs, we will implement a dedicated data type for handling sensitive information. This data type will:\n\n1. **Wrap sensitive data:** Instead of passing sensitive data as strings, we will encapsulate them within this dedicated type.\n2. **Override string representation:** The data type will override any methods that generate string representations (e.g., `__str__` in Python, `toString()` in Java) to return a masked or generic value like \"<redacted>\". This prevents accidental logging of the sensitive data.\n3. **Implement access control:**  The data type could optionally implement access control mechanisms to restrict access to the raw sensitive data, requiring explicit actions to retrieve it when necessary.\n\nThis approach offers several advantages:\n\n* **Centralized control:**  Provides a single point for managing and updating how sensitive data is handled across the codebase.\n* **Reduced risk of accidental exposure:** Minimizes the risk of inadvertently logging sensitive information by making it the default behavior to redact.\n* **Improved code clarity:** Clearly distinguishes sensitive data from regular strings, improving code readability and maintainability.\n\n**Examples:**\n\n* We could create a `Secret` class that stores the sensitive data and overrides the `__str__` method to return \"<redacted>\". \n* Libraries like `obfuscate` (Python) can be used for masking specific parts of log messages.\n\nThis decision favors security and maintainability over potential minor performance overhead introduced by the new data type. \n","GenTime":"2024-08-19 16:26:25","text":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n\n\n##Decision\nTo minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n"}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"Prediction":"## Decision\n\n**Migrate from the monolithic PostgreSQL database to a distributed system leveraging Event Sourcing for state management and a separate read-optimized data store for analytical queries.**\n\nThis decision addresses the identified issues as follows:\n\n* **Improved Analytics:** Decoupling storage and querying by introducing a separate read-optimized data store dedicated for analytical queries will eliminate performance bottlenecks on the main CI\/CD cluster caused by heavy analytical workloads.\n* **Enhanced Scalability:** Event Sourcing inherently supports distributed systems, allowing us to scale horizontally and overcome the limitations of central locking in PostgreSQL.\n* **Simplified Change Tracking:** Event Sourcing provides an immutable log of all events, making it significantly easier to track changes over time for auditing and rollback purposes. Each event represents a specific action, providing a clear history of modifications.\n* **Improved Debugging and Traceability:** The append-only nature of Event Sourcing offers an inherent audit log, allowing for easier debugging and tracing of failures by reconstructing the state at any point in time.\n* **Schema Flexibility:** Event Sourcing offers greater schema flexibility compared to traditional tabular structures. New event types can be introduced without altering existing ones, simplifying schema evolution and reducing migration complexities.\n* **Simplified Distributed State Management:** Event Sourcing naturally lends itself to distributed systems, as events can be propagated and processed asynchronously across multiple nodes. This simplifies the management of distributed state compared to traditional relational databases.\n\nThis approach will improve performance, scalability, auditability, and flexibility of the CI\/CD system while simplifying schema management and distributed state challenges. \n","GenTime":"2024-08-19 16:26:32","text":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n\n\n##Decision\nBased on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n"}
{"File Name":"agentframework\/0008-minify-build.md","Context":"## Context\\nAgentFramework is also use for Browser\\n","Decision":"We need minify build because package.json don't have a property called `minMain`\\n","tokens":11,"id":2359,"Prediction":"## Decision\n\nWe will design and implement a browser-compatible version of AgentFramework. This will involve addressing the specific constraints and security considerations of the browser environment, such as:\n\n* **Sandbox limitations:**  Browser environments operate within sandboxes, limiting access to the underlying operating system and resources. AgentFramework functionalities that rely on direct system access will need to be adapted or alternative solutions explored.\n* **JavaScript execution:** AgentFramework will likely be implemented in JavaScript to run within the browser. This may require adjustments to existing code or the development of a JavaScript-specific implementation.\n* **Security concerns:** Security is paramount in a browser environment. AgentFramework's browser implementation must prioritize secure communication channels, data encryption, and robust authentication mechanisms to protect against potential vulnerabilities. \n\nThe specific approach to addressing these challenges will depend on the exact functionalities of AgentFramework and the intended use case within the browser environment.  We will investigate existing browser-based agent frameworks and libraries for potential inspiration and integration opportunities. \n","GenTime":"2024-08-19 16:26:37","text":"## Context\\nAgentFramework is also use for Browser\\n\n\n##Decision\nWe need minify build because package.json don't have a property called `minMain`\\n"}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"Prediction":"## Decision\n\nWe will adopt `adr-tools` (https:\/\/github.com\/npryce\/adr-tools) for managing our Architectural Decision Records (ADRs). This lightweight command-line tool provides a standardized structure and simplifies the creation, modification, and maintenance of ADRs, reducing friction and promoting consistent documentation practices. Our previous positive experience with `adr-tools` reinforces its suitability for this project. \n","GenTime":"2024-08-19 16:26:39","text":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n\n\n##Decision\nInstall `adr-tools` locally and use it to help quickly generate consistent ADRs\\n"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"Prediction":"## Decision\n\neLife will adopt the Node.js Long Term Support (LTS) schedule for all projects. This means:\n\n* **We will officially support the current Active LTS version of Node.js.** This ensures we benefit from ongoing security and critical bug fixes.\n* **We will begin planning and testing for upgrades to new LTS versions as soon as they are announced.** This proactive approach mitigates risk and allows for smoother transitions. \n* **Projects are encouraged to use the latest LTS version whenever possible.** This ensures access to new features and performance improvements. However, individual project needs may justify using an older LTS version, as long as it remains officially supported by the Node.js project.\n* **We will support a single version of npm per Node.js version, specifically the one bundled with the respective Node.js release.** This simplifies dependency management and reduces compatibility issues. \n\n**Justification:**\n\n* **Security and Stability:**  Following the LTS schedule guarantees a baseline level of security and stability across all projects.\n* **Predictability:**  Knowing the release cadence of new LTS versions allows for better planning and resource allocation for upgrades. \n* **Community Support:**  LTS versions receive long-term support from the Node.js community, ensuring access to resources and assistance.\n* **Simplified Maintenance:**  Supporting a single, well-defined version of Node.js and npm minimizes complexity and reduces the potential for conflicts.\n\n**Consequences:**\n\n* **Upgrading Effort:**  Projects will need to be regularly upgraded to newer LTS versions. We will investigate tools and strategies to streamline this process.\n* **Dependency Compatibility:**  Projects heavily relying on specific npm versions might encounter compatibility issues when upgrading Node.js. We will provide guidance and support to address these situations.\n\n**This decision will be reviewed periodically to ensure its continued effectiveness and alignment with eLife's evolving technology landscape.** \n","GenTime":"2024-08-19 16:26:47","text":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n\n\n##Decision\nTo use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n"}
{"File Name":"mlmc\/0003-use-rabbit-mq-for-eda.md","Context":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n","Decision":"* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","tokens":53,"id":1575,"Prediction":"## Decision\n\nWe will use **RabbitMQ** as our message broker for the EDA architecture. \n\n## Justification\n\nRabbitMQ fulfills the defined decision drivers:\n\n* **Easy to use:** RabbitMQ is known for its relatively simple API and well-documented libraries in various programming languages, making it easy for developers to integrate with.\n* **Easy to learn:**  Numerous tutorials, guides, and a supportive community contribute to a manageable learning curve for RabbitMQ.\n* **Possibility to run from Docker:** RabbitMQ provides official Docker images, simplifying deployment and ensuring environment consistency.\n\n### Considered Alternatives\n\n* **Apache Kafka:** While highly scalable and performant, Kafka might be an overkill for the current needs and introduces a steeper learning curve compared to RabbitMQ.\n* **Redis:**  Although Redis offers pub\/sub functionality, its strength lies primarily in caching and in-memory data storage. Using it solely as a message broker might not be ideal for complex EDA scenarios.\n\n## Consequences\n\n* **Positive:** RabbitMQ's ease of use and readily available Docker image will accelerate development and deployment.\n* **Negative:** If future requirements demand extremely high throughput or complex message streaming scenarios, migrating from RabbitMQ to a more robust solution like Kafka might be necessary.\n\nThis decision will be re-evaluated if the messaging needs significantly change in terms of scale, complexity, or if alternative solutions offer significant advantages over RabbitMQ. \n","GenTime":"2024-08-19 16:26:53","text":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n\n\n##Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n"}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"Prediction":"## Decision \n","GenTime":"2024-08-19 16:26:54","text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n"}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"Prediction":"## Decision\n\nWe will integrate with Newslabs PSTT via its provided API ([mention specific API version or documentation link if available]).  This will allow us to leverage their speech-to-text capabilities without managing our own STT infrastructure. \n","GenTime":"2024-08-19 16:26:56","text":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n\n##Decision\nAshley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n"}
{"File Name":"interlok\/0003-deprecate-dynamicservicelocator.md","Context":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n","Decision":"Deprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","tokens":146,"id":2342,"Prediction":"## Decision\n\nWe will **replace the existing DynamicServiceLocator with a new, simplified configuration system.** This system will focus on storing mapping specifications and key information in a centralized database while eliminating the extraneous configuration options of the previous system. \n\nThis decision is driven by the following factors:\n\n* **Alignment with industry trends:** The industry is moving towards centralized processing for document trading networks.\n* **Simplification:** The current DynamicServiceLocator system is overly complex and difficult to understand, particularly within the UI.\n* **Reduced coupling:** The current file-based system introduces unnecessary coupling between components.\n* **Improved maintainability:** A centralized database will simplify management and updates to mapping specifications and key information.\n\nThis decision will result in a more streamlined and user-friendly configuration experience while aligning with the industry's best practices. \n","GenTime":"2024-08-19 16:27:00","text":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n\n\n##Decision\nDeprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n"}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"Prediction":"## Decision\n\nWe will implement a dynamic inheritance mechanism for themes, moving away from the current snapshot-based approach. This mechanism will allow child themes to inherit configuration fields, scripts, templates, and other assets from any parent theme, not just the default Storefront theme. \n\nThis dynamic inheritance will work in real-time, meaning any changes made to the parent theme will be reflected in the child theme without requiring reactivation. \n\nTo ensure flexibility and granularity, we will implement a system where inheritance can be controlled at a granular level. This means theme developers can choose to inherit specific elements (like config fields or templates) while overriding others. \n\nThis approach addresses the limitations of the current system by:\n\n* **Enabling inheritance from any theme:** Promotes theme reuse and reduces redundancy.\n* **Dynamic updates:** Ensures child themes stay up-to-date with parent theme changes.\n* **Granular control:** Provides flexibility for theme developers to customize inheritance behavior.\n\nThis decision aims to simplify theme development, improve maintainability, and create a more robust and flexible theming system. \n","GenTime":"2024-08-19 16:27:04","text":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n\n\n##Decision\nTo take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n"}
{"File Name":"tendermint\/adr-051-double-signing-risk-reduction.md","Context":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n","Decision":"We would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","tokens":85,"id":1944,"Prediction":"## Decision\n\n**Implement a double-signing risk reduction mechanism within the node daemon that analyzes the last N blocks before vote submission.** \n\nThis mechanism will specifically check for the presence of votes already cast by the validator's public key within the previous N blocks. If a match is detected, the node daemon will:\n\n1. **Halt the voting process immediately.**\n2. **Issue a critical alert to the validator operator,** clearly indicating the potential double-signing scenario and the detected duplicate vote. \n3. **Provide clear instructions on how to resolve the issue,** including steps for verifying the validator setup and stopping any duplicate instances.\n\n**Parameters:**\n\n* **N:** The optimal value for 'N' will be determined through further analysis, considering factors like chain dynamics and potential attack vectors. \n\n**Justification:**\n\nThis decision prioritizes the prevention of double-signing incidents, a critical threat to the blockchain's integrity. By incorporating this proactive check, we significantly reduce the risk of accidental double-signing due to validator misconfiguration or human error. The inclusion of alerts and guidance empowers validators to take immediate corrective action.\n\n**Consequences:**\n\n* **Increased node complexity:** Implementing this mechanism will require additional code and logic within the node daemon. \n* **Potential for false positives:**  Edge cases might exist where legitimate validator behavior is flagged as a potential double-signing risk. This necessitates robust testing and fine-tuning of the algorithm. \n* **Slight performance impact:** Analyzing the previous N blocks before each vote submission will introduce a minor performance overhead. However, this is deemed acceptable given the criticality of preventing double-signing. \n","GenTime":"2024-08-19 16:27:12","text":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n\n\n##Decision\nWe would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n"}
{"File Name":"caia\/0011-pin-mbtest-library-to-v2.5.1.md","Context":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n","Decision":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","tokens":315,"id":3507,"Prediction":"## Decision\n\nWe will address the failing \"caia\" Jenkins builds by updating the project's usage of the `get_actual_requests` method from the \"mbtest\" library. \n\nSince the method signature has changed from returning a `Mapping` to an `Iterable`, we will modify the test code to iterate over the returned `Iterable` instead of attempting to subscript it like a `Mapping`. \n\nThis decision prioritizes compatibility with the latest version of the \"mbtest\" library and ensures our test suite remains functional within the new \"GitHub organization\" pipeline. \n","GenTime":"2024-08-19 16:27:15","text":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n\n\n##Decision\nThe simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n"}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"Prediction":"## Decision\n\nWe will use the Serverless framework for provisioning and managing the infrastructure for this project. \n\n### Reasoning\n\n* **Reproducibility and Consistency:** Serverless promotes infrastructure as code, allowing us to define our infrastructure in code and version it. This ensures that our infrastructure is easily reproducible and consistent across environments.\n* **Reduced Manual Configuration:**  Using Serverless will significantly reduce manual configuration, leading to fewer errors and inconsistencies. This aligns with our goal of avoiding manual changes to services. \n* **Abstraction from Cloud Providers:** While we are initially focusing on AWS, Serverless provides a layer of abstraction over CloudFormation (and similar tools from other providers). This offers flexibility if we need to migrate to a different cloud provider in the future. \n* **Familiarity:**  Existing experience with Serverless within the team will shorten the learning curve and expedite development. \n\n### Consequences\n\n* **Vendor Lock-in:** While Serverless provides some level of cloud platform agnosticism, there is still a degree of lock-in to the Serverless framework itself.\n* **Learning Curve:** The team will need to invest time in learning the specifics of the Serverless framework and its best practices.\n* **Complexity:**  For simpler projects, using Serverless might introduce unnecessary complexity compared to directly using cloud provider tools like CloudFormation.  However, we believe the benefits outweigh this potential drawback in the long run. \n\n### Alternatives Considered\n\n* **Directly using CloudFormation (or equivalent):** This offers the most granular control over our infrastructure, but it comes with increased complexity and potential for inconsistencies if not managed carefully.\n* **Other Infrastructure-as-Code tools:**  Tools like Terraform offer similar benefits to Serverless with potentially broader cloud provider support. However, our existing familiarity with Serverless makes it the most efficient choice for this project. \n","GenTime":"2024-08-19 16:27:23","text":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n\n\n##Decision\nI will use Serverless to describe and provision my AWS components.\\n"}
{"File Name":"aws-sync-routes\/0005-uri.md","Context":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n","Decision":"The `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","tokens":64,"id":1050,"Prediction":"## Decision\n\n**Option 1: Extend the Existing REST API**\n\n* **Decision:** We will extend the existing REST API to include a new endpoint specifically for synchronizing routes to custom route tables. \n\n* **Reasons:**\n    *  Provides a clean, well-defined interface for the functionality.\n    *  Maintains consistency with existing API interactions.\n    *  Allows for future expansion and flexibility in handling route synchronization.\n\n* **Consequences:**\n    *  Requires modification to the API documentation and potentially client code.\n    *  Introduces a new endpoint that might not perfectly align with the existing API structure.\n\n**Option 2: Utilize Asynchronous Messaging (e.g., Message Queue)**\n\n* **Decision:**  We will **not** utilize asynchronous messaging for this specific functionality.\n\n* **Reasons:**\n    *  The log event triggering the synchronization might not contain all the necessary information to construct a comprehensive message.\n    *  Introducing a message queue adds complexity, especially if this functionality remains relatively isolated.\n\n**Option 3: Implement a Custom Solution (e.g., Webhook with Defined Payload)**\n\n* **Decision:** We will **not** implement a custom solution like a webhook.\n\n* **Reasons:**\n    *  Lacks the standardization and discoverability benefits of a well-defined REST API.\n    *  Could lead to increased maintenance overhead for a specialized solution. \n\n**Chosen Option:** **Option 1 - Extend the Existing REST API**\n\n**Rationale:** \n\nExtending the existing REST API provides the most maintainable and scalable approach. While it requires modifications, it ensures consistency and offers a clear interface for route synchronization, even with limited information initially available from the log event. We can design the API endpoint to accept a minimal set of parameters and then retrieve any additional required data internally. This approach balances functionality with architectural clarity. \n","GenTime":"2024-08-19 16:27:31","text":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n\n\n##Decision\nThe `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n"}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"Prediction":"## Decision\n\nWe will configure the PHP images with the following settings:\n\n* **memory_limit: -1** -  This disables PHP's internal memory limit, allowing the container orchestration platform to manage resource allocation. \n* **PHP-FPM pm.max_requests: 1** -  This setting ensures that each PHP-FPM worker process handles only one request before restarting. This prevents potential memory leaks from accumulating within a worker process over time and allows for more predictable resource usage.\n\n## Consequences\n\n**Advantages:**\n\n* **Simplified resource management:** Relying on the orchestration platform's resource management simplifies configuration and improves scalability.\n* **Improved resource utilization:** By removing PHP's internal memory limit, the container can utilize all available memory resources.\n* **Reduced risk of memory leaks:** By limiting each PHP-FPM worker process to a single request, the risk of memory leaks impacting subsequent requests is minimized.\n\n**Disadvantages:**\n\n* **Potential for container instability:** Without a PHP-defined memory limit, a misbehaving application could consume excessive memory and potentially crash the entire container.\n* **Increased monitoring complexity:**  Relying on the orchestrator's resource management necessitates monitoring container resource usage closely. \n\n**Mitigation Strategies:**\n\n* **Resource limits at the container level:**  Set resource limits (memory, CPU) within the orchestration platform to prevent containers from consuming excessive resources. \n* **Robust monitoring and alerting:** Implement comprehensive monitoring and alerting systems to detect and respond to resource exhaustion issues promptly. \n* **Consider alternative approaches:** For highly sensitive environments, explore alternative approaches such as using PHP extensions for memory management or running applications in smaller, more isolated containers.\n\nThis decision prioritizes scalability and integration with the orchestration platform while acknowledging the increased need for robust monitoring and potential risks associated with removing the PHP memory limit.\n","GenTime":"2024-08-19 16:27:39","text":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n\n\n##Decision\nSet php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n"}
{"File Name":"monitoring-doc\/0003-use-ecs-for-initial-beta-buildout.md","Context":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n","Decision":"We will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","tokens":110,"id":1376,"Prediction":"## Decision\n\nWe will adopt a container-based approach to infrastructure management, leveraging Docker for packaging and configuration management.  This decision aligns with industry best practices exemplified by platforms like Kubernetes and Amazon ECS and will allow us to:\n\n* **Promote consistency:**  Standardizing on Docker for packaging and configuration will reduce variations between teams and minimize reinventing the wheel.\n* **Improve maintainability:** Containerization enhances modularity, making infrastructure components easier to manage, update, and troubleshoot.\n* **Facilitate scalability:**  Docker's portability simplifies deployment across different environments and enables scaling infrastructure resources more efficiently.\n\nThis decision will involve:\n\n* **Transitioning existing infrastructure components to Docker containers.**\n* **Developing a centralized repository for sharing Docker images and configuration files.**\n* **Implementing a container orchestration platform (e.g., Kubernetes) to manage and automate container deployment and scaling.**\n\nWhile this transition will require an initial investment in training and tooling, the long-term benefits of increased consistency, maintainability, and scalability outweigh the costs.\n","GenTime":"2024-08-19 16:27:44","text":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n\n\n##Decision\nWe will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n"}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"Prediction":"## Decision\n\nWe will create an abstract `TrainingScheme` class with concrete subclasses for each training scheme. This will allow us to:\n\n1. **Encapsulate common logic:**  Shared functionality, such as the fixed 40% payment and the mechanism for setup payments, will be implemented in the abstract base class.\n2. **Isolate differences:**  Scheme-specific logic, such as the output payment calculations and banding systems, will be implemented within the concrete subclasses. \n3. **Prepare for future extensibility:**  Adding a third (or more) training scheme will involve creating a new subclass of `TrainingScheme`, minimizing code duplication and promoting maintainability.\n\nThis approach balances the need for shared functionality with the flexibility to accommodate differences between the training schemes, while also setting us up to easily incorporate future schemes.\n","GenTime":"2024-08-19 16:27:48","text":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n\n\n##Decision\n1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n"}
{"File Name":"LogLady\/0006-use-electron-updater-autoupdate.md","Context":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n","Decision":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","tokens":36,"id":1852,"Prediction":"## Decision\n\nWe will **not** be using the existing auto-update package already present in the project. Instead, we will utilize **[Name of chosen package, e.g., electron-updater]** for handling automatic updates.\n\nThis decision is based on the following factors:\n\n* **Popularity & Community Support:** [Name of chosen package] is widely adopted and boasts a large, active community. This translates to better documentation, readily available support, and a lower likelihood of encountering unresolved issues.\n* **Features & Ease of Integration:** [Name of chosen package] offers [List key features, e.g., seamless integration with code signing services, cross-platform compatibility, delta updates] which are crucial for our application's update mechanism. Additionally, its integration with our existing development workflow is expected to be smoother.\n* **Security:** [Name of chosen package] prioritizes secure update delivery through [Mention security features, e.g., code signing verification, encrypted transfer]. \n* **Future Maintenance:** Opting for a widely used package simplifies future maintenance as it's more likely to remain compatible with upcoming Electron versions and receive ongoing security updates.\n\nWhile the existing package might offer basic auto-update functionality, we prioritize the benefits outlined above for a more robust and maintainable solution. We will proceed by removing the unused package and integrating [Name of chosen package] into our project. \n","GenTime":"2024-08-19 16:27:54","text":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n\n\n##Decision\nWe will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n"}
{"File Name":"paas-csls-splunk-broker\/ADR004-deploy-broker-as-lambda.md","Context":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n","Decision":"We will deploy the Broker as an AWS Lambda\\n","tokens":77,"id":3295,"Prediction":"## Decision\n\nWe will deploy the Broker as a containerized application in AWS ECS (Elastic Container Service). \n\n## Reasons\n\n* **Consistency with Adapter Deployment:**  Deploying the Broker in AWS maintains consistency with the Adapter's deployment model and leverages our existing AWS infrastructure. \n* **Scalability and Availability:** ECS provides robust scalability and high availability, crucial for a critical component like the Broker. \n* **Integration with Existing Pipeline:**  We can easily integrate the Broker's container build and deployment into our existing CI\/CD pipeline.\n* **Language Support:** ECS supports Docker containers, aligning well with our Go-based Broker implementation. \n\n## Consequences\n\n* **Operational Overhead:**  Managing an ECS cluster introduces some operational overhead compared to a simpler deployment model.\n* **Cost Implications:** Using ECS incurs costs associated with running the cluster and its underlying resources. \n\n## Alternatives Considered:\n\n* **Deploying the Broker as a Lambda:** While possible, this approach might complicate interactions between the Broker and the Adapter, potentially impacting performance.\n* **Deploying to a separate VM:** This would increase management overhead compared to a containerized solution.\n\n## Status: Accepted\n\nThis decision will be reviewed if the scaling requirements of the Broker change significantly, or if a more integrated serverless solution for both Broker and Adapter emerges. \n","GenTime":"2024-08-19 16:28:00","text":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n\n##Decision\nWe will deploy the Broker as an AWS Lambda\\n"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"Prediction":"## Decision\n\nWe will adopt command-query separation (CQS) as a guiding principle for designing our software. This means:\n\n1. **All new methods will be classified as either commands or queries.**\n2. **Commands**:\n    * Will be responsible for causing state changes in the system.\n    * Will not return data, but may return an acknowledgement of success\/failure or an identifier of the changed resource.\n    * Will be named using verbs or verb phrases that clearly indicate the action being performed.\n3. **Queries**:\n    * Will be responsible for retrieving data from the system.\n    * Will not modify any state.\n    * Will return the requested data.\n    * Will be named using nouns or noun phrases that describe the data being retrieved.\n4. **Existing code in the `menu-generation` application will be reviewed and refactored** to align with CQS where it makes sense and brings tangible benefits.  We will prioritize refactoring based on the criticality and complexity of the existing code. \n\nThis decision aims to improve the readability, maintainability, and predictability of our codebase by clearly separating methods with side effects from those without. \n","GenTime":"2024-08-19 16:28:05","text":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n\n##Decision\nCommand-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n"}
{"File Name":"ethnicity-facts-and-figures-publisher\/0002-use-heroku-run-detached-for-static-site-build.md","Context":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n","Decision":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","tokens":172,"id":3361,"Prediction":"## Decision\n\nMigrate the scheduled tasks away from Heroku Scheduler and into a containerized environment managed by Kubernetes. \n\n## Reasons\n\n* **Reliability:** Kubernetes provides a more robust and fault-tolerant environment for running scheduled tasks compared to Heroku Scheduler. It allows for resource allocation, health checks, and automatic restarts, ensuring tasks complete successfully even if they encounter temporary issues. \n* **Flexibility:** Kubernetes offers greater control over task execution, allowing us to define specific resource limits, timeouts, and retry policies. This addresses the current problem of Heroku Scheduler terminating tasks that exceed the scheduled interval.\n* **Scalability:** As the platform grows and more scheduled tasks are introduced, Kubernetes provides the infrastructure to scale the scheduling system effectively.\n\n## Consequences\n\n* **Increased complexity:** Managing a Kubernetes cluster introduces additional operational overhead compared to the simplicity of Heroku Scheduler.\n* **Migration effort:** Migrating existing tasks to Kubernetes will require development effort to containerize tasks and configure the Kubernetes deployments.\n* **Cost implications:** Running a Kubernetes cluster might have cost implications depending on the chosen provider and resource usage.\n\n## Alternatives Considered\n\n* **Increasing Heroku Scheduler Interval:** While simple, this doesn't address the root issue of tasks occasionally exceeding the allocated time and could lead to cascading delays.\n* **Implementing a custom queuing system on Heroku:** This offers more control over task execution but adds significant complexity compared to a managed solution like Kubernetes.\n\n## Status\n\nProposed\n\n## Action Items\n\n* Research and select a suitable Kubernetes provider (e.g., AWS EKS, Google Kubernetes Engine, Azure Kubernetes Service).\n* Design and implement a containerized solution for the scheduled tasks.\n* Migrate existing scheduled tasks from Heroku Scheduler to the new Kubernetes cluster.\n* Establish monitoring and alerting for the Kubernetes cluster and scheduled tasks. \n","GenTime":"2024-08-19 16:28:13","text":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n\n\n##Decision\nWe will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"Prediction":"## Decision\n\nWe will refactor the Cosmos SDK event system to use **typed events** defined within each module. This approach will replace the current method of defining events within message handlers, which lacks a centralized and typed structure. \n\nThis decision is driven by the need to:\n\n* **Improve Event Consumability:** Typed events will eliminate the reliance on error-prone string matching and parsing, making it significantly easier for applications to consume and react to events.\n* **Enhance Developer Experience:** Defining events within modules provides a canonical source of truth and promotes code clarity and maintainability.\n* **Enable Event-Driven Applications:**  The growing Cosmos SDK ecosystem demands robust event handling for applications like wallets, relayers, DeFi protocols, and more. Typed events provide a solid foundation for building such applications.\n\nThis decision aligns with the experiences and best practices observed in Akash Network and Kubernetes development, emphasizing the value of typed events in complex, event-driven systems.\n","GenTime":"2024-08-19 16:28:18","text":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n\n\n##Decision\n**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n"}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"Prediction":"## Decision\n\nWe will migrate from apache-commons-logging to SLF4J for logging. This provides us with a flexible facade to use Log4j (already a dependency) and ensures compatibility with Java 9 and beyond. \n","GenTime":"2024-08-19 16:28:19","text":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n\n\n##Decision\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n"}
{"File Name":"wordpress-template\/0003-theme-namespace-naming-convention.md","Context":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n","Decision":"We'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","tokens":45,"id":2262,"Prediction":"## Decision\n\nWe will use a flat namespace for themes. This means each theme will have a unique name within the entire application. \n\n## Reasons\n\n* **Simplicity:**  A flat namespace is easy to understand and manage. \n* **Reduced Overhead:** Avoiding nested namespaces simplifies theme development and usage.\n* **Single Active Theme:** Since only one theme is active at a time, the risk of naming collisions is minimal.\n\n##  Consequences\n\n* **Child Themes:** We will need to implement a specific mechanism for child themes to ensure they can inherit and override elements from their parent theme while maintaining unique identifiers. This might involve prefixes, suffixes, or specific naming conventions for child themes.\n* **Potential Refactoring:** If, in the future, the application requires multiple active themes simultaneously, we might need to revisit this decision and potentially refactor to a hierarchical namespace. \n","GenTime":"2024-08-19 16:28:23","text":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n\n\n##Decision\nWe'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n"}
{"File Name":"stamper\/0007-docker.md","Context":"## Context\\nWe need to choose how we will deploy our app.\\n","Decision":"We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","tokens":15,"id":1239,"Prediction":"## Decision\n\nWe will use a containerized deployment strategy using Docker and orchestrate containers using Kubernetes. \n\n## Reasons\n\n* **Scalability and Availability:** Kubernetes offers robust scaling and self-healing capabilities, ensuring high availability and allowing us to easily adapt to changing traffic demands.\n* **Infrastructure Agnosticism:** Docker and Kubernetes provide platform independence, enabling us to deploy on various cloud providers or on-premise infrastructure without vendor lock-in.\n* **Simplified Deployment and Rollouts:** Containerization simplifies the deployment process and enables seamless rollouts and rollbacks with minimal downtime. \n* **Resource Optimization:** Kubernetes efficiently manages resource allocation, optimizing resource utilization and potentially reducing infrastructure costs.\n* **Microservice Architecture Support:**  Containerization aligns well with microservice architectures, allowing for independent deployments and scaling of individual services.\n\n## Status\n\nAccepted\n\n## Consequences\n\n* **Increased Complexity:** Containerization and orchestration introduce an additional layer of complexity requiring skilled personnel.\n* **Learning Curve:**  Adopting Docker and Kubernetes requires a learning curve for the development and operations teams.\n* **Initial Setup Time:** Setting up the containerization and orchestration infrastructure requires upfront time and resources.\n\n## Alternatives Considered\n\n* **Virtual Machines:** Deploying on virtual machines offers isolation and resource allocation but lacks the flexibility and scalability of containers.\n* **Platform-as-a-Service (PaaS):** Utilizing a PaaS simplifies deployment and management but may introduce vendor lock-in and limit customization options.\n\n## Action Items\n\n* **Research and select a suitable Kubernetes provider** (e.g., AWS EKS, Google Kubernetes Engine, Azure Kubernetes Service).\n* **Create Dockerfiles for all application components.**\n* **Define Kubernetes deployment manifests for the application.**\n* **Set up a continuous integration and continuous deployment (CI\/CD) pipeline for automated deployments.**\n* **Provide training to the development and operations teams on Docker and Kubernetes.** \n","GenTime":"2024-08-19 16:28:32","text":"## Context\\nWe need to choose how we will deploy our app.\\n\n\n##Decision\nWe use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n"}
{"File Name":"adr-tools\/0004-markdown-format.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":3568,"Prediction":"## Decision\n\nWe will use Markdown as the plain text format for storing architectural decision records. \n\n### Rationale\n\n* **Version Control Friendliness:** Markdown plays well with version control systems like Git, enabling clear diffing and history tracking.\n* **Tooling Compatibility:** The simple structure of Markdown allows tools to easily parse the content, modify record statuses, and insert hyperlinks for cross-referencing decisions.\n* **Ubiquitous Readability:** Markdown can be read comfortably in various environments: terminals, IDEs, version control browsers, and more.\n* **Basic Formatting Support:** Markdown offers sufficient formatting options (lists, code blocks, headings) to structure decisions effectively.\n* **Readable Rendering:** Markdown can be easily rendered into visually appealing HTML or PDF documents for comfortable reading and printing.\n\n### Considered Alternatives\n\n* **Plain text without any formatting:**  While simple, it lacks readability and expressiveness for complex decisions. \n* **ReStructured Text:**  While powerful, it has a steeper learning curve and might be overkill for our needs.\n* **AsciiDoc:** Similar to ReStructured Text in functionality but potentially more complex than needed.\n\n### Consequences\n\n* **Learning Curve:** Team members will need to be familiar with basic Markdown syntax.\n* **Limited Formatting:** Markdown doesn't support advanced formatting like tables or images directly (though they can be linked).\n* **Rendering Tool Dependency:** Viewing Markdown in a visually appealing way requires a rendering tool or plugin. \n","GenTime":"2024-08-19 16:28:39","text":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n\n\n##Decision\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n"}
{"File Name":"mario\/0008-use-one-s3-bucket-per-source.md","Context":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n","Decision":"Use one S3 bucket per data source.\\n","tokens":137,"id":3416,"Prediction":"## Decision\n\nWe will implement a folder-per-data-source strategy within the S3 bucket. Each data source will be assigned a unique, descriptive folder where it will upload its files. \n\nFor example:\n\n```\ns3:\/\/my-data-bucket\/\n  - data-source-a\/\n    - file1.csv\n    - file2.csv\n  - data-source-b\/\n    - data.json\n```\n\nThis approach offers several advantages:\n\n* **Source identification:**  The folder structure clearly identifies the origin of each file, enabling downstream processing to be triggered and configured accordingly. \n* **Flexibility:**  It accommodates various data sources without requiring complex configuration within the sending environment.\n* **Scalability:**  It allows for future growth and the addition of new data sources without impacting existing workflows. \n\n**Implementation Details:**\n\n* We will define a naming convention for data source folders to ensure consistency.\n* The data processing pipeline (triggered by Lambda) will use the folder structure to determine the source and apply the appropriate processing logic. \n* Documentation will be provided to data providers, specifying the designated folder for their data uploads.\n\n**Alternatives Considered:**\n\n* **S3 Object Tagging:** While tagging offers flexibility, it requires more control over the data uploading process, which may not be feasible with all data sources.\n* **Separate S3 Buckets:** This approach is less cost-effective and introduces management overhead.\n\nThis decision prioritizes simplicity, flexibility, and minimal dependencies on the data source environments. \n","GenTime":"2024-08-19 16:28:46","text":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n\n\n##Decision\nUse one S3 bucket per data source.\\n"}
{"File Name":"buy-for-your-school\/0020-use-accessible-autocomplete-for-autocomplete-fields.md","Context":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n","Decision":"We will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","tokens":26,"id":1255,"Prediction":"## Decision\n\nWe will implement autocomplete functionality using a third-party library to minimize development time and leverage existing solutions. The specific library will be chosen based on the following criteria:\n\n* **Performance:** The library should be able to handle a large number of potential suggestions without introducing noticeable lag.\n* **Flexibility:** The library should offer customization options to integrate seamlessly with our existing UI and data sources.\n* **Ease of use:**  The library should have clear documentation and be straightforward to integrate into our codebase.\n* **Support:** The library should be well-maintained and have a responsive support community. \n\nWe will evaluate and benchmark several popular libraries before making a final decision.  This evaluation will consider factors like data fetching strategies (e.g., local vs. remote), UI customization options, and developer experience.\n","GenTime":"2024-08-19 16:28:49","text":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n\n##Decision\nWe will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n"}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"Prediction":"## Decision\n\n**We will use WPF (Windows Presentation Foundation) with C# to develop the touch table application.** \n","GenTime":"2024-08-19 16:28:51","text":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n\n##Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n"}
{"File Name":"simple-server\/011-rollup-summary-tables-for-dashboards.md","Context":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n","Decision":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","tokens":123,"id":1701,"Prediction":"## Decision\n\nWe will implement a **dedicated reporting database** populated by an **extract, transform, load (ETL) process** from the primary database. \n\nThis decision is based on the following factors:\n\n* **Scalability:** A separate reporting database allows us to scale our reporting infrastructure independently from the primary operational database. This ensures consistent performance for both operational and reporting workloads.\n* **Simplicity:** ETL processes provide a clear and maintainable way to extract, transform, and load data into the reporting database. This simplifies the logic required within dashboards and reports.\n* **Flexibility:** This approach can accommodate a wide range of reporting needs, including ad-hoc queries, complex aggregations, and historical analysis, without impacting the performance of the primary database.\n* **Data consistency:** By using an ETL process, we can ensure that the reporting database contains a consistent and accurate snapshot of the operational data at specific points in time.\n\nThis decision will require careful consideration of the following aspects:\n\n* **ETL Tool Selection:** We need to evaluate and select an appropriate ETL tool that meets our performance, scalability, and maintainability requirements.\n* **Data Modeling:** Designing an efficient data model for the reporting database is crucial to optimize query performance and data storage.\n* **Monitoring and Alerting:** Implementing robust monitoring and alerting mechanisms will be essential to ensure the accuracy and reliability of the reporting data.\n\nWe will further document the specific details of our ETL process, reporting database schema, and monitoring strategy in separate architectural decision records.\n","GenTime":"2024-08-19 16:28:57","text":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n\n\n##Decision\nWe will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n"}
{"File Name":"PerformanceTestDocs\/0006-vellum-node-fails.md","Context":"## Context\\nVellum node fails\\n","Decision":"Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","tokens":9,"id":3952,"Prediction":"## Decision\n\n**Option 1: Automatic Retry with Exponential Backoff and Dead Letter Queue**\n\n* **Description:** When a Vellum node fails, the system will automatically retry the failed operation with an exponential backoff strategy. If the retries exhaust, the message or task will be moved to a dead letter queue for further investigation and manual intervention. \n\n* **Pros:** \n    * **Resilience:** Increases system resilience to transient failures.\n    * **Automation:** Reduces manual intervention for common errors.\n    * **Error Visibility:**  Provides a centralized location (dead letter queue) for tracking and analyzing persistent errors.\n\n* **Cons:**\n    * **Complexity:** Introduces additional components (retry logic, dead letter queue) and complexity.\n    * **Potential Data Duplication:**  Requires careful handling of message idempotency to avoid data duplication during retries. \n\n**Option 2: Failover to a Redundant Node**\n\n* **Description:** Deploy Vellum nodes in a redundant configuration (e.g., active\/passive or active\/active). When a node fails, the system automatically redirects traffic or tasks to a healthy node.\n\n* **Pros:**\n    * **High Availability:** Minimizes downtime and ensures continuous operation.\n    * **Transparency:** Failure and failover are transparent to users or other system components.\n\n* **Cons:** \n    * **Increased Resource Cost:** Requires additional resources for redundant nodes.\n    * **Complexity:** Implementing failover mechanisms adds complexity to the system.\n\n**Option 3: Manual Intervention**\n\n* **Description:**  When a Vellum node fails, the system triggers an alert, and manual intervention is required to investigate the root cause and restart or recover the failed node.\n\n* **Pros:** \n    * **Simplicity:** No additional components or complex logic required.\n\n* **Cons:**\n    * **Downtime:** Results in service disruption until the issue is resolved manually.\n    * **High Operational Overhead:** Requires manual monitoring and intervention. \n\n**Decision:** \n\nThe best option depends on the specific requirements and constraints of the system.\n\n* **For critical systems with high availability requirements, Option 2 (Failover to a Redundant Node) is the preferred approach.** \n* **For systems where some downtime is acceptable and cost is a major factor, Option 1 (Automatic Retry with Exponential Backoff and Dead Letter Queue) provides a good balance between resilience and cost-effectiveness.**\n* **Option 3 (Manual Intervention) is the least desirable option and should only be considered if other options are not feasible.**\n\n\n**Chosen Option:**  _[Choose the most appropriate option based on your specific context and provide a justification]_\n\n**Justification:** _[Explain the rationale behind the chosen option, considering the pros and cons of each option]_ \n","GenTime":"2024-08-19 16:29:09","text":"## Context\\nVellum node fails\\n\n\n##Decision\nVellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n"}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"Prediction":"## Decision\n\nWe will use **[Framework Name]** as our web framework. \n\n**Reasoning:**\n\n* **Popularity and Community Support:** [Framework Name] is a popular and well-maintained framework with a large and active community. This translates to extensive documentation, readily available resources, and a higher chance of finding solutions to potential problems. \n* **Features and Abstractions:** [Framework Name] offers a robust set of features and abstractions specifically designed to simplify web development tasks, aligning with our goal of minimizing effort spent on non-domain specific problems. This includes [mention specific features like routing, templating, database ORM, etc. - tailor this to the chosen framework and its relevance to the project].\n* **[Optional: Include additional specific reasons based on the chosen framework e.g. ease of learning, performance, scalability, security features].**\n\n**Alternatives Considered:**\n\n* **[Alternative Framework 1]:**  While [briefly mention positive aspects of this alternative], it [explain why it's not chosen, e.g., less mature, steeper learning curve, lacks specific features needed].\n* **[Alternative Framework 2]:**  Although [briefly mention positive aspects of this alternative], it [explain why it's not chosen, e.g.,  overly complex for our needs,  performance concerns].\n\n**Consequences:**\n\n* **Positive:** \n    * Faster development cycles due to the framework's abstractions and pre-built components.\n    * Reduced boilerplate code, leading to cleaner and more maintainable codebase.\n    * Easier onboarding for new developers due to the framework's popularity and available resources.\n* **Negative:** \n    * Potential learning curve for developers unfamiliar with [Framework Name].\n    * [Optional:  Mention any potential drawbacks specific to the chosen framework, e.g., performance limitations at a certain scale]. \n\nWe believe the positive consequences of using [Framework Name] outweigh the negative, given our context and priorities.\n","GenTime":"2024-08-19 16:29:18","text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n"}
{"File Name":"ensemblejs\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3940,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will document the context and consequences of a decision, facilitating communication and understanding among stakeholders. \n","GenTime":"2024-08-19 16:29:19","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"Prediction":"## Decision\n\nWe will implement a generic webhook system to retrieve task information and associate it with code changes. This approach satisfies the key drivers as follows:\n\n* **Simple Implementation:** Webhooks rely on simple HTTP requests, making the integration relatively straightforward.\n* **No Assumption About the Task Tracker:** This approach does not depend on the specific task tracker used. Any tracker capable of sending webhooks with relevant task data can be integrated.\n* **Support of a Set of Generic Fields Related to a Task:**  The webhook payload can include generic task fields (e.g., task ID, type, title, description) allowing for flexible analysis without hardcoding specific field names.\n\n### Details\n\n1.  **Webhook Endpoint:** Our system will expose a webhook endpoint capable of receiving and parsing task data payloads.\n2.  **Task Data Extraction:** We will define a standardized structure for the webhook payload, including generic fields like:\n    *   `task_id`: Unique identifier for the task\n    *   `task_type`: Category of the task (e.g., \"Feature Request\", \"Bug\", \"Chore\")\n    *   `task_title`: Short description of the task\n    *   `task_description`: Detailed explanation of the task (optional)\n3.  **Association with Code Changes:**  The webhook payload will be processed to extract relevant task information and associate it with code changes. The association mechanism will depend on the specific version control system (VCS) used, but generally, it will involve linking the task ID or other identifiers within commit messages.\n4.  **Metrics Generation:** With the association established, various metrics related to code changes can be generated, including the ratio of changes related to different task types, such as Feature Requests vs Bug fixes.\n\n### Consequences\n\nThis decision will result in:\n\n* **Flexibility:** The webhook system allows integration with various task trackers without extensive custom development.\n* **Scalability:** The system can handle a large number of tasks and code changes as it relies on simple HTTP requests.\n* **Future Extensibility:**  The generic nature of the webhook payload enables future expansion to incorporate additional task fields or event types without significant code changes. \n","GenTime":"2024-08-19 16:29:29","text":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n\n##Decision\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n"}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"Prediction":"## Decision\n\n**Option 4: Email a link to the form owner allowing the ability to download all attachments in one go.**\n\n## Reasons\n\n* **Reduced Email Noise:** This approach significantly reduces email clutter for both applicants and form owners, addressing the primary concern.\n* **No File Size Restrictions:**  By providing a download link, we eliminate the 10MB email attachment limit imposed by AWS SES. This allows for flexibility in form submissions, accommodating various file sizes and quantities. \n* **Improved Workflow Efficiency:**  Form owners can download all files at once, streamlining their workflow and reducing the risk of manual errors associated with handling numerous individual emails. \n* **Security:** The download link can be time-limited and\/or require authentication, ensuring secure access to the uploaded files.\n\n## Considered Options\n\n* **Option 1 (Attach as many files as possible):** While this reduces email volume to some extent, it still suffers from the 10MB limit and doesn't fully resolve the issue.\n* **Option 2 (Hard cap on 10MB):** This is too restrictive and may hinder legitimate use cases where larger file submissions are necessary.\n* **Option 3 (Admin interface):** This requires form owners to actively log in to retrieve files, potentially creating an extra step and not being as convenient as directly accessing them via email. \n\n## Consequences\n\n* **Development Effort:** Implementing a secure file download system will require development effort. \n* **Storage Costs:** Storing files for download will incur storage costs. However, this can be mitigated by implementing data retention policies. \n\nThis decision prioritizes user experience and workflow efficiency by significantly reducing email noise and removing file size limitations. The development effort and potential storage costs are justified by the benefits gained. \n","GenTime":"2024-08-19 16:29:37","text":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n\n\n##Decision\nWe feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n"}
{"File Name":"visit-plannr\/0002-dynamodb-composite-primary-key-design.md","Context":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n","Decision":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","tokens":260,"id":1990,"Prediction":"## Decision\n\nWe will use a simplified DynamoDB schema with a single primary key (partition key only) for the initial implementation. This decision prioritizes simplicity and speed of development over potential query optimization and physical storage locality benefits offered by a composite primary key. \n\n## Rationale\n\n- **Reduced Complexity:** Implementing a composite key with StreamName and Event Number adds complexity to the codebase, requiring event number tracking and management. This is not ideal in the early stages of development where agility and rapid iteration are paramount.\n- **Focus on Core Functionality:**  The current focus is on building core functionalities and demonstrating the application's value proposition. Optimizing for specific query patterns and physical storage locality is premature at this stage.\n- **Future Iteration:**  We acknowledge the potential benefits of a composite key in the future, especially as the application scales and query patterns become clearer. We will revisit this decision as part of a future iteration focusing on performance optimization.\n\n## Consequences\n\n- **Potential Query Limitations:**  Using a single partition key might limit our ability to perform efficient range queries based on Event Number within a specific StreamName. \n- **Suboptimal Physical Locality:**  Items belonging to the same StreamName might not be physically located together, potentially leading to slightly higher latency for queries retrieving events within a stream.\n\n## Action Items\n\n- Implement the DynamoDB schema with a single partition key.\n- Document the decision and rationale for future reference.\n- Create a backlog item to revisit the primary key design and evaluate the need for a composite key based on performance analysis and query patterns. \n","GenTime":"2024-08-19 16:29:44","text":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n\n\n##Decision\nFor now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n"}
{"File Name":"hello\/008-deployment.md","Context":"## Context\\nWe need a way to deploy our application to AWS.\\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"Prediction":"## Decision\n\nWe will use AWS Elastic Beanstalk for deploying our application. \n\n## Reasons\n\n* **Ease of Use:** Elastic Beanstalk offers a user-friendly interface and automates many deployment tasks, making it easier and faster to deploy and manage our application. This is especially beneficial given our team's current familiarity with AWS. \n* **Scalability and Availability:** Elastic Beanstalk leverages other AWS services like EC2, S3, and CloudWatch to provide built-in scalability and high availability, ensuring our application can handle traffic spikes and remain accessible. \n* **Cost-Effectiveness:** The pay-as-you-go pricing model of Elastic Beanstalk makes it cost-effective, especially for this initial deployment where we are unsure of the exact resource requirements. \n* **Integration with other AWS services:** Elastic Beanstalk integrates seamlessly with other AWS services, allowing for future extensibility as our application grows and requires additional functionalities like databases or message queues.\n\n## Considered Alternatives\n\n* **AWS CloudFormation\/Terraform:** While these Infrastructure-as-Code (IaC) tools offer greater flexibility and control over the infrastructure, they require a steeper learning curve and more time investment upfront, which may not be ideal for our current stage. \n* **Custom Deployment Scripts:** Developing and maintaining custom scripts for deploying and managing our application on EC2 instances would be time-consuming and require significant DevOps expertise, adding complexity to the process.\n\n## Consequences\n\n* **Potential Vendor Lock-in:** Choosing Elastic Beanstalk might lead to some vendor lock-in with AWS. However, the benefits of ease of use and faster deployment outweigh this concern, especially for our initial deployment.\n* **Limited Control:** Elastic Beanstalk abstracts some infrastructure management tasks, potentially limiting our control over certain configurations. However, it provides enough customization options for our current needs. \n\nWe will re-evaluate this decision as the application grows and our requirements evolve. If we require greater flexibility and control in the future, we can revisit using CloudFormation\/Terraform or other deployment strategies.\n","GenTime":"2024-08-19 16:29:52","text":"## Context\\nWe need a way to deploy our application to AWS.\\n\n\n##Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"Prediction":"## Decision\n\nTo address the issue of configuration drift and eliminate redundancy across deployments, we will consolidate our Kubernetes deployment configurations using a templating engine like Helm or Kustomize. \n\nHere's a breakdown of the decision:\n\n* **Problem:** Duplicated YAML files for different environments (e.g., GCP Iowa-a, AWS Frankfurt) lead to configuration drift and maintenance overhead.\n* **Solution:** Adopt a Kubernetes configuration templating tool like Helm or Kustomize. \n    * **Helm** offers a robust package management approach with templating and release management.\n    * **Kustomize** provides a more minimalist approach focused on overlaying changes onto base configurations. \n* **Benefits:**\n    * **Single Source of Truth:** Maintain a single source of truth for deployment configurations, reducing redundancy and inconsistencies.\n    * **Environment-Specific Values:** Use templates to inject environment-specific variables (e.g., region, resource names) during deployment.\n    * **Reusability:** Promote code reuse and simplify the deployment process for new services or environments.\n* **Next Steps:**\n    * **Tool Selection:** Evaluate and choose between Helm and Kustomize based on our specific needs and complexity.\n    * **Template Creation:** Create reusable templates for our existing \"bedrock\/www, snippets, basket\" services and any common infrastructure components.\n    * **Parameterization:** Identify and parameterize environment-specific variables.\n    * **Testing and Deployment:**  Thoroughly test the new templated deployments in a staging environment before rolling them out to production.\n\nThis approach aligns with our existing practices of configuration-as-code and automated deployments, further enhancing consistency and reducing the risk of human error. \n","GenTime":"2024-08-19 16:29:59","text":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n\n\n##Decision\nUse helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n"}
{"File Name":"hee-web-blueprint\/0013-use-bloomreach-channel-manager.md","Context":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n","Decision":"Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","tokens":141,"id":1205,"Prediction":"## Decision\n\n**Document Management:** Leverage Bloomreach's native content management capabilities for document management instead of integrating an external platform. This decision prioritizes platform cohesion, simplifies development, and reduces potential points of failure.\n\n**Channel Management:** Utilize Bloomreach's 'Channel' concept for managing multiple sites like Deenary and Speciality sites. This decision leverages the platform's inherent multi-site capabilities and offers benefits in terms of:\n\n* **Streamlined Site Creation:** Bloomreach's interface and tooling aim to simplify the creation and management of new sites within the platform.\n* **Component Reusability:**  The 'Channel' concept should enable the creation and sharing of reusable components (e.g., UI elements, content modules) across different sites, enhancing development efficiency and consistency. \n* **Content Segregation:** Bloomreach's permissions model, combined with the 'Channel' structure, will allow for granular control over content visibility and access, ensuring that each site's content remains appropriately segregated.\n* **Content Sharing Flexibility:** The platform should enable content sharing between channels where necessary, allowing for both top-down (from a central source to specific sites) and bottom-up (from specific sites to a central repository) sharing as required.\n* **Centralized Permissions:**  Bloomreach's existing permission system will be configured to accommodate the 'Channel' structure, allowing for the definition of user roles and permissions that align with the content segregation and sharing requirements. \n\n**Rationale:** This approach emphasizes leveraging the inherent capabilities of the chosen platform (Bloomreach). By maximizing the use of native features, we aim to simplify the architecture, reduce development complexity, and streamline ongoing maintenance. This decision prioritizes platform cohesion and minimizes reliance on external integrations where possible. \n","GenTime":"2024-08-19 16:30:07","text":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n\n##Decision\nBloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n"}
{"File Name":"devops-challenge\/0001-record-architecture-decisions.md","Context":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":112,"id":3488,"Prediction":"## Decision\n\n**Decision Title:** Adopt a Service Mesh Architecture for Microservices Platform\n\n**Decision Description:** We will implement a service mesh architecture to address the requirements of scalability, automation, fault tolerance, and single point of failure identification for our microservices platform. \n\n**Reasoning:**\n\n* **Scalability and Automation:** Service meshes excel at handling inter-service communication challenges, including service discovery, load balancing, and traffic routing. This allows for easier scaling of individual services based on load and automates many operational aspects.\n* **Resilience and Fault Tolerance:** Features like circuit breaking, retries, and timeouts, inherent in service mesh solutions, enhance the platform's resilience.  They minimize the impact of failing services and provide graceful degradation.\n* **Single Point of Failure Identification:** Service meshes provide observability features like distributed tracing and metrics collection. These features help identify performance bottlenecks and potential single points of failure within the system.\n\n**Consequences:**\n\n* **Increased Complexity:** Introducing a service mesh adds complexity to the infrastructure. We need to account for the learning curve associated with the chosen service mesh technology.\n* **Operational Overhead:** Managing and maintaining a service mesh introduces some operational overhead. \n* **Potential Performance Impact:** While service meshes offer performance benefits, they can introduce a minor performance overhead due to added network hops. \n\n**Status:** Accepted\n\n**Alternatives Considered:**\n\n* **Developing custom solutions:** Building in-house solutions for each requirement (service discovery, load balancing, etc.) was considered. However, this approach would require significant development effort and increase maintenance complexity.\n* **Utilizing individual libraries:** While leveraging separate libraries for specific functionalities (e.g., a library for service discovery, another for circuit breaking) is possible, it lacks the comprehensive and integrated approach of a service mesh. \n\n**Chosen Option:** We will proceed with implementing a service mesh architecture using [**Specific Service Mesh Technology - e.g., Istio, Linkerd, Consul**]. The choice of the specific technology will be based on a detailed evaluation of our specific requirements, existing infrastructure, and team expertise. \n","GenTime":"2024-08-19 16:30:16","text":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"spin-archetype-base\/0002-unified-terminology.md","Context":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n","Decision":"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","tokens":34,"id":2270,"Prediction":"## Decision\n\nTo mitigate the risk of ambiguity and confusion caused by using different terms for the same concept, we will establish and enforce a **Unified Domain Language (UDL)** throughout the project. \n\nThis decision entails:\n\n* **Creating a glossary:** We will maintain a living document (e.g., a shared spreadsheet or dedicated wiki page) that defines all key domain terms and their agreed-upon names in the code. \n* **Code reviews:** During code reviews, special attention will be paid to ensuring adherence to the UDL. \n* **Automated checks (optional):** We will investigate and potentially implement automated tooling to identify and flag potential UDL violations in code and documentation.\n* **Regular review and updates:** The glossary will be reviewed and updated regularly, especially after adding new features or making significant changes to the codebase. \n\nThis approach promotes consistency and clarity, making the code easier to understand and maintain.\n","GenTime":"2024-08-19 16:30:20","text":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n\n##Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n"}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each ADR will be a separate Markdown file stored in a dedicated \"docs\/adr\" directory within the project repository. \n","GenTime":"2024-08-19 16:30:22","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"Prediction":"## Decision\n\n**We will leverage Microsoft Dataverse (previously known as Common Data Service) to store references to the Azure Blob files.** \n\nThis approach offers several advantages:\n\n1. **Seamless Integration:** Dataverse is tightly integrated with Dynamics 365, allowing for easy data access and manipulation from both the existing Dynamics UI and the web frontend via the Web API.\n2. **Security & Permissions:** Dataverse inherits the robust security model of Dynamics 365, ensuring that access to file references remains consistent with existing user permissions.\n3. **Extensibility:** Dataverse entities can be customized with additional fields to store relevant metadata about the Azure Blob files, such as file type, size, upload date, etc.\n4. **Filtering & Queries:** Dataverse allows for complex filtering and queries via the Web API, enabling the web frontend to efficiently retrieve specific files based on various criteria. \n\n**Implementation Details:**\n\n* A new custom entity will be created in Dataverse to store the Azure Blob file references.\n* This entity will have a lookup field to the relevant Case\/Incident entity, establishing a clear relationship between permits and their associated files.\n* Attributes for storing relevant file metadata (e.g., file name, blob URL, file size) will be added to the custom entity.\n* The existing Dynamics UI will be updated to display a link to the Azure Blob file, dynamically generated using the stored reference in the Dataverse entity.\n* The web frontend will utilize the Dynamics 365 Web API to query and retrieve file references from the Dataverse entity, allowing users to access and download the files stored in Azure Blob storage.\n\nThis decision leverages the existing capabilities of the Microsoft ecosystem to provide a robust, secure, and maintainable solution for linking permits and files stored in Azure Blob storage. \n","GenTime":"2024-08-19 16:30:30","text":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n\n\n##Decision\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"Prediction":"## Decision\n\n**Implement a two-step data extraction and validation process:**\n\n1. **Daily Snapshot:** Implement a process to ingest the daily data updation status for each treasury from the Koshvani platform. This snapshot will serve as an indicator of potential data changes.\n\n2. **Targeted Extraction and Validation:** Based on the daily snapshot, identify treasuries with potential data updates. For these treasuries, implement a targeted extraction process that retrieves both data and transaction timestamps. Validate the extracted data through the following mechanisms:\n    - **Cross-reference with previous snapshots:** Compare extracted data with previous snapshots to identify discrepancies and ensure data consistency over time.\n    - **Timestamp verification:** Validate the transaction timestamps against system logs or other reliable sources to confirm their accuracy.\n    - **Data integrity checks:** Implement data quality checks specific to the data fields being extracted to ensure completeness, accuracy, and reasonableness.\n\nThis two-step approach allows for efficient data extraction by focusing on specific treasuries with potential updates while maintaining data accuracy through comprehensive validation. \n","GenTime":"2024-08-19 16:30:35","text":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n\n\n##Decision\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"Prediction":"## Decision\n\nWe will migrate from a single ingress controller to a per-namespace ingress controller model. This means each namespace requiring an ingress controller will have its own, separate from other namespaces. \n\n## Consequences\n\n**Advantages:**\n\n* **Improved Stability and Scalability:** Each ingress controller manages a smaller, namespace-specific config file, reducing the risk of resource exhaustion and crashes. This allows for easier scaling as new services and namespaces are added.\n* **Fault Isolation:** A failure in one namespace's ingress controller will not impact other namespaces, improving the overall resilience of the platform. \n* **Increased Security:** Namespace isolation limits the blast radius of potential security vulnerabilities within a single ingress controller. \n\n**Disadvantages:**\n\n* **Increased Cost:**  Each ingress controller will require its own load balancer, increasing the overall infrastructure cost (estimated to be an additional $25\/month per namespace requiring ingress). \n* **Increased Management Complexity:**  Deploying and managing multiple ingress controllers adds some operational overhead compared to a single, centralized controller.\n\n## Justification\n\nWhile the cost and management overhead will increase slightly, the improved stability, scalability, and security offered by the per-namespace ingress controller model outweigh the drawbacks. This change is crucial to ensure the platform can handle future growth and complexity while maintaining a high level of reliability.\n\n## Next Steps\n\n1. **Evaluate ingress controller options:**  Assess different ingress controllers suitable for a per-namespace model (e.g., nginx, Traefik, Istio).\n2. **Develop migration plan:** Define a phased approach for migrating existing ingresses to the new model, minimizing disruption to existing services.\n3. **Implement monitoring and alerting:** Set up robust monitoring and alerting for each ingress controller to proactively identify and address potential issues. \n4. **Document the new architecture:** Update documentation to reflect the per-namespace ingress controller setup, including deployment procedures and troubleshooting guides. \n","GenTime":"2024-08-19 16:30:43","text":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n\n##Decision\nWe will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n"}
{"File Name":"tendermint\/adr-073-libp2p.md","Context":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n","Decision":"Tendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","tokens":387,"id":1982,"Prediction":"## Decision\n\nWe will adopt LibP2P as the networking stack for Tendermint, replacing the existing legacy connection\/socket handling code. This decision is driven by the following factors:\n\n* **Decoupling and Modularity:** LibP2P provides a clean separation of concerns between peer management, message handling, and higher-level business logic. This addresses the entanglement of responsibilities present in the legacy codebase, enabling greater modularity and flexibility for future development.\n* **Focus on Core Functionality:** Utilizing LibP2P's mature and well-defined networking stack allows the Tendermint team to focus on Tendermint-specific features and optimizations, rather than maintaining low-level networking code.\n* **Ecosystem Benefits:** Integrating with the LibP2P ecosystem provides access to a wide range of existing tools, libraries, and community support, fostering collaboration and innovation within the Tendermint ecosystem.\n* **Interoperability Trade-off:** While migrating to LibP2P will require breaking compatibility with legacy Tendermint versions (pre-0.35), the long-term benefits in terms of maintainability, scalability, and ecosystem integration outweigh the short-term costs.\n\nThis decision aligns with the goals outlined in the P2P Roadmap RFC and paves the way for a more robust, scalable, and feature-rich Tendermint network. \n","GenTime":"2024-08-19 16:30:49","text":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n\n\n##Decision\nTendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n"}
{"File Name":"optimint\/adr-005-serialization.md","Context":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n","Decision":"`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","tokens":27,"id":3746,"Prediction":"## Decision\n\n**Use a language-agnostic, schema-driven serialization library like Protocol Buffers or Apache Avro.**\n\n**Reasoning:**\n\n* **Efficiency:** Both Protocol Buffers and Avro prioritize compact binary representation and efficient serialization\/deserialization, crucial for KV store optimization and network bandwidth reduction.\n* **Schema Evolution:**  Schema evolution support is essential for long-term maintainability as data structures evolve. Both libraries provide robust schema management and forward\/backward compatibility.\n* **Language Agnosticism:** Choosing a language-agnostic format like Protocol Buffers or Avro ensures interoperability, enabling communication between different parts of the system potentially written in different languages.\n* **Wide Adoption and Support:** Both libraries have active communities, extensive documentation, and mature implementations in various programming languages, minimizing development effort and ensuring future support.\n\n**Alternatives Considered:**\n\n* **Language-specific serialization (e.g., Java serialization, Python's pickle):**  These are often tightly coupled to the language, hindering interoperability and potentially leading to security vulnerabilities.\n* **JSON or XML:**  While human-readable, these formats are verbose and not as performant as binary serialization options for large-scale data processing.\n\n**Consequences:**\n\n* **Learning Curve:** Developers need to familiarize themselves with the chosen library and its schema definition language.\n* **Schema Management:**  A robust process for versioning and managing schema changes is necessary. \n\n**Note:** The choice between Protocol Buffers and Avro depends on specific project requirements and priorities. Benchmarking might be necessary to evaluate performance differences for specific use cases. \n","GenTime":"2024-08-19 16:30:57","text":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n\n\n##Decision\n`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n"}
{"File Name":"educational-platform\/0011-use-axon-event-publishing-mechanism.md","Context":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n","Decision":"Axon Framework will be used for integration events implementation.\\n","tokens":79,"id":2500,"Prediction":"## Decision\n\nWe will use Axon Framework for publishing and listening to integration events instead of the Spring-event related classes. \n","GenTime":"2024-08-19 16:30:58","text":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n\n\n##Decision\nAxon Framework will be used for integration events implementation.\\n"}
{"File Name":"monitoring-doc\/0002-configuration-management.md","Context":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n","Decision":"We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","tokens":109,"id":1372,"Prediction":"## Decision\n\nWe will migrate away from using `cloud.conf` for provisioning additional resources to base cloud instances. Instead, we will adopt a solution that leverages configuration management tools like Ansible or Puppet. This approach offers several advantages:\n\n* **Overcomes the 16kb configuration limit:** Configuration management tools do not impose such limitations, allowing for more complex and comprehensive resource provisioning.\n* **Eliminates duplication across instances:** By defining the desired state once, we can apply it consistently across all instances without redundancy.\n* **Facilitates rapid feedback testing:** Configuration management tools often provide features like dry runs and idempotent operations, enabling quicker iterations and reducing the risk of unintended consequences.\n\nThis decision aligns with our goal of enhancing maintainability, scalability, and agility in our infrastructure management practices. \n","GenTime":"2024-08-19 16:31:02","text":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n\n\n##Decision\nWe have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n"}
{"File Name":"contact-frontend\/0003-remove-login-redirection.md","Context":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n","Decision":"* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","tokens":314,"id":1754,"Prediction":"## Decision\n\n**We will remove the requirement for login for all contact-frontend standalone pages and decommission the authenticated routes.**\n\nThis decision is based on the following:\n\n* **Improved user experience:** Removing the login requirement simplifies the user journey and avoids unnecessary redirects, especially in scenarios where users might be logged out unexpectedly. This is crucial for ensuring a smooth experience, particularly when users are reporting issues. \n* **Enhanced accessibility:** Eliminating login barriers improves accessibility for all users, including those who might face challenges with login processes. This encourages more widespread user feedback and benefits the platform's development through diverse input.\n* **Reduced development complexity:** Maintaining separate code paths for authenticated and unauthenticated access increases complexity and testing burden. By standardizing on unauthenticated access, we streamline the codebase, reduce development time, and simplify future maintenance.\n\n**Consequences:**\n\n* The existing authenticated routes will be decommissioned and replaced with redirects to their unauthenticated counterparts.\n* The contact-frontend codebase will be refactored to remove the logic handling authentication for standalone pages, simplifying the code and reducing potential points of failure. \n* We will need to update documentation and potentially notify users about the change, although this impact should be minimal as all existing functionality will remain accessible.\n\nThis decision directly addresses the identified problem of unnecessary login requirements and prioritizes a better user experience, improved accessibility, and a more maintainable codebase. \n","GenTime":"2024-08-19 16:31:08","text":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n\n\n##Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n"}
{"File Name":"CrossyToad\/adr-0008-use-lenses.md","Context":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n","Decision":"It's Lens time!\\n","tokens":118,"id":2480,"Prediction":"## Decision\n\nWe will use the `lens` library to manage our game's nested state for the following reasons:\n\n* **Improved ergonomics for nested state manipulation:** Lenses provide a more concise and readable syntax for accessing and updating deeply nested data structures compared to traditional Haskell patterns. This will improve developer productivity and reduce the likelihood of errors.\n* **Potential for modularity with \"classy lenses\":** Classy lenses offer a way to abstract state access and modification for different game subsystems. This can lead to a more modular and maintainable codebase by reducing coupling between different parts of the game.\n\nHowever, we acknowledge that `lens` introduces a significant amount of abstraction and may have a steeper learning curve for some developers. To mitigate this, we will:\n\n* **Provide comprehensive documentation and examples:**  We will create internal documentation and tutorials on using lenses effectively within our game's codebase.\n* **Start with a gradual adoption:** We will initially focus on using lenses in areas where their benefits are most apparent (e.g., complex state updates). This will give the team time to learn and adapt to the library without overwhelming them.\n* **Regularly review and refactor our lens usage:** We will hold code reviews and discussions to ensure that our use of lenses remains clear, concise, and maintainable over time.\n\nWe believe that the benefits of using `lens` for our game's state management outweigh the potential drawbacks. The increased clarity, maintainability, and potential for modularity will contribute to a more robust and scalable codebase in the long run.\n","GenTime":"2024-08-19 16:31:15","text":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n\n\n##Decision\nIt's Lens time!\\n"}
{"File Name":"handbook\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":1996,"Prediction":"## Decision\n\nWe will adopt a single naming convention for all files and directories within the monorepo. This convention will be enforced through linting rules and integrated into our CI\/CD pipeline to prevent inconsistencies from being introduced.\n\n**Specifics:**\n\n* **Directory naming:**  [Choose one: `kebab-case`, `snake_case`, `PascalCase`, `camelCase`]\n* **File naming:** [Choose one: `kebab-case`, `snake_case`, `PascalCase`, `camelCase`] - **Must** be consistent with directory naming\n* **Component naming:**  [Choose one: `kebab-case`, `snake_case`, `PascalCase`, `camelCase`] \n* **Configuration files:** Retain the default naming conventions of the respective tools (e.g., `.eslintrc.json`, `tsconfig.json`).\n* **NX specific overrides:**  While NX offers defaults, we will configure it to align with the chosen naming convention for generated code and artifacts.\n\n**Rationale:**\n\n* **[Chosen casing type]:**  [Provide a brief justification for the chosen casing type, e.g., readability, commonality in the chosen tech stack, etc.]\n* **Linting and CI\/CD enforcement:** Automating the enforcement ensures adherence to the chosen convention and prevents future inconsistencies.\n\n**Consequences:**\n\n* **Benefits:**\n    * Improved codebase navigation and discoverability.\n    * Reduced cognitive load for developers.\n    * Easier maintenance and refactoring.\n* **Drawbacks:**\n    * Initial effort required to refactor existing code to adhere to the new convention.\n    * Potential for merge conflicts during the transition period. \n\n**Migration strategy:**\n\n* We will implement the new naming convention for all new code going forward.\n* For existing code, we will address the naming inconsistencies gradually through:\n    * **Dedicated refactoring sprints:** Allocating specific time for teams to refactor their respective sections of the codebase.\n    * **Incremental refactoring during feature development:** Encouraging developers to address naming inconsistencies within the scope of their current work.\n* We will leverage automated tools (e.g., codemods) to facilitate the refactoring process.\n","GenTime":"2024-08-19 16:31:25","text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n"}
{"File Name":"lbh-frontend-react\/0010-use-typedoc-to-generate-api-documentation.md","Context":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":1410,"Prediction":"## Decision\n\nWe will use TypeDoc to generate API documentation for our TypeScript code. \n\n## Reasoning\n\nTypeDoc offers a number of advantages over maintaining separate API documentation:\n\n* **Reduced duplication:** TypeDoc leverages TypeScript's type definitions, eliminating the need to duplicate type information in JSDoc comments.\n* **Automatic updates:**  API documentation stays consistent with the codebase as TypeDoc automatically reflects changes in types and comments.\n* **Improved maintainability:**  Keeping documentation and code aligned requires less manual effort, reducing the risk of discrepancies. \n* **JSDoc familiarity:** TypeDoc builds upon the familiar JSDoc syntax, easing the learning curve for developers already familiar with JSDoc.\n\nThis approach promotes accurate, up-to-date documentation with minimal overhead, leading to a better developer experience. \n","GenTime":"2024-08-19 16:31:28","text":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n\n\n##Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n"}
{"File Name":"alfresco-anaxes-shipyard\/0006-helm-chart-repository.md","Context":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n","Decision":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","tokens":266,"id":3602,"Prediction":"## Decision\n\nAfter careful consideration of the context and criteria, we have decided to use **GitHub Pages** as our Helm Chart Repository. \n\nThis decision is based on the following factors:\n\n* **Meets Key Criteria:** GitHub Pages fulfills our essential requirements:\n    * **AWS and Quay Agnostic:** It operates independently, eliminating dependencies on specific cloud providers or container registries.\n    * **Mature and Stable:** As a well-established platform, GitHub Pages offers a reliable and proven solution.\n    * **No IT Support Required:** Its ease of use and integration with existing workflows minimize IT overhead. \n    * **Custom Domain Support:** GitHub Pages allows for custom domain mapping, aligning with our branding needs. \n* **Cost-Effective:** Leveraging our existing GitHub organization makes this a highly cost-effective solution.\n* **Simplicity and Familiarity:**  The development team is already familiar with GitHub, simplifying adoption and ongoing management. \n\nWhile GitHub Pages does not inherently offer PaaS\/Cloud Storage features in the traditional sense, its static website hosting capabilities are sufficient for serving Helm charts. \n\nWe acknowledge that solutions like ChartMuseum and Harbor provide more advanced features, but for our current needs, the simplicity, familiarity, and cost-effectiveness of GitHub Pages outweigh the benefits of a more complex solution. This decision prioritizes a pragmatic approach, allowing us to quickly establish a functional and maintainable Helm Chart Repository. \n","GenTime":"2024-08-19 16:31:35","text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n\n\n##Decision\nWe will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n"}
{"File Name":"hee-web-blueprint\/0008-use-npm-to-host-hee-frontend-framework.md","Context":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n","Decision":"We have chosen to store our packages on NPM.\\n","tokens":47,"id":1202,"Prediction":"## Decision\n\nWe will use NPM as our package repository for front-end code. \n\n## Reasons\n\n* **Alignment with NHS Standards:** Utilizing NPM aligns with existing practices within the NHS UK and NHS Digital ecosystems, promoting interoperability and knowledge sharing.\n* **Mature Ecosystem:** NPM is a mature and widely adopted package manager, offering a vast repository of open-source packages and tools, potentially speeding up development. \n* **Community Support:** The large community surrounding NPM provides access to ample documentation, tutorials, and support forums, which can be beneficial for troubleshooting and knowledge acquisition. \n\n## Considered Options\n\n* **Other Package Managers (Yarn, pnpm):** While these offer potential performance or feature advantages, the alignment with existing NHS practices and the maturity of the NPM ecosystem outweigh these benefits in this context. \n* **Private Registry:** This option might be reconsidered if we have strong security or intellectual property concerns requiring stricter control over package distribution. \n\n## Consequences\n\n* **Potential for Dependency Conflicts:** Utilizing a public repository necessitates careful dependency management to avoid potential conflicts, especially considering the large number of existing NHS projects using NPM. \n* **Open Source Considerations:** Publishing packages publicly requires adherence to open-source best practices and licensing agreements.\n","GenTime":"2024-08-19 16:31:40","text":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n\n\n##Decision\nWe have chosen to store our packages on NPM.\\n"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"Prediction":"## Decision\n\nWe will migrate the HospitalRun Frontend from Redux to a combination of React Query and the Context API. \n\n**React Query** will be used for managing asynchronous data fetching, caching, and updates. This addresses the core functionality of fetching and managing data from PouchDB\/CouchDB, replacing Redux Thunk and simplifying our data handling logic. \n\n**React's Context API** will be employed for the limited use cases requiring global application state, such as session information management. This offers a lightweight and integrated solution within the React ecosystem for these specific scenarios. \n\nThis decision is driven by the following factors:\n\n* **Reduced Boilerplate:** Moving away from Redux significantly reduces the verbosity and boilerplate code currently present in the application.\n* **Improved Testing:** Isolating component testing becomes simpler as we decouple data fetching and state management from the components themselves. This allows for more focused and manageable tests.\n* **Leveraging Specialized Tools:** React Query offers a dedicated and optimized solution for asynchronous data management in React, while Context API provides a native mechanism for limited global state sharing.\n* **Alignment with Current Trends:** This migration aligns the HospitalRun Frontend with modern best practices and popular libraries within the React community, making the codebase more maintainable and accessible to developers familiar with these tools. \n","GenTime":"2024-08-19 16:31:46","text":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n\n\n##Decision\nHospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n"}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project. Each decision will be documented in a separate Markdown file within a dedicated \"docs\/adr\" directory at the root of the project repository. \n","GenTime":"2024-08-19 16:31:48","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"Prediction":"## Decision\n\nWe will use a **hybrid approach** to ensure sufficient memory capacity for AZ failure:\n\n1. **Quota-based reservation:** Maintain at least 50% of total org reserved memory capacity available across the remaining zones when one zone fails. This ensures a baseline capacity buffer. \n2. **Real-time monitoring and dynamic scaling:**  Continuously monitor actual memory usage using the `show-cf-memory-usage` tool. Implement dynamic scaling mechanisms to deploy additional cells if available capacity falls below a predefined threshold (e.g., 20% of total reserved capacity).\n\n## Justification\n\nWhile reserving 50% of total org reserved capacity provides a safety margin, it might lead to over-provisioning due to the discrepancies between reserved and actual memory usage. By incorporating real-time monitoring and dynamic scaling, we can:\n\n* **Optimize resource utilization:** Avoid unnecessary cell deployments and associated costs.\n* **Enhance availability:**  Proactively respond to unexpected spikes in memory consumption, ensuring application uptime even with fluctuating workloads.\n\nThis hybrid approach balances cost-effectiveness with high availability, ensuring our platform remains resilient to AZ failures. \n","GenTime":"2024-08-19 16:31:53","text":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n\n\n##Decision\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n"}
{"File Name":"katas\/003-katafication.md","Context":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"Prediction":"## Decision\n\nWe will implement a continuous integration (CI) pipeline that runs all katas against the target testing engine on every commit. \n\n## Rationale\n\nStoring failing tests in the repository is acceptable if we can guarantee that:\n\n1. **We know which tests are expected to fail and why.**  This information should be clearly documented within each kata.\n2. **We have a mechanism to automatically verify if a kata still fails as expected.** This prevents accidental fixes and ensures the kata remains relevant.\n\nA CI pipeline addresses both points:\n\n* **Automated Verification:** The pipeline will execute all katas and compare the results against the expected outcome (failure). If a kata unexpectedly passes, the pipeline will flag it, indicating a potential issue with the kata or the testing engine.\n* **Documentation:** The CI configuration can act as documentation, explicitly listing which katas are expected to fail and on which platform\/engine combination.\n\n## Consequences\n\n* **Increased Development Cost:** Setting up and maintaining the CI pipeline requires initial effort and ongoing maintenance.\n* **Potential for False Positives\/Negatives:** The CI pipeline needs careful configuration and robust test cases to minimize false positives (flagging correctly failing tests) and false negatives (missing unexpectedly passing tests). \n\nHowever, these drawbacks are outweighed by the benefits:\n\n* **Improved Reliability:**  The CI pipeline ensures the katas remain relevant and function as intended, providing a reliable resource for fixing failing tests. \n* **Reduced Risk of Regression:** The automated checks prevent accidental fixes, ensuring the katas consistently reproduce the intended failure.\n\nThis approach enables us to leverage failing tests as a valuable resource while minimizing the risks associated with storing them directly in the repository. \n","GenTime":"2024-08-19 16:32:00","text":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n\n\n##Decision\nAll tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n"}
{"File Name":"mediawiki-extensions-WikibaseLexeme\/0003-adr-with-all-team-members.md","Context":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n","Decision":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","tokens":214,"id":1563,"Prediction":"## Decision\n\nAll ADRs must undergo a time-boxed Request For Comments (RFC) process involving the entire development team for review and sign-off. This ensures wider feedback, avoids single-journey bias, and increases the overall quality and acceptance of architectural decisions. The RFC process will have a defined time limit to prevent delays in the journey team's progress. \n","GenTime":"2024-08-19 16:32:03","text":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n\n\n##Decision\nWe put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n"}
{"File Name":"claim-additional-payments-for-teaching\/0004-deployment-on-heroku.md","Context":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n","Decision":"We will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","tokens":84,"id":2093,"Prediction":"## Decision\n\nWe will use two separate Azure subscriptions for this project:\n\n1. **DfE Cloud Infrastructure Program Subscription:** This subscription will be used for production deployments and any environments subject to the same strict access restrictions. We will work with the DfE Cloud Infrastructure Program to provision and configure the necessary resources in this subscription.\n\n2. **Project-Specific Sandbox Subscription:** This subscription will be used for development, testing, prototyping, and user research. It will have less restrictive access controls to enable rapid iteration and experimentation.  We will strive to adhere to DfE security best practices while maintaining a flexible development environment. \n\nThis dual subscription approach offers several benefits:\n\n* **Agility:** Developers can rapidly iterate and deploy in the sandbox subscription without being hindered by production-level restrictions.\n* **Security:** Production environments remain protected within the DfE Cloud Infrastructure Program subscription, adhering to their security standards. \n* **Flexibility:** The project-specific subscription allows for experimentation with different Azure services and configurations without impacting the production environment.\n\nWe will explore options for automating deployments and infrastructure management across both subscriptions to ensure consistency and reduce manual effort.  As the project matures and features stabilize, we will transition them from the sandbox subscription to the DfE Cloud Infrastructure Program subscription following their established processes. \n","GenTime":"2024-08-19 16:32:08","text":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n\n\n##Decision\nWe will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n"}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"Prediction":"## Decision\n\nWe will implement the vector tile endpoint within the existing Django web application instead of setting up a dedicated `ST_AsMVT`-based vector tile server. \n","GenTime":"2024-08-19 16:32:10","text":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n\n\n##Decision\nWe have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n"}
{"File Name":"architecture\/0004-waiting-is-waste.md","Context":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n","Decision":"Any processes and workflows need to be very fast and fully automated.\\n","tokens":43,"id":1434,"Prediction":"## Decision\n\nWe will adopt a **highly asynchronous and event-driven architecture** to minimize blocking operations and developer wait times. This means:\n\n* **Asynchronous processing for long-running tasks:**  Operations like builds, deployments, tests, and data processing should be handled asynchronously.  Developers should be able to initiate these operations and continue working on other tasks without waiting for them to complete.\n* **Real-time feedback and notifications:**  The system should provide real-time updates and notifications about the status of asynchronous operations. Developers should be able to monitor progress and receive alerts about failures or completions without actively polling. \n* **Event-driven architecture for loose coupling:** Different components of the system should communicate through events. This allows for independent scaling and development of different parts of the system without introducing dependencies that lead to blocking.\n* **Prioritize tooling and automation:** Invest in tools and frameworks that support asynchronous workflows, automated testing, and continuous integration\/continuous delivery (CI\/CD). This will further minimize manual intervention and wait times for developers.\n\n**Rationale:**\n\nThis approach prioritizes developer productivity and agility by eliminating unnecessary wait times. Asynchronous processing and real-time feedback empower developers to work efficiently and respond to issues quickly.\n","GenTime":"2024-08-19 16:32:15","text":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n\n\n##Decision\nAny processes and workflows need to be very fast and fully automated.\\n"}
{"File Name":"terraform-aws-dynamodb\/0007-map-list-variables.md","Context":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n","Decision":"The module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","tokens":80,"id":2543,"Prediction":"## Decision\n\nWe will implement a solution where additional attributes for DynamoDB tables, local secondary indexes, and global secondary indexes are handled as structured objects instead of maps\/lists. This approach offers several advantages:\n\n**1. Type Safety and Validation:** \nRepresenting attributes as structured objects allows for defining specific types for each attribute. This enables compile-time type checking and validation, reducing the risk of runtime errors due to incorrect data types.\n\n**2. Code Clarity and Maintainability:**\nUsing structured objects instead of generic maps\/lists makes the code more readable and easier to understand. Developers can clearly see the structure and purpose of each attribute, improving maintainability.\n\n**3. Automated Handling of Range\/Hash Keys:**\nWithin the structured object representation, the logic for automatically including range\/hash keys when declared can be encapsulated. This removes the burden from the developer and ensures consistency in handling these keys.\n\n**4. Improved Developer Experience:**\nBy abstracting the complexity of map\/list manipulation and key management, we provide a more intuitive and user-friendly interface for developers interacting with the DynamoDB AWS Provider.\n\n**Implementation Details:**\n\n- Define specific classes or types to represent the structure of additional attributes for tables, LSIs, and GSIs.\n- Implement logic within these classes to automatically include range\/hash keys if declared in the object.\n- Update the DynamoDB AWS Provider to consume and process these structured objects, converting them to the required maps\/lists format internally before interacting with the DynamoDB API.\n\nThis approach strikes a balance between flexibility and structure, providing a robust and developer-friendly way to manage additional attributes in DynamoDB resources. \n","GenTime":"2024-08-19 16:32:23","text":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n\n\n##Decision\nThe module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n"}
{"File Name":"k8s-vagrant-centos-cluster\/0002-why-not-use-kubeadm.md","Context":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n","Decision":"Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","tokens":26,"id":3440,"Prediction":"## Decision\n\nWe will leverage kubeadm for bootstrapping the initial Kubernetes cluster, encompassing core components such as the API server, etcd, controller manager, and scheduler. This decision is based on the following rationale:\n\n* **Simplified Cluster Creation:** kubeadm streamlines the cluster setup process, automating complex configurations and reducing manual intervention.\n* **Production-Ready Best Practices:**  kubeadm promotes best practices aligned with production-grade Kubernetes deployments, ensuring a robust and reliable foundation.\n* **Active Community and Support:** As an official Kubernetes tool, kubeadm benefits from a vibrant community and robust support, facilitating troubleshooting and issue resolution. \n* **Flexibility and Extensibility:** While we leverage kubeadm for bootstrapping, we retain flexibility for customization.  We can extend the cluster with additional components and configurations tailored to our specific requirements. \n\nHowever, it's important to acknowledge potential considerations:\n\n* **Learning Curve:** Familiarity with kubeadm concepts and configuration options is necessary for effective utilization. \n* **Customization Trade-offs:** While kubeadm offers flexibility, extensive customization might necessitate deeper engagement with underlying Kubernetes configurations.\n\nWe will mitigate these considerations through:\n\n* **Documentation and Training:**  Comprehensive documentation and targeted training will ensure team proficiency with kubeadm.\n* **Phased Rollout:** We will adopt a phased approach to cluster deployment, allowing for iterative learning and adjustments based on experience.\n","GenTime":"2024-08-19 16:32:29","text":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n\n\n##Decision\nSetup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"Prediction":"## Decision\n\nDue to the exploratory nature of the Alpha phase, we will **not** integrate with the live production database for the Children's Social Care Placement Service. Instead, we will use a combination of:\n\n* **Mock data:** For initial prototyping and UI development, we will use static, manually-created mock data. This offers the fastest way to iterate on design and functionality without the complexity of a data store.\n* **In-memory database:** As the prototype matures and requires more dynamic data handling, we will use an in-memory database solution like SQLite. This offers lightweight persistence and basic data querying capabilities without the overhead of setting up and managing a full-fledged database server. \n\nThis approach allows for rapid prototyping and experimentation without jeopardizing the integrity of real user data. It also simplifies the development environment and reduces potential security risks associated with handling sensitive information.\n\nWe will re-evaluate this decision at the end of the Alpha phase and determine the most appropriate data persistence strategy for the subsequent Beta phase.\n","GenTime":"2024-08-19 16:32:33","text":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n\n\n##Decision\nThe placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n"}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"Prediction":"## Decision\n\n**Remove support for FreeTFT and TFTA tokens from the SDK.** \n\nThis decision entails the following actions:\n\n* Deprecate and remove all functions, classes, and modules specifically related to FreeTFT and TFTA tokens within the SDK.\n* Update the SDK documentation to reflect the removal of FreeTFT and TFTA support, outlining alternative solutions and migration paths for users relying on these functionalities.\n* Communicate the deprecation and removal plan to users with a clear timeline, emphasizing the rationale behind this decision. \n\n**This decision will be considered final unless significant technical or strategic challenges arise during implementation.**\n","GenTime":"2024-08-19 16:32:37","text":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n\n##Decision\nCompletely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n"}
{"File Name":"dos-server\/adr-14-persistent-file-links.md","Context":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n","Decision":"Handle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","tokens":187,"id":3369,"Prediction":"## Decision\n\nWe will use the Handle System to generate persistent identifiers for files ingested by DOS. \n\n## Reasoning\n\nWhile DOS can generate its own persistent identifiers, using the Handle System offers several advantages:\n\n* **Externalization:**  Decoupling identifier generation from DOS promotes reusability and portability. Other systems can leverage the Handle System for their persistent identifier needs. \n* **Existing Infrastructure:** The MIT Libraries already utilizes the Handle System in Dome and DSpace. Leveraging this existing infrastructure reduces development and maintenance efforts.\n* **Organizational Familiarity:**  Existing knowledge and experience with the Handle System within the organization translates to faster implementation and smoother operations.\n* **Mitigation of Future Changes:**  Using persistent identifiers through the Handle System eliminates the need to update ArchivesSpace links if the underlying DOS file URLs change (e.g., due to bucket name changes). This future-proofs the system and reduces maintenance overhead. \n","GenTime":"2024-08-19 16:32:41","text":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n\n\n##Decision\nHandle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n"}
{"File Name":"tendermint\/adr-063-privval-grpc.md","Context":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n","Decision":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","tokens":147,"id":1949,"Prediction":"## Decision\n\n**Migrate Tendermint's private validator client to use gRPC for communication with remote signers.**\n\nThis decision addresses the identified issues by:\n\n* **Replacing custom protocols:** gRPC offers a standardized and secure communication protocol, eliminating the need for `SecretConnection` and raw TCP\/Unix socket connections. \n* **Reducing integration complexity:** As a widely adopted protocol with multiple language implementations, gRPC simplifies integration for developers using various technology stacks.\n* **Improving security:** gRPC leverages TLS encryption by default, mitigating the man-in-the-middle attacks previously possible with the custom secure connection.\n* **Lowering maintenance burden:** By adopting a well-supported and documented protocol, maintaining and troubleshooting the communication layer becomes more straightforward. \n\nThis migration will result in a more secure, maintainable, and developer-friendly solution for integrating remote signers with Tendermint validators.\n","GenTime":"2024-08-19 16:32:45","text":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n\n\n##Decision\nAfter discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n"}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"Prediction":"## Decision\n\nWe will implement an offline access mechanism for the smart fridge using pre-generated access codes. \n\nHere's how it will work:\n\n1. **Unique Meal IDs:** Each meal will be assigned a unique ID by the kitchen during preparation. \n2. **Access Code Generation:** Upon purchase\/order confirmation, the system will generate a unique access code linked to the meal ID and the user's account. This code can be:\n    * **Pushed to User Device:**  Sent to the user's smartphone app.\n    * **Printed on Label:** Included on the meal's label for manual entry.\n3. **Offline Authentication:** When a user wants to access a meal:\n    * If online, the fridge will function as usual, verifying access via card or app.\n    * If offline, the user will input the meal's unique access code on the fridge's keypad. \n4. **Local Verification:** The fridge will validate the entered access code against a locally stored, periodically synced database of valid codes.\n5. **Access Granted:** Upon successful verification, the fridge will unlock and grant access to the corresponding meal compartment.\n\n## Benefits\n\n* **Resilience:** Enables meal access even during network outages, improving system reliability.\n* **Customization:** Supports customized meal delivery by linking access codes to specific users and meal IDs.\n* **Offline Functionality:** Enhances user experience by providing uninterrupted access to meals. \n\n## Drawbacks\n\n* **Security Considerations:** Requires robust security measures for local code storage and validation to prevent unauthorized access.\n* **Synchronization:** Demands a reliable mechanism for synchronizing the local access code database with the central system.\n* **User Education:**  Necessitates clear instructions and user interface elements to guide users through the offline access process.\n\nWe will further investigate secure local storage options, code generation algorithms, and synchronization mechanisms to ensure a secure and user-friendly offline access system. \n","GenTime":"2024-08-19 16:32:53","text":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n\n\n##Decision\nMeals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n"}
{"File Name":"beis-report-official-development-assistance\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":2387,"Prediction":"## Decision\n\n**Maintain using Rollbar as our primary error tracking tool for existing applications, while allowing new applications to adopt Sentry.**\n\n**Justification:**\n\n* **Consistency for existing projects:**  Switching all 14 applications from Rollbar to a new tool would require significant effort and introduce potential disruption. Sticking with Rollbar for these applications ensures consistency and avoids unnecessary migration costs. \n* **Flexibility for new projects:** Allowing new projects to adopt Sentry acknowledges the potential benefits of exploring alternative tools and caters to developer preferences.  This approach promotes exploring new technologies without disrupting existing workflows.\n* **Address environment naming inconsistencies:** For Rollbar, standardize environment naming conventions and update documentation to clearly explain how to configure `ROLLBAR_ENV` to match Rails environments. This will minimize confusion and improve error tracking accuracy.\n\n**Consequences:**\n\n* **Potential for tool sprawl:** Using two error tracking tools might increase maintenance overhead and complicate issue tracking across different applications. \n* **Limited leverage with Rollbar:** Continuing with Rollbar for a subset of applications may limit our negotiating power for pricing and features.\n* **Learning curve for new tool:**  Adopting Sentry will require investing time in learning the new tool and integrating it into new projects. \n\n**Action Items:**\n\n* **Document Rollbar environment naming conventions:** Create clear guidelines for setting `ROLLBAR_ENV` and update relevant documentation.\n* **Evaluate Sentry for future projects:** Define criteria for selecting error tracking tools and conduct a thorough evaluation of Sentry based on these criteria.\n* **Monitor tool usage and feedback:** Regularly gather feedback from developers regarding their experience with both Rollbar and Sentry to inform future decisions about error tracking tools. \n\nThis approach provides a balance between maintaining stability for existing projects and allowing flexibility for new ones while addressing the identified issues with Rollbar. Continuous monitoring and evaluation will be crucial to ensure that our chosen tools remain effective and meet our evolving needs. \n","GenTime":"2024-08-19 16:33:02","text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n"}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"Prediction":"## Decision\n\n**Adopt a comprehensive cloud resource tagging strategy to ensure consistent naming, identification, and management of all cloud assets.** This strategy will encompass the following:\n\n**1. Naming Convention:**\n\n* **Descriptive and Consistent:**  Resource names should be clear, concise, and follow a consistent pattern across all resource types. This will make it easy to identify and search for specific resources. \n* **Include Environment:**  Clearly distinguish resources across different environments (e.g., development, testing, production) using prefixes or suffixes in the name.\n* **Automated Where Possible:** Leverage infrastructure-as-code tools to enforce naming conventions during provisioning and prevent deviations.\n\n**2. Tagging Standard:**\n\n* **Mandatory Tags:** Define a set of mandatory tags that must be applied to all resources. These tags should capture essential information like:\n    * **Environment:**  (e.g., \"dev\", \"test\", \"prod\")\n    * **Application:** The application or service the resource belongs to.\n    * **Owner:**  Team or individual responsible for the resource.\n    * **Cost Center:**  The business unit responsible for the cost of the resource.\n* **Optional Tags:** Allow for optional tags to capture additional context-specific information, such as project, data sensitivity level, or compliance requirements.\n* **Tagging Policy:** Implement a tagging policy document that outlines the tagging standards, mandatory tags, naming conventions, and responsibilities for tagging resources.\n\n**3. Tooling and Automation:**\n\n* **Utilize Cloud-Native Tagging Tools:** Leverage the tagging capabilities provided by the cloud service provider (e.g., AWS Resource Groups and Tag Editor, Azure Resource Manager) to manage and enforce tags.\n* **Integrate with Infrastructure-as-Code:**  Embed tagging within IaC templates (e.g., Terraform, CloudFormation) to automate tagging during resource provisioning.\n* **Automated Tagging Validation:** Implement tools or scripts to automatically validate resource tags for compliance with the defined tagging policy.\n\n**4. Governance and Enforcement:**\n\n* **Establish Ownership and Accountability:** Define clear ownership and accountability for resource tagging across different teams and individuals.\n* **Regular Audits:** Conduct periodic audits to ensure compliance with the tagging strategy and identify any inconsistencies or missing tags.\n* **Automated Remediation:**  Implement automated processes to correct tagging errors or inconsistencies detected during audits.\n\nBy implementing a comprehensive cloud resource tagging strategy, we can:\n\n* Improve the organization and discoverability of cloud assets.\n* Simplify cost allocation and chargeback\/showback reporting.\n* Enhance resource management and optimization.\n* Strengthen security posture by identifying and managing resources based on their tags. \n","GenTime":"2024-08-19 16:33:13","text":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n\n\n##Decision\nThe naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n"}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"Prediction":"## Decision\n\n**We will implement a clearly defined API layer to decouple the kernel (backend) and Unity (frontend) components.** \n\nThis decision promotes a clean separation of concerns and technology agnosticism.\n\n**Justification:**\n\n* **Reduced Coupling:**  A well-defined API acts as a contract between the frontend and backend, allowing them to evolve independently without direct dependencies. \n* **Improved Maintainability:**  Changes in one layer (e.g., Unity UI update) will have minimal impact on the other (e.g., kernel business logic) as long as the API contract is respected.\n* **Enhanced Collaboration:**  Frontend and backend teams can work autonomously, reducing blockers and facilitating parallel development.\n* **Technology Flexibility:**  The API layer allows for future technology changes in either the kernel or Unity without requiring significant rework.\n\n**Consequences:**\n\n* **Initial Development Effort:**  Designing, building, and documenting the API layer will require upfront investment.\n* **Increased Communication Overhead:**  Clear communication and documentation of API changes are crucial to ensure both teams are aligned.\n\n**Next Steps:**\n\n1. **Define API Specifications:** Determine the resources, endpoints, request\/response formats, and authentication methods.\n2. **Technology Selection:**  Choose an appropriate technology stack for the API layer (e.g., RESTful API with JSON, GraphQL).\n3. **Implement and Document:** Build the API layer and provide comprehensive documentation for both frontend and backend teams.\n4. **Migrate Existing Logic:** Gradually migrate existing business logic and data access operations to the kernel side of the API.\n5. **Update Unity Integration:** Refactor Unity code to interact with the kernel exclusively through the defined API. \n","GenTime":"2024-08-19 16:33:20","text":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n\n\n##Decision\n* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n"}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"Prediction":"## Decision\n\nGiven the context of cost constraints, limited but recurring internet availability, and the increasing prevalence of mobile networks and cloud solutions, we will adopt a hybrid approach for OpenCHS's architecture:\n\n**1. Local-first Data Storage and Processing:**\n\n- OpenCHS will utilize a client-side database (e.g., SQLite) for data storage and processing on individual devices used by health workers.\n- This eliminates the need for a dedicated Postgres instance per installation, reducing costs and maintenance overhead for NGOs. \n- Health workers can work offline seamlessly, entering data and accessing existing patient information even without internet access.\n\n**2. Centralized Cloud Synchronization:**\n\n- A central cloud-based server will act as the primary data repository and synchronization hub.\n- OpenCHS will leverage intermittent internet connectivity at head offices to synchronize data between the central server and local devices.\n- Synchronization can be automated to occur periodically or triggered manually by users when a connection is available.\n\n**3. Future-Proofing with Cloud Services:**\n\n- The cloud-based infrastructure will allow for future scalability and integration with analytics, reporting, and other cloud-based services. \n- As 3G\/4G connectivity improves, OpenCHS can leverage these networks for more frequent data synchronization and potential real-time collaboration features.\n\n**Justification:**\n\nThis hybrid approach addresses all the concerns highlighted in the context:\n\n- **Cost-Effectiveness:** Local-first architecture eliminates the need for dedicated server infrastructure per installation, reducing costs and simplifying maintenance.\n- **Offline Availability:** Health workers can operate offline effectively, ensuring continuity of care even in areas with limited or no internet access.\n- **Data Centralization:** Cloud synchronization ensures data consistency, facilitates reporting and analysis, and enables data sharing among authorized stakeholders. \n- **Future Readiness:** The architecture is designed to adapt to future technological advancements, including wider 3G\/4G adoption and the increasing availability of cloud-based services. \n","GenTime":"2024-08-19 16:33:29","text":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n\n\n##Decision\nOpenchs server will have capabilities to be deployed on the cloud\\n"}
{"File Name":"tendermint\/adr-014-secp-malleability.md","Context":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n","Decision":"We can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","tokens":356,"id":1957,"Prediction":"## Decision\n\nTo mitigate the risks associated with the second layer of signature malleability in Secp256k1, we will implement deterministic signature generation and verification. This will involve the following:\n\n**1. Deterministic Signature Generation:**\n\n- We will adopt a standardized deterministic signature scheme such as **RFC 6979**. This scheme deterministically derives the signature nonce (`k`) from the message hash and private key, ensuring that for a given message and private key pair, the same signature is always generated.\n\n**2. Signature Normalization:**\n\n- During signature verification, we will enforce normalization of the `s` value in the signature. This means that if `s` is greater than half the curve order (`n\/2`), we will use its complement (`n - s`) instead. This ensures that only the smaller of the two possible `s` values is considered valid, eliminating the malleability arising from the two valid `(r, s)` and `(r, -s)` pairs.\n\n**3. Library Selection:**\n\n- We will utilize cryptographic libraries that support deterministic ECDSA signatures with the Secp256k1 curve and provide built-in signature normalization functionality. This will simplify implementation and ensure the use of well-tested and secure code.\n\n**Justification for Chosen Approach:**\n\n- Deterministic signature generation and signature normalization effectively address the second layer of malleability in Secp256k1 without requiring changes to the underlying elliptic curve cryptography.\n- This approach is widely adopted and standardized (e.g., RFC 6979), ensuring interoperability and compatibility with other systems.\n- Utilizing established cryptographic libraries simplifies implementation and minimizes the risk of introducing vulnerabilities.\n\n**Alternatives Considered:**\n\n- **Using a different elliptic curve cryptography:** While alternative ECC curves with inherent malleability protection exist, switching would introduce significant complexity and compatibility challenges.\n\n**Consequences of Decision:**\n\n- **Improved Security:** This approach mitigates the risks of transaction malleability, enhancing the security of our application.\n- **Simplified Development:** Removing signature malleability concerns eases downstream development by ensuring predictable transaction hashing.\n- **Compatibility:** Utilizing standardized techniques ensures interoperability with other systems and libraries.\n\nThis decision aims to balance security, ease of implementation, and compatibility, ultimately leading to a more robust and secure application. \n","GenTime":"2024-08-19 16:33:38","text":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n\n\n##Decision\nWe can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n"}
{"File Name":"dogma\/0008-location-of-testing-features.md","Context":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n","Decision":"For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","tokens":324,"id":1608,"Prediction":"## Decision\n\nWe will implement Dogma's testing features in a separate `dogmatest` module.\n\n## Reasoning\n\nWhile having the testing features in the `dogma` module offers some usability benefits, we believe the advantages of separate modules outweigh the drawbacks:\n\n* **Independent versioning and release cycles:** This provides greater flexibility for both maintainers and users. Bug fixes and new features can be released for the testing tools without requiring a new release of the core `dogma` module. \n* **Clearer separation of concerns:** Separating the testing utilities reinforces the distinction between the core framework and its tooling. This can lead to a cleaner codebase and potentially smaller download sizes for users who do not require the testing features.\n\nThe concern about users needing to determine compatible versions is acknowledged. However, we believe this can be effectively addressed through clear documentation and tooling. For example:\n\n* The `dogmatest` package can clearly document compatible `dogma` versions. \n* We can provide a simple CLI tool or helper functions that streamline the installation process and ensure version compatibility.\n\nBy carefully managing the separation and providing clear guidance to users, we can minimize potential friction and leverage the benefits of decoupling.\n","GenTime":"2024-08-19 16:33:44","text":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n\n\n##Decision\nFor the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n"}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"Prediction":"## Decision\n\nWe will proceed with option 1: **Have Atlas clients refuse to start if the `_locks` table is missing and provide a CLI to create it.**\n\n## Reasons\n\n- **Simplicity and Robustness:** This option offers the simplest and most robust solution. While requiring a manual step, its clarity and low risk of failure make it preferable.\n- **Automation Potential:**  The manual step can be easily automated through scripts or configuration management tools in most environments, minimizing operational overhead.\n- **Transparency:** This approach makes it evident to operators that the `_locks` table is essential and needs to be present. This promotes better understanding and control over the system's dependencies.\n\n## Rejected Alternatives\n\n- **Lock service instead of `_locks` table:** While enticing for automation, the risk of silent lock failures leading to undetected issues is unacceptable. \n- **Paxos-based table creation:** Introduces significant complexity and potential for errors, outweighing the benefits of automation.\n- **Dedicated `lockLeader` node:**  Imposes a single point of failure and adds configuration complexity across all nodes.\n\n## Consequences\n\n- **Initial Manual Step:**  Operators will need to manually create the `_locks` table before starting Atlas clients.\n- **CLI Tool Maintenance:** The CLI tool for `_locks` table creation needs to be maintained. \n\nThis decision prioritizes reliability and simplicity over complete automation. We believe the benefits of a robust and understandable solution outweigh the trade-off of a manual step, especially given the potential for automation in most operational contexts. \n","GenTime":"2024-08-19 16:33:51","text":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n\n\n##Decision\nWe decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n"}
{"File Name":"wikiindex\/adr-003-testing_library.md","Context":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n","Decision":"* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","tokens":30,"id":1387,"Prediction":"## Decision\n\nWe will use a mocking library that allows for easy mocking of functions, such as Mockito (Java), Moq (.NET), or Jest (JavaScript).  The specific library will be chosen based on the programming language of the project.\n","GenTime":"2024-08-19 16:33:53","text":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n\n\n##Decision\n* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"Prediction":"## Decision\n\nWe will use **Cucumber** as our acceptance testing framework. \n\n**Reasons:**\n\n* **Understandable by all:** Cucumber uses Gherkin, a plain-text language with a simple, Given-When-Then structure, making tests easily understandable for both technical and non-technical stakeholders. This aligns with the requirement for user-friendly test documentation.\n* **Reusable step definitions:** Cucumber promotes reusability through step definitions, which map Gherkin steps to actual code. This allows for efficient test creation and maintenance, addressing the need for easy definition of new tests.\n* **Behavior-driven development (BDD) alignment:** Cucumber is a core tool in the BDD approach, facilitating collaboration and communication between developers, testers, and business stakeholders. By including Cucumber tests within the application source code, we enable a seamless BDD workflow during development.\n* **Self-contained application sources:** Integrating Cucumber tests directly into the application's source code ensures self-containment. This allows for consistent execution of acceptance tests throughout the development lifecycle, supporting the BDD approach and enabling early identification of issues.\n\nThis decision promotes transparency, maintainability, and collaboration throughout the development process by employing a widely-used, user-friendly, and BDD-compatible framework like Cucumber.\n","GenTime":"2024-08-19 16:33:58","text":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n\n\n##Decision\n[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n"}
{"File Name":"cnp-design-documentation\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1077,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as a lightweight mechanism to document significant architectural decisions for this project. \n\nWe will follow the MADR (Markdown Architectural Decision Records) template for its simplicity and readability. Each ADR will be stored as a separate markdown file within a dedicated \"adr\" directory at the root of the project repository. \n\nThe ADRs will be linked to related documentation and code wherever applicable to maintain traceability. \n","GenTime":"2024-08-19 16:34:01","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"SiebenApp\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1922,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project.  Each ADR will follow a standardized template (e.g., MADR, Nygard) to ensure consistency and readability. \n","GenTime":"2024-08-19 16:34:03","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"libelektra\/spec_metadata.md","Context":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n","Decision":"Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","tokens":103,"id":1278,"Prediction":"## Decision\n\nWe will implement a two-pronged approach to address the challenge of storing `spec` metadata in storage plugins:\n\n**1. Standardized Metadata Handling:**\n\n- Define a restricted set of core metadata fields within the `spec:` namespace that are essential for basic `spec` functionality. These fields will be explicitly supported by all storage plugins.\n- For each supported storage plugin, develop a standardized mapping between these core `spec:` metadata fields and the native metadata capabilities of the underlying file format. \n- Where direct mapping is not possible, explore workarounds like utilizing dedicated metadata files alongside the primary data files or leveraging any extensible metadata features offered by the file format.\n\n**2. Namespaced Metadata Storage:**\n\n- For metadata fields outside the core `spec:` set, and for situations where users need to store arbitrary metadata, we will provide a generic namespaced metadata storage mechanism within each storage plugin. \n- This mechanism will treat metadata as key-value pairs, with namespaces (e.g., `spec:`, `user:`) acting as prefixes to ensure separation and prevent collisions.\n- The implementation of this namespaced storage will be plugin-specific and could involve various strategies like separate metadata files, database tables, or custom data structures within the primary data file (if the format allows).\n\nThis approach offers a balance between supporting essential `spec` functionality across all plugins while providing flexibility for handling arbitrary and namespaced metadata.\n","GenTime":"2024-08-19 16:34:09","text":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n\n\n##Decision\nDo not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n"}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"Prediction":"## Decision\n\nEvery configuration folder within the project repository will contain at least one dataflow diagram (DFD) in a standardized format. This DFD will represent the high-level data flow for the component or feature described by the configuration files within that folder. \n\n## Reasons\n\n* **Improved understanding and maintainability:** DFDs provide a clear and concise visual representation of how data flows through the system, making it easier for developers to understand and maintain the codebase.\n* **Facilitates onboarding:** New developers joining the team can quickly grasp the purpose and functionality of different components by referring to the DFDs.\n* **Reduced risk of errors:** Explicitly outlining data flow can help identify potential bottlenecks, inconsistencies, or errors in the design phase.\n* **Enhanced communication:** DFDs serve as a common language for both technical and non-technical stakeholders to discuss and understand the system's behavior.\n\n## Consequences\n\n* **Increased upfront effort:** Creating and maintaining DFDs will require additional effort from developers, especially during the initial design and implementation phases.\n* **Potential for diagram redundancy:** In some cases, high-level system DFDs might already exist. We need to carefully consider the level of detail and scope of DFDs within configuration folders to avoid redundancy.\n\n## Notes\n\n* The specific format and tool used for creating DFDs will be decided based on team preference and available resources.\n* We will establish guidelines and best practices for creating effective and informative DFDs.\n* Regular reviews of DFDs will be conducted to ensure they accurately reflect the current state of the codebase. \n","GenTime":"2024-08-19 16:34:16","text":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n\n\n##Decision\nWe decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n"}
{"File Name":"dogma\/0004-adr-process.md","Context":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n","Decision":"We have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","tokens":27,"id":1614,"Prediction":"## Decision\n\nWe will use a lightweight process for managing ADRs as follows:\n\n**1. Proposing an ADR:**\n\n* Create a new markdown file in the `\/docs\/adrs` directory with a descriptive filename following the format `ADR-XXXX-short-description.md`. (e.g.  `ADR-0001-use-postgres-as-database.md`)\n* Use the ADR template provided ([link to template if available]) to structure the ADR. Fill in all relevant sections.\n* Submit a Pull Request (PR) with the proposed ADR. Include a clear description of the problem and the proposed solution.\n\n**2. Discussing an ADR:**\n\n* All PRs containing ADRs are open for discussion. Team members are encouraged to review, comment, and propose changes to the ADR.\n* The discussion should focus on the technical merits and tradeoffs of the proposed decision, ensuring it aligns with the project's goals and constraints.\n\n**3. Accepting or Rejecting an ADR:**\n\n* The final decision to accept or reject an ADR rests with the [Decision Maker Role - e.g., Architect, Tech Lead].\n* A clear justification for the decision should be provided in the PR comments. \n* An accepted ADR is merged into the main branch.\n* A rejected ADR is closed with a comment explaining the reasoning behind the rejection.\n\n**4. Updating an ADR:**\n\n* If a decision needs to be revisited or amended, a new ADR should be created that supersedes the previous one.\n* The new ADR should reference the superseded ADR and clearly state the changes being made. \n\n**Tools**: We will leverage our existing version control system (e.g. Github, Gitlab) and PR workflow to manage the ADR process. \n\nThis lightweight process ensures that architectural decisions are documented, reviewed, and agreed upon while minimizing overhead and bureaucracy.\n","GenTime":"2024-08-19 16:34:25","text":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n\n\n##Decision\nWe have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n"}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as the mechanism for capturing important architectural decisions along with their context and consequences. Each significant decision will be documented in a separate ADR file using a lightweight Markdown format like the MADR template or similar. These ADRs will be stored in a dedicated version-controlled directory within the project repository. \n","GenTime":"2024-08-19 16:34:27","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"smarthub\/0003-create-backend-client-lib.md","Context":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n","Decision":"Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","tokens":29,"id":3692,"Prediction":"## Decision\n\nIntroduce a dedicated data access layer (DAL) to encapsulate all interactions with the off-chain data REST API. \n","GenTime":"2024-08-19 16:34:28","text":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n\n\n##Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n"}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"Prediction":"## Decision\n\nWe will design and document the myCloud API publicly by default. This means:\n\n* **API specification will be open-source:**  We will host the OpenAPI specification (or similar) in a public repository. \n* **Documentation will be publicly accessible:** Developer documentation will be published on a publicly available platform.\n\n## Reasoning\n\nThis decision aligns with our \"open by default\" principle and the lack of confidentiality requirements for the myCloud API. \n\n* **Transparency and Collaboration:** Openness fosters community involvement, potentially leading to valuable feedback and contributions. \n* **Security by Design:** Public scrutiny can help identify and mitigate security vulnerabilities early on.\n* **Improved Documentation:** Publicly available documentation encourages clarity and completeness. \n\n## Consequences\n\n* **Potential for increased scrutiny:** Publicly exposing our API design might lead to increased scrutiny from security researchers and competitors.\n* **Need for clear communication:** We need to clearly communicate the stability and support guarantees for different versions of the API. \n\nWe will mitigate the potential downsides by:\n\n* **Implementing robust security measures:** This includes rigorous testing, vulnerability scanning, and secure coding practices. \n* **Establishing a clear versioning strategy:**  This helps manage expectations about API changes and their impact on existing integrations.\n* **Engaging with the community:** We will actively encourage feedback and contributions to ensure the API's quality and security. \n","GenTime":"2024-08-19 16:34:34","text":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n\n\n##Decision\nWe release the Shared Jenkins Library under Apache 2.0.\\n"}
{"File Name":"community\/dr-012-Prettier_as_JavaScript_code_formatter.md","Context":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n","Decision":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","tokens":301,"id":3451,"Prediction":"## Decision\n\nWe will adopt Prettier (https:\/\/prettier.io) as our code formatter for all Kyma projects. This decision is based on the following factors:\n\n* **Enforcing a unified code style:** Prettier will help us achieve a consistent code style across all projects, improving readability and reducing noise in diffs.\n* **Simplicity and ease of use:** Prettier is known for its ease of configuration and minimal options, making it easy for developers to adopt.\n* **Language support:** Prettier supports all the languages used in Kyma projects, including JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON. \n* **Flexibility in integration:**  Prettier can be integrated into our workflow through various methods, including IDE plugins, pre-commit hooks, and the CLI, providing flexibility for our developers.\n\nBy enforcing a unified code style with Prettier, we aim to:\n\n* **Improve code readability and maintainability.**\n* **Reduce the time spent on code style discussions and reviews.**\n* **Enable developers to focus on delivering features rather than formatting code.**\n\nThis decision promotes consistency and efficiency within our development process. \n","GenTime":"2024-08-19 16:34:39","text":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n\n##Decision\nThe decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n"}
{"File Name":"hello\/001-record-decisions.md","Context":"## Context\\nWe want to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences. Each ADR will be a separate file living in the \/docs\/architecture\/decisions directory of the codebase, following a standardized template.\n","GenTime":"2024-08-19 16:34:41","text":"## Context\\nWe want to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"insight\/0002-tweepy.md","Context":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"Prediction":"## Decision\n\nWe will use **Tweepy** as our Python library for interacting with the Twitter API. \n\n## Reasons\n\n* **Widely Adopted and Well-Regarded:** Tweepy is the most popular Python library for Twitter API access, with a large and active community. This translates to extensive documentation, numerous tutorials, and readily available support.\n* **Comprehensive API Coverage:**  Tweepy provides a robust interface to various Twitter APIs, including those for accessing timelines, posting tweets, managing users, and streaming real-time data. This caters to our anticipated needs. \n* **Active Maintenance:** Tweepy is actively maintained and regularly updated to accommodate changes in the Twitter API, ensuring its reliability and compatibility. \n\n## Alternatives Considered\n\n* **Twitter API v2 Client:** While Twitter offers an official client library for its API v2, it's relatively new and lacks the extensive community support of Tweepy. \n* **Python-Twitter:** An older library, Python-Twitter lacks some features of Tweepy and has a smaller community, making it a less attractive option. \n\n## Consequences\n\n* **Ease of Development:**  Choosing a widely used library like Tweepy simplifies development due to readily available resources and a lower learning curve.\n* **Future Maintainability:** Tweepy's active maintenance ensures our integration will remain compatible with future Twitter API updates, reducing future technical debt. \n* **Community Support:** If we encounter challenges, the large Tweepy community offers a valuable resource for finding solutions and seeking guidance. \n","GenTime":"2024-08-19 16:34:48","text":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n\n\n##Decision\nWe choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n"}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"Prediction":"## Decision\n\n**Introduce a dedicated DynamoDB table for historical data.**\n\nThis table will store completed, failed, and other historical schedule events.  The main DynamoDB table will only store the current state of schedules and pending tasks.\n\n## Consequences\n\n* **Reduced contention:** Separating current state from historical data will drastically reduce concurrent access to the main DynamoDB table by the `task-1minute-enqueuer` and `task-1minute-sqs2sns` components, mitigating the throttling issue.\n* **Improved performance:** With reduced contention, both read and write operations on the main table will experience better performance. \n* **Simplified queries:** Queries for current state will be faster and simpler as they won't need to filter through historical data.\n* **Increased complexity:** Managing an additional DynamoDB table will add complexity to the infrastructure. \n* **Migration effort:**  Migrating existing historical data to the new table will require a one-time effort.\n\n## Status\n\nProposed\n\n## Rationale\n\nThis approach addresses the core issue of resource contention on the DynamoDB table without sacrificing the serverless architecture.  While it introduces some additional complexity, the benefits of improved performance and scalability outweigh the drawbacks. \n\n## Alternatives Considered\n\n* **Increasing DynamoDB capacity:** This is a temporary fix that doesn't address the underlying architectural issue.\n* **Implementing caching:** While caching could help reduce reads, it adds complexity and doesn't address write contention.\n* **Using a different database:** This would be a significant architectural change and is not warranted at this time. \n","GenTime":"2024-08-19 16:34:55","text":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n\n\n##Decision\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n"}
{"File Name":"zendesk_app_scaffold_react_redux\/replace-migration-scaffold-with-clean-scaffold-in-master-branch.md","Context":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n","Decision":"### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","tokens":124,"id":2378,"Prediction":"## Decision\n\nWe will create a separate branch specifically for v1 to v2 app migration.  This will allow us to maintain a clean and up-to-date scaffold on the master branch for new app development, while still providing the necessary tools and structure for developers migrating existing apps. \n\n## Reasoning\n\n* **Improved Developer Experience:**  New developers will see a streamlined scaffold on the master branch, immediately usable for creating new v2 apps. This avoids confusion and extra steps removing migration-specific code. \n* **Clear Separation of Concerns:** By isolating migration logic, the master branch remains focused on the ideal starting point for new v2 apps, promoting maintainability.\n* **Continued Support for Migration:** The dedicated migration branch ensures that existing tools like App Migrator and ZAT continue to function correctly without impacting the new app development experience.\n\n## Consequences\n\n* **Additional Branch Management:**  We'll need to maintain a separate branch and potentially handle merging updates from the master branch.\n* **Potential User Confusion:**  Clear documentation and communication will be crucial to direct developers to the appropriate branch based on their needs (new app vs. migration).\n\n## Status\n\nProposed\n\n## Action Items\n\n* Create a new branch for v1 to v2 app migration (e.g., `migration-v1-to-v2`).\n* Move all code and documentation specific to migration into this new branch.\n* Update the master branch to reflect a clean scaffold for new v2 app development.\n* Update the documentation to:\n    * Clearly guide developers to the correct branch.\n    * Explain the rationale behind the separation.\n* Communicate this change to the developer community. \n","GenTime":"2024-08-19 16:35:02","text":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n\n\n##Decision\n### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n"}
{"File Name":"cf-k8s-networking\/0006-rewrite-http-liveness-readiness-probes-for-healthchecks.md","Context":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n","Decision":"We have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","tokens":360,"id":2243,"Prediction":"## Decision\n\nWe will configure Istio to rewrite the `http` liveness and readiness probes to bypass the Envoy sidecar. This will ensure that probes continue to function correctly even when Istio `STRICT` mTLS is enabled. This decision is based on the following factors:\n\n* **Maintaining application health checks:** Liveness and readiness probes are critical for Kubernetes to monitor and manage the health of our applications. Without them, we risk undetected failures and service disruptions.\n* **Security benefits of STRICT mTLS:** We want to leverage the enhanced security provided by Istio's `STRICT` mTLS mode. This mode enforces mutual authentication between all services within the mesh, significantly reducing the risk of unauthorized communication.\n* **Minimal performance impact:** The probe rewrite mechanism provided by Istio is designed to have minimal performance impact as it leverages the existing `pilot-agent` process.\n\nThis approach strikes a balance between security and operational stability, allowing us to benefit from `STRICT` mTLS without compromising the functionality of our health checks. \n","GenTime":"2024-08-19 16:35:07","text":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n\n\n##Decision\nWe have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"Prediction":"## Decision\n\nWe will follow the Guice versioning scheme for `kotlin-guice` to maintain clarity and minimize upgrade friction for users. This means:\n\n* **Major Version Increments:** When Guice releases a new major version, `kotlin-guice` will also release a new major version, even if it requires breaking API changes. This ensures users understand the dependency linkage and potential compatibility issues.\n* **Best-Effort Compatibility:**  While a major version change allows for breaking changes, we will strive to maintain as much API compatibility with the previous `kotlin-guice` version as possible. Any unavoidable breaking changes will be clearly documented in the release notes with migration guidance.\n* **Timely Updates:** We will aim to release a new `kotlin-guice` version supporting the latest Guice major version within a reasonable timeframe after Guice's release. This minimizes the time users need to wait to leverage new features and improvements in Guice. \n","GenTime":"2024-08-19 16:35:12","text":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n\n\n##Decision\nWe will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n"}
{"File Name":"react-template\/0003-use-prettier-to-format-code.md","Context":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":3321,"Prediction":"## Decision\n\nWe will adopt Prettier as our standard code formatter for all projects and languages it supports. \n\n## Consequences\n\n* **Pros:**\n  * **Improved code consistency:**  Eliminates debates about code style, ensuring uniformity across projects.\n  * **Reduced code review time:** Developers can focus on logic rather than formatting.\n  * **Industry standard:** Easier onboarding for new developers familiar with Prettier.\n  * **Automated formatting:** Eliminates manual formatting, freeing up developer time.\n  * **Wide language support:**  Covers most, if not all, languages used within the JavaScript ecosystem.\n\n* **Cons:**\n  * **Opinionated:** Limited ability to customize certain style rules.\n  * **Initial setup required:**  Integration into existing projects and workflows.\n  * **Potential conflicts with existing linters:** Requires configuration to avoid redundancy or conflicts.\n\n## Status\n\nAccepted. \n\n## Next Steps\n\n1. **Configuration:**  Define a shared Prettier configuration file (e.g., `.prettierrc`) to enforce consistent formatting rules across projects.\n2. **Integration:** Integrate Prettier into existing development workflows:\n     *  Set up editor extensions for automatic formatting on save.\n     *  Configure linters to delegate formatting to Prettier.\n     *  Include Prettier in the CI\/CD pipeline to enforce formatting before merging code. \n3. **Documentation:**  Update documentation and onboarding materials to reflect the adoption of Prettier as the standard code formatter.\n4. **Team Training:** Conduct a brief training session to familiarize the team with Prettier's functionalities and the chosen configuration. \n","GenTime":"2024-08-19 16:35:19","text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n"}
{"File Name":"linshare-mobile-flutter-app\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3308,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences.  Each ADR will be a separate file living in the `\/docs\/adr` directory of our project's repository. We will use a descriptive file-naming convention like  `ADR-001-choosing-database.md`. \n","GenTime":"2024-08-19 16:35:21","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"alfa\/adr-003.md","Context":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"Prediction":"## Decision\n\nWe will use TypeScript to implement Alfa. \n\nThis decision is based on the following factors:\n\n* **Type Safety:** TypeScript's static type system will help us prevent a large class of bugs related to incorrect data types and API usage. This addresses our most common source of errors and will be crucial as the code base grows in size and complexity.\n* **Browser Compatibility:** TypeScript transpiles to JavaScript, ensuring that Alfa remains compatible with browsers and allows us to continue building browser-based tools.\n* **Familiarity and Tooling:** TypeScript is a superset of JavaScript, easing the learning curve for our team. It also boasts a rich ecosystem of tooling for development, debugging, and testing.\n* **Maintainability:** TypeScript's type annotations act as living documentation, improving code readability and maintainability in the long run. \n\nWhile other options like ReasonML or Elm offer more robust type systems, their adoption would require a steeper learning curve and potentially limit our access to the JavaScript ecosystem. TypeScript offers a good balance between type safety and practicality for our current needs.\n\nThis decision might be revisited in the future if the complexity of Alfa significantly increases or if the browser compatibility requirement changes.\n","GenTime":"2024-08-19 16:35:27","text":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n\n##Decision\nWe will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n"}
{"File Name":"frontend\/001-spa.md","Context":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n","Decision":"Build a SPA.\\n","tokens":89,"id":1096,"Prediction":"## Decision\n\nWe will implement a hybrid approach:\n\n1. **Primary Content via Traditional Web Pages:**  Maintain the core website structure with traditional page loads for content like articles, playlists, etc. This preserves SEO benefits and avoids the complexities of a full SPA. \n\n2. **Audio Player in a Persistent Mini-Player:** Develop a persistent mini-player element that is always visible (or easily accessible) on the page. This mini-player will handle all audio playback.\n\n3. **AJAX for Seamless Transitions:** When a user interacts with elements that trigger audio (e.g., clicking \"play\" on a track), use AJAX to load and update the audio source in the mini-player without a full page reload. \n\n## Rationale\n\nThis hybrid approach balances the user experience with development complexity:\n\n* **Improved UX over Pop-ups:** The mini-player offers a more integrated experience than a separate pop-up window, particularly on mobile devices. \n* **SEO Preservation:**  Traditional pages for primary content ensure search engines can easily index and rank our site.\n* **Simplified Development:**  While AJAX adds some complexity, it's less demanding than building and maintaining an entire SPA.\n\n## Consequences\n\n* **User Experience:** The success hinges on a well-designed and implemented mini-player. It needs to be intuitive, responsive, and unobtrusive while providing essential playback controls.\n* **Development Effort:**  AJAX integration requires careful planning and execution to ensure smooth audio transitions and error handling. \n* **Potential SEO Limitations:**  Depending on mini-player implementation and content loading, there might be minor SEO considerations (e.g., ensuring track information is accessible for indexing).\n\nThis decision allows us to prioritize a positive user experience for audio playback without entirely committing to a full SPA, minimizing potential drawbacks. We will closely monitor the implementation and make adjustments as needed to ensure optimal performance and user satisfaction. \n","GenTime":"2024-08-19 16:35:35","text":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n\n\n##Decision\nBuild a SPA.\\n"}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"Prediction":"## Decision\n\nWe will adhere to the principle of least privilege throughout the design and implementation of this software. This will involve:\n\n* **Role-based access control (RBAC):** Implementing RBAC will allow us to define granular permissions for different user roles, ensuring that users only have access to the resources and functionalities necessary for their specific tasks. \n* **Data access control:**  Limiting access to data based on user roles and responsibilities. This may involve implementing different data views for different user types, masking sensitive data elements, or using data encryption to restrict access.\n* **Service accounts with minimal privileges:** When using service accounts or automated processes, each will be granted the absolute minimum permissions required to perform its designated function.\n* **Regular review and auditing:** Access privileges will be regularly reviewed and audited to ensure they are still appropriate and to revoke any unnecessary permissions.\n\nThis approach minimizes the attack surface and potential damage in case of a security breach by:\n\n* Reducing the likelihood of unauthorized access to sensitive information.\n* Limiting the impact of compromised accounts or applications. \n* Improving system stability by preventing unintended or malicious modifications.\n\n**Rationale:**\n\nPrioritizing security and mitigating risks is paramount. The principle of least privilege offers a robust, proactive security measure that minimizes potential damage from breaches or misuse. This approach aligns with industry best practices and regulatory requirements for data protection and system security. \n","GenTime":"2024-08-19 16:35:41","text":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n\n\n##Decision\nWe will apply the principle of least privilege.\\n"}
{"File Name":"aspan_micro_front_ends\/0002-derived-attributes.md","Context":"## Context\\nDerived attributes are read-only.\\n","Decision":"Following attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","tokens":10,"id":3192,"Prediction":"## Decision\n\nDerived attributes will be implemented as read-only properties. This means they cannot be directly set by users or external systems. Instead, their values will be calculated based on the values of other attributes and\/or data within the system. \n","GenTime":"2024-08-19 16:35:43","text":"## Context\\nDerived attributes are read-only.\\n\n\n##Decision\nFollowing attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n"}
{"File Name":"status-react\/0002-extension-uri.md","Context":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"Prediction":"## Decision\n\nWe will define a dedicated URI schema to identify EDN files defining extensions. This schema will ensure unambiguous identification of extension definitions, enabling:\n\n* **Decentralized discovery and access:** Users and applications can locate and retrieve extensions directly from various decentralized storage platforms using the URI.\n* **Content negotiation:** The schema can be used to signal the expected format (EDN) to clients and servers, facilitating efficient content retrieval and parsing.\n* **Security and trust:** Implementing validation rules based on the URI schema can help prevent loading malicious or invalid extension definitions.\n\nWe will define the specific format and structure of the URI schema in a subsequent ADR, taking into account factors like compatibility with different storage platforms and extensibility for future needs. \n","GenTime":"2024-08-19 16:35:46","text":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n\n\n##Decision\nURI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n"}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"Prediction":"## Decision\n\nWe will adopt **AWS Image Builder** as the solution for generating and managing customized AMIs for the Modernisation Platform.\n\n## Reasons\n\n* **Reduced Operational Overhead:** While AWS Image Builder currently doesn't handle AMI lifecycle management, it significantly reduces the complexity and overhead associated with building and distributing AMIs compared to a custom approach or using Packer. \n* **Managed Service:** As a managed AWS service, Image Builder offers benefits like simplified infrastructure management, security updates, and integration with other AWS services.\n* **Flexibility and Control:** Image Builder provides flexibility through pipelines, recipes, and components, allowing us to customize AMI creation while maintaining a consistent and repeatable process.\n* **Future-Proofing:** AWS is actively developing Image Builder, and future updates may address current limitations like AMI lifecycle management. This aligns with our strategy of leveraging managed services for long-term sustainability. \n\n## Considered Options and Tradeoffs\n\n* **Custom approach:** Offers the most flexibility but introduces significant complexity and operational overhead. This is not feasible given our resource constraints and desire for a streamlined approach.\n* **Packer:** A widely used tool but still requires managing AMI distribution and lifecycle, adding extra complexity compared to Image Builder.\n* **AWS Image Builder:** Offers a balanced approach with reduced overhead, managed service benefits, and future growth potential despite the current lack of AMI lifecycle management.\n\n## Action Items\n\n* Investigate and implement a solution for managing the lifecycle of AMIs created by AWS Image Builder. This could involve developing custom scripts or integrating with third-party tools.\n* Define and document the process for creating, testing, and deploying AMIs using AWS Image Builder.\n* Train the team on AWS Image Builder and the established processes for consistent adoption. \n","GenTime":"2024-08-19 16:35:54","text":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n\n\n##Decision\nWe will create an AMI generation and management capability based on AWS Image Builder.\\n"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"Prediction":"## Decision\n\nWe will implement topic creation and credential supply within the same application. \n\n## Rationale\n\nWhile the Single Responsibility Principle encourages separation of concerns, in this case, the conceptual link between topic creation and credential supply outweighs the benefits of strict separation. Combining them within a single application will:\n\n* **Improve developer experience:** Developers working with topics will have a unified interface for both creating topics and managing credentials.\n* **Simplify deployment and management:** A single application is easier to deploy, manage, and scale compared to two separate applications.\n* **Reduce potential for inconsistency:** Handling both concerns within the same application minimizes the risk of inconsistencies arising from managing credentials separately from the topics they grant access to.\n\n## Consequences\n\n* **Potential for increased complexity:** Combining both functionalities within a single application could lead to increased complexity compared to two separate, simpler applications.\n* **Tighter coupling:** While conceptually related, combining the functionalities increases the coupling between them, potentially impacting future flexibility.\n\n## Mitigation\n\nTo mitigate the potential drawbacks, we will:\n\n* **Maintain clear separation of concerns within the application:** Implement separate modules or packages for topic creation and credential supply, with well-defined interfaces between them.\n* **Employ robust testing strategies:**  Implement thorough unit and integration tests to ensure the continued separation and functionality of both concerns within the application.\n\nThis approach balances the benefits of a unified application with the need for maintainability and flexibility. \n","GenTime":"2024-08-19 16:36:00","text":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\n\n##Decision\nWe will ignore the SRP in this instance, and keep the two concerns in the same application.\\n"}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"Prediction":"## Decision\n\nWe will use meaningful attributes derived from the data's organizational context to construct human-readable URIs for each data element on the ODH platform. This approach prioritizes usability and understandability over resolvability. \n\nSpecifically, we will:\n\n* **Adopt the `tag:` URI scheme** as defined in RFC 4151 for its human-readable format and suitability for representing real-world entities within the ODH platform.\n* **Construct URIs based on a hierarchical path** reflecting the organizational structure of the data. This path will be built using relevant attributes that ensure the uniqueness of each URI.\n* **Prioritize clarity and conciseness** when selecting attributes for URI construction, balancing the need for uniqueness with human readability.\n* **Develop and document a clear and consistent URI scheme**  that outlines the specific attributes and their order for different data types within the ODH platform. This scheme will serve as a guideline for generating and interpreting URIs across the system.\n\n## Consequences\n\n**Advantages:**\n\n* **Improved understandability:** Human-readable URIs make it easier for users to understand the meaning and relationships between data elements.\n* **Simplified data integration:** Meaningful URIs can facilitate data integration tasks by providing a common understanding of data entities across different systems.\n* **Enhanced data discoverability:**  Well-structured URIs can improve data discoverability by enabling users to infer the nature and location of resources based on their URIs.\n\n**Disadvantages:**\n\n* **Potential for URI collisions:** Careful consideration and planning are required to ensure the chosen attributes guarantee URI uniqueness across the entire dataset. \n* **Maintenance overhead:** Changes in data organization or attribute values may necessitate URI updates, requiring careful management to avoid breaking existing references.\n* **Lack of resolvability:** `tag:` URIs are not inherently resolvable, meaning they cannot be directly used to locate resources. This limitation may require additional mechanisms for resource retrieval. \n\n**Mitigation:**\n\n* **Implement robust validation rules:** To prevent URI collisions, we will implement strict validation rules during data ingestion and updates, ensuring uniqueness based on the defined URI scheme.\n* **Develop a centralized URI management system:** This system will help maintain consistency and prevent collisions while providing tools to manage potential changes in data organization or attributes.\n* **Provide alternative resource access methods:** Since `tag:` URIs are not resolvable, we will implement mechanisms like a dedicated API or query interface to retrieve resources based on their URIs or associated metadata.\n\nThis decision prioritizes a user-centric approach by focusing on human-readable and understandable URIs. We acknowledge the challenges associated with this choice and will implement the necessary safeguards and supporting mechanisms to ensure the long-term sustainability and effectiveness of the chosen URI scheme. \n","GenTime":"2024-08-19 16:36:12","text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n\n\n##Decision\nWe define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n"}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"Prediction":"## Decision\n\nWe will use **[Library\/Framework Name]** to graphically model nodes and edges in our web application.\n\n## Considered Options\n\n* **[Library\/Framework Name 1]:** [Briefly describe this option]\n* **[Library\/Framework Name 2]:** [Briefly describe this option]\n* **[Library\/Framework Name 3]:** [Briefly describe this option] \n\n## Decision Outcome\n\nWe chose **[Library\/Framework Name]** because it best meets our requirements. \n\n**Positive Consequences:**\n\n* **Angular Compatibility:** [Library\/Framework Name] offers dedicated support and integration with Angular, making development smoother.\n* **Feature Richness:**  [Library\/Framework Name] provides all the necessary features for creating nodes, connections, and potentially other functionalities we might need (e.g., custom styling, layout algorithms).\n* **Usability:**  [Library\/Framework Name] has clear documentation, a user-friendly API, and potentially helpful community resources, simplifying development and maintenance.\n\n**Negative Consequences:**\n\n* **[Potential drawback 1]:** [Describe the drawback and its impact]. However, we believe this is outweighed by [mention positive aspect(s) that make it acceptable].\n* **[Potential drawback 2]:** [Describe the drawback and its impact]. To mitigate this, we plan to [mention your mitigation strategy].\n\n## Pros and Cons of the Options <!-- optional -->\n\n| Option                   | Pros                                                            | Cons                                           |\n|--------------------------|------------------------------------------------------------------|------------------------------------------------|\n| [Library\/Framework Name 1] | [List the pros of this option]                               | [List the cons of this option]                |\n| [Library\/Framework Name 2] | [List the pros of this option]                               | [List the cons of this option]                |\n| [Library\/Framework Name 3] | [List the pros of this option]                               | [List the cons of this option]                |\n\n## Links <!-- optional -->\n\n* [Link 1]: [Description of Link 1]\n* [Link 2]: [Description of Link 2] \n","GenTime":"2024-08-19 16:36:21","text":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n\n\n##Decision\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n"}
{"File Name":"dlp-lux\/0003-Shibboleth.md","Context":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n","Decision":"Lux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","tokens":167,"id":3214,"Prediction":"## Decision\n\nGiven the shared need for Shibboleth authentication and overlapping attributes between Lux and Curate, we will investigate creating a centralized Shibboleth service for both applications. This service will:\n\n1. **Handle Shibboleth integration:** Manage communication with Emory's Shibboleth Identity Provider, including attribute requests.\n2. **Provide a unified authentication endpoint:** Offer a single point of access for both Lux and Curate, simplifying their authentication logic.\n3. **Standardize attribute handling:** Define a consistent format for user attributes received from Shibboleth and pass them to the applications.\n4. **Support multiple environments:** Handle authentication for production, testing, and development environments for both applications. \n5. **(Optional) Facilitate single sign-on:** Potentially enable users to authenticate once and access both applications seamlessly.\n\nWhile Lux requires new user sign-ups, this functionality will be addressed outside the centralized Shibboleth service. Lux will manage new user registration and potentially integrate it with the centralized service to assign appropriate roles and permissions based on user attributes. \n\nThis approach promotes code reusability, simplifies maintenance, and reduces potential inconsistencies between the applications' authentication implementations. However, it requires careful planning and coordination to ensure both applications' needs are met. \n","GenTime":"2024-08-19 16:36:27","text":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n\n\n##Decision\nLux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n"}
{"File Name":"ea-talk\/0008-define-appropriate-schema-types.md","Context":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n","Decision":"We will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","tokens":117,"id":1106,"Prediction":"## Decision\n\nWe will adopt the following schema naming convention and usage pattern:\n\n* **<application_name>_core:**  This schema will contain all tables, views, and other database objects that are essential to the core functionality of a specific application. Only the application itself and database administrators should have direct access to this schema.\n* **<application_name>_view:** This schema will contain views that expose data from the _core schema in a controlled and secure manner. These views will be tailored for specific use cases, such as reporting, data analysis, or integration with other applications. Access to this schema will be granted based on specific needs.\n* **<application_name>_api:**  This schema will contain stored procedures, functions, and other database logic that encapsulates business rules and data access patterns.  Applications will interact with the database primarily through this schema, ensuring data integrity and security. Access will be restricted to the application and administrators.\n\n**Rationale:**\n\nThis approach addresses the concerns raised in the discussion by:\n\n* **Improving organization:** By separating objects by purpose, we enhance maintainability and reduce complexity within the database.\n* **Enhancing security:**  Limiting direct access to the core data and exposing it selectively through views and API layers minimizes the risk of unauthorized access or data manipulation.\n* **Facilitating collaboration:** Clearly defined schema boundaries and access controls simplify collaboration between developers, DBAs, and other stakeholders.\n\n**Consequences:**\n\n* **Increased initial setup effort:** This approach requires more upfront planning and configuration compared to a single-schema model.\n* **Potential performance overhead:** Additional abstraction layers with views and stored procedures may introduce minor performance overhead. However, this can be mitigated through careful design and optimization.\n\n**Next Steps:**\n\n* Document the new schema naming convention and usage guidelines in the DB Working Group documentation.\n* Develop and implement a process for creating and managing schemas according to the new guidelines.\n* Update existing applications and databases to adhere to the new schema structure gradually, prioritizing new development and critical applications.\n* Monitor the impact of the new schema strategy on development efficiency, security posture, and application performance. \n","GenTime":"2024-08-19 16:36:36","text":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n\n\n##Decision\nWe will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n"}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"Prediction":"## Decision\n\nMigrate from using the free tier of https:\/\/postcodes.io\/ to the in-house postcode search API. \n\n**Reasoning:**\n\nWhile the free tier of postcodes.io has been sufficient until now, the strategic direction to bring high-value, multi-product APIs in-house necessitates this migration. Developing in-house expertise and control over this critical functionality outweighs the benefits of continuing to rely on an external, free service.\n\n**Consequences:**\n\n* **Positive:**\n    * Increased control over the postcode search functionality, including data quality, performance, and feature development.\n    * Reduced reliance on external providers for a critical component.\n    * Potential cost savings in the long term compared to using a paid service from postcodes.io.\n    * Alignment with the strategic direction of developing in-house APIs for core functionalities. \n* **Negative:**\n    * Initial migration effort required, including updating code and potentially data formats.\n    * Ongoing maintenance and development effort for the in-house API.\n\n**Action Items:**\n\n* Define a migration plan, including a timeline and resources.\n* Update the application code to utilize the in-house postcode search API.\n* Conduct thorough testing to ensure a smooth transition and data integrity.\n* Decommission the use of https:\/\/postcodes.io\/ after successful migration.\n* Monitor the performance and stability of the in-house API post-migration. \n","GenTime":"2024-08-19 16:36:42","text":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n\n##Decision\nThe decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n"}
{"File Name":"linshare-mobile-flutter-app\/0006-download-file-manager.md","Context":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n","Decision":"We decided that LinShare could currently download a file one by one.\\n","tokens":80,"id":3297,"Prediction":"## Decision\n\nWe will introduce a batch download feature in LinShare Flutter application for downloading multiple files simultaneously. We will **not** directly utilize `flutter_downloader`'s queuing mechanism for batch downloads. \n\nInstead, we will implement a custom batch download manager on top of `flutter_downloader` that will:\n\n* Handle the selection of multiple files across \"My Space,\" \"Received Shares,\" and \"Shared Spaces.\"\n* Allow users to initiate, pause, resume, and cancel batch downloads.\n* Display overall progress and individual file download status within the batch.\n* Utilize `flutter_downloader`'s `enqueue` function to download each file within the batch sequentially or concurrently (configurable by the user).\n* Potentially explore parallel downloading techniques if concurrent downloads via `flutter_downloader` do not meet performance requirements.\n\nThis approach provides better control over the batch download process, improves user experience with dedicated UI elements and progress tracking, and allows for future enhancements like download prioritization and parallel downloads. \n","GenTime":"2024-08-19 16:36:47","text":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n\n\n##Decision\nWe decided that LinShare could currently download a file one by one.\\n"}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"Prediction":"## Decision\n\nWe will adopt JSON (JavaScript Object Notation) as the standard format for messages exchanged by the routing protocol. \n\n## Reasons\n\n* **Interoperability:** JSON is a language-agnostic data format, ensuring compatibility across different router implementations potentially written in various programming languages.\n* **Human Readability:** JSON's human-readable structure simplifies debugging and protocol analysis.\n* **Existing Libraries:** Robust and widely available JSON parsing libraries exist for almost every programming language, facilitating easy integration.\n* **Extensibility:** JSON's flexible schema allows for future additions and modifications to the message format without breaking backward compatibility.\n* **Lightweight:** Compared to alternatives like XML, JSON offers a more compact representation, minimizing network bandwidth consumption. \n\n## Consequences\n\n* **Learning Curve:** Developers unfamiliar with JSON might need to learn the basics, although the learning curve is relatively low. \n* **Parsing Overhead:** While generally considered lightweight, JSON parsing still requires some computational resources. However, the impact on performance is expected to be minimal with modern hardware and efficient libraries. \n* **Security Considerations:**  While JSON itself doesn't inherently address security, we acknowledge the need to implement appropriate security measures like message signing and encryption on top of the chosen format. \n","GenTime":"2024-08-19 16:36:52","text":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n\n\n##Decision\nWe use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n"}
{"File Name":"linshare-mobile-android-app\/0008-download-with-downloadmanager-service.md","Context":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n","Decision":"Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","tokens":99,"id":1647,"Prediction":"## Decision\n\nWe will adopt the system's `DownloadManager` for handling background downloads instead of maintaining our own custom solutions.\n\n## Reasons\n\n* **Leverage Platform Features:** Utilizing the built-in `DownloadManager` allows us to benefit from its robust functionality and rely on the platform's expertise in managing downloads effectively.\n* **Reduced Complexity:**  Adopting `DownloadManager` eliminates the need for us to develop, test, and maintain our own background downloading mechanism, freeing up resources for other development priorities.\n* **Improved User Experience:** The `DownloadManager` offers a consistent download experience for users across different applications, adhering to platform conventions.\n* **System Integration:** The `DownloadManager` integrates seamlessly with other system components like network management and notifications, providing a more reliable and efficient solution.\n* **Future-Proofing:** Future platform updates and enhancements to the `DownloadManager` will automatically benefit our application without requiring code changes. \n","GenTime":"2024-08-19 16:36:57","text":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n\n##Decision\nInstead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n"}
{"File Name":"libelektra\/capabilities.md","Context":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n","Decision":"Add `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","tokens":66,"id":1318,"Prediction":"## Decision\n\nWe will introduce a new KeySet flag called `KEY_IS_VALID_IN_STORAGE` to explicitly track whether a KeySet can be stored in a specific storage plugin. This flag will be set during `kdbGet` based on the limitations of the currently active storage plugin. Plugins will be modified to check this flag before attempting to store a KeySet and return an error if the flag is not set.\n\n## Reasoning\n\nRelying solely on implicit limitations and scattered documentation creates confusion for users and developers. Explicitly tracking the validity of a KeySet for storage provides several benefits:\n\n* **Improved User Experience:** Clear error messages when attempting to store an invalid KeySet, instead of silent failures or unexpected behavior.\n* **Enhanced Plugin Development:**  Provides a standardized mechanism for plugins to enforce storage limitations, simplifying plugin development and maintenance.\n* **Clearer Documentation:**  The `KEY_IS_VALID_IN_STORAGE` flag serves as a clear indicator of storage limitations, allowing for more accurate and concise documentation.\n\n## Consequences\n\n* **Increased Code Complexity:** Requires modifications to both the core library and storage plugins to manage the new flag. \n* **Potential Performance Overhead:** Checking the flag during storage operations might introduce a minor performance overhead. However, this is expected to be negligible in most cases.\n\n## Alternatives Considered\n\n* **Status Codes:** Returning specific error codes from `kdbSet` to indicate storage limitations. However, this approach would lack clarity and make error handling more complex.\n* **Plugin-Specific Validation:**  Each plugin could implement its own validation logic. This would lead to code duplication and inconsistency across plugins.\n\n## Status\n\nProposed\n\n## Open Questions\n\n* Should the `KEY_IS_VALID_IN_STORAGE` flag be a persistent property of a KeySet, or should it be dynamically set and cleared?\n* How should existing plugins be migrated to support the new flag? \n","GenTime":"2024-08-19 16:37:05","text":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n\n\n##Decision\nAdd `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n"}
{"File Name":"govuk-docker\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":127,"id":1669,"Prediction":"## Decision\n\nWe will not port the historical architectural decisions recorded in the original `govuk-docker` repo to ADRs. \n\nWhile these documents have historical value, they are not actively maintained and are increasingly out of date.  Attempting to port and reconcile them with the current state of the project would be a significant undertaking with limited value.\n\nInstead, we will:\n\n1. **Deprecate the existing documentation:** Add a clear statement to the existing documentation in the `govuk-docker` repository stating that it is deprecated and no longer maintained. We will link this statement to this ADR for context.\n2. **Start fresh with ADRs:** For all future architectural decisions, we will use the ADR format outlined in this project. This will ensure that all decisions are recorded consistently and kept up-to-date.\n3. **Refer to historical context when needed:** If a new architectural decision requires revisiting a decision documented in the old format, we will summarize the relevant information within the new ADR. This will provide sufficient context without requiring us to maintain the outdated documentation.\n\nThis approach allows us to benefit from a structured ADR process while acknowledging that maintaining outdated documentation is not a priority. \n","GenTime":"2024-08-19 16:37:11","text":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"celestia-core\/adr-018-ABCI-Validators.md","Context":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n","Decision":"### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","tokens":44,"id":4119,"Prediction":"## Decision\n\nThis ADR proposes renaming and refactoring certain validator-related ABCI messages to improve clarity and align them with their intended usage. Specifically:\n\n- **Rename `ValidatorUpdate` to `ValidatorSetUpdate`**: This clarifies that the message pertains to updates to the entire validator set, not just a single validator.\n- **Refactor `Validator` message within `ValidatorSetUpdate`**:  Replace the nested `PubKey` field within the `Validator` message with a top-level `Pubkey` field in `ValidatorSetUpdate`. This simplifies the message structure and aligns it with other ABCI messages.\n- **Introduce `ValidatorAddress`**:  A new type will be introduced to represent validator addresses, improving code readability and type safety.\n\nThese changes aim to simplify the ABCI interface, enhance code clarity, and improve the developer experience when interacting with validator-related functionalities. \n","GenTime":"2024-08-19 16:37:15","text":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n\n\n##Decision\n### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n"}
{"File Name":"HES_pipeline\/data_storage_and_access.md","Context":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n","Decision":"The raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","tokens":83,"id":3253,"Prediction":"## Decision\n\nA staged data pipeline will be implemented to ingest, clean, transform, and store the HES data. This pipeline will consist of the following stages:\n\n1. **Ingestion:** Raw HES data will be ingested from the source system into a staging area. The ingestion process will handle different data formats and potential data quality issues. \n2. **Cleaning and Validation:** Data in the staging area will undergo a cleaning and validation process. This will involve:\n    - Handling missing values \n    - Removing duplicates\n    - Validating data types and ranges\n    - Standardizing data formats\n3. **Transformation and Feature Engineering:**  Cleaned data will be transformed into a structured format suitable for analysis. This stage will also include:\n    - Creating derived variables based on existing ones\n    - Aggregating data at different levels (e.g., patient, hospital) \n    - Joining different datasets based on relevant keys\n4. **Storage:**  Processed data, including the original variables, derived variables, and derived datasets, will be stored in a data warehouse or data lake. This storage solution should be optimized for analytical queries and data exploration.\n\n## Reasoning\n\nA staged data pipeline offers several advantages:\n\n* **Improved Data Quality:** Cleaning and validation steps ensure high-quality data for analysis.\n* **Reproducibility:**  The pipeline automates data processing, making it easy to reproduce results and incorporate new data. \n* **Efficiency:**  Pre-processing the data eliminates repetitive cleaning and preparation tasks, saving time and resources for analysts. \n* **Scalability:**  The pipeline can be scaled to handle increasing data volumes and evolving data requirements.\n* **Flexibility:** The modular design allows for easy adaptation to new data sources or changes in data structure. \n\nThis approach provides a structured, efficient, and scalable solution to manage and analyze the large and complex HES data.\n","GenTime":"2024-08-19 16:37:23","text":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n\n\n##Decision\nThe raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n"}
{"File Name":"stentor\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2166,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context and consequences. Each ADR will be a separate file living in the \/docs\/architecture\/decisions directory of our repository, following a standardized template (e.g., MADR or Y-Statements).  We will use a simple numbering scheme for ADR filenames to reflect their order of creation. \n","GenTime":"2024-08-19 16:37:25","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"Prediction":"## Decision\n\nWe will develop a single-page application (SPA) frontend using a modern JavaScript framework like React, Vue.js, or Angular. This SPA will communicate with our API using RESTful principles and JSON for data exchange.\n\n## Reasons\n\n* **Improved User Experience:** SPAs offer a more responsive and interactive user experience compared to traditional multi-page applications. This is because SPAs dynamically update content without requiring full page reloads.\n* **API Centricity:** Using an SPA allows us to keep the API at the core of our architecture. This allows for greater flexibility and scalability as the application grows and potentially integrates with other platforms or services.\n* **Cross-Device Compatibility:** Modern JavaScript frameworks provide robust tools and libraries for building responsive user interfaces that adapt to different screen sizes and devices. This ensures our frontend is accessible on desktops, tablets, and mobile phones.\n* **Large Developer Community and Resources:** Choosing a popular framework like React, Vue.js, or Angular grants us access to a vast ecosystem of libraries, tools, and community support, making development and maintenance easier. \n\n## Consequences\n\n* **Increased Development Complexity:** Building and maintaining an SPA requires expertise in JavaScript frameworks and related technologies.\n* **Initial Loading Time:** SPAs may have a slightly longer initial load time as they download the entire application upfront. However, this can be mitigated with techniques like code splitting and lazy loading.\n* **SEO Considerations:** SPAs can present challenges for search engine optimization (SEO) due to their dynamic nature.  However, solutions like server-side rendering (SSR) and pre-rendering can address these concerns.\n","GenTime":"2024-08-19 16:37:32","text":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n\n\n##Decision\nThe frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n"}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"Prediction":"## Decision\n\nWe will adopt OpenAPI (formerly known as Swagger) as the Interface Definition Language (IDL) to describe our APIs for the following reasons:\n\n* **Widely adopted and supported:** OpenAPI is a widely adopted standard with extensive tooling support for various languages and platforms. This ensures broad compatibility and facilitates integration with existing and future tools.\n* **Language-agnostic:** OpenAPI is language-agnostic, allowing us to define APIs independently of the implementation language used by different modules. This promotes flexibility and maintainability as technology stacks evolve.\n* **Facilitates Documentation and Communication:**  OpenAPI definitions serve as clear and concise documentation for our APIs, improving communication between development teams and stakeholders.\n* **Enables Code Generation:**  OpenAPI tools can generate server stubs, client libraries, and API documentation automatically, reducing boilerplate code and ensuring consistency. \n\nTo maintain these interfaces, we will:\n\n* **Version Control:** Store OpenAPI specifications in version control alongside the codebase to track changes and ensure alignment.\n* **Centralized Repository:** Maintain a centralized repository for all API definitions to provide a single source of truth.\n* **Automated Validation:** Integrate automated validation tools within the CI\/CD pipeline to ensure that API implementations adhere to the defined specifications.\n* **Design-First Approach:** Encourage a design-first approach, where API definitions are finalized before implementation to foster better architectural decisions and reduce inconsistencies. \n","GenTime":"2024-08-19 16:37:38","text":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n\n\n##Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n"}
{"File Name":"hee-web-blueprint\/0003-use-github-actions-for-automation.md","Context":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n","Decision":"Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","tokens":27,"id":1196,"Prediction":"## Decision\n\nWe will use **[Platform Name]** to automate the build, test, and deployment of the HEE National Website Platform.\n\n## Justification\n\nSeveral factors were considered when evaluating potential platforms, including:\n\n* **Ease of use:** The chosen platform should be user-friendly and easy to learn, even for team members with limited DevOps experience.\n* **Scalability:** The platform should be able to handle the current and future needs of the HEE National Website Platform, including potential increases in traffic and website complexity.\n* **Integration:** The platform should integrate seamlessly with our existing tools and workflows, such as our version control system (e.g., Git), code repository (e.g., GitHub\/GitLab), and testing frameworks.\n* **Cost:** The platform's cost should be reasonable and align with the project's budget.\n* **Security:** The platform should have robust security features to protect sensitive data and ensure the integrity of our deployments.\n\nBased on our evaluation, **[Platform Name]** emerged as the best option because:\n\n* **[Reason 1]:**  e.g., It offers a user-friendly interface with comprehensive documentation and a strong community for support. \n* **[Reason 2]:** e.g., It provides excellent scalability and can be easily integrated with our existing AWS infrastructure. \n* **[Reason 3]:** e.g., It offers native integrations with our chosen testing framework and supports automated testing pipelines.\n* **[Reason 4]:** e.g., It offers a cost-effective pricing model that aligns with our budget.\n* **[Reason 5]:** e.g., It prioritizes security with features like granular access control, audit logs, and compliance certifications.\n\n**Alternatives Considered:**\n\n* **[Alternative platform 1]:** Briefly explain why this option was not selected, highlighting its limitations compared to the chosen platform.\n* **[Alternative platform 2]:**  Briefly explain why this option was not selected, highlighting its limitations compared to the chosen platform. \n\n## Consequences\n\n* **Positive:** \n    *  Improved efficiency and speed of deployments.\n    * Reduced risk of errors during deployments.\n    * Increased consistency and reliability of our website.\n    *  Improved collaboration and communication within the development team.\n* **Negative:** \n    * Potential learning curve for team members unfamiliar with the chosen platform.\n    * Possible integration challenges with existing tools and workflows. \n    *  Ongoing maintenance and potential costs associated with using the chosen platform.\n\n\nThis decision will be reviewed [State review frequency or event triggering review] to ensure it continues to meet the needs of the project. \n","GenTime":"2024-08-19 16:37:50","text":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n\n\n##Decision\nHaving determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n"}
{"File Name":"texas-holdem-code-challenge\/0008-use-typescript.md","Context":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n","Decision":"Use TypeScript.\\n","tokens":193,"id":2290,"Prediction":"## Decision\n\nWe will implement option **(2): Use a type system to check that the plumbing matches up, don't unit test the main function.**\n\n## Reasons\n\n* **Focus on component testing:** Our design already emphasizes small, unit-testable components.  The main function primarily orchestrates these components; therefore, robust component tests should suffice.\n* **Type system benefits:** Leveraging TypeScript provides compile-time checks for the data flow and interactions between components, significantly reducing the risk of \"plumbing\" errors that unit testing the main function would target.\n* **Development efficiency:** Compared to writing extensive mocks for unit testing the main function (option 1), relying on TypeScript's type system offers a less verbose and faster feedback loop during development. \n* **Maintainability:**  A type-driven approach improves maintainability by clearly defining interfaces and dependencies, making the codebase easier to understand and refactor.\n\n## Consequences\n\n* **Investment in type definitions:**  We will need to invest time in creating and maintaining accurate TypeScript type definitions. \n* **Integration testing criticality:**  Thorough integration testing becomes even more critical as it becomes the primary means to catch errors in the main function's orchestration logic. \n* **Potential for runtime errors:** While TypeScript mitigates many potential issues, some runtime errors related to data validation or external dependencies might not be caught until integration testing or even production. \n\n## Action Items\n\n* **Integrate TypeScript:** Set up the project to use TypeScript and configure necessary tooling. \n* **Define component interfaces:** Define clear TypeScript interfaces for all components involved in the main function's orchestration.\n* **Implement rigorous integration tests:** Develop a comprehensive suite of integration tests covering various scenarios and edge cases to validate the main function's behavior.\n","GenTime":"2024-08-19 16:37:58","text":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n\n\n##Decision\nUse TypeScript.\\n"}
{"File Name":"tendermint\/adr-061-p2p-refactor-scope.md","Context":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n","Decision":"The P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","tokens":227,"id":1979,"Prediction":"## Decision\n\nWe will refactor the `p2p` package in two phases:\n\n**Phase 1: Stabilization and Incremental Improvements (Next 6 months):**\n\n* **Address critical issues:**  Focus on resolving DoS vulnerabilities, incorrect behavior, and the most severe performance bottlenecks. \n* **Improve testability:**  Introduce unit and integration tests to increase code coverage and enable safer future changes.\n* **Refactor selectively:**  While addressing the above, refactor tightly coupled code and leaky abstractions opportunistically. Full architectural overhaul will be tackled in Phase 2. \n* **Do not adopt QUIC or other major protocol changes yet.** This phase focuses on stabilization and laying the groundwork for more substantial changes in Phase 2.\n\n**Phase 2:  Architectural Overhaul (To be scheduled after Phase 1):**\n\n* **Collaborate with Informal Systems:**  Work closely with Informal Systems on the design and implementation of their P2P networking support in Tendermint-rs, leveraging their expertise and potentially sharing code where applicable.\n* **Evaluate and adopt modern protocols:**  Seriously consider adopting QUIC or other standardized protocols that meet our performance, security, and maintainability requirements. \n* **Implement a modular and extensible architecture:** Design for flexibility to allow for future protocol upgrades and feature additions.\n\nThis phased approach balances the urgency of addressing critical issues with the long-term goal of building a robust and maintainable P2P layer. It also allows for valuable collaboration with Informal Systems and a well-informed decision on protocol adoption. \n","GenTime":"2024-08-19 16:38:04","text":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\n\n##Decision\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n"}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"Prediction":"## Context and Problem Statement\n\nOur web application needs to implement user authentication for security and to enable personalized experiences. We need to decide whether to build a custom authentication system in-house or integrate with an existing third-party provider. \n\n## Decision Drivers \n\n* **Time to Market:** Rapidly launching core features is a high priority.\n* **Security Expertise:**  Our team has limited experience with building secure authentication systems.\n* **Development Costs:** Minimizing development costs is crucial for our budget. \n* **Future Feature Requirements:** We may need to support social logins and two-factor authentication in the future. \n","GenTime":"2024-08-19 16:38:08","text":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n"}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"Prediction":"## Decision\n\nWe will develop a bespoke grant management system. This system will be a web application that allows applicants to submit applications, reviewers to assess them, and administrators to manage the entire process. We will prioritize building a system tailored to our specific grant program requirements, ensuring a streamlined and efficient workflow.\n","GenTime":"2024-08-19 16:38:09","text":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n\n##Decision\nViewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n"}
{"File Name":"BMMRO\/2019-11-20_firestore.md","Context":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n","Decision":"To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","tokens":23,"id":2488,"Prediction":"## Decision\n\nWe will use a relational database management system (RDBMS) to store user-provided data from various forms. \n\n**Reasons for this decision:**\n\n* **Structured Data:** Forms typically collect structured data, which is well-suited for storage and querying in an RDBMS. \n* **Data Integrity:** RDBMS systems excel at enforcing data integrity through constraints, ensuring data accuracy and consistency.\n* **Scalability:**  RDBMS systems can scale to accommodate growing data volumes and user traffic.\n* **Mature Ecosystem:** RDBMS technology offers a mature ecosystem with a wide range of tools, libraries, and community support. \n\n**Specific RDBMS Selection:**\n\nThe choice of a specific RDBMS (e.g., PostgreSQL, MySQL, SQL Server) will be made in a subsequent ADR after evaluating factors such as:\n\n* **Scalability Requirements:**  Anticipated data growth and user traffic.\n* **Budget Constraints:**  Licensing costs and infrastructure requirements.\n* **Team Expertise:**  Existing database skills and experience within the team.\n* **Integration Needs:** Compatibility with other software components in the system.\n\nThis decision provides a high-level direction and will be further refined in later stages of the architectural process. \n","GenTime":"2024-08-19 16:38:15","text":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n\n##Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n"}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"Prediction":"## Decision\n\nWe will implement debouncing for profile\/scene deployments to mitigate the risk of DDoS and wasted bandwidth. This will be achieved by introducing a delay after a user action (e.g., mute\/unmute) before triggering a deployment. If another action occurs within this delay, the timer will be reset. Only after the delay, and if no further actions have occurred, will the profile\/scene deployment be executed. \n\nThis approach ensures that:\n* Rapid, consecutive actions from a user will only result in a single deployment.\n* Legitimate, individual actions will still trigger deployments with a minor, acceptable delay. \n\nThe specific debounce time will be determined through testing and analysis of typical user behavior to balance responsiveness with resource efficiency. \n","GenTime":"2024-08-19 16:38:18","text":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n\n\n##Decision\nWe decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n"}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"Prediction":"## Decision\n\nWe will favor the use of Value Objects (VOs) throughout the `Wegwijs` codebase to represent simple entities where equality is determined by value rather than identity. This decision aims to leverage the benefits of type safety, immutability, and improved code clarity that VOs provide.\n","GenTime":"2024-08-19 16:38:20","text":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n\n\n##Decision\nUse a Value Object wherever possible.\\n"}
{"File Name":"architecture\/0002-apply-clean-code-guidelines.md","Context":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n","Decision":"I will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","tokens":57,"id":1416,"Prediction":"## Decision\n\nWe will adopt a comprehensive approach to ensure code understandability at all levels:\n\n**1. Code Level:**\n\n* **Coding Style Guide:** Enforce a consistent coding style using a style guide (e.g., PEP 8 for Python, Google Java Style Guide) and linters. \n* **Meaningful Naming:** Prioritize clear, descriptive names for variables, functions, classes, and files. \n* **Comments and Documentation:** Encourage writing clear and concise comments explaining the \"why\" behind the code, not just the \"what.\" Utilize docstrings for documenting functions and classes.\n* **Modular Design:** Break down complex tasks into smaller, well-defined modules\/functions\/classes, adhering to the Single Responsibility Principle (SRP).\n\n**2. Application\/Component Level:**\n\n* **Architectural Documentation:** Maintain up-to-date architectural documentation (e.g., C4 Model, UML diagrams) visualizing the application's structure and interactions between components.\n* **Code Organization:** Structure code logically into folders and subfolders based on functionality or features.\n* **README Files:** Include a clear and concise README file for each application\/component explaining its purpose, dependencies, how to run it, and relevant information for developers.\n\n**3. System\/Microservices Level:**\n\n* **Service Discovery and Documentation:** Implement a service discovery mechanism (if applicable) to provide a central repository of available services and their endpoints. Document each service's API using tools like Swagger or OpenAPI.\n* **System Architecture Diagrams:** Maintain high-level architecture diagrams illustrating the relationships and interactions between different applications\/microservices.\n* **Knowledge Sharing Sessions:** Conduct regular knowledge sharing sessions within the team to discuss architectural decisions, code design patterns, and best practices.\n\n**Tooling:**\n\n* Utilize static code analysis tools to identify potential code complexity issues and enforce coding standards.\n* Employ code documentation generators to automatically generate documentation from docstrings and code annotations.\n\n**Continuous Improvement:**\n\n* Regularly review and refactor code to improve readability and maintainability. \n* Foster a culture of constructive code reviews where code understandability is a key focus area.\n\nThis multi-faceted approach emphasizes clarity, consistency, and accessibility of information across different levels of the codebase, ensuring maintainability, onboarding efficiency, and reduced knowledge silos within the team. \n","GenTime":"2024-08-19 16:38:30","text":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n\n##Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n"}
{"File Name":"molgenis-service-armadillo\/0006-use-rdata-format-as-data-input.md","Context":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n","Decision":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","tokens":83,"id":1326,"Prediction":"## Decision\n\n**Decouple the MOLGENIS \"Armadillo\" service from specific data providers through the introduction of a Data Provider Interface (DPI).**\n\nThis interface will define a set of abstract methods for interacting with data, allowing the \"Armadillo\" service to remain agnostic of the underlying data storage and retrieval mechanisms. \n\n**Rationale:**\n\nThis approach addresses the concerns raised in the context by:\n\n* **Promoting reusability:** The DPI makes the service usable by other parties who can implement the interface for their specific data sources.\n* **Maintaining MOLGENIS integration:**  MOLGENIS can implement the DPI to integrate with the service, ensuring continued compatibility.\n* **Enabling independent release cycles:** Decoupling data provider logic from the core service allows for independent development and release cycles.\n* **Fostering community contributions:** A well-defined DPI lowers the barrier for external developers to contribute to the service and integrate new data providers. \n\n**Consequences:**\n\n* **Increased initial development effort:**  Designing and implementing a comprehensive and well-documented DPI will require upfront investment.\n* **Potential performance overhead:**  Abstraction through an interface might introduce a slight performance overhead compared to a tightly coupled implementation. This should be carefully measured and mitigated where necessary. \n\n**Next Steps:**\n\n1. Define the Data Provider Interface (DPI) with methods for common data operations (e.g., query, retrieval, update).\n2. Refactor the \"Armadillo\" service to utilize the DPI for all data interactions.\n3. Develop a reference implementation of the DPI for the existing MOLGENIS data provider.\n4. Document the DPI and provide examples to facilitate adoption by other developers. \n","GenTime":"2024-08-19 16:38:37","text":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n\n\n##Decision\nWe implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n"}
{"File Name":"verify-hub\/0001-record-architechture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4206,"Prediction":"## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation to capture significant architectural decisions for this project.  Each ADR will be a separate Markdown file stored in a dedicated \"docs\/adr\" directory within the project repository. \n","GenTime":"2024-08-19 16:38:39","text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"Prediction":"## Decision\n\nWe will use **[Name of Message Broker\/Queue System, e.g., Kafka, RabbitMQ, AWS SQS, etc.]** as our message broker for implementing asynchronous, event-driven communication between modules.\n\n## Justification\n\n* **[Specific reasons for choosing this message broker]**\n    * **Scalability and Performance:** [Explain how the chosen message broker addresses the project's scalability and performance needs.]\n    * **Feature Set:**  [Mention relevant features offered by the broker, e.g., guaranteed delivery, pub\/sub capabilities, different exchange types, etc.]\n    * **Integration:** [Discuss ease of integration with existing technologies and programming languages used in the project.]\n    * **Community and Support:** [Highlight the strength of community support, available documentation, and potential for troubleshooting help.]\n    * **Cost:** [Consider the cost implications of using this particular message broker (if applicable).]\n\n* **Alternatives Considered:**\n    * [List other message brokers that were evaluated, e.g., RabbitMQ, NATS, etc.]\n\n* **Tradeoffs:**\n    * [Explain the tradeoffs considered in selecting this message broker. For example, a highly scalable and feature-rich broker might have higher operational complexity or cost.]\n\n## Consequences\n\n* **Positive:**\n    * Improved system scalability and flexibility due to decoupled modules.\n    * Enhanced responsiveness and fault tolerance.\n    * Better support for real-time data processing and event-driven workflows.\n\n* **Negative:**\n    * Increased architectural complexity compared to synchronous communication.\n    * Potential challenges in handling message ordering and ensuring data consistency.\n    * Requires careful consideration of error handling and retry mechanisms in an asynchronous environment.\n\nThis decision will be reviewed and potentially revised if:\n\n* **[Specify conditions for review]:** \n    * For example, significant changes in scalability requirements, new feature needs, or the emergence of more suitable message broker technologies. \n","GenTime":"2024-08-19 16:38:48","text":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n\n\n##Decision\nSolution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n"}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"Prediction":"## Decision\n\nWe will use a relational database management system (RDBMS) with PostgreSQL as the specific implementation for the CHIP API's persistence layer. \n\n## Reasoning\n\n* **Simplicity and Flexibility:** RDBMS provide a well-understood and standardized way of modeling and querying data.  They offer flexibility to adapt to evolving data requirements, which aligns with the anticipated growth of the application.\n* **Power and Reliability:** PostgreSQL is renowned for its data integrity, reliability, and robustness. It supports complex queries and transactions, ensuring data consistency and availability. \n* **Wide Adoption and Support:** Both RDBMS and PostgreSQL have extensive community support and readily available resources, making it easier to find talent and troubleshoot issues. \n\n## Consequences\n\n* **Increased Complexity:** Compared to a simple key-value store, setting up and maintaining an RDBMS introduces some complexity. However, the trade-off in flexibility and scalability outweighs this initial overhead.\n* **Potential Performance Considerations:** While PostgreSQL is highly performant, meticulous database design and query optimization might be needed as the data grows significantly.\n\nThis decision provides a solid and scalable foundation for the CHIP API's persistence needs, allowing for future expansion and data growth while maintaining data integrity and reliability. \n","GenTime":"2024-08-19 16:38:53","text":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n\n\n##Decision\nA RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"Prediction":"## Decision\n\n**Use an asymmetric algorithm (RSA or ECDSA) for signing JWTs.**\n\n## Reasons\n\n* **Enhanced Security for Multi-Tenant Environments:** Asymmetric algorithms allow each tenant to have their own key pair. Tenants can only sign JWTs with their private key, and verification is done using their public key. This eliminates the security risk of a single shared secret being compromised.\n* **Scalability:** Distributing public keys for verification is easier and more secure than managing shared secrets in a multi-tenant system.\n* **Standardized Practices:**  Asymmetric cryptography is widely accepted and recommended for securing JWTs in multi-tenant architectures.\n\n## Considered Options\n\n* **HMAC with a shared secret:** While simpler to implement, this approach is unsuitable for multi-tenant environments due to the security implications of a shared secret.\n\n## Consequences\n\n* **Increased Complexity:** Asymmetric algorithms are more complex to implement compared to HMAC.\n* **Performance Overhead:** Signature generation and verification with asymmetric cryptography are generally slower than HMAC. However, this overhead is often negligible in practice, especially when considering the security benefits.\n\n##  Further Considerations\n\n* **Key Management:** Implement a robust key management system to securely store, rotate, and revoke tenant keys.\n* **Algorithm Choice:** Evaluate RSA and ECDSA based on specific security requirements, performance considerations, and available libraries. \n","GenTime":"2024-08-19 16:38:59","text":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n\n##Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"Prediction":"## Decision\n\nWe will implement a custom serializer for the `steps` attribute of the workflow resource. This serializer will convert the ES6 Map, wrapped by a MobX Observable, into an array of key-value pairs. This approach addresses the concerns raised with using existing libraries:\n\n1. **Preserves Order:** Representing the map as an array of key-value pairs maintains the order of steps, which is crucial for the workflow functionality.\n2. **Transparency:** A custom serializer provides a clear and explicit method for type conversion, making it understandable for developers working with other Zooniverse client libraries (Ruby, Python). This promotes consistency across different parts of the platform.\n\nThis decision prioritizes clarity, maintainability, and the specific requirements of the workflow feature over the convenience of using existing libraries. \n","GenTime":"2024-08-19 16:39:03","text":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n\n\n##Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"Prediction":"## Decision\n\nWe will develop the new library of composable functions as a separate codebase and artifact, `tamr_client`. We will continue to support `tamr_unify_client`, utilizing the new `tamr_client` internally to refactor and implement new features.\n\n## Consequences\n\n* **Pros:**\n    * Minimizes disruption to existing users of `tamr_unify_client`.\n    * Allows for a clean and iterative transition to the new architecture.\n    * Enables us to experiment with the new architecture without impacting the existing codebase.\n    * Provides an opportunity to improve the public API of `tamr_unify_client` over time.\n* **Cons:**\n    * Increases the maintenance burden in the short term, as we will need to maintain two codebases.\n    * Adds some complexity to the development process, as changes may need to be coordinated across both codebases.\n\n## Migration Plan\n\n1. Develop the new `tamr_client` library with the desired composable function architecture.\n2. Refactor `tamr_unify_client` to utilize `tamr_client` internally for existing functionality.\n3. Implement new features in `tamr_unify_client` using the new `tamr_client` library.\n4. Provide clear documentation and migration guides for users who wish to transition from `tamr_unify_client` to `tamr_client` directly.\n5. Eventually, we may deprecate `tamr_unify_client` once adoption of `tamr_client` is sufficiently high.\n\nThis approach balances the needs of our existing user base with our desire to improve the architecture of our codebase. \n","GenTime":"2024-08-19 16:39:11","text":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n\n\n##Decision\nImplement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n"}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"Prediction":"## Decision\n\nWe will revert to extracting the `Grant-wise expenditure` section directly from Koshvani. This decision is based on the successful resolution of the site crash issues that initially prevented us from doing so. \n","GenTime":"2024-08-19 16:39:12","text":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n\n##Decision\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n"}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"Prediction":"## Decision\n\nWe will use English for all API endpoints, UI elements, and resource field names.\n\n## Reasons\n\n* **Wider reach and interoperability:** English is the de facto language for software development and international communication. Using English makes our API accessible to a global audience, including developers from other countries and potential integrations with third-party systems.\n* **Future-proofing:**  The Flemish Government may interact with international systems or organizations in the future. Using English from the outset prevents potential localization challenges and costs down the line.\n* **Maintainability:** Sticking to one language simplifies code maintenance, documentation, and onboarding of new developers, especially those who may not be fluent in Dutch.\n* **Availability of resources:** English resources for libraries, frameworks, and learning materials are more readily available, supporting efficient development and problem-solving. \n\n## Consequences\n\n* **Potential language barrier for some Flemish users:** Using English may present a slight barrier for some Flemish government employees or citizens who are not fluent in English.\n* **Perceived lack of cultural sensitivity:** Choosing English over Dutch might be perceived by some as a disregard for local language and culture. \n\n## Mitigations\n\n* **Provide clear and concise documentation in both English and Dutch:** This will ensure that all users, regardless of their English proficiency, can understand and interact with the API and UI.\n* **Offer localized user interfaces:** While the core functionality and data will be exposed in English, we can explore providing localized UI layers or translations for specific user groups within the Flemish Government.\n* **Engage with stakeholders:** Communicate the rationale behind this decision to relevant stakeholders within the Flemish Government and address any concerns they may have. \n\nThis decision prioritizes long-term benefits of accessibility, maintainability, and future-proofing over the potential short-term inconvenience of using English as the primary language. \n","GenTime":"2024-08-19 16:39:20","text":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n\n\n##Decision\nWe will use Dutch terms because it is an external requirement dictated by the organisation.\\n"}
{"File Name":"grout\/adr-2-rename-package.md","Context":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"Prediction":"## Decision\n\nWe will rename the project to **\"Forge\"**. This name evokes a similar feeling of construction and flexibility as \"Ashlar,\" but it is more concise, distinct, and readily available across various naming contexts (e.g., domain name, social media handles). \n\nWhile the other options considered (Joist, Lintel, Silicone, Grout) relate to construction materials, they are either too technical, lack a strong metaphorical connection to the software's purpose, or could be misconstrued. \"Forge,\" on the other hand, implies the act of creation and shaping, aligning well with a flexible software tool. \n","GenTime":"2024-08-19 16:39:23","text":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n\n\n##Decision\nI propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"Prediction":"## Decision\n\nWe will develop a simulated Hub environment for user research purposes. This simulated environment will:\n\n* **Expose a realistic API surface** mimicking the expected behavior of the real Hub, focusing on the core interactions required for the Verify journey.\n* **Provide mock data and responses** to allow for a complete and consistent user experience within the defined scope of the research.\n* **Allow for customization and scenario injection** to facilitate testing of various user flows and edge cases.\n\n**Reasoning:**\n\n* A simulated environment offers a controlled and consistent testing ground, independent of the Hub's development lifecycle and potential instability.\n* It enables focused testing on the Service Provider's integration with the Hub, isolating variables and providing clearer insights into usability and performance within that specific area.\n* A customizable simulation allows for exploration of various user journeys and edge cases relevant to the research objectives, providing more comprehensive data.\n\n**Consequences:**\n\n* **Development effort:** Creating and maintaining the simulated Hub environment requires dedicated resources and time. \n* **Potential for divergence:** The simulated environment, while mimicking the real Hub, may not perfectly reflect its behavior, potentially leading to slightly skewed research findings. \n\n**Mitigation:**\n\n* **Prioritize core functionality:** Focus the simulation development efforts on the key API endpoints and data flows essential for the Verify journey within the research scope. \n* **Regular alignment with Hub development:** Maintain close communication with the Hub development team to ensure the simulated environment reflects the latest API specifications and functionalities as closely as possible. \n* **Clearly communicate limitations:** Acknowledge the inherent limitations of a simulated environment when presenting research findings and interpreting user feedback. \n","GenTime":"2024-08-19 16:39:30","text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n"}
