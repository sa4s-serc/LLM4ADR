Context,Decision,Prediction
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe PriveXMl results generated by ETL on the processing of data feed files must be capable of categorization based on specific tenants (Company, Client & Account) data. Due to the fact such data will change overtime based on client requests, every sections of PriveXML Contents (Ticker + Position + Transaction) section should generally be able to be connected to giving tenant. But using just the tenant Id or primary key as a relationship marker isn't suitable, as during migration and over-all during the life of our archives, seperation of the tenant changes and the PriveXML events will import insertion but increase complexity on the read/consumption end as we may need to re-read all changes for tenant before being able to consume the PriveXML to generate current state.\n\n## Decision\n","We increase complexity during insertion by including all necessary tenant information as part of a PriveXML event and also provide event objects which could only just contain specific changes to client information within the same topic. This then enforces us to think of this topic as a general topic for ETL which may contain an event to process a new PriveXML result or process tenant data change.\nMore so, the topics for each tenant must be tenant specific due to necessary policies and requirements to create seperation of data on physical levels.\n",Implement a Historical Tenant record with valid from / valid to date range for each Tenant in the Tenant table.  PriveXML will be related based on the Historical TenantId.  Historical Tenant will be updated during ETL tenant Migrations to ensure there is no loss in data and no change of PrvieXML above date change.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently have certain specific feeds being parsed using pentaho, these are legacy feeds which transform data feed files into PriveXMl format. These feed parsers will require migration into new CDH ETL feeds parsers based on defined standard library.\n\n## Decision\n","Due to limited resources and time constraints, we will defer the feed migration from pentaho to the CDH ETL till adequate human resources are available, more so an indept understanding of exactly how the parsers work is required, which will require talking with Christopher on this.\n",Migrate the legacy feed parsers from Pentaho to the new CDH ETL feed parsers based on the defined standard library.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, we move files around in S3 from Archive folders into bulkupload folders which is cumbersome and error prone,\nmore so this is being done to allow us trigger data feed processing from the dev admin.\nWe need a means of reducing the flow steps to go from files to processing and reduce surface level access for the ETL team.\n\n## Decision\n","- ETL will have a dedicated database tables which will serve the need to both register and retrieve data feed files in the\nmost optimized manner necessary on a per feed basis. This database tables will be appropriately updated by the ETL service\nbased on update events from either the uploader service, or the windows-specific uploader jars.\nThe following operations then becomes possible:\n1. Ability to query for specific files based on upload date (the date it was uploaded to S3).\n2. Ability to query for a specific batch of files related to specific data feeds.\n3. Ability to get data files specific to a given owner and/or provider.\n4. Ability to get data files specific to a given provider.\n- Due to we creating a database to optimize a query to retrieve and manage data feed files, then we require a way to keep\nthe ETL database tables up to date with new files from uploader service. Therefore, the ETL system will listen for events coming\nfrom both a lambda function which will be called by S3 when new files are added to the specific bucket, and the new uploader\nservice which will house necessary logic for retrieving such data feed files from their sources. Once all uploading logic\nhave being migrated to the uploader, we will de-commission the lambda function and directly have the uploader service inform the\nETL service as regards new files.\nSee\n![Data Feed Delivery](../assets/images/workflows/image8.png)\n","Implement a Lambda function that will be triggered by new files/objects created in the Archive folder in S3. This Lambda function will then move the file to the bulkupload folder, triggering the data feed processing automatically. This will reduce the number of manual steps required to process files and reduce the surface level access for the ETL team.\n\n**Benefits:**\n- Reduced manual effort and error-prone steps\n- Automated data feed processing\n- Reduced surface level access for the ETL team\n\n**Drawbacks:**\n- Additional cost for running the Lambda function\n- Potential for Lambda function failures"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMonolith will undergo segmentation where we plan to pull out the Client Data Hub out of the monolith as a separate service for better scaling and API management. The functionality of the client data hub to provided tenant, transaction and position related data will stay the same, but the data feed parsing within it's perview will also be moved out into an [ETL Service](./etl.md).\nThe client data hub provides the following services:\n1. Delivery of tenant information (Accounts, Company, and Client data).\n2. Delivery of Tenant Transaction (Data Feed and Systems) for specified periods of time.\n3. Delivery of Tenant Positions for their related transactions for porfolio tracking.\n4. Processing of reconciliation request on tenant accounts and positions.\nThis requires us to consider migration procedures for moving existing data from the monolith into the new CDH and ETL related databases.\n\n## Decision\n",Create a specific services (ETL and CDH) where an ETL service and CDH service will be responsible for the processing of delivered PriveXML for consumption and delivery of client transactions + positions into data tables. Both services will communicate across a pubsub event bus.\nThe following is expected:\n- CDH will expose an API by which it will handle all tenant and transaction related requests from monolith.\n- CDH will listen on specified event topic on message bus from which all tenant update requests will be received as events\n- CDH will publish to the monolith events on updated tenant data by which monolith will update it's records.\n- CDH will have all tenant data moved from monolith into it's control (to be discussed in CDH Tech).\n- CDH will consume all PriveXML events to update it's records of transactions + tickers + positions.\n![CDH Architecture](../assets/images/aab_workshop/aab_5.jpg)\n,"**Decision:**\n\nThe Client Data Hub (CDH) and [ETL Service](./etl.md) should be developed as separate services using Java Spring Boot and Docker.\n\n**Rationale:**\n\n* **Scalability:** Separating the CDH into a separate service will allow it to be scaled independently, ensuring better performance and reliability.\n* **API Management:** Moving the API management functionality to the CDH will provide more control and flexibility over the exposure of tenant data.\n* **Data Feed Parsing Optimization:** Offloading data feed parsing to the ETL Service will improve the efficiency of the CDH by focusing it on its core data delivery functions.\n* **Proven Technology Stack:** Java Spring Boot and Docker are well-established and widely-used technologies, ensuring stability and support.\n* **Data Migration Considerations:** The data migration process will need to be carefully planned and executed to ensure data integrity and minimize disruption during the transition."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wish to segment into separate processes where the data feed files processing is handled by the ETL service and the CDH service is reponsible for consuming these produced output which then are materialized into records which is used in response to request to the CDH service. This means ETL service must be able to communicate to the CDH service loosely without direct connection or dependence between either.\n\n## Decision\n,We have chosen an event based communication where the CDH and ETL service communicate results between each other over an event queue based on specified topics (deployed onsite within geozone of CDH and ETL services).\n![Event Queue](../assets/images/workflows/image3.png)\n,"Implement a message queue broker to facilitate communication between the ETL service and the CDH service. This message queue broker will act as an intermediary, allowing the ETL service to produce messages containing the processed data feed files and the CDH service to consume these messages and materialize the records accordingly. This approach ensures loose coupling and independence between the two services, enabling them to operate autonomously."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the parsing process for datafeeds is still part of the existing monolith codebase, which brings it's own share of problems with scaling and feed parsing managemnent. Monolith also has a hard dependency on AWS S3 which must be broken out to allow us specfically move these dependencies outside as they are not relatively core to the ETL processing architecture but are generally how we organize and move input into desired place for access and delivery.\nA hard requirement for the ETL service is the ability to ensure processed datafeed consistently have relational information with their tenants as specific feeds\nhave specific constraints on how they are accessed and stored.\nAnother hard requirements is to organize how data feed files are accessed and retrieved from S3, this currently has issues due to the need to directly access S3, and move files and directories into the bulkupload directories for processing by the monolith. Whilst the alternative of storing both file, metadata and file binary into the database simplifies these access issues, we create more problems in the management of the database files (without using FileStream optimization in SQLServer), increasing cost of backup and replication.\n\n## Decision\n","ETL will be moved into an external service of it's own with the following responsibilities:\n- Embodiment of all parser logic.\n- Delivery of agreed parser format (currently PriveXML) into message queues.\n- Standardized library for parsing delivery and logic.\n- Standardized database tables for data feed file delivery and access.\n- Standardized database tables for tenant data (Company, Client, Accounts).\n- Creates tenant specific events for delivery for tenant specific datafeed.\nAs regards data feed file access problem\n- Manage synchronization of uploaded files events into database from uploader service.\n","**Decision:** Migrate the datafeed parsing process to a separate microservice.\n\n**Rationale:**\n\nThe current parsing process is tightly coupled with the monolith, which introduces scalability and maintenance challenges. By migrating the parsing process to a separate microservice, we can:\n\n* **Improve scalability:** The microservice can be independently scaled to handle increased data volume or traffic.\n* **Reduce complexity:** The separation of the parsing process from the monolith will make the codebase cleaner and easier to maintain.\n* **Break the dependency on AWS S3:** The microservice can be designed to access data from S3 using an API or SDK, which will decouple it from the specific implementation of S3.\n* **Ensure relational information with tenants:** The microservice can be responsible for associating processed datafeed data with the appropriate tenants.\n* **Improve organization and retrieval of data feed files from S3:** The microservice can provide a consistent and optimized way to access and retrieve data feed files from S3, eliminating the need for direct S3 access.\n* **Avoid database file management issues:** By storing only metadata and file binary in the database, we can minimize the cost of backup and replication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\n\n## Decision\n","1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\n![Target Parser Flow](../assets/images/workflows/image1.png)\n","**Decision:**\n\nWe will adopt a **Data Feed File Parsing Framework** to handle the processing of data feed files for ETL. This framework will provide a standardized approach for defining the expected inputs and outputs of parsers and their integration into the overall parsing process.\n\n**Key Points:**\n\n* **Parser Definition:** Individual parsers will be defined using a common schema that specifies the following:\n    * Supported file formats and structures\n    * Expected data fields and their extraction rules\n    * Validation rules to check the integrity of parsed data\n* **Integration with ETL Process:** The framework will provide a mechanism to seamlessly integrate parsers into the ETL process, including:\n    * Mapping parsed data to target data structures\n    * Handling errors and exceptions during parsing\n    * Reporting on data quality and conformance\n* **Modularity and Extensibility:** The framework will be designed to be modular and extensible, allowing for easy addition of new parsers and updates to existing ones as required.\n* **Data Validation and Quality Control:** The framework will include built-in data validation mechanisms to ensure the integrity and accuracy of parsed data, including both structural and semantic checks.\n\n**Benefits:**\n\n* Improved data quality and reliability\n* Reduced development time and effort\n* Standardized parsing process across multiple data sources\n* Increased flexibility and scalability for future changes\n* Enhanced data transparency and governance"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMonolith and a few deployed jars handling delivery of data feed files into S3 for the existing parsing processes powering the monolith client data hub system, this increases cost on the monolith's systems which require more vertical scaling of resources to manage, more so, due to the monolith lock, any fix or update is locked to the monolith release SDLC.\nConsidering these functions serve to move files from source to destination we need to migrate them as external services to both CDH and the monolith, these are then bundled into a single service responsible for the delivery of new data feed files into the S3 Archives and CDH feed data stores.\n\n## Decision\n","Migration of all monolith related uploading logic into external service which is responsible for the timely retreival, delivery and storage of data feed files from their respective sources. The service is responsible for ensuring the ETL service database is always up to date, by deliverying events on file additions into the archive storage regardless of what storage is being used by event delivery.\n![Data Feed Delivery](../assets/images/workflows/image8.png)\n",Migrate the monolith and deployed jars responsible for data feed file delivery into S3 to external services for both CDH and the monolith.\n\n**Benefits:**\n\n* Reduces cost on the monolith's systems by eliminating the need for vertical scaling of resources.\n* Improves agility by decoupling data feed file delivery from the monolith release SDLC.\n* Enhances scalability by bundling the data feed file delivery functions into a single service.\n\n**Risks:**\n\n* Potential for data loss or corruption during migration.\n* Increased complexity due to the introduction of new services.\n* Requires additional resources to manage and maintain the external services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\nrequirements during onboarding and processing of feed files, the following issues where considered:\n- Will such logic require specialized implementation across feeds?\n- Are such logic generic and require one time implementation or will require continous change/update?\n- What are the benefits of moving such logic into ETL instead of CDH.\n\n## Decision\n","The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\nensuring the final result is always consistent to march what the CDH service requires.\n","Move all the logic related to onboarding and processing of feed files to ETL services, such logic is generic and does not require specialized implementation across feeds, it will require one time implementation which will be generic enough to handle all possible feed files. Moving such logic to ETL will provide the following benefits:\n- Decoupling of CDH from ETL services.\n- Centralized management of feed files processing logic.\n- Improved maintainability and extensibility of the ETL pipeline.\n- Reduced code duplication and complexity within CDH."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\n\n## Decision\n","1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\n![Target Parser Flow](../assets/images/workflows/image1.png)\n","**Decision:** Implement a data feed pipeline with the following characteristics:\n\n* **Input:** Data feed files in CSV or JSON format, conforming to a predefined schema.\n* **Parsing:** Utilize vendor-specific parsers to extract data from the input files.\n* **Output:** Parsed data in a consistent intermediate format, suitable for further processing in the ETL pipeline.\n* **Validation:** Perform data quality checks and validations during parsing to ensure data integrity.\n* **Error Handling:** Handle parsing errors gracefully, providing error messages and logging for debugging purposes.\n* **Scalability:** Design the pipeline to support multiple data feed files concurrently, with the ability to scale as needed.\n* **Extensibility:** Allow for the easy integration of new data feed sources and parser implementations in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDuring the parsing of different feeds it has become clear that some feed files contain partial information related to Positions, Transactions and Tickers (think DBS, BOS, Credit Suisse) which end up creating inconsistent and incomplete PriveXML/JSON result files with partial information which if handled in CDH will break decision made in [0002 CDH ETL Logic Boundaries](./0002-cdh-etl-logic-boundaries.md).\nFrom existing processing of feed files, such inconsistency is resolved by reviewing previous results based on time ranges back into the past which allows us reconcile such inconsistencies, by creating a clear picture due to information on Tickers, Positions and Transactions as required.\nProblems:\n- Each parser falling into such use case will escalate processing of a batch to process more files per requests.\n- Increased processing time for processing a batch of files due to the need to reprocess (clean, parse, normalize) feed data.\n- We must scale vertically as needed to meet resource requirements for constraints of reprocessing more files per feed request (if necessary only).\n2. Reuse previous parsing results\nSince previous feed files were already processed to create their corresponding PriveXML/JSON which contains the cleaned and normalized results of such a batch of feed files for each time range (t-1, t-2,..., etc). If these are stored within an appropriate store (S3, MySQL, Redis) which will allow immediate access as such result do not change, then we can skip reprocessing of the original source files and use this as the source of the reconciliation processing during feed parsing of new feed batches.\nBenefits:\n- Each parser still only ever processing the provided batch (same memory and cpu usage)\n- Reprocessing of source files only ever occurs when such result does not yet exists (saving cpu cycles per feed)\n- Each parser retrieves previous results based on specific critierais with a single query to retrive as many t-n time range.\nProblems:\n- If relation database table is used then we create a shared resource across horizontal scaling of ETL service.\n(We can mitigate this by using a scalable store (e.g s3) that can handle as much request for files as needed).\n- We must implement more code to provide cleaner abstractions for retrieving/querying previous results (setup cost)\n- We must still fall back to processing source files if the result does not yet exists in db (so why not do it all the time?).\n**TradeOffs: Performance over Code Complexity**\nBecause if we choose option 1 then we increase complexity in execution time as each parsing data can requests multiple files for reprocessing to reconcile inconsistency, while option 2 means we must manage another database if going on a relational store to persist such processing results for later read-heavy querying.\n\n## Decision\n","We chosen to following the solution prescribed in Option 2, using a relation database table to store records to such PriveXML/JSON results which will be based on processing date and feed category for later retrieval. This can be stored as blobbed or have the contents stored externally (on S3) where the table maintains necessary pointers to location for retrieval through a library.\n","**Reuse previous parsing results**\n\n**Rationale**\n\nThe benefits of reusing previous parsing results outweigh the drawbacks. By reusing previous results, we can skip the time-consuming process of reprocessing source files, which will improve the performance of the ETL process. Additionally, reusing previous results will reduce the amount of code that needs to be written and maintained, which will make the ETL process more efficient and easier to manage.\n\n**Trade-offs**\n\nThe main trade-off is that we will need to implement more code to provide cleaner abstractions for retrieving/querying previous results. However, this trade-off is outweighed by the benefits of reusing previous results.\n\n**Consequences**\n\nBy reusing previous parsing results, we will improve the performance of the ETL process and reduce the amount of code that needs to be written and maintained."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\nAll account types have a set of balances that are composed of `Coins`. The `x/bank` module keeps\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\napplication.\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\noperations in the Cosmos SDK work off of units of 10^6.\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\nas a result the total amount of asset types increases. We propose to have `x/bank` additionally keep\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\nUX and remove the requirement for making any assumptions on the unit of denomination.\n\n## Decision\n","The `x/bank` module will be updated to store and index metadata by `denom`, specifically the ""base"" or\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\nMetadata may also include a non-zero length list of denominations. Each entry containts the name of\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\nand on exchanges. The values in those fields link to an entry in the list of denominations.\nThe list in `denom_units` and the `display` entry may be changed via governance.\nAs a result, we can define the type as follows:\n```protobuf\nmessage DenomUnit {\nstring denom    = 1;\nuint32 exponent = 2;\nrepeated string aliases = 3;\n}\nmessage Metadata {\nstring description = 1;\nrepeated DenomUnit denom_units = 2;\nstring base = 3;\nstring display = 4;\n}\n```\nAs an example, the ATOM's metadata can be defined as follows:\n```json\n{\n""description"": ""The native staking token of the Cosmos Hub."",\n""denom_units"": [\n{\n""denom"": ""uatom"",\n""exponent"": 0,\n""aliases"": [\n""microatom""\n],\n},\n{\n""denom"": ""matom"",\n""exponent"": 3,\n""aliases"": [\n""milliatom""\n]\n},\n{\n""denom"": ""atom"",\n""exponent"": 6,\n}\n],\n""base"": ""uatom"",\n""display"": ""atom"",\n}\n```\nGiven the above metadata, a client may infer the following things:\n- 4.3atom = 4.3 * (10^6) = 4,300,000uatom\n- The string ""atom"" can be used as a display name in a list of tokens.\n- The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\nan explicit decision to choose a different representation.\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\nas the base framework for this already exists in the Cosmos SDK.\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x/bank` module which is also\nindexed by the base `denom`.\n```go\ntype GenesisState struct {\nSendEnabled   bool        `json:""send_enabled"" yaml:""send_enabled""`\nBalances      []Balance   `json:""balances"" yaml:""balances""`\nSupply        sdk.Coins   `json:""supply"" yaml:""supply""`\nDenomMetadata []Metadata  `json:""denom_metadata"" yaml:""denom_metadata""`\n}\n```\n",Add metadata to the `denom` field in the `Coins` type to provide a richer UX for clients and apps built atop Cosmos-SDK-based chains. This metadata will help clients and apps understand the unit of denomination for each `denom` and remove the requirement for making any assumptions on the unit of denomination.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis idea was first conceived of in order to fulfill the use case of the\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\nmembers would be elected by a governing community and would fulfill the role of\ncoordinating the community under emergency situations. This thinking\ncan be further abstracted into the conception of ""blockchain specialization\ngroups"".\nThe creation of these groups are the beginning of specialization capabilities\nwithin a wider blockchain community which could be used to enable a certain\nlevel of delegated responsibilities. Examples of specialization which could be\nbeneficial to a blockchain community include: code auditing, emergency response,\ncode development etc. This type of community organization paves the way for\nindividual stakeholders to delegate votes by issue type, if in the future\ngovernance proposals include a field for issue type.\n\n## Decision\n","A specialization group can be broadly broken down into the following functions\n(herein containing examples):\n- Membership Admittance\n- Membership Acceptance\n- Membership Revocation\n- (probably) Without Penalty\n- member steps down (self-Revocation)\n- replaced by new member from governance\n- (probably) With Penalty\n- due to breach of soft-agreement (determined through governance)\n- due to breach of hard-agreement (determined by code)\n- Execution of Duties\n- Special transactions which only execute for members of a specialization\ngroup (for example, dCERT members voting to turn off transaction routes in\nan emergency scenario)\n- Compensation\n- Group compensation (further distribution decided by the specialization group)\n- Individual compensation for all constituents of a group from the\ngreater community\nMembership admittance to a specialization group could take place over a wide\nvariety of mechanisms. The most obvious example is through a general vote among\nthe entire community, however in certain systems a community may want to allow\nthe members already in a specialization group to internally elect new members,\nor maybe the community may assign a permission to a particular specialization\ngroup to appoint members to other 3rd party groups. The sky is really the limit\nas to how membership admittance can be structured. We attempt to capture\nsome of these possiblities in a common interface dubbed the `Electionator`. For\nits initial implementation as a part of this ADR we recommend that the general\nelection abstraction (`Electionator`) is provided as well as a basic\nimplementation of that abstraction which allows for a continuous election of\nmembers of a specialization group.\n``` golang\n// The Electionator abstraction covers the concept space for\n// a wide variety of election kinds.\ntype Electionator interface {\n// is the election object accepting votes.\nActive() bool\n// functionality to execute for when a vote is cast in this election, here\n// the vote field is anticipated to be marshalled into a vote type used\n// by an election.\n//\n// NOTE There are no explicit ids here. Just votes which pertain specifically\n// to one electionator. Anyone can create and send a vote to the electionator item\n// which will presumably attempt to marshal those bytes into a particular struct\n// and apply the vote information in some arbitrary way. There can be multiple\n// Electionators within the Cosmos-Hub for multiple specialization groups, votes\n// would need to be routed to the Electionator upstream of here.\nVote(addr sdk.AccAddress, vote []byte)\n// here lies all functionality to authenticate and execute changes for\n// when a member accepts being elected\nAcceptElection(sdk.AccAddress)\n// Register a revoker object\nRegisterRevoker(Revoker)\n// No more revokers may be registered after this function is called\nSealRevokers()\n// register hooks to call when an election actions occur\nRegisterHooks(ElectionatorHooks)\n// query for the current winner(s) of this election based on arbitrary\n// election ruleset\nQueryElected() []sdk.AccAddress\n// query metadata for an address in the election this\n// could include for example position that an address\n// is being elected for within a group\n//\n// this metadata may be directly related to\n// voting information and/or privileges enabled\n// to members within a group.\nQueryMetadata(sdk.AccAddress) []byte\n}\n// ElectionatorHooks, once registered with an Electionator,\n// trigger execution of relevant interface functions when\n// Electionator events occur.\ntype ElectionatorHooks interface {\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\nAfterMemberAccepted(addr sdk.AccAddress)\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\n}\n// Revoker defines the function required for a membership revocation rule-set\n// used by a specialization group. This could be used to create self revoking,\n// and evidence based revoking, etc. Revokers types may be created and\n// reused for different election types.\n//\n// When revoking the ""cause"" bytes may be arbitrarily marshalled into evidence,\n// memos, etc.\ntype Revoker interface {\nRevokeName() string      // identifier for this revoker type\nRevokeMember(addr sdk.AccAddress, cause []byte) error\n}\n```\nCertain level of commonality likely exists between the existing code within\n`x/governance` and required functionality of elections. This common\nfunctionality should be abstracted during implementation. Similarly for each\nvote implementation client CLI/REST functionality should be abstracted\nto be reused for multiple elections.\nThe specialization group abstraction firstly extends the `Electionator`\nbut also further defines traits of the group.\n``` golang\ntype SpecializationGroup interface {\nElectionator\nGetName() string\nGetDescription() string\n// general soft contract the group is expected\n// to fulfill with the greater community\nGetContract() string\n// messages which can be executed by the members of the group\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\n// logic to be executed at endblock, this may for instance\n// include payment of a stipend to the group members\n// for participation in the security group.\nEndBlocker(ctx sdk.Context)\n}\n```\n",To establish blockchain specialization groups within the community to enable delegated responsibilities and enhance community organization.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user’s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user/computer.\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don’t provide a native secret store.\n\n## Decision\n",We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https://github.com/99designs/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user’s machine.\n,"The decision is to use a Secret Store to store key material and metadata.\n\nA Secret Store is a secure, encrypted storage mechanism for sensitive data, such as passwords, API keys, and certificates. It provides a common abstraction layer to the many different backends and a reasonable fallback for minimal platforms that don’t provide a native secret store.\n\nUsing a Secret Store will address the security concerns raised in the context. By storing key material and metadata in an encrypted format, it will be more difficult for attackers to access and use this information.\n\nIn addition, using a Secret Store will provide a consistent way to access and manage secrets across different platforms. This will make it easier for developers to build and deploy applications that use secrets.\n\nThe following are some of the benefits of using a Secret Store:\n\n* **Improved security:** Secrets are stored in an encrypted format, making them more difficult for attackers to access.\n* **Centralized management:** Secrets can be managed from a central location, making it easier to track and control their use.\n* **Improved convenience:** Developers can access secrets without having to manage the underlying infrastructure.\n* **Increased flexibility:** Secret Stores can be used to store a variety of different types of secrets, including passwords, API keys, and certificates.\n\nOverall, using a Secret Store is a good decision for improving the security and manageability of secrets."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current ""naive"" IBC Relayer strategy currently establishes a single predetermined IBC channel atop a single connection between two clients (each potentially of a different chain).  This strategy then detects packets to be relayed by watching for `send_packet` and `recv_packet` events matching that channel, and sends the necessary transactions to relay those packets.\nWe wish to expand this ""naive"" strategy to a ""passive"" one which detects and relays both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.\nIn order to accomplish this, we propose adding more comprehensive events to expose channel metadata for each transaction sent from the `x/ibc/04-channel/keeper/handshake.go` and `x/ibc/04-channel/keeper/packet.go` modules.\nHere is an example of what would be in `ChanOpenInit`:\n```go\nconst (\nEventTypeChannelMeta = ""channel_meta""\nAttributeKeyAction = ""action""\nAttributeKeyHops = ""hops""\nAttributeKeyOrder = ""order""\nAttributeKeySrcPort = ""src_port""\nAttributeKeySrcChannel = ""src_channel""\nAttributeKeySrcVersion = ""src_version""\nAttributeKeyDstPort = ""dst_port""\nAttributeKeyDstChannel = ""dst_channel""\nAttributeKeyDstVersion = ""dst_version""\n)\n// ...\n// Emit Event with Channel metadata for the relayer to pick up and\n// relay to the other chain\n// This appears immediately before the successful return statement.\nctx.EventManager().EmitEvents(sdk.Events{\nsdk.NewEvent(\ntypes.EventTypeChannelMeta,\nsdk.NewAttribute(types.AttributeKeyAction, ""open_init""),\nsdk.NewAttribute(types.AttributeKeySrcConnection, connectionHops[0]),\nsdk.NewAttribute(types.AttributeKeyHops, strings.Join(connectionHops, "","")),\nsdk.NewAttribute(types.AttributeKeyOrder, order.String()),\nsdk.NewAttribute(types.AttributeKeySrcPort, portID),\nsdk.NewAttribute(types.AttributeKeySrcChannel, chanenlID),\nsdk.NewAttribute(types.AttributeKeySrcVersion, version),\nsdk.NewAttribute(types.AttributeKeyDstPort, counterparty.GetPortID()),\nsdk.NewAttribute(types.AttributeKeyDstChannel, counterparty.GetChannelID()),\n// The destination version is not yet known, but a value is necessary to pad\n// the event attribute offsets\nsdk.NewAttribute(types.AttributeKeyDstVersion, """"),\n),\n})\n```\nThese metadata events capture all the ""header"" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.\n### Handling Channel Open Attempts\nIn the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current ""naive"" relayer.\nTo implement this behavior, we propose replacing the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`.  The typical implementation, in `x/ibc-transfer/module.go` would be compatible with the current ""naive"" relayer, as follows:\n```go\nfunc (am AppModule) OnAttemptChanOpenTry(\nctx sdk.Context,\nchanKeeper channel.Keeper,\nportCap *capability.Capability,\nmsg channel.MsgChannelOpenTry,\n) (*sdk.Result, error) {\n// Require portID is the portID transfer module is bound to\nboundPort := am.keeper.GetPort(ctx)\nif boundPort != msg.PortID {\nreturn nil, sdkerrors.Wrapf(porttypes.ErrInvalidPort, ""invalid port: %s, expected %s"", msg.PortID, boundPort)\n}\n// BEGIN NEW CODE\n// Assert our protocol version, overriding the relayer's suggestion.\nmsg.Version = types.Version\n// Continue the ChanOpenTry.\nres, chanCap, err := channel.HandleMsgChannelOpenTry(ctx, chanKeeper, portCap, msg)\nif err != nil {\nreturn nil, err\n}\n// END OF NEW CODE\n// ... the rest of the callback is similar to the existing OnChanOpenTry\n// but uses msg.* directly.\n```\nHere is how this callback would be used, in the implementation of `x/ibc/handler.go`:\n```go\n// ...\ncase channel.MsgChannelOpenTry:\n// Lookup module by port capability\nmodule, portCap, err := k.PortKeeper.LookupModuleByPort(ctx, msg.PortID)\nif err != nil {\nreturn nil, sdkerrors.Wrap(err, ""could not retrieve module from port-id"")\n}\n// Retrieve callbacks from router\ncbs, ok := k.Router.GetRoute(module)\nif !ok {\nreturn nil, sdkerrors.Wrapf(port.ErrInvalidRoute, ""route not found to module: %s"", module)\n}\n// Delegate to the module's OnAttemptChanOpenTry.\nreturn cbs.OnAttemptChanOpenTry(ctx, k.ChannelKeeper, portCap, msg)\n```\nThe reason we do not have a more structured interaction between `x/ibc/handler.go` and the port's module (to explicitly negotiate versions, etc) is that we do not wish to constrain the app module to have to finish handling the `MsgChannelOpenTry` during this transaction or even this block.\n\n## Decision\n","- Expose events to allow ""passive"" connection relayers.\n- Enable application-initiated channels via such passive relayers.\n- Allow port modules to control how to handle open-try messages.\n","The passive IBC Relayer will expand upon the current ""naive"" strategy by detecting and relaying both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.  In order to accomplish this, comprehensive events will be added to expose channel metadata for each transaction sent from the `x/ibc/04-channel/keeper/handshake.go` and `x/ibc/04-channel/keeper/packet.go` modules.  These metadata events capture all the ""header"" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.  In the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current ""naive"" relayer.  To implement this behavior, we replace the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Summary\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\n### Exceptional cases\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\n1. The chain which the client is following has halted and is no longer producing blocks/headers, so no updates can be made to the client\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\n### Security model\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\n\n## Decision\n","We elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\n1. `allow_governance_override_after_expiry` (boolean, default false)\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\n1. `allow_governance_override_after_misbehaviour` (boolean, default false)\n1. Add a new governance proposal type, `ClientUpdateProposal`, in the `x/ibc` module\n1. Extend the base `Proposal` with a client identifier (`string`) and a header (`bytes`, encoded in a client-type-specific format)\n1. If this governance proposal passes, the client is updated with the provided header, if and only if:\n1. `allow_governance_override_after_expiry` is true and the client has expired (`Expired()` returns true)\n1. `allow_governance_override_after_misbehaviour` is true and the client has been frozen (`Frozen()` returns true)\n1. In this case, additionally, the client is unfrozen by calling `Unfreeze()`\nNote additionally that the header submitted by governance must be new enough that it will be possible to update the light client after the new header is inserted into the client state (which will only happen after the governance proposal has passed).\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https://github.com/cosmos/ics/tree/master/spec/ics-007-tendermint-client#upgrades).\n","**Decision:** Implement a governance proposal type to allow governance to manually force-update a client with a new header after a delay period.\n\n**Rationale:**\n\n* This mechanism provides a recovery mechanism for exceptional cases where funds would otherwise be permanently locked.\n* The security model is not substantially altered, as two-thirds of the validator set can already sign arbitrary data.\n* The delay period allows for community discussion and consensus before the client is updated.\n\n**Consequences:**\n\n* Users will have a recovery mechanism for exceptional cases.\n* Governance will have the ability to manually update clients, but this will require a delay period and community consensus.\n* The security model will remain in place, as two-thirds of the validator set can already sign arbitrary data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md) and\n[ARD 020](./adr-019-protobuf-transaction-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nThis ADR continues from [ARD 020](./adr-020-protobuf-transaction-encoding.md)\nto specify the encoding of queries.\n\n## Decision\n","### Custom Query Definition\nModules define custom queries through a protocol buffers `service` definition.\nThese `service` definitions are generally associated with and used by the\nGRPC protocol. However, the protocol buffers specification indicates that\nthey can be used more generically by any request/response protocol that uses\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\nEach module with custom queries should define a service canonically named `Query`:\n```proto\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\n}\n```\n#### Handling of Interface Types\nModules that use interface types and need true polymorphism generally force a\n`oneof` up to the app-level that provides the set of concrete implementations of\nthat interface that the app supports. While app's are welcome to do the same for\nqueries and implement an app-level query service, it is recommended that modules\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\nThere is a concern on the transaction level that the overhead of `Any` is too\nhigh to justify its usage. However for queries this is not a concern, and\nproviding generic module-level queries that use `Any` does not preclude apps\nfrom also providing app-level queries that return use the app-level `oneof`s.\nA hypothetical example for the `gov` module would look something like:\n```proto\n// x/gov/types/types.proto\nimport ""google/protobuf/any.proto"";\nservice Query {\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\n}\nmessage AnyProposal {\nProposalBase base = 1;\ngoogle.protobuf.Any content = 2;\n}\n```\n### Custom Query Implementation\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https://github.com/gogo/protobuf)\ngrpc plugin, which for a service named `Query` generates an interface named\n`QueryServer` as below:\n```go\ntype QueryServer interface {\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\n}\n```\nThe custom queries for our module are implemented by implementing this interface.\nThe first parameter in this generated interface is a generic `context.Context`,\nwhereas querier methods generally need an instance of `sdk.Context` to read\nfrom the store. Since arbitrary values can be attached to `context.Context`\nusing the `WithValue` and `Value` methods, the SDK should provide a function\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\n`context.Context`.\nAn example implementation of `QueryBalance` for the bank module as above would\nlook something like:\n```go\ntype Querier struct {\nKeeper\n}\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\nreturn &balance, nil\n}\n```\n### Custom Query Registration and Routing\nQuery server implementations as above would be registered with `AppModule`s using\na new method `RegisterQueryServer(grpc.Server)` which could be implemented simply\nas below:\n```go\n// x/bank/module.go\nfunc (am AppModule) RegisterQueryServer(server grpc.Server) {\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\n}\n```\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\nquery routing table (with the routing method being described below).\nThe signature for this method matches the existing\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\nquery server implementation described above.\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\nand method name (ex. `QueryBalance`) combined with `/`s to form a full\nmethod name (ex. `/cosmos_sdk.x.bank.v1.Query/QueryBalance`). This gets translated\ninto an ABCI query as `custom/cosmos_sdk.x.bank.v1.Query/QueryBalance`. Service handlers\nregistered with `QueryRouter.RegisterService` will be routed this way.\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\n`sdk.Query` and `QueryRouter` infrastructure.\nThis basic specification allows us to reuse protocol buffer `service` definitions\nfor ABCI custom queries substantially reducing the need for manual decoding and\nencoding in query methods.\n### GRPC Protocol Support\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\nproxy server that routes requests in the GRPC protocol to ABCI query requests\nunder the hood. In this way, clients could use their host languages' existing\nGRPC implementations to make direct queries against Cosmos SDK app's using\nthese `service` definitions. In order for this server to work, the `QueryRouter`\non `BaseApp` will need to expose the service handlers registered with\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\nlaunch the proxy server on a separate port in the same process as the ABCI app\nwith a command-line flag.\n### REST Queries and Swagger Generation\n[grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway) is a project that\ntranslates REST calls into GRPC calls using special annotations on service\nmethods. Modules that want to expose REST queries should add `google.api.http`\nannotations to their `rpc` methods as in this example below.\n```proto\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balance/{address}/{denom}""\n};\n}\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balances/{address}""\n};\n}\n}\n```\ngrpc-gateway will work direcly against the GRPC proxy described above which will\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\ngenerate Swagger definitions automatically.\nIn the current implementation of REST queries, each module needs to implement\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\napproach, there will be no need to generate separate REST query handlers, just\nquery servers as described above as grpc-gateway handles the translation of protobuf\nto REST as well as Swagger definitions.\nThe SDK should provide CLI commands for apps to start GRPC gateway either in\na separate process or the same process as the ABCI app, as well as provide a\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\nfile.\n### Client Usage\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\ninterface like:\n```go\ntype QueryClient interface {\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\n}\n```\nVia a small patch to gogo protobuf ([gogo/protobuf#675](https://github.com/gogo/protobuf/pull/675))\nwe have tweaked the grpc codegen to use an interface rather than concrete type\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\nfor ABCI client queries.\n1Context` will receive a new method `QueryConn` that returns a `ClientConn`\nthat routes calls to ABCI queries\nClients (such as CLI methods) will then be able to call query methods like this:\n```go\nclientCtx := client.NewContext()\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\nparams := &types.QueryBalanceParams{addr, denom}\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\n```\n### Testing\nTests would be able to create a query client directly from keeper and `sdk.Context`\nreferences using a `QueryServerTestHelper` as below:\n```go\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\nqueryClient := types.NewQueryClient(queryHelper)\n```\n",TBC
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to support building highly secure, robust and interoperable blockchain\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\nevidence can be submitted, evaluated and verified resulting in some agreed upon\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\nFurthermore, such a mechanism is paramount for any\n[IBC](https://github.com/cosmos/ics/blob/master/ibc/2_IBC_ARCHITECTURE.md) or\ncross-chain validation protocol implementation in order to support the ability\nfor any misbehavior to be relayed back from a collateralized chain to a primary\nchain so that the equivocating validator(s) can be slashed.\n\n## Decision\n","We will implement an evidence module in the Cosmos SDK supporting the following\nfunctionality:\n- Provide developers with the abstractions and interfaces necessary to define\ncustom evidence messages, message handlers, and methods to slash and penalize\naccordingly for misbehavior.\n- Support the ability to route evidence messages to handlers in any module to\ndetermine the validity of submitted misbehavior.\n- Support the ability, through governance, to modify slashing penalties of any\nevidence type.\n- Querier implementation to support querying params, evidence types, params, and\nall submitted valid misbehavior.\n### Types\nFirst, we define the `Evidence` interface type. The `x/evidence` module may implement\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\nIn addition, other modules may implement their own `Evidence` types in a similar\nmanner in which governance is extensible. It is important to note any concrete\ntype implementing the `Evidence` interface may include arbitrary fields such as\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\nWhen submitting evidence to the `x/evidence` module, the concrete type must provide\nthe validator's consensus address, which should be known by the `x/slashing`\nmodule (assuming the infraction is valid), the height at which the infraction\noccurred and the validator's power at same height in which the infraction occurred.\n```go\ntype Evidence interface {\nRoute() string\nType() string\nString() string\nHash() HexBytes\nValidateBasic() error\n// The consensus address of the malicious validator at time of infraction\nGetConsensusAddress() ConsAddress\n// Height at which the infraction occurred\nGetHeight() int64\n// The total power of the malicious validator at time of infraction\nGetValidatorPower() int64\n// The total validator set power at time of infraction\nGetTotalPower() int64\n}\n```\n### Routing & Handling\nEach `Evidence` type must map to a specific unique route and be registered with\nthe `x/evidence` module. It accomplishes this through the `Router` implementation.\n```go\ntype Router interface {\nAddRoute(r string, h Handler) Router\nHasRoute(r string) bool\nGetRoute(path string) Handler\nSeal()\n}\n```\nUpon successful routing through the `x/evidence` module, the `Evidence` type\nis passed through a `Handler`. This `Handler` is responsible for executing all\ncorresponding business logic necessary for verifying the evidence as valid. In\naddition, the `Handler` may execute any necessary slashing and potential jailing.\nSince slashing fractions will typically result from some form of static functions,\nallow the `Handler` to do this provides the greatest flexibility. An example could\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\nby governance. The `Evidence` type should provide all the external information\nnecessary in order for the `Handler` to make the necessary state transitions.\nIf no error is returned, the `Evidence` is considered valid.\n```go\ntype Handler func(Context, Evidence) error\n```\n### Submission\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\nhandled by the `x/evidence` module's `SubmitEvidence`.\n```go\ntype MsgSubmitEvidence struct {\nEvidence\n}\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\nreturn err.Result()\n}\n// emit events...\nreturn Result{\n// ...\n}\n}\n```\nThe `x/evidence` module's keeper is responsible for matching the `Evidence` against\nthe module's router and invoking the corresponding `Handler` which may include\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\n```go\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\nhandler := keeper.router.GetRoute(evidence.Route())\nif err := handler(ctx, evidence); err != nil {\nreturn ErrInvalidEvidence(keeper.codespace, err)\n}\nkeeper.setEvidence(ctx, evidence)\nreturn nil\n}\n```\n### Genesis\nFinally, we need to represent the genesis state of the `x/evidence` module. The\nmodule only needs a list of all submitted valid infractions and any necessary params\nfor which the module needs in order to handle submitted evidence. The `x/evidence`\nmodule will naturally define and route native evidence types for which it'll most\nlikely need slashing penalty constants for.\n```go\ntype GenesisState struct {\nParams       Params\nInfractions  []Evidence\n}\n```\n","The Cosmos SDK will implement a generic slashing mechanism in which arbitrary evidence can be submitted, evaluated and verified resulting in some agreed upon penalty for any misbehavior committed by a validator. The slashing mechanism will be designed to support a wide range of misbehavior types, including equivocation (double-voting), signing when unbonded, and signing an incorrect state transition. The slashing mechanism will also be extensible to support new misbehavior types in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\nBy default, when telemetry is enabled, the application should track and expose metrics that are\nstored in-memory. The operator may choose to enable additional sinks, where we support only\n[Prometheus](https://prometheus.io/) for now, as it's battle-tested, simple to setup, open source,\nand is rich with ecosystem tooling.\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\nmetrics may be added or removed at will and without much friction. To do this, we will use the\n[go-metrics](https://github.com/armon/go-metrics) library.\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\nwill be exposed via `/metrics?format={text|prometheus}` via the API server.\n\n## Decision\n","We will add an additional configuration block to `app.toml` that defines telemetry settings:\n```toml\n###############################################################################\n###                         Telemetry Configuration                         ###\n###############################################################################\n[telemetry]\n# Prefixed with keys to separate services\nservice-name = {{ .Telemetry.ServiceName }}\n# Enabled enables the application telemetry functionality. When enabled,\n# an in-memory sink is also enabled by default. Operators may also enabled\n# other sinks such as Prometheus.\nenabled = {{ .Telemetry.Enabled }}\n# Enable prefixing gauge values with hostname\nenable-hostname = {{ .Telemetry.EnableHostname }}\n# Enable adding hostname to labels\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\n# Enable adding service to labels\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\n```\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\n```go\n// Metrics defines a wrapper around application telemetry functionality. It allows\n// metrics to be gathered at any point in time. When creating a Metrics object,\n// internally, a global metrics is registered with a set of sinks as configured\n// by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\n// dump of formatted recent metrics will be sent to STDERR.\ntype Metrics struct {\nmemSink           *metrics.InmemSink\nprometheusEnabled bool\n}\n// Gather collects all registered metrics and returns a GatherResponse where the\n// metrics are encoded depending on the type. Metrics are either encoded via\n// Prometheus or JSON if in-memory.\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\nswitch format {\ncase FormatPrometheus:\nreturn m.gatherPrometheus()\ncase FormatText:\nreturn m.gatherGeneric()\ncase FormatDefault:\nreturn m.gatherGeneric()\ndefault:\nreturn GatherResponse{}, fmt.Errorf(""unsupported metrics format: %s"", format)\n}\n}\n```\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\nhandler accordingly.\n```go\nfunc (s *Server) Start(cfg config.Config) error {\n// ...\nif cfg.Telemetry.Enabled {\nm, err := telemetry.New(cfg.Telemetry)\nif err != nil {\nreturn err\n}\ns.metrics = m\ns.registerMetrics()\n}\n// ...\n}\nfunc (s *Server) registerMetrics() {\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\nformat := strings.TrimSpace(r.FormValue(""format""))\ngr, err := s.metrics.Gather(format)\nif err != nil {\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(""failed to gather metrics: %s"", err))\nreturn\n}\nw.Header().Set(""Content-Type"", gr.ContentType)\n_, _ = w.Write(gr.Metrics)\n}\ns.Router.HandleFunc(""/metrics"", metricsHandler).Methods(""GET"")\n}\n```\nApplication developers may track counters, gauges, summaries, and key/value metrics. There is no\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\n```go\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\ndefer metrics.MeasureSince(time.Now(), ""MintCoins"")\n// ...\n}\n```\n","To implement telemetry, we will use the go-metrics library to expose metrics from modules and other core parts of the Cosmos SDK. Metrics will be stored in-memory by default, and operators can choose to enable additional sinks, such as Prometheus. When telemetry is enabled, metrics will be exposed via `/metrics?format={text|prometheus}` via the API server. Operators can enable telemetry with specific configuration options."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\nIdeally, we would have:\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\n\n## Decision\n","Re-structure the `/docs` folder of the SDK github repo as follows:\n```\ndocs/\n├── README\n├── intro/\n├── concepts/\n│   ├── baseapp\n│   ├── types\n│   ├── store\n│   ├── server\n│   ├── modules/\n│   │   ├── keeper\n│   │   ├── handler\n│   │   ├── cli\n│   ├── gas\n│   └── commands\n├── clients/\n│   ├── lite/\n│   ├── service-providers\n├── modules/\n├── spec/\n├── translations/\n└── architecture/\n```\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\n- `README`: Landing page of the docs.\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https://github.com/cosmos/sdk-application-tutorial/) will be highlighted, as well as the `godocs`.\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\n- `clients`: Contains specs and info about the various SDK clients.\n- `spec`: Contains specs of modules, and others.\n- `modules`: Contains links to `godocs` and the spec of the modules.\n- `architecture`: Contains architecture-related docs like the present one.\n- `translations`: Contains different translations of the documentation.\nWebsite docs sidebar will only include the following sections:\n- `README`\n- `intro`\n- `concepts`\n- `clients`\n`architecture` need not be displayed on the website.\n","Move all documentation related to specific SDKs, frameworks, or tools to their respective GitHub repositories. This includes:\n\n* SDK documentation to the SDK repository\n* Hub documentation to the Hub repository\n* Lotion documentation to the Lotion repository\n\nKeep all other documentation, such as FAQs, whitepapers, and high-level material about Cosmos, on the website."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFull implementation of the [IBC specification](https://github.com/cosmos/ics) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\nas described in [ICS 5](https://github.com/cosmos/ics/tree/master/spec/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\nobject-capability keys at this time.\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L132))\nand passed to Keepers as fixed arguments ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L160)). Keepers cannot create or store capability keys during transaction execution — although they could call `NewKVStoreKey` and take the memory address\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional — were this not the case, the keys would be predictable and couldn't serve as object capabilities).\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\nThis ADR proposes such an interface & mechanism.\n\n## Decision\n","The SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\ninvoked.\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\nchain is running.\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\nThe persistent `KVStore` tracks which capability is owned by which modules.\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\na reverse mapping that map from module name, capability name to the capability index.\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\ngo-map to retrieve the capability at the original memory location.\nThe `CapabilityKeeper` will define the following types & functions:\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\na name. A `String()` method is provided for debugging.\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\n```golang\ntype Capability struct {\nindex uint64\n}\n```\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\n```golang\ntype CapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleNames   map[string]interface{}\nsealed        bool\n}\n```\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\npassed by other modules.\n```golang\ntype ScopedCapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleName    string\n}\n```\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\nIt MUST be called before `InitialiseAndSeal`.\n```golang\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\nif k.sealed {\npanic(""cannot scope to module via a sealed capability keeper"")\n}\nif _, ok := k.scopedModules[moduleName]; ok {\npanic(fmt.Sprintf(""cannot create multiple scoped keepers for the same module name: %s"", moduleName))\n}\nk.scopedModules[moduleName] = struct{}{}\nreturn ScopedKeeper{\ncdc:      k.cdc,\nstoreKey: k.storeKey,\nmemKey:   k.memKey,\ncapMap:   k.capMap,\nmodule:   moduleName,\n}\n}\n```\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\ncreation of any new `ScopedCapabilityKeeper`s.\n```golang\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\nif ck.sealed {\npanic(""capability keeper is sealed"")\n}\npersistentStore := ctx.KVStore(ck.persistentKey)\nmap := ctx.KVStore(ck.memKey)\n// initialise memory store for all names in persistent store\nfor index, value := range persistentStore.Iter() {\ncapability = &CapabilityKey{index: index}\nfor moduleAndCapability := range value {\nmoduleName, capabilityName := moduleAndCapability.Split(""/"")\nmemStore.Set(moduleName + ""/fwd/"" + capability, capabilityName)\nmemStore.Set(moduleName + ""/rev/"" + capabilityName, index)\nck.capMap[index] = capability\n}\n}\nck.sealed = true\n}\n```\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\nreference. The newly created capability is automatically persisted; the calling module need not\ncall `ClaimCapability`.\n```golang\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\n// check name not taken in memory store\nif capStore.Get(""rev/"" + name) != nil {\nreturn nil, errors.New(""name already taken"")\n}\n// fetch the current index\nindex := persistentStore.Get(""index"")\n// create a new capability\ncapability := &CapabilityKey{index: index}\n// set persistent store\npersistentStore.Set(index, Set.singleton(sck.moduleName + ""/"" + name))\n// update the index\nindex++\npersistentStore.Set(""index"", index)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to index\nmemStore.Set(sck.moduleName + ""/rev/"" + name, index)\n// set the in-memory mapping from index to capability pointer\ncapMap[index] = capability\n// return the newly created capability\nreturn capability\n}\n```\n`AuthenticateCapability` can be called by any module to check that a capability\ndoes in fact correspond to a particular name (the name can be untrusted user input)\nwith which the calling module previously associated it.\n```golang\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\n// return whether forward mapping in memory store matches name\nreturn memStore.Get(sck.moduleName + ""/fwd/"" + capability) === name\n}\n```\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\nso that future `GetCapability` calls will succeed.\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\nthey will all own it.\n```golang\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\npersistentStore := ctx.KVStore(sck.persistentKey)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to capability\nmemStore.Set(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.add(sck.moduleName + ""/"" + name)\npersistentStore.Set(capability.Index(), owners)\n}\n```\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\nThe module is not allowed to retrieve capabilities which it does not own.\n```golang\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\n// fetch the index of capability using reverse mapping in memstore\nindex := memStore.Get(sck.moduleName + ""/rev/"" + name)\n// fetch capability from go-map using index\ncapability := capMap[index]\n// return the capability\nreturn capability\n}\n```\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\nmore owners exist, the capability will be deleted globally.\n```golang\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\npersistentStore := ctx.KVStore(sck.persistentKey)\nname := capStore.Get(sck.moduleName + ""/fwd/"" + capability)\nif name == nil {\nreturn error(""capability not owned by module"")\n}\n// delete forward mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/fwd/"" + capability, name)\n// delete reverse mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.remove(sck.moduleName + ""/"" + name)\nif owners.size() > 0 {\n// there are still other owners, keep the capability around\npersistentStore.Set(capability.Index(), owners)\n} else {\n// no more owners, delete the capability\npersistentStore.Delete(capability.Index())\ndelete(capMap[capability.Index()])\n}\n}\n```\n### Usage patterns\n#### Initialisation\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\n```golang\nck := NewCapabilityKeeper(persistentKey, memoryKey)\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(""mod1""), ....)\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(""mod2""), ....)\n// other initialisation logic ...\n// load initial state...\nck.InitialiseAndSeal(initialContext)\n```\n#### Creating, passing, claiming and using capabilities\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\nModule 1 would have the following code:\n```golang\ncapability := scopedCapabilityKeeper.NewCapability(ctx, ""resourceABC"")\nmod2Keeper.SomeFunction(ctx, capability, args...)\n```\n`SomeFunction`, running in module 2, could then claim the capability:\n```golang\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\nk.sck.ClaimCapability(ctx, capability, ""resourceABC"")\n// other logic...\n}\n```\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\n```golang\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\ncapability := k.sck.GetCapability(ctx, name)\nmod1.UseResource(ctx, capability, ""resourceABC"")\n}\n```\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\n```golang\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\nif !k.sck.AuthenticateCapability(name, capability) {\nreturn errors.New(""unauthenticated"")\n}\n// do something with the resource\n}\n```\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\n","To implement the full ICS specification, the Cosmos SDK must enable the creation and authentication of object-capability keys during transaction execution.\n\nTo achieve this, the following interface will be added to the Cosmos SDK:\n\n```go\ntype CapabilityKeeper interface {\n  RegisterKey(ctx sdk.Context, key *store.KVStoreKey) error\n  GetKey(ctx sdk.Context, name string) (*store.KVStoreKey, bool)\n}\n```\n\nThe `CapabilityKeeper` interface provides methods to register and retrieve `store.KVStoreKey` objects during transaction execution.\n\nTo generate unique memory addresses for capability keys, the following mechanism will be used:\n\n1. Each `store.KVStoreKey` object will be assigned a unique identifier when it is registered with the `CapabilityKeeper`.\n2. When the application is started or restarted, the unique identifiers for all registered `store.KVStoreKey` objects will be stored in the Merkle tree.\n3. When a `store.KVStoreKey` object is needed during transaction execution, the `CapabilityKeeper` will retrieve the unique identifier for the object from the Merkle tree and generate a new memory address for the object based on the identifier.\n\nThis mechanism ensures that the memory addresses for capability keys are unique and consistent across all machines, even if the application is started or restarted.\n\nTo revert capability creation on tx failure, the following mechanism will be used:\n\n1. When a transaction is executed, the `CapabilityKeeper` will create a new map of capability keys.\n2. If the transaction succeeds, the new map of capability keys will be stored in the Merkle tree.\n3. If the transaction fails, the new map of capability keys will be discarded.\n\nThis mechanism ensures that capability keys are only created if the transaction succeeds."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nSpecifically, the client-side migration path primarily includes tx generation and\nsigning, message construction and routing, in addition to CLI & REST handlers and\nbusiness logic (i.e. queriers).\nWith this in mind, we will tackle the migration path via two main areas, txs and\nquerying. However, this ADR solely focuses on transactions. Querying should be\naddressed in a future ADR, but it should build off of these proposals.\nBased on detailed discussions ([\#6030](https://github.com/cosmos/cosmos-sdk/issues/6030)\nand [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078)), the original\ndesign for transactions was changed substantially from an `oneof` /JSON-signing\napproach to the approach described below.\n\n## Decision\n","### Transactions\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\n`sdk.Msg`s are encoding with `Any` in transactions.\nOne of the main goals of using `Any` to encode interface values is to have a\ncore set of types which is reused by apps so that\nclients can safely be compatible with as many chains as possible.\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\nformat that can serve a wide variety of use cases without breaking client\ncompatibility.\nIn order to facilitate signing, transactions are separated into `TxBody`,\nwhich will be re-used by `SignDoc` below, and `signatures`:\n```proto\n// types/types.proto\npackage cosmos_sdk.v1;\nmessage Tx {\nTxBody body = 1;\nAuthInfo auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\n// A variant of Tx that pins the signer's exact binary represenation of body and\n// auth_info. This is used for signing, broadcasting and verification. The binary\n// `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\n// becomes the ""txhash"", commonly used as the transaction ID.\nmessage TxRaw {\n// A protobuf serialization of a TxBody that matches the representation in SignDoc.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\nbytes auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\nmessage TxBody {\n// A list of messages to be executed. The required signers of those messages define\n// the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\n// Each required signer address is added to the list only the first time it occurs.\n//\n// By convention, the first required signer (usually from the first message) is referred\n// to as the primary signer and pays the fee for the whole transaction.\nrepeated google.protobuf.Any messages = 1;\nstring memo = 2;\nint64 timeout_height = 3;\nrepeated google.protobuf.Any extension_options = 1023;\n}\nmessage AuthInfo {\n// This list defines the signing modes for the required signers. The number\n// and order of elements must match the required signers from TxBody's messages.\n// The first element is the primary signer and the one which pays the fee.\nrepeated SignerInfo signer_infos = 1;\n// The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\nFee fee = 2;\n}\nmessage SignerInfo {\n// The public key is optional for accounts that already exist in state. If unset, the\n// verifier can use the required signer address for this position and lookup the public key.\nPublicKey public_key = 1;\n// ModeInfo describes the signing mode of the signer and is a nested\n// structure to support nested multisig pubkey's\nModeInfo mode_info = 2;\n// sequence is the sequence of the account, which describes the\n// number of committed transactions signed by a given address. It is used to prevent\n// replay attacks.\nuint64 sequence = 3;\n}\nmessage ModeInfo {\noneof sum {\nSingle single = 1;\nMulti multi = 2;\n}\n// Single is the mode info for a single signer. It is structured as a message\n// to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\nmessage Single {\nSignMode mode = 1;\n}\n// Multi is the mode info for a multisig public key\nmessage Multi {\n// bitarray specifies which keys within the multisig are signing\nCompactBitArray bitarray = 1;\n// mode_infos is the corresponding modes of the signers of the multisig\n// which could include nested multisig public keys\nrepeated ModeInfo mode_infos = 2;\n}\n}\nenum SignMode {\nSIGN_MODE_UNSPECIFIED = 0;\nSIGN_MODE_DIRECT = 1;\nSIGN_MODE_TEXTUAL = 2;\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\n}\n```\nAs will be discussed below, in order to include as much of the `Tx` as possible\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\nraw signatures themselves live outside of what is signed over.\nBecause we are aiming for a flexible, extensible cross-chain transaction\nformat, new transaction processing options should be added to `TxBody` as soon\nthose use cases are discovered, even if they can't be implemented yet.\nBecause there is coordination overhead in this, `TxBody` includes an\n`extension_options` field which can be used for any transaction processing\noptions that are not already covered. App developers should, nevertheless,\nattempt to upstream important improvements to `Tx`.\n### Signing\nAll of the signing modes below aim to provide the following guarantees:\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\nis signed\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\nthe final gas is fully dependent on what I am signing\nThese guarantees give the maximum amount confidence to message signers that\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\n#### `SIGN_MODE_DIRECT`\nThe ""direct"" signing behavior is to sign the raw `TxBody` bytes as broadcast over\nthe wire. This has the advantages of:\n- requiring the minimum additional client capabilities beyond a standard protocol\nbuffers implementation\n- leaving effectively zero holes for transaction malleability (i.e. there are no\nsubtle differences between the signing and encoding formats which could\npotentially be exploited by an attacker)\nSignatures are structured using the `SignDoc` below which reuses the serialization of\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\n```proto\n// types/types.proto\nmessage SignDoc {\n// A protobuf serialization of a TxBody that matches the representation in TxRaw.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\nbytes auth_info = 2;\nstring chain_id = 3;\nuint64 account_number = 4;\n}\n```\nIn order to sign in the default mode, clients take the following steps:\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\n2. Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n3. Sign the encoded `SignDoc` bytes.\n4. Build a `TxRaw` and serialize it for broadcasting.\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\nbytes encoded in `TxRaw` not based on any [""canonicalization""](https://github.com/regen-network/canonical-proto3)\nalgorithm which creates added complexity for clients in addition to preventing\nsome forms of upgradeability (to be addressed later in this document).\nSignature verifiers do:\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\n2. Create a list of required signer addresses from the messages.\n3. For each required signer:\n- Pull account number and sequence from the state.\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\n- Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n- Verify the signature at the the same list position against the serialized `SignDoc`.\n#### `SIGN_MODE_LEGACY_AMINO`\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\nsupported transaction signing. Once wallets and exchanges have had a\nchance to upgrade to protobuf based signing, this option will be disabled. In\nthe meantime, it is foreseen that disabling the current Amino signing would cause\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\nCosmos Hub and other chains may choose to disable Amino signing immediately.\nLegacy clients will be able to sign a transaction using the current Amino\nJSON format and have it encoded to protobuf using the REST `/tx/encode`\nendpoint before broadcasting.\n#### `SIGN_MODE_TEXTUAL`\nAs was discussed extensively in [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078),\nthere is a desire for a human-readable signing encoding, especially for hardware\nwallets like the [Ledger](https://www.ledger.com) which display\ntransaction contents to users before signing. JSON was an attempt at this but\nfalls short of the ideal.\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\nencoding which will replace Amino JSON. This new encoding should be even more\nfocused on readability than JSON, possibly based on formatting strings like\n[MessageFormat](http://userguide.icu-project.org/formatparse/messages).\nIn order to ensure that the new human-readable format does not suffer from\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\nto generate sign bytes.\nMultiple human-readable formats (maybe even localized messages) may be supported\nby `SIGN_MODE_TEXTUAL` when it is implemented.\n### Unknown Field Filtering\nUnknown fields in protobuf messages should generally be rejected by transaction\nprocessors because:\n- important data may be present in the unknown fields, that if ignored, will\ncause unexpected behavior for clients\n- they present a malleability vulnerability where attackers can bloat tx size\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\nnot `TxBody`)\nThere are also scenarios where we may choose to safely ignore unknown fields\n(https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-624400188) to\nprovide graceful forwards compatibility with newer clients.\nWe propose that field numbers with bit 11 set (for most use cases this is\nthe range of 1024-2047) be considered non-critical fields that can safely be\nignored if unknown.\nTo handle this we will need a unknown field filter that:\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\nunsigned parts of `AuthInfo` if present based on the signing mode)\n- rejects unknown fields in all messages (including nested `Any`s) other than\nfields with bit 11 set\nThis will likely need to be a custom protobuf parser pass that takes message bytes\nand `FileDescriptor`s and returns a boolean result.\n### Public Key Encoding\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\nso a natural solution might be to use `Any` as we are doing for other interfaces.\nThere are, however, a limited number of public keys in existence and new ones\naren't created overnight. The proposed solution is to use a `oneof` that:\n- attempts to catalog all known key types even if a given app can't use them all\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\nEx:\n```proto\nmessage PublicKey {\noneof sum {\nbytes secp256k1 = 1;\nbytes ed25519 = 2;\n...\ngoogle.protobuf.Any any_pubkey = 15;\n}\n}\n```\nApps should only attempt to handle a registered set of public keys that they\nhave tested. The provided signature verification ante handler decorators will\nenforce this.\n### CLI & REST\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\nin the client can be interfaces, similar to how we described in [ADR 019](./adr-019-protobuf-state-encoding.md),\nthe client logic will now need to take a codec interface that knows not only how\nto handle all the types, but also knows how to generate transactions, signatures,\nand messages.\n```go\ntype AccountRetriever interface {\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\n}\ntype Generator interface {\nNewTx() TxBuilder\nNewFee() ClientFee\nNewSignature() ClientSignature\nMarshalTx(tx types.Tx) ([]byte, error)\n}\ntype TxBuilder interface {\nGetTx() sdk.Tx\nSetMsgs(...sdk.Msg) error\nGetSignatures() []sdk.Signature\nSetSignatures(...sdk.Signature)\nGetFee() sdk.Fee\nSetFee(sdk.Fee)\nGetMemo() string\nSetMemo(string)\n}\n```\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\na `Context` which should have all of these fields pre-populated.\nEach client method should then use one of the `Init` methods to re-initialize\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\ngenerate or broadcast a transaction. For example:\n```go\nimport ""github.com/spf13/cobra""\nimport ""github.com/cosmos/cosmos-sdk/client""\nimport ""github.com/cosmos/cosmos-sdk/client/tx""\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\nreturn &cobra.Command{\nRunE: func(cmd *cobra.Command, args []string) error {\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\nmsg := NewSomeMsg{...}\ntx.GenerateOrBroadcastTx(clientCtx, msg)\n},\n}\n}\n```\n",This ADR does not contain a Decision section.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion/consideration.\n\n## Decision\n","We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\nThus, we propose the following mechanism:\n### Params:\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\n- There is a new gov param called `MaxVotingPeriodExtension`.\n### Mechanism\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\n### Dealing with Unbonding/Redelegation\nThere is one thing that needs to be addressed.  How to deal with redelegation/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\n### Delegators\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and ""unextend"" as that would contradict the ""voting power length can only be ratcheted up"" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\n","The voting period for governance proposals will be variable, depending on the proposal's complexity and level of controversy. The governance team will assess each proposal and determine the appropriate voting period.\n\n**Pros:**\n\n* Allows for more efficient handling of non-contentious proposals\n* Provides more time for discussion and consideration of complex or contentious proposals\n* Ensures that all proposals receive the appropriate level of attention\n\n**Cons:**\n\n* May require additional effort from the governance team to assess each proposal\n* Could potentially lead to delays in the implementation of important proposals\n\n**Alternatives Considered:**\n\n* Keeping the voting period the same for all proposals\n* Implementing a tiered voting system, with different voting periods for different tiers of proposals\n\n**Decision Rationale:**\n\nThe benefits of a variable voting period outweigh the drawbacks. By allowing the voting period to vary, the governance team can ensure that each proposal receives the appropriate level of attention and consideration. This will lead to more efficient and effective governance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order for the Cosmos SDK to implement the [IBC specification](https://github.com/cosmos/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\n\n## Decision\n","The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\n```golang\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\ninfo := HistoricalInfo{\nHeader: ctx.BlockHeader(),\nValSet: keeper.StakingKeeper.GetAllValidators(ctx), // note that this must be stored in a canonical order\n}\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\nn := keeper.GetParamRecentHeadersToStore()\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\n// continue handling request\n}\n```\nAlternatively, the application MAY store only the hash of the validator set.\nThe application MUST make these past `n` committed headers available for querying by SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x/staking` or `x/ibc`).\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\n","The Cosmos SDK will introduce the `ConsensusState` interface that modules can implement to provide a representation of the consensus state at a given height. Modules will also implement a `ConsensusParam` interface, which will provide information about the consensus parameters such as the chain id, block gas limit, and minimum gas prices. These interfaces will be used by the IBC module to generate proofs of consensus states and consensus parameters, which can be used to verify the validity of handshakes on other chains."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the Cosmos SDK utilizes [go-amino](https://github.com/tendermint/go-amino/) for binary\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\nFrom the Amino docs:\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\n> support. See the [Proto3 spec](https://developers.google.com/protocol-buffers/docs/proto3) for more\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\n>\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\nAmino also aims to have the following goals (not a complete list):\n- Binary bytes must be decode-able with a schema.\n- Schema must be upgradeable.\n- The encoder and decoder logic must be reasonably simple.\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\nclients written in various languages while providing virtually little in the way of true backwards\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1</sup>. This is\nlargely reflected in the performance of simulations and application transaction throughput.\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\n- Language agnostic\n- Platform agnostic\n- Rich client support and thriving ecosystem\n- High performance\n- Minimal encoded message size\n- Codegen-based over reflection-based\n- Supports backward and forward compatibility\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\nmade to address client-side encoding.\n\n## Decision\n","We will adopt [Protocol Buffers](https://developers.google.com/protocol-buffers) for serializing\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\nwill provide three concrete implementations of the `Marshaler` interface: `AminoCodec`, `ProtoCodec`,\nand `HybridCodec`.\n- `AminoCodec`: Uses Amino for both binary and JSON encoding.\n- `ProtoCodec`: Uses Protobuf for or both binary and JSON encoding.\n- `HybridCodec`: Uses Amino for JSON encoding and Protobuf for binary encoding.\nUntil the client migration landscape is fully understood and designed, modules will use a `HybridCodec`\nas the concrete codec it accepts and/or extends. This means that all client JSON encoding, including\ngenesis state, will still use Amino. The ultimate goal will be to replace Amino JSON encoding with\nProtbuf encoding and thus have modules accept and/or extend `ProtoCodec`.\n### Module Codecs\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\n`Marshaler` that will be a `HybridCodec`. This migration is simple as things will just work as-is.\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\n[gogoprotobuf](https://github.com/gogo/protobuf) Value types.\nExample:\n```go\nts, err := gogotypes.TimestampProto(completionTime)\nif err != nil {\n// ...\n}\nbz := cdc.MustMarshalBinaryBare(ts)\n```\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\nto the module and will contain method contracts that know how to serialize the needed interfaces.\nExample:\n```go\n// x/auth/types/codec.go\ntype Codec interface {\ncodec.Marshaler\nMarshalAccount(acc exported.Account) ([]byte, error)\nUnmarshalAccount(bz []byte) (exported.Account, error)\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\n}\n```\n### Usage of `Any` to encode interfaces\nIn general, module-level .proto files should define messages which encode interfaces\nusing [`google.protobuf.Any`](https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/any.proto).\nAfter [extension discussion](https://github.com/cosmos/cosmos-sdk/issues/6030),\nthis was chosen as the preferred alternative to application-level `oneof`s\nas in our original protobuf design. The arguments in favor of `Any` can be\nsummarized as follows:\n* `Any` provides a simpler, more consistent client UX for dealing with\ninterfaces than app-level `oneof`s that will need to be coordinated more\ncarefully across applications. Creating a generic transaction\nsigning library using `oneof`s may be cumbersome and critical logic may need\nto be reimplemented for each chain\n* `Any` provides more resistance against human error than `oneof`\n* `Any` is generally simpler to implement for both modules and apps\nThe main counter-argument to using `Any` centers around its additional space\nand possibly performance overhead. The space overhead could be dealt with using\ncompression at the persistence layer in the future and the performance impact\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\nwith user experience as the higher order concern.\nNote, that given the SDK's decision to adopt the `Codec` interfaces described\nabove, apps can still choose to use `oneof` to encode state and transactions\nbut it is not the recommended approach. If apps do choose to use `oneof`s\ninstead of `Any` they will likely lose compatibility with client apps that\nsupport multiple chains. Thus developers should think carefully about whether\nthey care more about what is possibly a pre-mature optimization or end-user\nand client developer UX.\n### Safe usage of `Any`\nBy default, the [gogo protobuf implementation of `Any`](https://godoc.org/github.com/gogo/protobuf/types)\nuses [global type registration]( https://github.com/gogo/protobuf/blob/master/proto/properties.go#L540)\nto decode values packed in `Any` into concrete\ngo types. This introduces a vulnerability where any malicious module\nin the dependency tree could registry a type with the global protobuf registry\nand cause it to be loaded and unmarshaled by a transaction that referenced\nit in the `type_url` field.\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\nvalues into concrete types through the `InterfaceRegistry` interface which\nbears some similarity to type registration with Amino:\n```go\ntype InterfaceRegistry interface {\n// RegisterInterface associates protoName as the public name for the\n// interface passed in as iface\n// Ex:\n//   registry.RegisterInterface(""cosmos_sdk.Msg"", (*sdk.Msg)(nil))\nRegisterInterface(protoName string, iface interface{})\n// RegisterImplementations registers impls as a concrete implementations of\n// the interface iface\n// Ex:\n//  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\nRegisterImplementations(iface interface{}, impls ...proto.Message)\n}\n```\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\nto communicate the list of concrete types that satisfy an interface to clients.\nIn .proto files:\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\nmay be used via code generation, reflection &/or static linting.\nThe same struct that implements `InterfaceRegistry` will also implement an\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\n```go\ntype InterfaceUnpacker interface {\n// UnpackAny unpacks the value in any to the interface pointer passed in as\n// iface. Note that the type in any must have been registered with\n// RegisterImplementations as a concrete type for that interface\n// Ex:\n//    var msg sdk.Msg\n//    err := ctx.UnpackAny(any, &msg)\n//    ...\nUnpackAny(any *Any, iface interface{}) error\n}\n```\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\nusage of `Any`, it just introduces a security and introspection layer for\ngolang usage.\n`InterfaceRegistry` will be a member of `ProtoCodec` and `HybridCodec` as\ndescribed above. In order for modules to register interface types, app modules\ncan optionally implement the following interface:\n```go\ntype InterfaceModule interface {\nRegisterInterfaceTypes(InterfaceRegistry)\n}\n```\nThe module manager will include a method to call `RegisterInterfaceTypes` on\nevery module that implements it in order to populate the `InterfaceRegistry`.\n### Using `Any` to encode state\nThe SDK will provide support methods `MarshalAny` and `UnmarshalAny` to allow\neasy encoding of state to `Any` in `Codec` implementations. Ex:\n```go\nimport ""github.com/cosmos/cosmos-sdk/codec""\nfunc (c *Codec) MarshalEvidence(evidenceI eviexported.Evidence) ([]byte, error) {\nreturn codec.MarshalAny(evidenceI)\n}\nfunc (c *Codec) UnmarshalEvidence(bz []byte) (eviexported.Evidence, error) {\nvar evi eviexported.Evidence\nerr := codec.UnmarshalAny(c.interfaceContext, &evi, bz)\nif err != nil {\nreturn nil, err\n}\nreturn evi, nil\n}\n```\n### Using `Any` in `sdk.Msg`s\nA similar concept is to be applied for messages that contain interfaces fields.\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\nan interface:\n```protobuf\n// x/evidence/types/types.proto\nmessage MsgSubmitEvidence {\nbytes submitter = 1\n[\n(gogoproto.casttype) = ""github.com/cosmos/cosmos-sdk/types.AccAddress""\n];\ngoogle.protobuf.Any evidence = 2;\n}\n```\nNote that in order to unpack the evidence from `Any` we do need a reference to\n`InterfaceRegistry`. In order to reference evidence in methods like\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\ninterfaces before they're needed.\n### Unpacking Interfaces\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\ninterfaces wrapped in `Any` before they're needed, we create an interface\nthat `sdk.Msg`s and other types can implement:\n```go\ntype UnpackInterfacesMessage interface {\nUnpackInterfaces(InterfaceUnpacker) error\n}\n```\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\nstruct itself with a public getter `GetCachedValue() interface{}`.\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\nand stored in `cachedValue` for reference later.\nThen unpacked interface values can safely be used in any code afterwards\nwithout knowledge of the `InterfaceRegistry`\nand messages can introduce a simple getter to cast the cached value to the\ncorrect interface type.\nThis has the added benefit that unmarshaling of `Any` values only happens once\nduring initial deserialization rather than every time the value is read. Also,\nwhen `Any` values are first packed (for instance in a call to\n`NewMsgSubmitEvidence`), the original interface value is cached so that\nunmarshaling isn't needed to read it again.\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\n`GetEvidence` as follows:\n```go\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\nvar evi eviexported.Evidence\nreturn ctx.UnpackAny(msg.Evidence, *evi)\n}\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\n}\n```\n### Amino Compatibility\nOur custom implementation of `Any` can be used transparently with Amino if used\nwith the proper codec instance. What this means is that interfaces packed within\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\nhave been registered properly with Amino).\nIn order for this functionality to work:\n- **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\nnow a wrapper which properly handles `Any`**\n- **all new code should use `Marshaler` which is compatible with both amino and\nprotobuf**\n- Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\n### Why Wasn't X Chosen Instead\nFor a more complete comparison to alternative protocols, see [here](https://codeburst.io/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\n### Cap'n Proto\nWhile [Cap’n Proto](https://capnproto.org/) does seem like an advantageous alternative to Protobuf\ndue to it's native support for interfaces/generics and built in canonicalization, it does lack the\nrich client ecosystem compared to Protobuf and is a bit less mature.\n### FlatBuffers\n[FlatBuffers](https://google.github.io/flatbuffers/) is also a potentially viable alternative, with the\nprimary difference being that FlatBuffers does not need a parsing/unpacking step to a secondary\nrepresentation before you can access data, often coupled with per-object memory allocation.\nHowever, it would require great efforts into research and full understanding the scope of the migration\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\nuntrusted inputs.\n",Migrate the Cosmos SDK state machine to utilize Protobuf for state serialization.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\n\n## Decision\n","In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`’s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\nDetailed changes:\n### 1) (Un)Marshal accounts directly using amino\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\n```go\n// GenesisState - all auth state that must be provided at genesis\ntype GenesisState struct {\nParams   Params           `json:""params"" yaml:""params""`\nAccounts []GenesisAccount `json:""accounts"" yaml:""accounts""`\n}\n```\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\n```go\n// InitGenesis - Init store state from genesis data\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\nak.SetParams(ctx, data.Params)\n// load the accounts\nfor _, a := range data.Accounts {\nacc := ak.NewAccount(ctx, a) // set account number\nak.SetAccount(ctx, acc)\n}\n}\n// ExportGenesis returns a GenesisState for a given context and keeper\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\nparams := ak.GetParams(ctx)\nvar genAccounts []exported.GenesisAccount\nak.IterateAccounts(ctx, func(account exported.Account) bool {\ngenAccount := account.(exported.GenesisAccount)\ngenAccounts = append(genAccounts, genAccount)\nreturn false\n})\nreturn NewGenesisState(params, genAccounts)\n}\n```\n### 2) Register custom account types on the `auth` codec\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\nAn example custom account definition:\n```go\nimport authtypes ""github.com/cosmos/cosmos-sdk/x/auth/types""\n// Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\nfunc init() {\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, ""cosmos-sdk/ModuleAccount"")\n}\ntype ModuleAccount struct {\n...\n```\nThe `auth` codec definition:\n```go\nvar ModuleCdc *codec.LegacyAmino\nfunc init() {\nModuleCdc = codec.NewLegacyAmino()\n// register module msg's and Account interface\n...\n// leave the codec unsealed\n}\n// RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\nModuleCdc.RegisterConcrete(o, name, nil)\n}\n```\n### 3) Genesis validation for custom account types\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\n```go\ntype GenesisAccount interface {\nexported.Account\nValidate() error\n}\n```\nThen the `auth` `ValidateGenesis` function becomes:\n```go\n// ValidateGenesis performs basic validation of auth genesis data returning an\n// error for any failed validation criteria.\nfunc ValidateGenesis(data GenesisState) error {\n// Validate params\n...\n// Validate accounts\naddrMap := make(map[string]bool, len(data.Accounts))\nfor _, acc := range data.Accounts {\n// check for duplicated accounts\naddrStr := acc.GetAddress().String()\nif _, ok := addrMap[addrStr]; ok {\nreturn fmt.Errorf(""duplicate account found in genesis state; address: %s"", addrStr)\n}\naddrMap[addrStr] = true\n// check account specific validation\nif err := acc.Validate(); err != nil {\nreturn fmt.Errorf(""invalid account found in genesis state; address: %s, error: %s"", addrStr, err.Error())\n}\n}\nreturn nil\n}\n```\n### 4) Move add-genesis-account cli to `auth`\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\n### 5) Update module and vesting accounts\nUnder the new scheme, module and vesting account types need some minor updates:\n- Type registration on `auth`'s codec (shown above)\n- A `Validate` method for each `Account` concrete type\n",**Decision:** Implement account export/import in the auth keeper.\n\n**Rationale:**\n\n* **Centralized account management:** Moving account export/import to the auth keeper centralizes the logic and reduces the need for external tooling.\n* **Improved extensibility:** Allowing the auth keeper to handle all account types simplifies the process of adding new custom accounts to the SDK.\n* **Reduced maintenance burden:** Developers using custom accounts no longer have to fork and modify `genaccounts`.\n* **Enhanced security:** Exporting and importing accounts through the auth keeper ensures that the process is secure and tamper-proof.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[ICS 26 - Routing Module](https://github.com/cosmos/ics/tree/master/spec/ics-026-routing-module) defines a function [`handlePacketRecv`](https://github.com/cosmos/ics/tree/master/spec/ics-026-routing-module#packet-relay).\nIn ICS 26, the routing module is defined as a layer above each application module\nwhich verifies and routes messages to the destination modules. It is possible to\nimplement it as a separate module, however, we already have functionality to route\nmessages upon the destination identifiers in the baseapp. This ADR suggests\nto utilize existing `baseapp.router` to route packets to application modules.\nGenerally, routing module callbacks have two separate steps in them,\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\nin order to increase developer ergonomics by reducing boilerplate\nverification code.\nFor atomic multi-message transaction, we want to keep the IBC related\nstate modification to be preserved even the application side state change\nreverts. One of the example might be IBC token sending message following with\nstake delegation which uses the tokens received by the previous packet message.\nIf the token receiving fails for any reason, we might not want to keep\nexecuting the transaction, but we also don't want to abort the transaction\nor the sequence and commitment will be reverted and the channel will be stuck.\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\n\n## Decision\n","`PortKeeper` will have the capability key that is able to access only the\nchannels bound to the port. Entities that hold a `PortKeeper` will be\nable to call the methods on it which are corresponding with the methods with\nthe same names on the `ChannelKeeper`, but only with the\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\neasily construct a capability-safe `PortKeeper`. This will be addressed in\nanother ADR and we will use insecure `ChannelKeeper` for now.\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\n`Result.Code == CodeTxBreak`.\n```go\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\nmsgs := tx.GetMsgs()\n// AnteHandler\nif app.anteHandler != nil {\nanteCtx, msCache := app.cacheTxContext(ctx)\nnewCtx, err := app.anteHandler(anteCtx, tx)\nif !newCtx.IsZero() {\nctx = newCtx.WithMultiStore(ms)\n}\nif err != nil {\n// error handling logic\nreturn res\n}\nmsCache.Write()\n}\n// Main Handler\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\nresult = app.runMsgs(runMsgCtx, msgs)\n// BEGIN modification made in this ADR\nif result.IsOK() || result.IsBreak() {\n// END\nmsCache.Write()\n}\nreturn result\n}\n```\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\n`AnteDecorator` will iterate over the messages included in the transaction, type\n`switch` to check whether the message contains an incoming IBC packet, and if so\nverify the Merkle proof.\n```go\ntype ProofVerificationDecorator struct {\nclientKeeper ClientKeeper\nchannelKeeper ChannelKeeper\n}\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\nfor _, msg := range tx.GetMsgs() {\nvar err error\nswitch msg := msg.(type) {\ncase client.MsgUpdateClient:\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\ncase channel.MsgPacket:\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\ncase chanel.MsgAcknowledgement:\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\ncase channel.MsgTimeoutPacket:\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\ncase channel.MsgChannelOpenInit;\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\ndefault:\ncontinue\n}\nif err != nil {\nreturn ctx, err\n}\n}\nreturn next(ctx, tx, simulate)\n}\n```\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\nrespectively.\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\n`VerifyTimeout` will be extracted out into separated functions,\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\nwhich will be called by the application handlers after the execution.\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\nverified by the counter-party chain and increments the sequence to prevent\ndouble execution. `DeleteCommitment` will delete the commitment stored,\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\nof ordered channel.\n```go\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\n}\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\n}\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\nif channel.Ordering == types.ORDERED [\nchannel.State = types.CLOSED\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\n}\n}\n```\nEach application handler should call respective finalization methods on the `PortKeeper`\nin order to increase sequence (in case of packet) or remove the commitment\n(in case of acknowledgement and timeout).\nCalling those functions implies that the application logic has successfully executed.\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\nwhich will persist the state changes that has been already done but prevent any further\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\nincreased in the previous IBC packets(thus preventing double execution) without\nproceeding to the following messages.\nIn any case the application modules should never return state reverting result,\nwhich will make the channel unable to proceed.\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\nunder the routing module specification. Instead of define each channel handshake callback\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\n`CheckOpen` will find the correct `ChennelChecker` using the\n`PortID` and call it, which will return an error if it is unacceptable by the application.\nThe `ProofVerificationDecorator` will be inserted to the top level application.\nIt is not safe to make each module responsible to call proof verification\nlogic, whereas application can misbehave(in terms of IBC protocol) by\nmistake.\nThe `ProofVerificationDecorator` should come right after the default sybil attack\nresistent layer from the current `auth.NewAnteHandler`:\n```go\n// add IBC ProofVerificationDecorator to the Chain of\nfunc NewAnteHandler(\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\nreturn sdk.ChainAnteDecorators(\nNewSetUpContextDecorator(), // outermost AnteDecorator. SetUpContext must be called first\n...\nNewIncrementSequenceDecorator(ak),\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), // innermost AnteDecorator\n)\n}\n```\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\nExample application-side usage:\n```go\ntype AppModule struct {}\n// CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\nif channel.Ordering != UNORDERED {\nreturn ErrUncompatibleOrdering()\n}\nif channel.CounterpartyPort != ""bank"" {\nreturn ErrUncompatiblePort()\n}\nif channel.Version != """" {\nreturn ErrUncompatibleVersion()\n}\nreturn nil\n}\nfunc NewHandler(k Keeper) Handler {\nreturn func(ctx Context, msg Msg) Result {\nswitch msg := msg.(type) {\ncase MsgTransfer:\nreturn handleMsgTransfer(ctx, k, msg)\ncase ibc.MsgPacket:\nvar data PacketDataTransfer\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\nreturn err\n}\nreturn handlePacketDataTransfer(ctx, k, msg, data)\ncase ibc.MsgTimeoutPacket:\nvar data PacketDataTransfer\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\nreturn err\n}\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\n// interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\n// MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\ncase ibc.MsgChannelOpen:\nreturn handleMsgChannelOpen(ctx, k, msg)\n}\n}\n}\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\nif err != nil {\nreturn sdk.ResultFromError(err)\n}\nreturn sdk.Result{}\n}\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\nif err != nil {\n// TODO: Source chain sent invalid packet, shutdown channel\n}\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) // WriteAcknowledgement increases the sequence, preventing double spending\nreturn sdk.Result{}\n}\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\nif err != nil {\n// This chain sent invalid packet or cannot recover the funds\npanic(err)\n}\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\n// packet timeout should not fail\nreturn sdk.Result{}\n}\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\nreturn sdk.Result{}\n}\n```\n","**Decision:** Utilize the existing `baseapp.router` to route packets to application modules, and implement the verification step in the `AnteHandler` to reduce boilerplate verification code. Introduce a new `CodeType`, `CodeTxBreak`, to handle the specific case of IBC related state modification in atomic multi-message transactions, ensuring that such modifications are preserved even if the application side state change reverts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained.\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto ""circuit-break"" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established.\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to ""pre-approve"" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.\n\n## Decision\n","The dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the\nimplementation of:\n- continuous voting\n- slashing due to breach of soft contract\n- revoking a member due to breach of soft contract\n- emergency disband of the entire dCERT group (ex. for colluding maliciously)\n- compensation stipend from the community pool or other means decided by\ngovernance\nThis system necessitates the following new parameters:\n- blockly stipend allowance per dCERT member\n- maximum number of dCERT members\n- required staked slashable tokens for each dCERT member\n- quorum for suspending a particular member\n- proposal wager for disbanding the dCERT group\n- stabilization period for dCERT member transition\n- circuit break dCERT privileges enabled\nThese parameters are expected to be implemented through the param keeper such\nthat governance may change them at any given point.\n### Continuous Voting Electionator\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n- All delegation addresses may submit votes at any point which updates their\npreferred representation on the dCERT group.\n- Preferred representation may be arbitrarily split between addresses (ex. 50%\nto John, 25% to Sally, 25% to Carol)\n- In order for a new member to be added to the dCERT group they must\nsend a transaction accepting their admission at which point the validity of\ntheir admission is to be confirmed.\n- A sequence number is assigned when a member is added to dCERT group.\nIf a member leaves the dCERT group and then enters back, a new sequence number\nis assigned.\n- Addresses which control the greatest amount of preferred-representation are\neligible to join the dCERT group (up the _maximum number of dCERT members_).\nIf the dCERT group is already full and new member is admitted, the existing\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\n- In the split situation where the dCERT group is full but a vying candidate\nhas the same amount of vote as an existing dCERT member, the existing\nmember should maintain its position.\n- In the split situation where somebody must be kicked out but the two\naddresses with the smallest number of votes have the same number of votes,\nthe address with the smallest sequence number maintains its position.\n- A stabilization period can be optionally included to reduce the\n""flip-flopping"" of the dCERT membership tail members. If a stabilization\nperiod is provided which is greater than 0, when members are kicked due to\ninsufficient support, a queue entry is created which documents which member is\nto replace which other member. While this entry is in the queue, no new entries\nto kick that same dCERT member can be made. When the entry matures at the\nduration of the  stabilization period, the new member is instantiated, and old\nmember kicked.\n### Staking/Slashing\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed.\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes.\n### dCERT membership transactions\nActive dCERT members\n- change of the description of the dCERT group\n- circuit break a message route\n- vote to suspend a dCERT member.\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: ""disable all staking-delegation messages"", or\n""disable all distribution messages"". This could be accomplished by verifying\nthat the message route has not been ""circuit-broken"" at CheckTx time (in\n`baseapp/baseapp.go`).\n""unbreaking"" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired.\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n","Create a specialization group called the Decentralized Computer Emergency Response Team (dCERT) to coordinate between various actors within a blockchain community during emergency scenarios, such as validators, bug-hunters, and developers. Grant the dCERT group the privilege to temporarily disable a particular message path, enabled/disabled globally with a governance parameter. Consider expanding the roles of dCERT in the future to include ""pre-approving"" security updates on behalf of the community."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\n\n## Decision\n","### Design\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\n```\nslash_amount = k * power // power is the faulting validator's voting power and k is some on-chain constant\n```\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\n```\nslash_amount = k * (power_1 + power_2 + ... + power_n) // where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\n```\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\n```\nslash_amount = k * ((power_1)^(1/r) + (power_2)^(1/r) + ... + (power_n)^(1/r))^r // where k and r are both on-chain constants\n```\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\n#### Correlation across non-sybil validators\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\n#### Parameterization\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\n#### Griefing\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\n### Implementation\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define ""recent slashes"" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define ""recent slashes"" as ones that have occured withing the last `jail period`.\n```\ntype SlashEvent struct {\nAddress                     sdk.ValAddress\nSqrtValidatorVotingPercent  sdk.Dec\nSlashedSoFar                sdk.Dec\n}\n```\nThese slash events will be pruned from the queue once they are older than their respective ""recent slash period"".\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the ""Square of Sum of Roots"" formula introduced above.\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\n","**Decision:** Implement a mechanism to charge large validators and their delegators a fee proportional to the amount of consensus power they hold.\n\n**Rationale:**\n\n* This fee aligns incentives and internalizes the negative externalities of centralization.\n* Validators and delegators who contribute to centralization will bear the associated costs, discouraging excessive concentration of consensus power.\n* The fees can be used to offset the risks associated with centralization, such as funding security audits or measures to prevent fork attacks.\n* This approach provides a transparent and fair method for distributing the costs of centralization to those who benefit from it.\n* The fee structure can be adjusted over time based on the desired level of decentralization and the amount of consensus power held by large validators."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe specification for IBC cross-chain fungible token transfers\n([ICS20](https://github.com/cosmos/ics/tree/master/spec/ics-020-fungible-token-transfer)), needs to\nbe aware of the origin of any token denomination in order to relay a `Packet` which contains the sender\nand recipient addressed in the\n[`FungibleTokenPacketData`](https://github.com/cosmos/ics/tree/master/spec/ics-020-fungible-token-transfer#data-structures).\nThe Packet relay sending works based in 2 cases (per\n[specification](https://github.com/cosmos/ics/tree/master/spec/ics-020-fungible-token-transfer#packet-relay) and [Colin Axnér](https://github.com/colin-axner)'s description):\n1. Sender chain is acting as the source zone. The coins are transferred\nto an escrow address (i.e locked) on the sender chain and then transferred\nto the receiving chain through IBC TAO logic. It is expected that the\nreceiving chain will mint vouchers to the receiving address.\n2. Sender chain is acting as the sink zone. The coins (vouchers) are burned\non the sender chain and then transferred to the receiving chain though IBC\nTAO logic. It is expected that the receiving chain, which had previously\nsent the original denomination, will unescrow the fungible token and send\nit to the receiving address.\nAnother way of thinking of source and sink zones is through the token's\ntimeline. Each send to any chain other than the one it was previously\nreceived from is a movement forwards in the token's timeline. This causes\ntrace to be added to the token's history and the destination port and\ndestination channel to be prefixed to the denomination. In these instances\nthe sender chain is acting as the source zone. When the token is sent back\nto the chain it previously received from, the prefix is removed. This is\na backwards movement in the token's timeline and the sender chain\nis acting as the sink zone.\n### Example\nAssume the following channel connections exist and that all channels use the port ID `transfer`:\n- chain `A` has channels with chain `B` and chain `C` with the IDs `channelToB` and `channelToC`, respectively\n- chain `B` has channels with chain `A` and chain `C` with the IDs `channelToA` and `channelToC`, respectively\n- chain `C` has channels with chain `A` and chain `B` with the IDs `channelToA` and `channelToB`, respectively\nThese steps of transfer between chains occur in the following order: `A -> B -> C -> A -> C`. In particular:\n1. `A -> B`: sender chain is source zone. `A` sends packet with `denom` (escrowed on `A`), `B` receives `denom` and mints and sends voucher `transfer/channelToA/denom` to recipient.\n2. `B -> C`: sender chain is source zone. `B` sends packet with `transfer/channelToA/denom` (escrowed on `B`), `C` receives `transfer/channelToA/denom` and mints and sends voucher `transfer/channelToB/transfer/channelToA/denom` to recipient.\n3. `C -> A`: sender chain is source zone. `C` sends packet with `transfer/channelToB/transfer/channelToA/denom` (escrowed on `C`), `A` receives `transfer/channelToB/transfer/channelToA/denom` and mints and sends voucher `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` to recipient.\n4. `A -> C`: sender chain is sink zone. `A` sends packet with `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` (burned on `A`), `C` receives `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom`, and unescrows and sends `transfer/channelToB/transfer/channelToA/denom` to recipient.\nThe token has a final denomination on chain `C` of `transfer/channelToB/transfer/channelToA/denom`, where `transfer/channelToB/transfer/channelToA` is the trace information.\nIn this context, upon a receive of a cross-chain fungible token transfer, if the sender chain is the source of the token, the protocol prefixes the denomination with the port and channel identifiers in the following format:\n```typescript\nprefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/denom\n```\nExample: transferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to\nEthermint's port `EthermintPort` and channel `EthermintChannel` results in `100\nEthermintPort/EthermintChannel/uatom`, where `EthermintPort/EthermintChannel/uatom` is the new\ndenomination on the receiving chain.\nIn the case those tokens are transferred back to the Hub (i.e the **source** chain), the prefix is\ntrimmed and the token denomination updated to the original one.\n### Problem\nThe problem of adding additional information to the coin denomination is twofold:\n1. The ever increasing length if tokens are transferred to zones other than the source:\nIf a token is transferred `n` times via IBC to a sink chain, the token denom will contain `n` pairs\nof prefixes, as shown on the format example above. This poses a problem because, while port and\nchannel identifiers have a maximum length of 64 each, the SDK `Coin` type only accepts denoms up to\n64 characters. Thus, a single cross-chain token, which again, is composed by the port and channels\nidentifiers plus the base denomination, can exceed the length validation for the SDK `Coins`.\nThis can result in undesired behaviours such as tokens not being able to be transferred to multiple\nsink chains if the denomination exceeds the length or unexpected `panics` due to denomination\nvalidation failing on the receiving chain.\n2. The existence of special characters and uppercase letters on the denomination:\nIn the SDK every time a `Coin` is initialized through the constructor function `NewCoin`, a validation\nof a coin's denom is performed according to a\n[Regex](https://github.com/cosmos/cosmos-sdk/blob/a940214a4923a3bf9a9161cd14bd3072299cd0c9/types/coin.go#L583),\nwhere only lowercase alphanumeric characters are accepted. While this is desirable for native denominations\nto keep a clean UX, it presents a challenge for IBC as ports and channels might be randomly\ngenerated with special and uppercase characters as per the [ICS 024 - Host\nRequirements](https://github.com/cosmos/ics/tree/master/spec/ics-024-host-requirements#paths-identifiers-separators)\nspecification.\n\n## Decision\n","The issues outlined above, are applicable only to SDK-based chains, and thus the proposed solution\nare do not require specification changes that would result in modification to other implementations\nof the ICS20 spec.\nInstead of adding the identifiers on the coin denomination directly, the proposed solution hashes\nthe denomination prefix in order to get a consistent length for all the cross-chain fungible tokens.\nThis will be used for internal storage only, and when transferred via IBC to a different chain, the\ndenomination specified on the packed data will be the full prefix path of the identifiers needed to\ntrace the token back to the originating chain, as specified on ICS20.\nThe new proposed format will be the following:\n```golang\nibcDenom = ""ibc/"" + hash(trace path + ""/"" + base denom)\n```\nThe hash function will be a SHA256 hash of the fields of the `DenomTrace`:\n```protobuf\n// DenomTrace contains the base denomination for ICS20 fungible tokens and the source tracing\n// information\nmessage DenomTrace {\n// chain of port/channel identifiers used for tracing the source of the fungible token\nstring path = 1;\n// base denomination of the relayed fungible token\nstring base_denom = 2;\n}\n```\nThe `IBCDenom` function constructs the `Coin` denomination used when creating the ICS20 fungible token packet data:\n```golang\n// Hash returns the hex bytes of the SHA256 hash of the DenomTrace fields using the following formula:\n//\n// hash = sha256(tracePath + ""/"" + baseDenom)\nfunc (dt DenomTrace) Hash() tmbytes.HexBytes {\nreturn tmhash.Sum(dt.Path + ""/"" + dt.BaseDenom)\n}\n// IBCDenom a coin denomination for an ICS20 fungible token in the format 'ibc/{hash(tracePath + baseDenom)}'.\n// If the trace is empty, it will return the base denomination.\nfunc (dt DenomTrace) IBCDenom() string {\nif dt.Path != """" {\nreturn fmt.Sprintf(""ibc/%s"", dt.Hash())\n}\nreturn dt.BaseDenom\n}\n```\n### `x/ibc-transfer` Changes\nIn order to retrieve the trace information from an IBC denomination, a lookup table needs to be\nadded to the `ibc-transfer` module. These values need to also be persisted between upgrades, meaning\nthat a new `[]DenomTrace` `GenesisState` field state needs to be added to the module:\n```golang\n// GetDenomTrace retrieves the full identifiers trace and base denomination from the store.\nfunc (k Keeper) GetDenomTrace(ctx Context, denomTraceHash []byte) (DenomTrace, bool) {\nstore := ctx.KVStore(k.storeKey)\nbz := store.Get(types.KeyDenomTrace(traceHash))\nif bz == nil {\nreturn &DenomTrace, false\n}\nvar denomTrace DenomTrace\nk.cdc.MustUnmarshalBinaryBare(bz, &denomTrace)\nreturn denomTrace, true\n}\n// HasDenomTrace checks if a the key with the given trace hash exists on the store.\nfunc (k Keeper) HasDenomTrace(ctx Context, denomTraceHash []byte)  bool {\nstore := ctx.KVStore(k.storeKey)\nreturn store.Has(types.KeyTrace(denomTraceHash))\n}\n// SetDenomTrace sets a new {trace hash -> trace} pair to the store.\nfunc (k Keeper) SetDenomTrace(ctx Context, denomTrace DenomTrace) {\nstore := ctx.KVStore(k.storeKey)\nbz := k.cdc.MustMarshalBinaryBare(&denomTrace)\nstore.Set(types.KeyTrace(denomTrace.Hash()), bz)\n}\n```\nThe `MsgTransfer` will validate that the `Coin` denomination from the `Token` field contains a valid\nhash, if the trace info is provided, or that the base denominations matches:\n```golang\nfunc (msg MsgTransfer) ValidateBasic() error {\n// ...\nreturn ValidateIBCDenom(msg.Token.Denom)\n}\n```\n```golang\n// ValidateIBCDenom validates that the given denomination is either:\n//\n//  - A valid base denomination (eg: 'uatom')\n//  - A valid fungible token representation (i.e 'ibc/{hash}') per ADR 001 https://github.com/cosmos/cosmos-sdk/blob/master/docs/architecture/adr-001-coin-source-tracing.md\nfunc ValidateIBCDenom(denom string) error {\ndenomSplit := strings.SplitN(denom, ""/"", 2)\nswitch {\ncase strings.TrimSpace(denom) == """",\nlen(denomSplit) == 1 && denomSplit[0] == ""ibc"",\nlen(denomSplit) == 2 && (denomSplit[0] != ""ibc"" || strings.TrimSpace(denomSplit[1]) == """"):\nreturn sdkerrors.Wrapf(ErrInvalidDenomForTransfer, ""denomination should be prefixed with the format 'ibc/{hash(trace + \""/\"" + %s)}'"", denom)\ncase denomSplit[0] == denom && strings.TrimSpace(denom) != """":\nreturn sdk.ValidateDenom(denom)\n}\nif _, err := ParseHexHash(denomSplit[1]); err != nil {\nreturn Wrapf(err, ""invalid denom trace hash %s"", denomSplit[1])\n}\nreturn nil\n}\n```\nThe denomination trace info only needs to be updated when token is received:\n- Receiver is **source** chain: The receiver created the token and must have the trace lookup already stored (if necessary _ie_ native token case wouldn't need a lookup).\n- Receiver is **not source** chain: Store the received info. For example, during step 1, when chain `B` receives `transfer/channelToA/denom`.\n```golang\n// SendTransfer\n// ...\nfullDenomPath := token.Denom\n// deconstruct the token denomination into the denomination trace info\n// to determine if the sender is the source chain\nif strings.HasPrefix(token.Denom, ""ibc/"") {\nfullDenomPath, err = k.DenomPathFromHash(ctx, token.Denom)\nif err != nil {\nreturn err\n}\n}\nif types.SenderChainIsSource(sourcePort, sourceChannel, fullDenomPath) {\n//...\n```\n```golang\n// DenomPathFromHash returns the full denomination path prefix from an ibc denom with a hash\n// component.\nfunc (k Keeper) DenomPathFromHash(ctx sdk.Context, denom string) (string, error) {\nhexHash := denom[4:]\nhash, err := ParseHexHash(hexHash)\nif err != nil {\nreturn """", Wrap(ErrInvalidDenomForTransfer, err.Error())\n}\ndenomTrace, found := k.GetDenomTrace(ctx, hash)\nif !found {\nreturn """", Wrap(ErrTraceNotFound, hexHash)\n}\nfullDenomPath := denomTrace.GetFullDenomPath()\nreturn fullDenomPath, nil\n}\n```\n```golang\n// OnRecvPacket\n// ...\n// This is the prefix that would have been prefixed to the denomination\n// on sender chain IF and only if the token originally came from the\n// receiving chain.\n//\n// NOTE: We use SourcePort and SourceChannel here, because the counterparty\n// chain would have prefixed with DestPort and DestChannel when originally\n// receiving this coin as seen in the ""sender chain is the source"" condition.\nif ReceiverChainIsSource(packet.GetSourcePort(), packet.GetSourceChannel(), data.Denom) {\n// sender chain is not the source, unescrow tokens\n// remove prefix added by sender chain\nvoucherPrefix := types.GetDenomPrefix(packet.GetSourcePort(), packet.GetSourceChannel())\nunprefixedDenom := data.Denom[len(voucherPrefix):]\ntoken := sdk.NewCoin(unprefixedDenom, sdk.NewIntFromUint64(data.Amount))\n// unescrow tokens\nescrowAddress := types.GetEscrowAddress(packet.GetDestPort(), packet.GetDestChannel())\nreturn k.bankKeeper.SendCoins(ctx, escrowAddress, receiver, sdk.NewCoins(token))\n}\n// sender chain is the source, mint vouchers\n// since SendPacket did not prefix the denomination, we must prefix denomination here\nsourcePrefix := types.GetDenomPrefix(packet.GetDestPort(), packet.GetDestChannel())\n// NOTE: sourcePrefix contains the trailing ""/""\nprefixedDenom := sourcePrefix + data.Denom\n// construct the denomination trace from the full raw denomination\ndenomTrace := types.ParseDenomTrace(prefixedDenom)\n// set the value to the lookup table if not stored already\ntraceHash := denomTrace.Hash()\nif !k.HasDenomTrace(ctx, traceHash) {\nk.SetDenomTrace(ctx, traceHash, denomTrace)\n}\nvoucherDenom := denomTrace.IBCDenom()\nvoucher := sdk.NewCoin(voucherDenom, sdk.NewIntFromUint64(data.Amount))\n// mint new tokens if the source of the transfer is the same chain\nif err := k.bankKeeper.MintCoins(\nctx, types.ModuleName, sdk.NewCoins(voucher),\n); err != nil {\nreturn err\n}\n// send to receiver\nreturn k.bankKeeper.SendCoinsFromModuleToAccount(\nctx, types.ModuleName, receiver, sdk.NewCoins(voucher),\n)\n```\n```golang\nfunc NewDenomTraceFromRawDenom(denom string) DenomTrace{\ndenomSplit := strings.Split(denom, ""/"")\ntrace := """"\nif len(denomSplit) > 1 {\ntrace = strings.Join(denomSplit[:len(denomSplit)-1], ""/"")\n}\nreturn DenomTrace{\nBaseDenom: denomSplit[len(denomSplit)-1],\nTrace:     trace,\n}\n}\n```\nOne final remark is that the `FungibleTokenPacketData` will remain the same, i.e with the prefixed full denomination, since the receiving chain may not be an SDK-based chain.\n### Coin Changes\nThe coin denomination validation will need to be updated to reflect these changes. In particular, the denomination validation\nfunction will now:\n- Accept slash separators (`""/""`) and uppercase characters (due to the `HexBytes` format)\n- Bump the maximum character length to 128, as the hex representation used by Tendermint's\n`HexBytes` type contains 64 characters.\nAdditional validation logic, such as verifying the length of the hash, the  may be added to the bank module in the future if the [custom base denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755) is integrated into the SDK.\n### Positive\n- Clearer separation of the source tracing behaviour of the token (transfer prefix) from the original\n`Coin` denomination\n- Consistent validation of `Coin` fields (i.e no special characters, fixed max length)\n- Cleaner `Coin` and standard denominations for IBC\n- No additional fields to SDK `Coin`\n### Negative\n- Store each set of tracing denomination identifiers on the `ibc-transfer` module store\n- Clients will have to fetch the base denomination every time they receive a new relayed fungible token over IBC. This can be mitigated using a map/cache for already seen hashes on the client side. Other forms of mitigation, would be opening a websocket connection subscribe to incoming events.\n### Neutral\n- Slight difference with the ICS20 spec\n- Additional validation logic for IBC coins on the `ibc-transfer` module\n- Additional genesis fields\n- Slightly increases the gas usage on cross-chain transfers due to access to the store. This should\nbe inter-block cached if transfers are frequent.\n","**Decision:**\n\nTo address the problem of denomination length and character restrictions in cross-chain fungible token transfers, the following solution is proposed:\n\n**Prefix Denominations with a Hashed Trace**\n\nInstead of directly appending port and channel identifiers to the denomination, a hashed trace will be used as a prefix. The hash function will take the concatenation of the port and channel identifiers as input and produce a fixed-length hash. This hash will be prepended to the base denomination to form the final denomination.\n\n**Specifics:**\n\n* The hash function will be a collision-resistant cryptographic hash function, such as SHA256 or SHA512.\n* The hash will be truncated to a maximum length of 32 characters (for SHA256) or 64 characters (for SHA512).\n* The hashed trace will be encoded using Base64 or another suitable encoding scheme to ensure that it does not contain any special characters or uppercase letters.\n* The final denomination will have the following format:\n\n```typescript\nhashedTrace/denom\n```\n\n**Example:**\n\nTransferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to Ethermint's port `EthermintPort` and channel `EthermintChannel` would result in the following denomination:\n\n```typescript\nSHA256(HubPort/HubChannel)/uatom\n```\n\n**Benefits:**\n\n* This solution solves both problems identified:\n    * The hashed trace will have a fixed and manageable length, regardless of the number of cross-chain transfers.\n    * The hashed trace will not contain any special characters or uppercase letters, ensuring compatibility with the SDK's `Coin` type.\n* It maintains the ability to trace the history of a token's movements across chains.\n* It does not require any changes to the IBC protocol or the SDK's `Coin` type.\n\n**Implementation:**\n\n* The IBC module will be responsible for generating the hashed trace and prepending it to the denomination of cross-chain fungible tokens.\n* The SDK will be updated to accept denominations with the hashed trace prefix.\n\n**Testing:**\n\n* Extensive testing will be conducted to ensure the validity and robustness of the solution, including tests for:\n    * Denomination length validation\n    * Character restrictions\n    * Traceability of token history\n    * Compatibility with existing IBC implementations"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\nmediocre.\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\nwhich are repetitive and hard to maintain.\nSecond, this makes it harder to align with the object capability theorem: the right to access the\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\nFinally, because the getter/setter functions are defined as methods of a module's `Keeper`, the reviewers\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\nThere is no static way to know which part of the state that the function is accessing (and which is not).\n\n## Decision\n","We will define a type named `Value`:\n```go\ntype Value struct {\nm   Mapping\nkey []byte\n}\n```\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\nspace it will access and `Value.key` defines the exact key for the reference.\nWe will define a type named `Mapping`:\n```go\ntype Mapping struct {\nstoreKey sdk.StoreKey\ncdc      *codec.LegacyAmino\nprefix   []byte\n}\n```\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\nWe will define the following core methods for the `Value` type:\n```go\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Value) Get(ctx Context, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\n// Get stored data as raw byte slice\nfunc (Value) GetRaw(ctx Context) []byte {}\n// Marshal and set a raw value\nfunc (Value) Set(ctx Context, o interface{}) {}\n// Check if a raw value exists\nfunc (Value) Exists(ctx Context) bool {}\n// Delete a raw value value\nfunc (Value) Delete(ctx Context) {}\n```\nWe will define the following core methods for the `Mapping` type:\n```go\n// Constructs key-value pair reference corresponding to the key argument in the Mapping space\nfunc (Mapping) Value(key []byte) Value {}\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\n// Get stored data as raw byte slice\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\n// Marshal and set a raw value\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\n// Check if a raw value exists\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\n// Delete a raw value value\nfunc (Mapping) Delete(ctx Context, key []byte) {}\n```\nEach method of the `Mapping` type that is passed the arugments `ctx`, `key`, and `args...` will proxy\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\nIn addition, we will define and provide a common set of types derived from the `Value` type:\n```go\ntype Boolean struct { Value }\ntype Enum struct { Value }\ntype Integer struct { Value; enc IntEncoding }\ntype String struct { Value }\n// ...\n```\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\nin core methods are replaced by explicit return types.\nFinally, we will define a family of types derived from the `Mapping` type:\n```go\ntype Indexer struct {\nm   Mapping\nenc IntEncoding\n}\n```\nWhere the `key` argument in core method is typed.\nSome of the properties of the accessor types are:\n- State access happens only when a function which takes a `Context` as an argument is invoked\n- Accessor type structs give rights to access the state only that the struct is referring, no other\n- Marshalling/Unmarshalling happens implicitly within the core methods\n","The SDK modules should natively support a new `KVStore` interface that provides a modular and user-friendly way to access state. This interface should include methods for getting and setting values, as well as for deleting keys. The interface should also be designed in a way that makes it easy to align with the object capability theorem."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nProtocol Buffers provide a basic [style guide](https://developers.google.com/protocol-buffers/docs/style)\nand [Buf](https://buf.build/docs/style-guide) builds upon that. To the\nextent possible, we want to follow industry accepted guidelines and wisdom for\nthe effective usage of protobuf, deviating from those only when there is clear\nrationale for our use case.\n### Adoption of `Any`\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\ninterface types (as opposed to `oneof`) makes package naming a central part\nof the encoding as fully-qualified message names now appear in encoded\nmessages.\n### Current Directory Organization\nThus far we have mostly followed [Buf's](https://buf.build) [DEFAULT](https://buf.build/docs/lint-checkers#default)\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout)\nwhich although being convenient for developing code comes with the warning\nfrom Buf that:\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\n### Adoption of gRPC Queries\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\npath. In the future, gRPC queries may be allowed from within persistent scripts\nby technologies such as CosmWasm and these query routes would be stored within\nscript binaries.\n\n## Decision\n","The goal of this ADR is to provide thoughtful naming conventions that:\n* encourage a good user experience for when users interact directly with\n.proto files and fully-qualified protobuf names\n* balance conciseness against the possibility of either over-optimizing (making\nnames too short and cryptic) or under-optimizing (just accepting bloated names\nwith lots of redundant information)\nThese guidelines are meant to act as a style guide for both the SDK and\nthird-party modules.\nAs a starting point, we should adopt all of the [DEFAULT](https://buf.build/docs/lint-checkers#default)\ncheckers in [Buf's](https://buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout),\nexcept:\n* [PACKAGE_VERSION_SUFFIX](https://buf.build/docs/lint-checkers#package_version_suffix)\n* [SERVICE_SUFFIX](https://buf.build/docs/lint-checkers#service_suffix)\nFurther guidelines to be described below.\n### Principles\n#### Concise and Descriptive Names\nNames should be descriptive enough to convey their meaning and distinguish\nthem from other names.\nGiven that we are using fully-qualifed names within\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\nkeep names concise, without going overboard. The general rule of thumb should\nbe if a shorter name would convey more or else the same thing, pick the shorter\nname.\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\nSuch conciseness makes names both more pleasant to work with and take up less\nspace within transactions and on the wire.\nWe should also resist the temptation to over-optimize, by making names\ncryptically short with abbreviations. For instance, we shouldn't try to\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\nThe goal is to make names **_concise but not cryptic_**.\n#### Names are for Clients First\nPackage and type names should be chosen for the benefit of users, not\nnecessarily because of legacy concerns related to the go code-base.\n#### Plan for Longevity\nIn the interests of long-term support, we should plan on the names we do\nchoose to be in usage for a long time, so now is the opportunity to make\nthe best choices for the future.\n### Versioning\n#### Don't Allow Breaking Changes in Stable Packages\nAlways use a breaking change detector such as [Buf](https://buf.build) to prevent\nbreaking changes in stable (non-alpha or beta) packages. Breaking changes can\nbreak smart contracts/persistent scripts and generally provide a bad UX for\nclients. With protobuf, there should usually be ways to extend existing\nfunctionality instead of just breaking it.\n#### Omit v1 suffix\nInstead of using [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix),\nwe can omit `v1` for packages that don't actually have a second version. This\nallows for more concise names for common use cases like `cosmos.bank.Send`.\nPackages that do have a second or third version can indicate that with `.v2`\nor `.v3`.\n#### Use `alpha` or `beta` to Denote Non-stable Packages\n[Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix)\n(ex. `v1alpha1`) _should_ be used for non-stable packages. These packages should\nlikely be excluded from breaking change detection and _should_ generally\nbe blocked from usage by smart contracts/persistent scripts to prevent them\nfrom breaking. The SDK _should_ mark any packages as alpha or beta where the\nAPI is likely to change significantly in the near future.\n### Package Naming\n#### Adopt a short, unique top-level package name\nTop-level packages should adopt a short name that is known to not collide with\nother names in common usage within the Cosmos ecosystem. In the near future, a\nregistry should be created to reserve and index top-level package names used\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\n[ICS](https://github.com/cosmos/ics) specifications could consider a\nshort top-level package like `ics23` based upon the standard number.\n#### Limit sub-package depth\nSub-package depth should be increased with caution. Generally a single\nsub-package is needed for a module or a library. Even though `x` or `modules`\nis used in source code to denote modules, this is often unnecessary for .proto\nfiles as modules are the primary thing sub-packages are used for. Only items which\nare known to be used infrequently should have deep sub-package depths.\nFor the Cosmos SDK, it is recommended that that we simply write `cosmos.bank`,\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\ntypes can go straight in the `cosmos` package or we can introduce a\n`cosmos.base` package if needed. Note that this naming _will not_ change\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\n`x/bank`.\n### Message Naming\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\ntypes which are used in transactions will retain the `Msg` prefix as that provides\nhelpful context.\n### Service and RPC Naming\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\nimplement a gRPC query service. We should consider the principle of conciseness\nfor query service and RPC names as these may be called from persistent script\nmodules such as CosmWasm. Also, users may use these query paths from tools like\n[gRPCurl](https://github.com/fullstorydev/grpcurl). As an example, we can shorten\n`/cosmos_sdk.x.bank.v1.QueryService/QueryBalance` to\n`/cosmos.bank.Query/Balance` without losing much useful information.\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`/\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\non the `Query` service, the request and response types would be `QueryBalanceRequest`\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\nand `BalanceResponse`.\n#### Use just `Query` for the query service\nInstead of [Buf's default service suffix recommendation](https://github.com/cosmos/cosmos-sdk/pull/6033),\nwe should simply use the shorter `Query` for query services.\nFor other types of gRPC services, we should consider sticking with Buf's\ndefault recommendation.\n#### Omit `Get` and `Query` from query service RPC names\n`Get` and `Query` should be omitted from `Query` service names because they are\nredundant in the fully-qualified name. For instance, `/cosmos.bank.Query/QueryBalance`\njust says `Query` twice without any new information.\n","**Decision:** Follow the Buf style guide for Protobuf package and file naming, with the following exceptions:\n\n* **Package Naming:**\n    * For interfaces encoded using `google.protobuf.Any`, use the fully-qualified message name as the package name.\n* **File Naming:**\n    * Continue to disable the `PACKAGE_DIRECTORY_MATCH` linter check, but ensure that all proto files are organized in a logical and consistent manner.\n* **gRPC Query Paths:**\n    * For gRPC queries, use the full gRPC service path as a prefix for the ABCI query path.\n\n**Rationale:**\n\n* Following the Buf style guide for Protobuf package and file naming provides consistency and alignment with industry best practices.\n* Using the fully-qualified message name as the package name for interfaces encoded using `google.protobuf.Any` ensures that the encoded message structure is self-describing.\n* Disabling the `PACKAGE_DIRECTORY_MATCH` linter check provides flexibility in organizing proto files while still maintaining a logical structure.\n* Utilizing the full gRPC service path as a prefix for ABCI query paths makes the query routes more explicit and easier to understand."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Protobuf](https://developers.google.com/protocol-buffers/docs/proto3)\nseralization is not unique (i.e. there exist a practically unlimited number of\nvalid binary representations for a protobuf document)<sup>1</sup>. For signature\nverification in Cosmos SDK, signer and verifier need to agree on the same\nserialization of a SignDoc as defined in\n[ADR-020](./adr-020-protobuf-transaction-encoding.md) without transmitting the\nserialization. This document describes a deterministic serialization scheme for\na subset of protobuf documents, that covers this use case but can be reused in\nother cases as well.\n\n## Decision\n","The following encoding scheme is proposed to be used by other ADRs.\n### Scope\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\nserialization, such that every protobuf parser can parse it.\nNo maps are supported in version 1 due to the complexity of defining a\nderterministic serialization. This might change in future. Implementations must\nreject documents containing maps as invalid input.\n### Serialization rules\nThe serialization is based on the\n[protobuf 3 encoding](https://developers.google.com/protocol-buffers/docs/encoding)\nwith the following additions:\n1. Fields must be serialized only once in ascending order\n2. Extra fields or any extra data must not be added\n3. [Default values](https://developers.google.com/protocol-buffers/docs/proto3#default)\nmust be omitted\n4. `repeated` fields of scalar numeric types must use\n[packed encoding](https://developers.google.com/protocol-buffers/docs/encoding#packed)\nby default.\n5. Variant encoding of integers must not be longer than needed.\nWhile rule number 1. and 2. should be pretty straight forward and describe the\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\nbetween unset fields and fields set to the default value<sup>2</sup>. At\nserialization level however, it is possible to set the fields with an empty\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\nwhere a property can be empty (`""""`, `0`), `null` or undefined, leading to 3\ndifferent documents.\nOmitting fields set to default values is valid because the parser must assign\nthe default value to fields missing in the serialization<sup>3</sup>. For scalar\ntypes, omitting defaults is required by the spec<sup>4</sup>. For `repeated`\nfields, not serializing them is the only way to express empty lists. Enums must\nhave a first element of numeric value 0, which is the default<sup>5</sup>. And\nmessage fields default to unset<sup>6</sup>.\nOmitting defaults allows for some amount of forward compatibility: users of\nnewer versions of a protobuf schema produce the same serialization as users of\nolder versions as long as newly added fields are not used (i.e. set to their\ndefault value).\n### Implementation\nThere are three main implementation strategies, ordered from the least to the\nmost custom development:\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\n[gogoproto](https://pkg.go.dev/github.com/gogo/protobuf/gogoproto) is known to\nbe compliant by in most cases, but not when certain annotations such as\n`nullable = false` are used. It might also be an option to configure an\nexisting serializer accordingly.\n- **Normalize default values before encoding them.** If your serializer follows\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\nyou can normalize default values to unset. This can be done when working with\n[protobuf.js](https://www.npmjs.com/package/protobufjs):\n```js\nconst bytes = SignDoc.encode({\nbodyBytes: body.length > 0 ? body : null, // normalize empty bytes to unset\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, // normalize empty bytes to unset\nchainId: chainId || null, // normalize """" to unset\naccountNumber: accountNumber || null, // normalize 0 to unset\naccountSequence: accountSequence || null, // normalize 0 to unset\n}).finish();\n```\n- **Use a hand-written serializer for the types you need.** If none of the above\nways works for you, you can write a serializer yourself. For SignDoc this\nwould look something like this in Go, building on existing protobuf utilities:\n```go\nif !signDoc.body_bytes.empty() {\nbuf.WriteUVarInt64(0xA) // wire type and field number for body_bytes\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\nbuf.WriteBytes(signDoc.body_bytes)\n}\nif !signDoc.auth_info.empty() {\nbuf.WriteUVarInt64(0x12) // wire type and field number for auth_info\nbuf.WriteUVarInt64(signDoc.auth_info.length())\nbuf.WriteBytes(signDoc.auth_info)\n}\nif !signDoc.chain_id.empty() {\nbuf.WriteUVarInt64(0x1a) // wire type and field number for chain_id\nbuf.WriteUVarInt64(signDoc.chain_id.length())\nbuf.WriteBytes(signDoc.chain_id)\n}\nif signDoc.account_number != 0 {\nbuf.WriteUVarInt64(0x20) // wire type and field number for account_number\nbuf.WriteUVarInt(signDoc.account_number)\n}\nif signDoc.account_sequence != 0 {\nbuf.WriteUVarInt64(0x28) // wire type and field number for account_sequence\nbuf.WriteUVarInt(signDoc.account_sequence)\n}\n```\n### Test vectors\nGiven the protobuf definition `Article.proto`\n```protobuf\npackage blog;\nsyntax = ""proto3"";\nenum Type {\nUNSPECIFIED = 0;\nIMAGES = 1;\nNEWS = 2;\n};\nenum Review {\nUNSPECIFIED = 0;\nACCEPTED = 1;\nREJECTED = 2;\n};\nmessage Article {\nstring title = 1;\nstring description = 2;\nuint64 created = 3;\nuint64 updated = 4;\nbool public = 5;\nbool promoted = 6;\nType type = 7;\nReview review = 8;\nrepeated string comments = 9;\nrepeated string backlinks = 10;\n};\n```\nserializing the values\n```yaml\ntitle: ""The world needs change 🌳""\ndescription: """"\ncreated: 1596806111080\nupdated: 0\npublic: true\npromoted: false\ntype: Type.NEWS\nreview: Review.UNSPECIFIED\ncomments: [""Nice one"", ""Thank you""]\nbacklinks: []\n```\nmust result in the serialization\n```\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\n```\nWhen inspecting the serialized document, you see that every second field is\nomitted:\n```\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\n1: ""The world needs change \360\237\214\263""\n3: 1596806111080\n5: 1\n7: 2\n9: ""Nice one""\n9: ""Thank you""\n```\n",Use the **Canonical Protobuf encoding** for serialization in cases where unique serialization is required.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https://github.com/cosmos/cosmos-sdk/issues/5467) and [4982](https://github.com/cosmos/cosmos-sdk/issues/4982) for additional context.\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\n\n## Decision\n","Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\n### Account interface (x/auth)\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\nnow be stored in & managed by the bank module.\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\naccount balance.\nVesting accounts will continue to store original vesting, delegated free, and delegated\nvesting coins (which is safe since these cannot contain arbitrary denominations).\n### Bank keeper (x/bank)\nThe following APIs will be added to the `x/bank` keeper:\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\ncore functionality or persistence.\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\nbut retrieval of all balances for a single account is presumed to be more frequent):\n```golang\nvar BalancesPrefix = []byte(""balances"")\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\nif !balance.IsValid() {\nreturn err\n}\nstore := ctx.KVStore(k.storeKey)\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\nbz := Marshal(balance)\naccountStore.Set([]byte(balance.Denom), bz)\nreturn nil\n}\n```\nThis will result in the balances being indexed by the byte representation of\n`balances/{address}/{denom}`.\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\naccount balance by denomination found in the (un)delegation amount. As a result,\nany mutations to the account balance by will made by denomination.\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\ndirectly instead of calling `GetCoins()` / `SetCoins()` (which no longer exist).\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\naccount balances.\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\npossible as to not load the entire account balance.\n### Supply module\nThe supply module, in order to implement the total supply invariant, will now need\nto scan all accounts & call `GetAllBalances` using the `x/bank` Keeper, then sum\nthe balances and check that they match the expected total supply.\n","The decision is to store non-zero balances in a skiplist, which will allow for efficient loading and storage of large numbers of denominations. This will mitigate the potential denial-of-service concern that could be caused by too many denominations being stored in an `sdk.Coins` struct."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\n[runTx()](https://github.com/cosmos/cosmos-sdk/blob/bad4ca75f58b182f600396ca350ad844c18fc80b/baseapp/baseapp.go#L539)\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\nmight be handled in a ""standard"" way (middleware) alongside the others.\nWe propose middleware-solution, which could help developers implement the following cases:\n* add external logging (let's say sending reports to external services like [Sentry](https://sentry.io));\n* call panic for specific error cases;\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\nin order to customize it.\n\n## Decision\n","### Design\n#### Overview\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\ncan be found [here](https://github.com/cosmos/cosmos-sdk/pull/6053).\n#### Implementation details\n##### Recovery handler\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\n`recover()` from the `builtin` package.\n```go\ntype RecoveryHandler func(recoveryObj interface{}) error\n```\nHandler should type assert (or other methods) an object to define if object should be handled.\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\nAn example:\n```go\nfunc exampleErrHandler(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(error)\nif !ok { return nil }\nif someSpecificError.Is(err) {\npanic(customPanicMsg)\n} else {\nreturn nil\n}\n}\n```\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\n##### Recovery middleware\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\nchain processing.\n```go\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\nif err := handler(recoveryObj); err != nil {\nreturn nil, err\n}\nreturn next, nil\n}\n}\n```\nFunction receives a `recoveryObj` object and returns:\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\n`OutOfGas` middleware example:\n```go\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\nif !ok { return nil }\nreturn sdkerrors.Wrap(\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\n""out of gas in location: %v; gasWanted: %d, gasUsed: %d"", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\n),\n)\n}\nreturn newRecoveryMiddleware(handler, next)\n}\n```\n`Default` middleware example:\n```go\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nreturn sdkerrors.Wrap(\nsdkerrors.ErrPanic, fmt.Sprintf(""recovered: %v\nstack:\n%v"", recoveryObj, string(debug.Stack())),\n)\n}\nreturn newRecoveryMiddleware(handler, nil)\n}\n```\n##### Recovery processing\nBasic chain of middlewares processing would look like:\n```go\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\nif middleware == nil { return nil }\nnext, err := middleware(recoveryObj)\nif err != nil { return err }\nif next == nil { return nil }\nreturn processRecovery(recoveryObj, next)\n}\n```\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\n`default` handler which must return an `error`.\n##### BaseApp changes\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\n```go\ntype BaseApp struct {\n// ...\nrunTxRecoveryMiddleware recoveryMiddleware\n}\nfunc NewBaseApp(...) {\n// ...\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\n}\nfunc (app *BaseApp) runTx(...) {\n// ...\ndefer func() {\nif r := recover(); r != nil {\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\nerr, result = processRecovery(r, recoveryMW), nil\n}\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\n}()\n// ...\n}\n```\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\n```go\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\nfor _, h := range handlers {\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\n}\n}\n```\nThis method would prepend handlers to an existing chain.\n",**Decision:** Implement a middleware-based solution for customizing error handling in BaseApp.\n\n**Rationale:**\n\n* Middleware is a flexible approach that allows developers to implement a wide range of error handling customizations without the need to rewrite the entire BaseApp.\n* It provides a standardized way to handle both custom error cases and special cases like `sdk.ErrorOutOfGas`.\n* It enables the implementation of specific use cases such as external logging and panic handling for specific error cases.\n* The proposed solution addresses the specific need for customizing error handling in BaseApp to facilitate deterministic node execution in the face of communication breaks with a sidecar service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https://github.com/tendermint/tendermint/issues/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\n\n## Decision\n","### Pseudo procedure for consensus key rotation\n- create new random consensus key.\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\n- start validating with new consensus key.\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\n### Considerations\n- consensus key mapping information management strategy\n- store history of each key mapping changes in the kvstore.\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\n- the state machine does not need any historical mapping information which is past more than unbonding period.\n- key rotation costs related to LCD and IBC\n- LCD and IBC will have traffic/computation burden when there exists frequent power changes\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\n- limits\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\n- parameters can be decided by governance and stored in genesis file.\n- key rotation fee\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\n- evidence module\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\n- abci.ValidatorUpdate\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\n- new genesis parameters in `staking` module\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\n### Workflow\n1. The validator generates a new consensus keypair.\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\n```go\ntype MsgRotateConsPubKey struct {\nValidatorAddress  sdk.ValAddress\nNewPubKey         crypto.PubKey\n}\n```\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\n4. `RotateConsPubKey`\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\n- checks if the signing account has enough balance to pay `KeyRotationFee`\n- pays `KeyRotationFee` to community fund\n- overwrites `NewPubKey` in `validator.ConsPubKey`\n- deletes old `ValidatorByConsAddr`\n- `SetValidatorByConsAddr` for `NewPubKey`\n- Add `ConsPubKeyRotationHistory` for tracking rotation\n```go\ntype ConsPubKeyRotationHistory struct {\nOperatorAddress         sdk.ValAddress\nOldConsPubKey           crypto.PubKey\nNewConsPubKey           crypto.PubKey\nRotatedHeight           int64\n}\n```\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\n```go\nabci.ValidatorUpdate{\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\nPower:  0,\n}\nabci.ValidatorUpdate{\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\nPower:  v.ConsensusPower(),\n}\n```\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\n- Note : All above features shall be implemented in `staking` module.\n","Validator consensus key rotation feature will be implemented in Cosmos-SDK. The implementation will be kept as simple as possible, without considering multiple consensus keys concept."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\n- From which repo would components be published?\n- Which repos would consume components from NPM?\n- Should the Storybook live inside the Client repo?\n## Decision Drivers\nN/A\n\n## Decision\n","N/A\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally — before publishing the component to the hosted Storybook and NPM.\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\n1. Duplicate component code to Storybook repo\n2. Publish completed components to NPM\n3. Remove the original component code from the Client and import via NPM\n### Positive Consequences\n### Negative consequences\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\nIf possible, also avoid having people working simultaneously on a component that consumes / is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\n",**Decision:**\n\n* **Components will be published from the DPE Client repository.**\n* **Both the DPE Client and Storybook repositories will consume components from NPM.**\n* **The Storybook will live outside of the Client repository.**\n\n**Rationale:**\n\n* Publishing components from the DPE Client repository allows for centralized management and versioning of components.\n* Consuming components from NPM in both repositories ensures that both projects have access to the latest versions of components.\n* Keeping the Storybook outside of the Client repository allows for more flexibility and easier collaboration with other teams.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCorvus Tenancy, which provides the underpinnings of the Marain Tenancy service, has always had a concept of a root tenant (dating from its earlier, pre-open-source incarnation). This ADR captures aspects of this root tenant that are non-obvious. (We learned that it was non-obvious because some code has been written that was unaware of the special status of the root tenant.)\n\n## Decision\n","There is a special tenant known as the Root Tenant. It has a well-known id, `f26450ab1668784bb327951c8b08f347`. It is special in three respects:\n* tenants are hierarchical, and the root tenant forms the root of that hierarchy\n* the tenanted storage mechanisms will all fall back to the root tenant to find default connection settings if the tenant being used has not defined tenant-specific settings\n* within Marain services, the root tenant is always represented by a special in-memory instance of the `RootTenant` type, whereas all other tenants are managed by the tenancy service\nThat third item is there to support the second: because each service puts its own `RootTenant` into the DI service collection, as a singleton, it becomes possible for the service to attach whatever service-specific fallback settings it requires. We describe the root tenant as ""synthetic"" because each service creates its own object to represent the root tenant, whereas all other objects representing tenants are obtained via the `Marain.Tenancy` service, typically through the `ClientTenantProvider`.\nWe contemplated separating out the first two concerns (which might enable us not to need the third characteristic above) because it has been a source of confusion in the past. However, for the time being we are planning to keep it this way because that alternative approach would require us to introduce an extra mechanism to support these kinds of defaults.\n",The root tenant is a special case in Corvus. It is the only tenant that can create other tenants. It is also the only tenant that can update the configuration of the Corvus service. All other tenants are created by the root tenant and are owned by the root tenant.\n\nThe root tenant is not visible to users of the Marain Tenancy service. It is only used internally by the Corvus service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe original build of `Marain.Tenancy` provided two main things:\n- It provided an API over the existing `Corvus.Tenancy` project, allowing us to build multiple services around the same tenancy system without each service needing to be aware of the underlying persistence mechanism for tenants.\n- It provided a drop in replacement for direct use of the `Corvus.Tenancy.Abstractions.ITenantProvider`, in the form of the `ClientTenantProvider` in `Marain.Tenancy.ClientTenantProvider`.\nHowever, the service did not impose any particular tenancy model on consumers; it simply exposed the capabilities of the `Corvus.Tenancy` library.\nSubsequently, as the functionality of the remaining Marain services was extended to properly support multi-tenancy, it became necessary to define the tenancy model that would be shared across all Marain Services. This is documented in `Marain.Instance` ADRs [0005 - Multitenancy approach for Marain](https://github.com/marain-dotnet/Marain.Instance/blob/master/docs/adr/0005-multitenancy-approach-for-marain.md) and [0006 - Process for onboarding new tenants and enrolling them to use Marain services](https://github.com/marain-dotnet/Marain.Instance/blob/master/docs/adr/0006-process-for-onboarding-new-tenants.md)\nThe additional code required to support this, available in [the `Marain.TenantManagment` repository](https://github.com/marain-dotnet/Marain.TenantManagement) was built as a wrapper around the existing `Marain.Tenancy` API and is currently only exposed via a Command Line Interface.\nThis has worked reasonably well to date; the CLI and code provided by Marain.TenantManagement is in use in multiple locations to work with tenants in several different ways, but all based on the underlying model defined in the ADRs linked to above.\nHowever, it has become apparent that in Marain, the two are essentially inseperable. All Marain services (bar Marain.Tenancy) validate requests to ensure that the tenant making the request is enrolled to use the service being called, which requires use of code referenced from `Marain.TenantManagement` and `Marain.Tenancy`.\nIn addition, new requirements (such as the need for a service to be able to obtain a list of enrolled clients) are difficult to implement efficiently without further blurring the lines between the two libraries.\nThis has brought into focus the fact that `Marain.Tenancy` is something of an outlier in the Marain world; despite being the service used to manage Marain tenants, it is the only service in the Marain ecosystem that does not fully ""buy in"" to the Marain tenancy model. This means that as we expand the model to cover new scenarios (including the creation of additional tenant management tooling to simplify onboarding and offboarding, and potentially begin to integrate billing, metering, and so on) we will likely end up either continuing to add extensions to the `Marain.TenantManagement` library, or adding a separate API that acts as a wrapper around `Marain.Tenancy`.\n\n## Decision\n","To simplify our approach, we will modify `Marain.Tenancy` so that that API it exposes reflects the tenancy model used by the Marain ecosystem. This means that existing functionality provided by the `Marain.TenantManagement` extensions will become API endpoints - e.g. enrollment, client/service tenant creation, etc.\nThe existing CLIs provided with `Marain.TenantManagement` and `Marain.Tenancy` will be merged and separated out into their own project. They will then form the basis of CLI tooling required for standard tasks in all Marain services. As well as making the tooling simpler to use (by having all functionality available through one tool rather than spread across multiple), this will ensure we maintain good practice in ensuring that all of these tasks can be carried out via their relevant APIs.\n","Marain.Tenancy will be split into two distinct libraries:\n- `Marain.Tenancy`: This will be a low-level library that exposes the raw underlying functionality of the `Corvus.Tenancy` library.\n- `Marain.TenancyManagement`: This will be a higher-level library that wraps the functionality of `Marain.Tenancy` and exposes it in a way that is consistent with the Marain tenancy model. This library will also provide additional functionality such as the ability to manage tenants and enroll them to use Marain services.\nThis split will allow Marain services to continue to use the functionality of `Marain.Tenancy` without having to be aware of the underlying implementation details. It will also allow us to more easily extend the Marain tenancy model in the future, without breaking existing code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\nOne of the basic principles of efficient JSON parsing in the new world is that you don't build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the ""modifiable entities"" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the ""modifiable entities"" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\n\n## Decision\n","Since `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\n* fetch a serialized representation of an ITenant from the web API\n* modify that representation to reflect the changes you wish to make\n* PUT that serialized representation of an ITenant back to the web API\nwe now use this procedure instead:\n* send a PATCH request in describing the changes required in JSON Patch format\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application/json-patch+json` content type:\n```json\n[{\n""path"": ""/name"",\n""op"": ""replace"",\n""value"": ""NewTenantName""\n}]\n```\nJSON Patch supports multiple changes in a single request, e.g.:\n```json\n[\n{\n""op"": ""add"",\n""path"": ""/properties/StorageConfiguration__corvustenancy"",\n""value"": {\n""AccountName"": ""mardevtenancy"",\n""Container"": null,\n""KeyVaultName"": ""mardevkv"",\n""AccountKeySecretName"": ""mardevtenancystore"",\n""DisableTenantIdPrefix"": false\n}\n},\n{\n""op"": ""add"",\n""path"": ""/properties/Foo__bar"",\n""value"": ""Some string""\n},\n{\n""op"": ""add"",\n""path"": ""/properties/Foo__spong"",\n""value"": 42\n}\n]\n```\nThe `op` can be set to `remove` to delete properties.\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\n```csharp\nawait tenantStore.UpdateTenantAsync(tenantId, name: ""NewTenantName"");\n```\nAdding or changing a property looks like this:\nawait tenantStore.UpdateTenantAsync(\ntenantId,\npropertiesToSetOrAdd: new Dictionary<string, object>()\n{\n{ ""StorageConfiguration__corvustenancy"", myStorageConfig },\n{ ""SomeOtherSetting"": 42 },\n});\n","We should change tenancy so that it represents an immutable object, with any write behavior going directly to the Property Bag abstraction."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSwitching on the C# 8.0 nullable references feature for Menes has revealed some ambiguities around whether certain properties of exceptions are meant to be nullable.\nIn many cases, the only reason for ambiguity is that we have followed a pattern of defining various ""standard constructors"", such as default constructors, exception-message-only constructors, and deserializing constructors.\n\n## Decision\n","Menes exceptions will not have any of these standard exceptions except in cases where there are no required properties (e.g., the exception's type tells you everything you need to know).\nProperties that always have non-null values in practice will declare this formally by having non-nullable types.\nWe will remove all deserializing constructors, and remove the `[Serializable]` attribute from all exceptions that have them. This has been motivated by the use of nullable references, because deserializing constructors cause some challenges there, but this is a distinct issue. Menes exceptions are all designed for use within a Menes-based service. Menes is designed to implement service boundaries, and by definition, if we ever attempt to throw a Menes-defined exception across a process boundary, we've made a mistake.\n","**The following properties of exceptions shall be made nullable using the C# 9.0 nullable reference annotation:**\n\n* `Exception.HResult`\n* `Exception.InnerException`\n* `Exception.Message`\n* `Exception.Source`\n\n**Reasoning:**\n\n* The nullability of these properties is often ambiguous, as they may be null in certain scenarios (e.g., when deserializing an exception from a JSON payload). By making them nullable, we eliminate the need for explicit null checks or default value assignments.\n* Making these properties nullable does not introduce any significant performance overhead or potential security vulnerabilities.\n* The nullability annotation provides valuable information to developers, making it clear that these properties may be null and should be handled accordingly.\n\n**Consequences:**\n\n* Code that relies on the non-nullability of these properties will need to be updated to account for the possibility of null values.\n* This may introduce additional null checks or default value assignments in some cases.\n* However, the overall benefits of improved clarity and reduced ambiguity outweigh the minor potential drawbacks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\n\n## Decision\n","Menes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\n","Use the `Nullable` NuGet package to provide nullable reference attributes for older runtimes, but only if the target runtime is `netstandard2.0`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn real-world OpenAPI schema, we have discovered that people sometimes omit the `type: object` from their object definitions. We believe that this *is* valid Open API schema.\n```yaml\nPet:\nrequired:\n- id\n- name\nproperties:\nid:\ntype: integer\nformat: int64\nname:\ntype: string\ntag:\ntype: string\n```\nHowever, there are other scenarios where you are *not* expected to supply the `type` property. Specifically, the `anyOf`, `oneOf`, `allOf` cases.\n```yaml\nsomeEntity:\nanyOf:\n- type: string\n- type: object\n- type: array\n- type: boolean\n- type: integer\n- type: number\n```\n\n## Decision\n","Menes will support these semantics. We have updated our schema validation to support this by translating the missing `type` element into the internal schema type `None`, rather than translating to `Object`.\n","The `type` field is required for OpenAPI schemas, even for object definitions. The field should be set to `object` for object definitions. For `anyOf`, `oneOf`, and `allOf`, the `type` field is not required."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nModiapersonoversikt (denne frontenden) blir utviklet som en selvstendig frontend som på sikt skal erstatte frontenden i dagens modiabrukerdialog. For å kunne levere fortløpende ny funksjonalitet til saksbehandlerene, ønsker vi å levere ofte og smått.\n\n## Decision\n",Visittkortet dras inn som en enkeltstående react-komponent inn til modiabrukerdialog.\n,Lever i små trinn for å levere ny funksjonalitet raskt og fortløpende til saksbehandlerne.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To implement an Architectural Decision Record (ADR) template and process for capturing and documenting architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/).\nGitHub Actions will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repo, which has a basic use case will provide some\nexposure to the service.\n\n## Decision\n",The decision is to replace Travis CI with GitHub Actions.\n,Utilize GitHub Actions for Continuous Integration and Delivery (CI/CD).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently used: 2.1.0\nThe latest stable version of Spring Boot is 2.2.1:\nhttps://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes\nMore frequent but smaller upgrades are recommended.\n\n## Decision\n,Spring Boot will be upgraded to 2.2.1.\nAccording to release notes no migration needed on Mokka side.\n,Upgrade Spring Boot to version 2.2.1.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMokka's stubs engine is required to provide set of new features that will allow better (wider) support of request matching,\n(i.e. headers, cookies, variety of patterns), proxying and recording interactions.\nIt has been considered if further development of Mokka stubs engine should be continued or we should research and adopt one of existing, grown up and production ready solution.\nWireMock (http://wiremock.org/) was chosen as one of the most popular mock server with strong and active open source community. It provides consistent and reach API (both: Java and HTTP JSON).\nThrough years it became one of the mostly chosen library for mocking external services in JUnit tests and also the core part of Spring Cloud Contract library.\nBy reusing WireMock standards, Mokka may benefit in easier integration with already existing solutions, products and projects.\nTable below comparises features of Mokka and WireMock in context of stubbing/mocking engine.\nPlease mind it does not considers GUI or user management features as what Mokka provides is not considered to be change.\nCompared versions:\n* WireMock version: 2.26.0<br>\n* Mokka version: 0.5.0\n| Feature | Mokka | WireMock |\n| ------- | ----  | -------- |\n| Stubs Management (CRUD) | + | + |\n| Stubs Management - Disable | + | - |\n| SOAP/HTTP Support | + <BR> BASIC: No matching by request headers/cookies, no multipart support | + |\n| REST Support | + <br> BASIC: No matching by request headers/cookies, no multipart support| + |\n| Create stubs from OpenAPI spec | - | - |\n| JMS Support | + <br> Embedded ActiveMQ | - |\n| Proxying | -  | +  |\n| Record and Playback | -  | +  |\n| Simulating Faults | - | + |\n| Response templating | + <BR> Using Groovy | + <BR> Using Handlebars |\n| Stubs stored in | Database | JSON files |\n| Stubs modifications history | + | - |\n| Interactions log | + <br> Stored in database | + <br> Stored in memory |\n| File serving | + | + |\n| Fake Payment Gateway | + <BR> BlueMedia | - |\n| Admin API | - | + <BR>JSON API, Java API |\nSo the WireMock already covers most of the requirements. The main differences:\n* ""Stubs Management - Disable"" - use Stub Mapping Metada to define ""active"" property? __TODO__\n* ""Create stubs from OpenAPI spec"" - have to implemented in Mokka and WireMock anyway. __TODO__\n* ""JMS Support"" - to be verified __TODO__\n* ""Response templating"" - using ""Handlebars"" are not necessarily a drawback. It seems to be far more elegant and easy to use than Groovy. Also safer from security point of view. __TODO__\n* ""Stubs stored in"" - we can try to provide database-based custom implementation of `MappingsSource`\n* ""Stubs modifications history"" - __TODO__??\n* ""Interactions log"" - Saving to database may be achieved by providing custom implementation of `RequestJournal`. __TODO__??\n* ""Fake Payment Gateway"" - Mokka implementation can be reused\nThe following solutions were considered:\n* continue with Mokka stubs engine\n* \+ flexibility and open architectural decisions in future\n* \- large codebase to maintain\n* \- requires high contribution to develop all missing features\n* adopt WireMock sources and maintain them to fulfill all the requirements\n* \+ large set of production-ready features provided at start\n* \+ flexibility and open architectural decisions in future\n* \- large codebase to maintain\n* \- no easy way to upgrade WireMock\n* \- no support of WireMock community\n* adopt WireMock as library and use all possible extension points\n* \+ WireMock core implementation remains untouched, new feature are provided using well designed extensions points\n* \+ small codebase to maintain\n* \+ WireMock bugs are handled by the community\n* \+ upgrading WireMock should be straightforward (as long as no breaking changes are introduced to WireMock APIs)\n* \- some future architectural decisions or feature considerations may be limited by WireMock architecture\n* \- in unknown feature WireMock license may change or community support may drop\n* \- we fully rely on WireMock development lifecycle\n\n## Decision\n",Adopt WireMock as library and use all possible extension points.\n,Adopt WireMock as library and use all possible extension points.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To record the architectural decisions made on this project, we will use an Architectural Decision Record (ADR). ADRs are documents that capture the context, decision, and rationale for an architectural decision. They provide a single source of truth for architectural decisions and help to ensure that all stakeholders are aware of and aligned on the decisions made."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMesh Configuration Protocol (MCP) is a [protocol](https://github.com/istio/api/tree/master/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\n\n## Decision\n,"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\n",MCP uses gRPC for message serialization and transport
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEnable one way TLS between front-end and envoy per host+domain that is\nspecified via gateway config.\n\n## Decision\n,"#### Gateway Configuration\nThis is achieved by sending the following config from copilot.\n```\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\nname: mygateway\nspec:\nselector:\nistio: ingressgateway # use istio default ingress gateway\nservers:\n- port:\nnumber: 443\nname: https-httpbin\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: /etc/istio/ingressgateway-certs/tls.crt\nprivateKey: /etc/istio/ingressgateway-certs/tls.key\nhosts:\n- ""httpbin.example.com""\n- port:\nnumber: 443\nname: https-bookinfo\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: /etc/istio/ingressgateway-bookinfo-certs/tls.crt\nprivateKey: /etc/istio/ingressgateway-bookinfo-certs/tls.key\nhosts:\n- ""bookinfo.com""\n```\nIn the config above each cert and key in the array of servers represent a\nhost+domain and the path to each cert and the key is arbitrarily chosen.\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\n```\nfrontend_tls_keypairs:\nexample:\n- cert_chain: |\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\n-----END RSA PRIVATE KEY-----\n- cert_chain: |\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\n-----END RSA PRIVATE KEY-----\n```\n#### Cert Storage\nThe placement of the certs and keys on the envoy VM is done using a separate\nprocess specific to this purpose. This process will be in charge of knowing\nwhere the certs are located and placing the certs on the correct paths. It is\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\nare stored, and having a specific process to manage this will reduce duplication\nand mitigate skew.\n",Implement one way TLS between front-end and envoy using a wildcard certificate and SNI headers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrevious packaging of Copilot in istio release relied on the fact that you would\nbe building copilot on the local machine (bosh pre-packaging).  This meant that\nyou could reliably fetch all of your dependencies using dep (which was included\nas a blob in the release).\nWhen we moved to get rid of pre-packaging and instead do all packaging on a bosh\nvm (just known as packaging) we ended up missing one key external dependency for\ndep to work (git). Including git as part of release would have meant adding\nanother blob and packaging step just for git.\n\n## Decision\n,We removed the .gitignore of the vendor directory and checked-in all of the\nsource code that dep was placing in that directory at build time.\n,**Decision**: Depend on a pre-installed version of git in the Go runtime.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\n\n## Decision\n,We have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\n,Increase the GRPC message size to 32 MB to resolve the bottleneck between cc-route-syncer and copilot.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\nacross all diego cells at a specified time interval (at the time of writing 10\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\nimpact the performance of the BBS (large response sets coming back).\n\n## Decision\n,We want to use the [Event package](https://github.com/cloudfoundry/bbs/blob/master/doc/events.md)\nto get the event stream for each ActualLRP. We will also use a bulk sync every\n60 seconds to catch any events that were missed.\n,Upgrade diego to use an ActualLRP syncing model which fetches LRPs on a periodic basis and incrementally updates the ActualLRP cache with a REST call (HTTP POST).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) template to document and track architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the project, we need an API for a map: interface, search, marker placement, satellite or road map imagery. There are several options for maps, will be a primary mode of interacting with the site.\n* Google maps\n* OpenLayers\n* TomTom\n* MapBox\n* HERE\n* Mapfit\nMain factor are cost, and ease of use (documentation for the API)\nGoogle maps are highly customizable in style and appearance, and configerable for marker placement, information windows, and interface/controls.\n\n## Decision\n","Upon examining the options, Google Maps was considered the most mature, easy-to-use and well-supported option. The API has excellent documentation and example code. The interface will be familiar to the majority of site users.\n","Based on the given factors of cost and ease of use, the decision is to use OpenLayers for the project's mapping needs. OpenLayers has a relatively low cost and provides comprehensive documentation for its API, making it easy to integrate and use within the project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\n- Marker placement on maps\n- Destinations being correctly added and drawn\n- Trips being correctly written to and received from session storage\n\n## Decision\n","The testing framework chosen for automated testing is Jest. This framework is used because:\n- It has a simple installation and configuration process for Node.js\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https://testawaywego.azurewebsites.net website since it is the website used for development.\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\n","In order to ensure thorough validation of the core functionality of the website, a comprehensive testing strategy encompassing unit testing, end-to-end testing, and manual testing will be employed.\n\n**Unit Testing**\n\n* **Purpose:** To validate the functionality of individual units of code, such as functions, within back-end models and database-related code.\n* **Benefits:**\n    * Isolates and tests specific code units independently, ensuring their correctness.\n    * Provides quick feedback, enabling early detection of errors.\n\n**End-to-End Testing**\n\n* **Purpose:** To test the functionality of the website from beginning to end, simulating actual user actions.\n* **Benefits:**\n    * Verifies the correct flow and integration of different components of the website.\n    * Ensures that the website behaves as expected from a user's perspective.\n\n**Manual Testing**\n\n* **Purpose:** To perform high-level functionality assessment and confirmation, focusing on specific aspects such as:\n    * Marker placement on maps\n    * Correct addition and drawing of destinations\n    * Proper writing and retrieval of trips from session storage\n* **Benefits:**\n    * Provides a comprehensive and thorough evaluation of website functionality.\n    * Captures real-world usage scenarios and edge cases that may be missed by automated testing.\n\n**Collaboration and Communication**\n\n* Testing teams will collaborate closely with developers to ensure that test cases cover all essential functionality.\n* Clear and concise communication will be maintained throughout the testing process to facilitate timely resolution of issues.\n* Automated tests will be updated and maintained regularly to reflect any changes in the codebase."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\n\n## Decision\n","4 Sprints planned, consecutively. Will only start on 17th April, but have a ""Sprint 0"" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\n* Release will be every Tuesday, by 8pm\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\n* Friday coding sessions together from 12pm - 4pm\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\nRebecca: Product Manager\nTyson: SCRUM Master\nTheese roles will be alternated throughout the project each week.\n",Allow Thabang to work remotely from 16-25 April to ensure productivity during the major submission period.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to join a group, a potential member must be invited (it's not correct to add a person to a group without asking permission.) A person invited can either be a member of the website (have an account) or be a new user (no account registered yet). In order to cover both of these scenarios, and to avoid the website being a ""walled garden"" with a tiny set of users, and to encourage potential future growt, a mechanism to invite users could be an email, sent by an existing member, to any valid email address with an invitation to join. This could be in the form of a token with a payload, or more simply, an extra table in the DB, linking the invited person's email to a trip ID.\n\n## Decision\n","The mechanism of an external invitation with a specific link requires the ability to send an email (prefereably attractively  styled and clearly phrased, to avoid being rejected as unsolicited or junk email). The node module 'nodemailer' was selected as appropriate, for its wide support, mature development and ease of use, and 0 dependecies.\n","The mechanism for inviting new members to a group will be an email, sent by an existing member, to any valid email address with an invitation to join. The email will contain a token with a payload that includes the group ID and the email address of the invited person. When the invited person clicks on the link in the email, they will be taken to a page where they can create an account or log in if they already have one. Once they have logged in, they will be added to the group."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA list of destinations should be reorderable, not fixed\n\n## Decision\n","A trip is made up of a list of destinations. This list should be able to be reordered, on the main site or the mobile version of the site. Draggable would be the best, but a button for moving an extry up and down will also work.\n",Use a reorderable list component.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDestinations need to be entered into a trip somehow. The two most obvious choices seem to be by typing (some kind of auto-completion feature) or by clicking directly on a map, to set markers. These paradigms are the dominant ones in most existing APIs and site/map websites.\n\n## Decision\n",We will aim to support both autocomplete AND clicking on the map. This would be the most convenient for users of the site.\n,Incorporate both typing and map-select methods for specifying destinations. This will allow users to choose the most convenient method for their needs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\n\n## Decision\n",Using a well known and widely known/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login/registration page through the Google Login API\n,"**Decision:** Integrate the Google Login API for user authentication.\n\n**Rationale:**\n\n* **Convenience and efficiency:** Using the Google Login API will simplify the login process for users, reducing the need for them to create and remember separate login credentials for our application.\n* **Security enhancement:** By leveraging the Google Login API, we can utilize the security measures implemented by Google, such as two-factor authentication, to enhance the overall security of our application.\n* **Time-saving:** Integrating the Google Login API will enable us to expedite the completion of the Sprint 1 User story related to login, allowing us to allocate resources to other development tasks.\n* **User experience improvement:** Providing multiple login options, including Google Login, improves the user experience by offering flexibility and reducing barriers to entry.\n* **Data compatibility:** The Google Login API can provide a standardized format for user data, which will facilitate easier integration with our existing database and other systems."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size/priority/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\n\n## Decision\n","Using the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for ""To Do"", ""In progress"", and ""Completed"".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\nhttps://github.com/witseie-elen4010/2019-005-project/projects/1\n","Implement a central KANBAN board using Jira or Trello, with the following columns:\n\n* **Backlog:** User stories that have not been started yet\n* **In Progress:** User stories that are currently being worked on\n* **Code Review:** User stories that have been completed and are undergoing code review\n* **Testing:** User stories that have passed code review and are being tested\n* **Done:** User stories that have been tested and are complete\n\nLabel user stories with the following information:\n\n* **Size:** Small, Medium, Large\n* **Priority:** High, Medium, Low\n* **Difficulty:** Easy, Medium, Hard\n* **Value:** High, Medium, Low"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo perform Continual Integration and development, with weekly releases, it would be convenient and useful to have a testing branch as well. Accidental pull requests into the main branch may introduce features that have not been tested from the interfac/front-end. It is difficult to automate these front-end interface tests, and there may be factors not present in a localhost/express server that only become apparent in an online scanario.\nThe use of **master** branch as the release branch is useful, as 'master' is usually the most protected on GitHub, with the most warnings about deleting, modifying, etc.\nCode reviews ar essential from all developers, to become familiar with each other's code, and to learn about javascript, and web-development. THis way we all learn from each other, and also learn good review and communicaton practice.\n\n## Decision\n","**master** will be the release branch\n**development** will be the main development/test branch. This will also be made into the ""default"" branch for all pull requests, to avoid accidentaly PR into master\n**feature** branches must be made off development, with unique names. All pull requests for completed features to be made into ""development"".\n* All PRs must be reviewed by at least two developers to merge into ""development""\n* All PRs must be reviewed by at the three other developers to merge into ""master""\n* All PRs must pass all tests (Jest, Travis, and Coveralls) in order to be considered valid for a merge\n* Stale reviews will be automatically dismissed if a new commit is pushed to the same branch\n* Accepted PRs for completed features (User Stories) should be deleted after sucessfully merging\n",Create a `testing` branch to perform front-end interface testing before merging into the `master` branch.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving a fixed structure for a project has may advantages, limiting spread of files across multiple folders and contraining locations to known places. THere is an advantage is letting a folder strucute emerge oganically, but also a large risk, as things can break when low-level file locations change, necesitating logs of bug fixing and refactoring. Having a rigid initial structure canb lead to later restrictions, or imposed complexity.\n\n## Decision\n",The following folder strucure is adopted:\n.\n├── app\n│   ├── controllers\n│   ├── models\n│   ├── public\n│   │   ├── css\n│   │   ├── img\n│   │   └── js\n│   ├── routes\n│   └── views\n├── docs\n│   ├── adr\n│   ├── misc\n│   ├── project_artifacts\n│   └── templates\n├── node_modules\n├── test\n└── local_only\n**Update** Removed folders originally specified that were found to not be required during project development: 'log' and 'utility'\n,"**Decision:**\n\nAdopt a hybrid approach, leveraging both a defined folder structure and organic emergence:\n\n**Structured Core:**\n- Establish a well-defined folder structure for core application components, ensuring consistency and easy navigation.\n- Use this core structure for critical code and data that requires stability and control.\n\n**Organic Substructures:**\n- Allow for the organic emergence of substructures within specific application modules or feature areas.\n- Provide guidelines and best practices to encourage logical organization and prevent excessive nesting.\n- Monitor and review substructures periodically to identify potential issues and ensure alignment with the project's goals.\n\n**Benefits:**\n\n- Ensures a stable and predictable foundation for the core application.\n- Provides flexibility and adaptability for evolving requirements in specific areas.\n- Balances the advantages of both rigid and organic structures, minimizing potential risks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\nenforce them. The question is when should those annotations be enforced and/or if we need to enable\nanother settings as well?\n### Decision\nSince we are using the [`symfony/phpunit-bridge`][phpunit-bridge], we decide to leverage the\n[`Symfony\Bridge\PhpUnit\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\n`@covers` annotations whilst still benefit from it.\nThis however does not allow to completely forgo its usage due to the following cases:\n- A test testing more than one class, requiring multiple `@covers` annotations\n- A test case testing a ""test class"", i.e. code reserved for testing purposes\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\nsetting `general_phpdoc_annotation_remove` has been refused.\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\n`@covers` annotation is necessary or not, no further action has been voted for automating this\nprocess.\n### Status\nAccepted ([#1060][1060])\n[code-coverage-listener]: https://symfony.com/doc/current/components/phpunit_bridge.html#code-coverage-listener\n[phpunit-bridge]: https://packagist.org/packages/symfony/phpunit-bridge\n[php-cs-fixer]: https://github.com/FriendsOfPHP/PHP-CS-Fixer\n[1060]: https://github.com/infection/infection/pull/1060\n\n## Decision\n","Since we are using the [`symfony/phpunit-bridge`][phpunit-bridge], we decide to leverage the\n[`Symfony\Bridge\PhpUnit\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\n`@covers` annotations whilst still benefit from it.\nThis however does not allow to completely forgo its usage due to the following cases:\n- A test testing more than one class, requiring multiple `@covers` annotations\n- A test case testing a ""test class"", i.e. code reserved for testing purposes\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\nsetting `general_phpdoc_annotation_remove` has been refused.\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\n`@covers` annotation is necessary or not, no further action has been voted for automating this\nprocess.\n### Status\nAccepted ([#1060][1060])\n[code-coverage-listener]: https://symfony.com/doc/current/components/phpunit_bridge.html#code-coverage-listener\n[phpunit-bridge]: https://packagist.org/packages/symfony/phpunit-bridge\n[php-cs-fixer]: https://github.com/FriendsOfPHP/PHP-CS-Fixer\n[1060]: https://github.com/infection/infection/pull/1060\n","Leverage the `Symfony\Bridge\PhpUnit\CoverageListener` in `phpunit.xml.dist` to avoid the requirement for `@covers` annotations while still benefiting from them. However, `@covers` annotations are still necessary in cases where a test covers multiple classes or a ""test class"" reserved for testing purposes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\n`self`.\nWhilst ""incorrect"", this usage does not break anything. Besides:\n- [PHUnit documentation][phpunit-doc] itself uses this by default\n- `$this` is much more widely used than `self` in this context in the community\n- all Infection code uses `$this`\nThere is not much shortcomings from using this other than the ""incorrectness"" of using a static\nmethod as a non-static one.\n### Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\nthe decision is to keep the usage of `$this` over `self` in the codebase.\n### Status\nAccepted ([#1061][1061])\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\n[1061]: https://github.com/infection/infection/pull/1061\n\n## Decision\n","Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\nthe decision is to keep the usage of `$this` over `self` in the codebase.\n### Status\nAccepted ([#1061][1061])\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\n[1061]: https://github.com/infection/infection/pull/1061\n",Keep the usage of `$this` over `self` in the codebase.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nUsing `@inheritdoc` was done inconsistently across the codebase so the decision of whether we use it\nsystematically or remove it systematically had to be done.\nA number of points:\n- [PHPDoc][phpdoc-inheritance] provides inheritance of the docblocks by default when appropriate\n- Static analysers such as PHPStan or Psalm can do without at the time of writing\nAlso it has a very limited value.\n### Decision\nDo not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\nbe removed when submitting pull requests.\n### Status\nAccepted ([#860][860])\n[phpdoc-inheritance]: https://docs.phpdoc.org/guides/inheritance.html\n[860]: https://github.com/infection/infection/issues/860\n\n## Decision\n,Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must\nbe removed when submitting pull requests.\n### Status\nAccepted ([#860][860])\n[phpdoc-inheritance]: https://docs.phpdoc.org/guides/inheritance.html\n[860]: https://github.com/infection/infection/issues/860\n,Do not use `@inheritdoc` tags or any of its variants. The `@inheritdoc` tags and its variants must be removed when submitting pull requests.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n// ...\ntry {\n// the statement that fail\n$this->fail();\n} catch (Exception $e) {\n// ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n// ...\n$this->expectException($exception)\n// the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n\n## Decision\n","As recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n","Leverage the `expectException*()` API when possible, as recommended by Sebastian Bergmann."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGiven Dapr is going out with language specific Actor SDKs, we formally introduced an Actor API into Dapr to make Actors are first-class citizen in Dapr. The goal of this review was to ensure Dapr can provide strong support of Service Fabric stateful actors programming model so that we can offer a migration path to the majority of existing actor users.\n\n## Decision\n","### Dapr\n* A separate Actor interface is defined.\n* Actors should support multiple reminders and timers.\n* Actor state access methods are encapsulated in the Actor interface itself.\n* Actor interface shall support updating a group of key-value states in a single operation.\n* Actor interface shall support deletion of an actor. If the actor is activated when the method is called, the in-flight transaction is allowed to complete, then the actor is deactivated, deleted, with associated state removed.\n### Non-Dapr\n* Transaction across multiple API calls is left for future versions, if proven necessary. Due to single-threaded guarantee, such transaction scope might be unnecessary. However, if developer expects an Actor code to behave atomically (in an implied transaction scope), we may have to implement this.\n","**Decision**\n\nDapr will support the Service Fabric stateful actor programming model by introducing an Actor API that aligns with the Service Fabric actor model semantics. This will provide a migration path for existing Service Fabric actor users to Dapr and enable them to leverage the benefits of Dapr's distributed application runtime, including simplified actor state management, automatic service discovery, and resilient communication."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before / after PR is merged or by a scheduler.\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\n\n## Decision\n","### Test environments\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\n* **Local machine**. contributors or developers will use [Minikube](https://github.com/kubernetes/minikube) to validate their changes and run new tests before creating Pull Request.\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) before/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\n### Bring up test cluster\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https://github.com/kubernetes/test-infra/tree/master/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\n### CI/CD and test result report for tests\nMany Kubernetes-related projects use [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), and [Testgrid](https://github.com/kubernetes/test-infra/tree/master/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\nInstead, Dapr will use [Azure Pipeline](https://azure.microsoft.com/en-us/services/devops/pipelines/) to run e2e tests and its [test report feature](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\n","**Decision:** To bring up a **Kubernetes cluster** with the **Dapr components** deployed, to run the **E2E tests** against the deployed components, and to report the **test results** in **JUnit XML format** using **JUnit 5**."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\n\n## Decision\n",We should consider adding universal namespace capabilities to Dapr.\n,Implement a cluster-aware service discovery mechanism that allows services in one cluster to discover and address services in other clusters by their fully qualified names in a universal namespace. This mechanism should be used in conjunction with a service registration mechanism that enables services to register their availability and capabilities with the discovery mechanism.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAuthenticode signing of binaries.\n\n## Decision\n,* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\n,Implement Authenticode signing for all released binaries.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\nSee https://github.com/dapr/java-sdk/issues/17\n\n## Decision\n,* Java 8 should be the minimum version supported for Dapr's Java SDK.\n* Java 11 should be used in samples and user documentation to encourage adoption.\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\n,"The minimum Java version supported by Dapr's Java SDK should be Java 8.\n\nJava 8 is still the mainly used version by the Java community in 2019. It is also the last LTS version of Java. This means that it is still supported by Oracle and will continue to receive security updates.\n\nJava 11 is the latest LTS version of Java. However, it is not yet as widely used as Java 8. This means that there may be some compatibility issues with older applications.\n\nBy supporting Java 8, Dapr's Java SDK can reach a wider audience. It can also avoid any potential compatibility issues with older applications."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\nWe needed a way to construct an accepted and constant way of naming our Docker images.\n\n## Decision\n","* An image will conform to the following format: \<namespace>/\<repository>:\<tag>\n* A valid tag conforms to the following format: \<version>-\<architecture>, or just \<version>, then arch is assumed Linux\n","As we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures, we have settled on the following naming convention for our Docker images:\n\n\n- Name: `repository-name`.`image-name`\n- Version: `major`.`minor`.`patch`\n- Architecture: `amd64` or `arm64`\n\nFor example, an image for the `front-end` repository with the `web` image, version `1.0.0`, and architecture `amd64` would be named as `front-end.web:1.0.0-amd64`.\n\nThis naming convention will help us to easily identify and manage our Docker images. It will also allow us to consistently version and architecture our images, which will be beneficial for our development and deployment processes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\nAn API change is needed to account for the requested type of operation.\n\n## Decision\n","### Naming\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\n### Types\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\nDid not provide meaningful value.\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\n### API structure\nIt was decided that the API url will be kept as: `http://localhost:<port>/v1.0/bindings/<name>`.\nThe verb for the HTTP API will remain POST/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\nThis is not a breaking change.\n### Schema and versioning\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\n",Update the API to allow for bi-directional streaming.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe reviewed storage API design for completeness and consistency.\n\n## Decision\n,"* All requests/responses use a single parameter that represents the request/response object. This allows us to extend/update request/response object without changing the API.\n* Add Delete() method\n* Support bulk operations: BulkDelete() and BulkSet(). All operations in the bulk are expected to be completed within a single transaction scope.\n* Support a generic BulkOperation() method, which is carried out as a single transaction.\n* Transaction across multiple API requests is postponed to future versions.\n* Actor state operations are moved to a new Actor interface. Please see [API-002-actor-api-design](./API-002-actor-api-design.md).\n",**Decision:** Approve the storage API design as complete and consistent.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNot adding content-type to state store, pubsub and bindings.\n\n## Decision\n","* We will not add content-type since it is a persisted metadata and it can cause problems such as:\n* Long term support since metadata persisted previously would need to be supported indefinitely.\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\nOriginal issue and discussion: https://github.com/dapr/dapr/issues/2026\n","**Decision:** Do not add content-type to state store, pubsub and bindings.\n\n**Rationale:**\n\n* **Simplicity:** Adding content-type introduces additional complexity and maintenance overhead.\n* **Compatibility:** Removing content-type ensures compatibility with existing systems and integrations.\n* **Performance:** Content-type adds unnecessary data to messages, potentially impacting performance.\n* **No functional impact:** The absence of content-type does not affect the functionality of the system.\n* **Protocol compliance:** PubSub and state store protocols do not require content-type for message content."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n\n## Decision\n",* We'll keep CLI versioning and runtime versioning separate.\n* CLI will pull down latest runtime binary during the *init()* command.\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\n,"Uncouple CLI versions from runtime versions.\n\n**Rationale:**\n\n* **Decouples development and deployment:** Allows CLI to evolve independently of runtime, enabling faster iteration and feature development.\n* **Enhances flexibility:** Developers can choose the latest CLI version with new features while using a stable runtime version for production.\n* **Reduces maintenance overhead:** Separate update processes simplify maintenance and minimize potential conflicts.\n* **Improves compatibility:** Enables running multiple runtime versions with a single CLI version, providing flexibility in migration scenarios.\n* **Facilitates experimentation:** Users can test newer CLI versions without committing to runtime upgrades, reducing risk and encouraging innovation."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere was a proposal to introducing versioning for HTTP App Callbacks. The goal of this review was to understand if a versioning was required and how it could handle situations post v1.0 of DAPR\n\n## Decision\n,"- Introducing versioning to app callback APIs would require changes to the user applications which is not feasible\n- There would be no way for DAPR runtime to find out the app callback version before hand\nWe decided not to introduce such a versioning scheme on the app callback APIs. Post v1.0, if required, the versioning could be implemented inside the payload but not on the API itself. A missing version in the payload could imply v1.0.\n","**Decision:** Versioning for HTTP App Callbacks is not required at this time.\n\n**Rationale:**\n\n* The current implementation of DAPR is stable and there are no anticipated breaking changes for HTTP App Callbacks in the foreseeable future.\n* Adding versioning would introduce complexity and overhead to the system.\n* The benefits of versioning are outweighed by the risks and costs.\n\n**Consequences:**\n\n* HTTP App Callbacks will not be versioned.\n* If DAPR undergoes a重大版本更新 in the future, the impact on HTTP App Callbacks will need to be carefully evaluated."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis decision record is to support multiple state stores support in Dapr. We agreed on the decision to introduce the breaking change in API\nto support multi state store with no backward compatibility.\nWith this change , the state API allows the app to target a specific state store by store-name, for example:\nv1.0/state/storeA/\nv1.0/state/storeB/\nEarlier this breaking change, the API is v1.0/state/`<key>`\nWe have reviewed multi storage API design for completeness and consistency.\n\n## Decision\n","*  New state store API is v1.0/state/`<store-name>`/\n*  If user is using actors and like to persist the state then user must provide actorStateStore: true in the configuration yaml.\nIf the attribute is not specified or multiple actor state stores are configured, Dapr runtime will log warning.\nThe actor API to save the state will fail in both these scenarios where actorStore is not specified or multiple actor stores\nare specified.\n*  It is noted that after this breaking change, actor state store has to be specified unlike earlier where first state store is picked up by default.\n* It is noted that this breaking change will also require a CLI change to generate the state store YAML for redis with actorStateStore.\n* To provide multiple stores, user has to provide separate YAML for each store and giving unique name for the store.\n* It is noted that the param's keyPrefix represents state key prefix, it's value included ${appid} is the microservice appid, ${name} is the CRDs component's unique name, ${none} is non key prefix and the custom key prefix\nFor example, below are the 2 sample yaml files in which redis store is used as actor state store while mongodb store is not used as actor state store.\n```\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\nname: myStore1  # Required. This is the unique name of the store.\nspec:\ntype: state.redis\nmetadata:\n- name: keyPrefix\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\n- name: <KEY>\nvalue: <VALUE>\n- name: <KEY>\nvalue: <VALUE>\n- name: actorStateStore  # Optional. default: false\nvalue : true\n```\n```\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\nname: myStore2 # Required. This is the unique name of the store.\nspec:\ntype: state.mongodb\nmetadata:\n- name: keyPrefix\nvalue: none # Optional. default appid. such as: appid, none, name and custom key prefix\n- name: <KEY>\nvalue: <VALUE>\n- name: <KEY>\nvalue: <VALUE>\n```\nSo with the above example, the state APIs will be : v1.0/state/myStore1/`<key>`\nand v1.0/state/myStore2/`<key>`\n","**Decision:** Introduce a breaking change in the API to support multi-state store support with no backward compatibility.\n\n**Rationale:**\n\n* The previous API (v1.0/state/`<key>`) did not allow the app to target a specific state store.\n* The new API (v1.0/state/`<store-name>/`<key>`) allows the app to target a specific state store, which is necessary for supporting multiple state stores.\n* The breaking change is necessary to ensure that the new API is not compatible with the old API, which could lead to unexpected behavior.\n* The multi-storage API design has been reviewed for completeness and consistency."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\n\n## Decision\n,"### Integration build release\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\n### Official build release\n#### Pre-release build\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\n**Pre-release process**\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\n```\n$ git tag ""v0.1.0-alpha.0"" -m ""v0.1.0-alpha.0""\n$ git push --tags\n```\n3. CI creates new build and push the images with only version tag\n4. Test and validate the functionalities with the specific version\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\n7. Repeat from 4 to 6 until all bugs are fixed\n#### Release the stable version to users\nOnce all bugs are fixed, we will create the release note under [./docs/release_notes](https://github.com/dapr/dapr/tree/master/docs/release_notes) and run CI release manually in order to deliver the stable version to users.\n### Release Patch version\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\n","To safely release new Dapr binaries and corresponding configurations without interrupting users, implement the following process:\n\n**Pre-Release:**\n\n1. Announce the planned release date and changes in a blog post or other public forum.\n2. Update the Dapr website and documentation with the new version information.\n3. Test the new release internally on a test cluster to ensure stability.\n4. Make a snapshot of the Dapr repository before the release.\n\n**Release Day:**\n\n1. Release the new Dapr binaries to the public repository.\n2. Update the Dapr Helm charts to reference the new binaries.\n3. Create a release branch in the Dapr repository and cherry-pick the necessary changes.\n4. Publish the new release notes to the Dapr website and documentation.\n\n**Post-Release:**\n\n1. Monitor the release closely for any issues or blockers.\n2. Address any reported issues promptly.\n3. Update the Dapr website and documentation as needed.\n4. Merge the release branch back into the main branch."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\n\n## Decision\n","### Concurrency model\n* Dapr supports two flavors of optimistic concurrency: first-write wins and last-write wins. First-write wins is implemented through ETag.\n* User code can express concurrency intention with a *config* annotation attached to a request. See **Config annotation** for details.\n* Future version of Dapr may support call throttling through application channel.\n* We'll choose last-write wins as the default.\n### Consistency model\n* Dapr supports both eventual consistency and strong consistency.\n* Actors always use strong consistency.\n* We'll choose eventual consistency as default for services other than actors.\n### Actor Transaction\n* Dapr-compatible Actor state stores shall support ACID transaction.\n* Dapr doesn't mandate specific transaction isolation level at this point. However, when deemed necessary, we can easily add those to **Config annotation** as needed.\n### Config annotation\n* User payload can contain an optional **config** annotation/element that expresses various constraints and policies to be applied to the call, including:\n* Concurrency model: first-write or last-write\n* Consistency model: strong or eventual\n* Retry policies:\n* Interval\n* Pattern: linear, expotential\n* Circuit-breaker Timeout (before an open circuit-breaker is reset)\n### State store configuration probe\n* An Dapr-compatible state store shall provide an endpoint that answers to configuration probe and returns (among others):\n* Supported concurrency model\n* Supported consistency model\n* A state store instance shall return the specific configuration of the current instance.\n* It's considered out of scope to require state store to dynamically apply new configurations.\n### Dapr\n* Update state store API spec to reflect above decisions\n* Create backlog of issues to implement above decisions\n",Components will define behaviors in the API spec and these behaviors will be implemented in the implementation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have defined gRPC services and protobuf messages without convention, which results in the duplicated protobuf definitions and inconsistent names of services and messages. Thus, this record defines the minimum-level coding convention for Protobuf message to improve the quality of grpc/protobuf message definitions.\n\n## Decision\n","* Use `google.protobuf.Any` data field only if the message field conveys serialized protobuf message with type url. Otherwise, use the explicit data type or protobuf message.\n* Use `Request` suffix for gRPC request message name and `Response` suffix for gRPC response message name\n* Do not use `Client` and `Service` suffix for gRPC service name e.g. (x) DaprClient, DaprService\n* Avoid the duplicated protobuf message definitions by defining the messages in shared proto\n* Define and use enum type if field accepts only predefined values.\n","**Decision:**\n\nEstablish a coding convention for Protobuf message definitions to ensure consistency, reduce duplication, and improve readability.\n\n**Rationale:**\n\n* **Consistency:** A coding convention will ensure that all Protobuf messages follow the same naming conventions, avoiding confusion and inconsistencies.\n* **Reduced Duplication:** By following a convention, we can eliminate duplicate Protobuf definitions and centralize message definitions in a single location.\n* **Improved Readability:** A consistent coding style will make Protobuf messages easier to read and understand, reducing the time spent on deciphering the code.\n* **Maintainability:** A well-defined coding convention will make it easier to maintain and update Protobuf messages, ensuring long-term code quality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\n\n## Decision\n","### Dapr\n* All components use the same **Component** manifests, identified by a component **type**.\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\n","We will introduce different manifest types for State Stores, Message Buses, and Bindings.\n\n**Rationale**\n\nIntroducing different manifest types for State Stores, Message Buses, and Bindings will allow us to better organize and manage our manifests. It will also make it easier to identify the different types of resources that we are working with.\n\n**Consequences**\n\n* We will need to create new manifest types for State Stores, Message Buses, and Bindings.\n* We will need to update our tooling to support the new manifest types.\n* We will need to update our documentation to reflect the new manifest types."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https://github.com/dapr/cli/issues/411).\n\n## Decision\n,"* Calling `dapr init` will\n* Install `daprd` binary in `$HOME/.dapr/bin` for Linux/MacOS and `%USERPROFILE%\.dapr\bin` for Windows.\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\n* Create the default `components` folder in `$HOME/.dapr/bin` for Linux/MacOS or `%USERPROFILE\.dapr\bin` for Windows.\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\n* Create a default configuration file in `$HOME/.dapr/config.yaml` for Linx/MacOS and `%USERPROFILE%\.dapr\config.yaml` for Windows for enabling tracing by default.\n* Calling `dapr init --slim` will\n* Install the binaries `daprd` and `placement` in `$HOME/.dapr/bin` for Linux/MacOS and `%USERPROFILE%\.dapr\bin` for Windows.\n* Create an empty default `components` folder in `$HOME/.dapr/bin` for Linux/MacOS or `%USERPROFILE\.dapr\bin` for Windows.\n* Calling `dapr uninstall` will\n* Remove the folder containing binaries `bin` the default path `$HOME/.dapr` for Linux/MacOS and `%USERPROFILE%\.dapr` for Windows.\n* Remove the docker dapr_placement if Docker is installed.\n* Calling `dapr uninstall --all`\n* Remove the folder containing binaries `bin` the default path `$HOME/.dapr` for Linux/MacOS and `%USERPROFILE%\.dapr` for Windows.\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\n* Remove the default folder `$HOME/.dapr` in Linux/MacOS and `%USERPROFILE%\.dapr` in Windows.\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME/.dapr` for Linux/MacOS and `%USERPROFILE%\.dapr` for Windows.\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\n* The `dapr` CLI by default will expect the `daprd` in `$HOME/.dapr/bin` for Linux/MacOS and `%USERPROFILE%\.dapr\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\n","**Decision:**\n\nThe behavior of `init` and `uninstall` commands on Self Hosted mode will change to match the behavior in Hosted mode, as follows:\n\n* `init` will no longer create or modify Kubernetes resources.\n* `uninstall` will no longer delete Kubernetes resources.\n\n**Rationale:**\n\nThis change is necessary to ensure consistency between the Self Hosted and Hosted modes of Dapr. This will simplify user experience and reduce confusion.\n\n**Consequences:**\n\n* Users must manually create and delete Kubernetes resources when using Dapr in Self Hosted mode.\n* The `init` and `uninstall` commands will no longer be able to automatically create or delete Kubernetes resources."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGo community has the multiple http server implementations, such as go net/http, fasthttp, gin, to serve HTTP Restful API. This decision records describes which http server implementation uses in Dapr.\n\n## Decision\n",* Use [fasthttp server](https://github.com/valyala/fasthttp) implementation because fasthttp offers [the best performance and lowest resource usages](https://github.com/valyala/fasthttp#http-server-performance-comparison-with-nethttp) for the existing HTTP 1.1 server\n* Use [fasthttpadaptor](https://godoc.org/github.com/valyala/fasthttp/fasthttpadaptor) if you need to convert fasthttp request context to net/http context.\n,"The decision is to use **Go net/http** as the default HTTP server implementation in Dapr.\n\n**Rationale:**\n\n* **Widely adopted:** Go net/http is the standard library for HTTP server in Go, and it is widely adopted by the Go community. This makes it easy for developers to find resources and support for using net/http.\n* **Stable and mature:** Go net/http has been in development for many years and is considered stable and mature. This means that it is less likely to have bugs or security vulnerabilities.\n* **Extensible:** Go net/http is extensible, allowing developers to add custom functionality to the server. This makes it possible to use net/http to build complex and sophisticated HTTP applications.\n* **High performance:** Go net/http is a high-performance HTTP server implementation. It is able to handle a large number of concurrent requests efficiently.\n\n**Consequences:**\n\n* Developers using Dapr will need to use Go net/http to build their HTTP applications.\n* Dapr will provide support for using Go net/http in its documentation and tutorials.\n* Other HTTP server implementations may be supported in the future, but Go net/http will remain the default implementation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we keep building up Dapr features, it becomes apparent that we need to refactor the existing code base to reinforce component modularity. This will improve testability and maintainability in long run. And this refactor also lays the foundation of opening up extensible points (such as Bindings) to the community.\n\n## Decision\n","### Dapr\n* Formally separate hosting and API implementations. Hosting provides communication protocols (HTTP/gRPC) as different access heads to the same Dapr API implementation.\n* Ensure consistency between gRPC and HTTP interface.\n* Separate binding implementations to a separate repository.\n* Use smart defaults for configurable parameters.\n* Rename Dapr runtime binary from **dapr** to **daprd**.\n### Non-Dapr\n* We may consider allowing Dapr to dynamically load bindings during runtime. However, we are not going to implement this unless it's justified by customer asks.\n* A unified configuration file that includes paths to individual configuration files.\n* Provide a Discovery building block with hopefully pluggable discovery mechanisms (such as a custom DNS).\n","To refactor the Dapr code base to improve component modularity, testability, and maintainability. This refactor will also lay the foundation for opening up extensible points to the community."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDapr exposes APIs for building blocks which can be invoked over http/gRPC by the user code. Making raw http/gRPC calls from user code works but it doesn't provide a good strongly typed experience for developers.\n\n## Decision\n,"* Dapr provides language specific SDKs for developers for C#, Java, Javascript, Python, Go, Rust, C++. There may be others in the future\n- For the current release, the SDKs are auto-generated from the Dapr proto specifications using gRPC tools.\n- In future releases, we will work on creating and releasing strongly typed SDKs for the languages, which are wrappers on top of the auto-generated gRPC SDKs (e.g. C# SDK shipped for state management APIs with the 0.1.0 release.) This is the preferred approach. Creating purely handcrafted SDKs is discouraged.\n* For Actors, language specific SDKs are written as Actor specific handcrafted code is preferred since this greatly simplifies the user experience. e.g. The C# Actor SDK shipped with the 0.1.0 release.\n",Create a set of client helper libraries for each building block in each language to provide a strongly typed experience for developers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\n\n## Decision\n,"### Dapr\n* All messaging APIs are grouped under a **messaging** namespace/package.\n* We define three distinct messaging interfaces:\n- **direct**\nOne-to-one messaging between two parties: a sender sending message to a recipient.\n- **broadcast**\nOne-to-many messaging: a sender sending message to a list of recipients.\n- **pub-sub**\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\n",We will adopt the following naming conventions for messaging interfaces:\n\n- Request interfaces will be named using the `Request` suffix.\n- Response interfaces will be named using the `Response` suffix.\n- Event interfaces will be named using the `Event` suffix.\n- Command interfaces will be named using the `Command` suffix.\n- Message interfaces will be named using the `Message` suffix.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe reviewed parity of state store APIs .\n\n## Decision\n,"* GetState APIs continue to have Single Key Get and Bulk Get APIs behaviour as current 0.10.0 version.\n* SaveState API will continue to have one SaveState API endpoint. If user wants to save single key, same save state API will be used\nfor passing single item in the bulk set.\nPotential issues arises if following new single key save state API is introduced:\n`Post : state/{storeName}/{key}`\nThis will conflict with\n- State Transaction API, if the key is ""transaction""\n- GetBulkState API, if the key is ""bulk""\nSo the decision is to continue the Save State API behaviour as current 0.10.0 version.\n* Bulk Delete API might come in future versions based on the scenarios.\n",Implement both in-memory and etcd state stores.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\n\n## Decision\n",We should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\n,We will provide a set of pre-built tracing components that user code can optionally use to automatically trace incoming requests.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile we were considering how to ship the CSSK, the question of how it\nshould be organized came up several times. The crux of the issue was\nwhether the more reusable parts (for example, the event dispatch bits)\nshould be separated into a library, or whether it should all just live\nin one big pile of code. Currently, the CSSK is organized in the\nlatter manner.\nSeparating the infrastructure code out into a separate library would\nmake it more obvious which parts of the code are intended to be edited\nby the developer. Right now, it’s not immediately obvious where to\nmake changes, and what bits are there to support the development\nexperience.\nSeparating the infrastructure code out into a separate library would\nalso make it far more inconvenient to make changes to it. Developers\nwould need to update the lib, possibly creating their own fork, and\nthen ensure that the app code was using the appropriate version. This\nis extremely likely to happen because the infrastructure code is\nneither comprehensive nor mature.\n\n## Decision\n","We have decided to keep it all together, but clearly separate the\n“library” code from the “app” code by using separate directories for\nthose two types of code: “lib” and “app”.\n","**Decision:** The CSSK will be organized into a single codebase, with the more reusable parts separated into a library.\n\n**Rationale:**\n\n* **Clarity:** Separating the reusable parts into a library will make it more obvious which parts of the code are intended to be edited by the developer. This will improve the development experience and reduce the risk of unintended changes.\n* **Flexibility:** Keeping the CSSK in a single codebase will make it more flexible and easier to maintain. Developers will be able to make changes to both the reusable parts and the application code without having to manage multiple dependencies.\n* **Convenience:** Updating the reusable parts will be easier if they are part of the same codebase. Developers will not have to create their own fork or manage multiple versions of the library.\n\n**Consequences:**\n\n* **Increased maintenance overhead:** The reusable parts will need to be maintained separately from the application code. This could increase the maintenance overhead for the project.\n* **Potential for code duplication:** There is a risk that some code may be duplicated between the reusable parts and the application code. This could lead to maintenance issues down the road."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe API for publishing release notes is not advanced and is just a giant blob of JSON containing every release in the database. A GitLab Job runs on a schedule and reads this blob, splits it into a file per release, and commits those changes to a GitHub repo. This job is slow and is something else to maintain and monitor separate from Nucleus. So the decision was between improving the API to only send the releases that had changed since the last sync, or to push changes to GitHub as soon as the're made. The latter has the advantages of happening very quickly after the change is saved, and having the context of the Nucleus user who made the change which can also be recorded in the Git commit.\n\n## Decision\n",We've decided to go with pushing changes directly to GitHub via the GitHub API and using an async worker system to do it. The async system chosen was [Spinach][].\n,Push changes to GitHub as soon as they're made.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Establish an Architectural Decision Record (ADR) process and a central repository for documenting all architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe 'core mechanic' of this game is the player being in control of allied plants.  There may be other cases of player control of other in-game entities as dev continues, but this is core.\nGiving creatures and the player character pluggable command/control is more dev overhead.  As a spare time project, any choice for extra complexity deserves extra scrutiny.  On the same wave, though, the outcomes which can justify the extra effort are a larger set:  Not just important functionality, but significant learning and straight pleasure in craftsmanship can be enough reason.\nThis offers options for reuse.\nIf our guy is terrified, the player's InputCommandSource can be temporarily replaced with a FleeingCommandSource (or some such).  When the player is in charge of other entities, they get an InputCommandSource themselves, and will naturally be controlled by the player when their moments to act arrive in the schedule.\n\n## Decision\n","I'll do this.  It feels correct, and it trends toward the sort of wins and frustrations I'm hoping for in this project.  That is, where my decisions, rather than the guts of frameworks, are what I'm wrestling most often.\n",Implement an InputCommandSource system with pluggable control capabilities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI feel that ADRs in some form may be very beneficial at Envisage, but two things make me reluctant to push the practice right now.  First, ignorance--I've never done this, so I can't talk personally about the experience.  Second, the enthusiasm of first encounter.  Both of these are addressed by actually doing it for a while.\nCopper Bend is a learning/entertainment project.  Trying new techniques and technologies is a primary goal.\nThe Envisage products are very different from this one.  What works well here won't necessarily cross over.\n\n## Decision\n","I will avoid tooling, and simply handcraft the documents.  I'll keep them in the main project repo.  I'll review periodically, trimming the present-but-useless and adding the missing-yet-promising.\nI won't worry about synchronizing the format between ADR docs.\n","**Decision:**\n\nExperiment with ADRs on the Copper Bend project to explore their benefits and drawbacks in a low-stakes environment.\n\n**Rationale:**\n\n* **Overcoming ignorance:** Hands-on experience will provide valuable insights into the practical implementation of ADRs.\n* **Mitigating the enthusiasm of first encounter:** Testing ADRs in a controlled setting will allow the team to assess their effectiveness objectively before committing to widespread adoption.\n* **Contextual relevance:** Copper Bend is a suitable environment for experimenting with ADRs due to its focus on learning and trying new techniques.\n* **Differentiation from Envisage products:** The unique nature of Copper Bend will help the team identify specific benefits and challenges of ADRs that may not be applicable to other Envisage products.\n\n**Additional Considerations:**\n\n* The team should document their experiences with ADRs, both positive and negative, to inform future decisions.\n* The experiment should be time-boxed to avoid prolonged investment in an ineffective practice.\n* If ADRs prove beneficial in Copper Bend, a gradual rollout to other Envisage products can be considered."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\n\n## Decision\n",As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\n,Implement a RESTful API in the simulator to communicate with the WebApp. The API will expose endpoints to control the simulator and to retrieve data from it.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nComponents might be more grokable if they were structured consistently. This ADR proposes conventions\n\n## Decision\n,"TL;DR:\n```\nprimer-react/\n├─ src/\n│  ├─ Breadcrumbs/\n│  │  ├─ index.ts                    // Just re-exporting?\n│  │  ├─ Breadcrumbs.tsx             // Primary component\n│  │  ├─ BreadcrumbsItem.tsx         // Subcomponent (include parent component name to increase findability in most IDEs)\n│  │  ├─ Breadcrumbs.mdx             // Documentation. Always .mdx, not .md\n│  │  ├─ Breadcrumbs.stories.tsx\n│  │  ├─ Breadcrumbs.test.tsx        // Unit tests\n│  │  ├─ Breadcrumbs.types.test.tsx  // Type tests\n│  │  ├─ Breadcrumbs.yml             // Component metadata (Possible future)\n│  │  └─ __snapshots__/\n┆  ┆\n```\n### Rules\n- Every component should have its own PascalCased directory directly under `src/`\n- Subcomponents should be properties of the exported parent component (e.g., `Breadcrumbs.Item`)\n- Replacements of existing components should use an incrementing number (e.g., `Breadcrumbs2` rather than `NewBreadcrumbs`)\n",Define component structure conventions for improved code readability.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nThroughout the last few years folks from the Design Infrastructure, Web Systems, and UI Platform teams have discussed the idea of using custom elements for behaviors in Primer React. The main goal of using custom elements in Primer React is to be able to author behaviors once and reuse them in any framework. Several experiments have been conducted which are listed above.\n### Assumptions\nDe-duplication is not our highest or only priority. Attempts at de-duplication must be weighed against changes to the maintainer, developer, and customer experience.\n### Findings\n#### Developer experience regressions\n- Custom elements rendering their own subtrees (ShadowDOM) requires polyfills for as-yet implemented specifications. This means Primer React will accumulate added complexity if we were to implement Custom Elements with ShadowDOM.\n- Implementing Custom Elements in Primer React will require a division of client side and server side code, as custom elements should only be executed in a browser environment. Currently Primer React is ""isomorphic"" - in that the code can be executed anywhere that React can be, which includes NodeJS server runtimes, as well as the client side. While not insurmountable this does mean Primer React will accumulate added complexity, which likely will be surfaced to the user.\n- While it's possible to add server side libraries to enable Custom Elements to be rendered on the Server, this adds more complexity and is antithetical to the usage patterns of custom elements.\n- As of this writing, you cannot style custom elements with styled-components[^1]. This means that if a component wants to use a custom element to get behaviors and you also want to style that component, you must use another wrapper div to apply styles. This is a bug in styled-components and should be fixed in the next release.\n#### Incompatibility with some React tools\nSome of our GitHub custom elements such as `details-dialog` and `details-menu` make assumptions about the DOM tree. For example, `details-dialog` expects a `details` element to wrap the custom element and uses this assumption[^2] to determine whether or not clicks are happening inside or outside of the dialog and closes the dialog if the click happened outside of the dialog. This makes sense in most cases and is a nice way of enforcing proper usage of the details element, but breaks down when used with [React Portals](https://reactjs.org/docs/portals.html) which are often used to ensure menus are displayed correctly in cases where a parent has an overflow: hidden applied to it, or incompatible z-index.\n#### Extensibility\nBuilding behaviors in React Hooks gives us the ability to provide things like state and state change hooks to the consumer of the component. This allows the user to build on additional behaviors to the component based on the state or other variables provided to the component consumer. Doing the same with custom elements would require listening to events on the document[^3] and reacting to them. This is certainly do-able, but goes against some of the foundational principles of React (reacting to changes in the DOM vs changes in React state).\n#### Organizational overhead\n- GitHub’s custom elements are all managed in different repos which introduces more maintenance overhead.\n- You'd need to npm link while developing if you want to test changes out with the presentational components themselves instead of making changes and seeing updates instantly. npm link usually doesn't work well with hot module reloading either.\n- You'd need to draft & publish releases to both libraries every time you want to update the behavior\n- If the behaviors are shared between github.com and Primer React, you'd need to do careful testing in both environments to make sure that changes don't create any regressions. That greatly widens the context that engineers need to keep in mind every time a change is made.\n- Reacting to changes will take a bit more time as we’ll need to orchestrate releases between custom elements and Primer React - as opposed to having behaviors already present in Primer React which can be versioned in lockstep.\n- Engineers who want to contribute to Primer React Components to build new components and behaviors would need to be familiar with both custom elements and React, two very different paradigms, and context switch between the two.\n#### Other\n- The custom element and web component API progress slower than React due to changes needing to go through the whatwg standards process.\n#### Risks of not switching to custom elements for behaviors\n- We spend extra time building behaviors in React that have already been built in our [custom elements library](https://github.github.io/web-systems-documentation/#custom-elements).\n- There are currently 19 behaviors/components listed on the custom elements documentation site. Several of these we have already implemented in React in either Primer React, Doctocat, or other React applications at GitHub which can be upstreamed (details-dialog, details-menu, clipboard-copy, text-expander, autocomplete, task-list via drag and drop hooks, tab-container, text-expander).\n- We decide not to invest further in React at GitHub and have wasted time we could have spent building more custom elements.\n- This seems unlikely as there seems to be clear consensus that we will continue to build more and more highly interactive products.\n- The React library is abandoned and becomes obsolete.\n- This is a risk with any technology that we may use, seems highly unlikely in the near term.\n- While also a possibility for custom elements, the track record demonstrates deprecations of Web APIs is extremely rare and has a long deprecation path.\n- Behaviors in github.com using custom elements and behaviors in Primer REact diverge, leading to an inconsistent experience.\n- This is probably the biggest risk we face, but moving to custom elements isn’t necessarily the only or best solution. We should explore other ways of detecting divergence such as integration tests.\n\n## Decision\n","### Custom elements\nDue to the challenges listed above and our priorities listed in the [Assumptions](#assumptions) section, we are not investing time in building out behaviors with custom elements in our Primer React library. Instead, we should spend time expanding coverage using React Hooks and focus on finding other approaches for making sure implementation of behaviors in our different stacks are consistent (such as integration tests).\n### Vanilla JavaScript behaviors\nSome behaviors can be implemented as vanilla JavaScript without introducing additional complexity to Primer React or its consumers. In cases where this is possible, behaviors will be implemented with no dependencies except the DOM and consumed within React Hooks to provide their functionality to Primer React.\nIn general, _portions of behaviors_ that affect or rely on **user interactions and events**, **shared state**, or **CSS styles** should be kept in React Hooks. Parts of the behavior that can be implemented in isolation of these concepts should be built with no dependency on React or other libraries.\n[^1]: https://codesandbox.io/s/demo-styling-custom-element-g973d?file=/src/index.tsx\n[^2]: https://github.com/github/details-dialog-element/blob/main/src/index.ts#L195\n[^3]: https://github.com/github/details-dialog-element#details-dialog-close\n",Do not use Custom Elements for behaviors in Primer React.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur component prop APIs have, at times been a bit of a mess. We've seen:\n- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)\n- Explicit plans to change some of those (e.g., the deprecation of Styled System props)\n- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)\nThis ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.\n\n## Decision\n","### 🟢 `sx`\nAll components that ultimately render to the DOM should accept an `sx` prop.\nThe `sx` prop (of type `SystemStyleObject`) should generally set styles for the root HTML element rendered by the component. An exception would be components like `<Dialog>`, whose outermost HTML element is a backdrop. In that case, it would be appropriate for `sx` styles to apply to child of the backdrop that is more likely to need styling overrides.\n### 🟢 `ref`\nAll components that ultimately render to the DOM should accept a `ref` prop. That `ref` prop should most often be passed to the root HTMLElement rendered by the component, although occasionally a different descendent node may make more sense.\nSee also: [Discussion on `ref` props (internal)](https://github.com/github/primer/discussions/131)\n### 🟡 `as`\nOnly components with a clear need for polymorphism should accept an `as` prop. Reasonable cases include:\n- Components that need functionality from the component passed to the `as` prop, like a `<Button>` that renders a React Router link.\n- Components whose accessibility are improved by using semantically appropriate HTML elements, like an ActionList\nWhen a Primer component user passes an `as` prop to a component, it should be done in a way that is consistent with the component’s intended use. In some situations we can enforce that with a narrowed type for our `as` prop.\nSee also: [Discussion on `as` props (internal)](https://github.com/github/primer/discussions/130)\n### 🟡 DOM props: Limited\nAll components that accept an `as` prop should accept props en masse for the element specified by the `as` prop (excluding props of the same name already used by the component). _Additionally_, some other elements that do _not_ accept an `as` prop should accept the props for their root HTML element when those props are fundamental to the component’s function (e.g., `<TextInput>` should accept DOM props for its underlying `<input>`).\n### 🔴 Styled System props\nComponents should not accept Styled System props (except our utility components: `Box` and `Text`)\n_Reasoning:_ Utility components are meant to provide a convenient API for writing styles (including styles that reference theme and other context managed within Primer). Non-utility components implement specific design patterns where additional styling is available for exceptional cases.\nSee also: [Discussion on the deprecation of styled-system props (internal)](https://github.com/github/primer/discussions/132)\n### 🔴 `theme`\nComponents should not accept a `theme` prop (with the exception of `ThemeProvider`).\n_Reasoning:_ The `theme` prop doesn't enable anything that can't be done with `<ThemeProvider>`, and promotes the anti-pattern of per-component theme overrides.\n### `children`\nI'm intentionally withholding advocacy about `children` prop types because I expect that topic will be covered by a future ADR.\n### Sequencing\n1. Deprecate remaining unwanted Styled System props (should be done? Let's verify.)\n1. Release an eslint rule to disallow Styled System props\n1. Release an eslint rule to disallow `theme`\n1. Migrate all usage within PRC\n1. Assist GitHub projects with migration\n1. Remove support for unwanted props\n1. Update docs to reflect the standards in this ADR\n",**Codify prop API conventions and move towards a consistent API across components**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPrimer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.\n`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.\n\n## Decision\n",We will rewrite Primer React components in TypeScript.\n,"**Migrate the Primer React components library to TypeScript**. This will enable us to:\n\n* **Maintain type definitions more easily and accurately**. TypeScript will automatically generate and maintain type definitions based on the implementation code, eliminating the need for manual maintenance and reducing the likelihood of errors.\n* **Catch type errors at compile time**. TypeScript will enforce type safety, which will help us catch type errors at compile time, improving the quality and reliability of our code.\n* **Improve the developer experience**. TypeScript will provide autocompletion, type checking, and other features that will make it easier for developers to use the Primer React components library and build applications with it."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKitsune supports many locales,\nand has content which we want to be searchable in those locales.\nElasticsearch has support for many language-specific analyzers:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.9/analysis-lang-analyzer.html\nSearch v1 used per-document analyzers,\nthat is to say, within the same index:\n```\ndoc_1: { ""content"": ""Hello world"" }\ndoc_2: { ""content"": ""Hallo Welt"" }\n```\n`doc_1.content` could be analyzed using an english analyzer,\nand `doc_2.content` could be analyzed using a german analyzer.\nWell before version 7 ES removed this feature,\nand now all fields of the same name across an index must be analyzed the same,\nso we must take a different approach with the current Search implementation.\nWe can either place separate locales in their own index,\nand set up locale-specific analyzers for the same field name across indices.\nOr we can keep separate locales within the same index,\nand define unique field names for each field which needs to be analyzed under a specific locale.\n\n## Decision\n","Heavily influenced by: https://www.elastic.co/blog/multilingual-search-using-language-identification-in-elasticsearch\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\nwith the appropriate analyzer defined on that key,\nsuch that:\n```\ndoc_1: { ""content"": { ""en-US"": ""Hello world"" }}\ndoc_2: { ""content"": { ""de"": ""Hallo Welt"" }}\n```\n`doc_1.content.en-US` is analyzed using an english analyzer,\nand `doc_2.content.de` is analyzed using a german analyzer.\n",We should place separate locales in their own index. This will allow us to set up locale-specific analyzers for the same field name across indices. This is a more flexible and scalable approach than keeping separate locales within the same index and defining unique field names for each field that needs to be analyzed under a specific locale.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we are re-implementing our search in ElasticSearch v7,\nwe must re-implement Ask a Question (AAQ) search.\nThere is one primary use-case for storing AAQ documents in ES which Search v1 supports,\nwhich we must continue to be able to do in the redesigned Search:\nsearching for an AAQ thread as a unit.\nThere are other secondary use-cases which we may want to support when storing AAQ documents in ES.\nA non-exhaustive list of these are:\n-   Searching within AAQ threads\n-   Searching within questions and their solutions\n-   Aggregating answers to create contribution data\nWe also want search to be _fast_,\nso should model our data to avoid nested fields and parent-child relationships,\nand use de-normalization wherever possible:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.9/tune-for-search-speed.html#_document_modeling\n\n## Decision\n","We will model our data in ES based on what makes most sense for our expected use-cases,\nand what will make those fast and efficient,\nrather than feeling like we must have a 1:1 copy of our data structure in our database.\nIn this vein, we will use a structure of two document ""types"" within one index,\n`QuestionDocument` and `AnswerDocument`,\nwhere a `QuestionDocument` will exist for each `Question` which exists in the database,\nand an `AnswerDocument` will exist for each `Answer` which exists in the database.\n`AnswerDocument` will be a subclass of `QuestionDocument` so will inherit all of its fields,\nand we will set the value of those fields to the value of the `Question` associated with its `Answer`.\nFor instance, if in database:\n```\nanswer.created => 2020-10-27\nanswer.question.created => 2020-11-01\n```\nin elastic:\n```\nanswer_document.created => 2020-10-27\nanswer_document.question_created => 2020-11-01\n```\n`QuestionDocument` will also have an `answer_content` field,\nwhich contains the content of all a Question's Answers.\nWe will set this to null in the `AnswerDocument`.\n",We will model the AAQ data as follows:\n-   Store all AAQ threads in a single ES document\n-   Store each question and solution in a subdocument within the thread document\n-   Store each answer in a subdocument within the question document
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** To implement an Architectural Decision Record (ADR) process to document and track architectural decisions made on the project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to record and communicate architectural decisions.\n* They help ensure that decisions are well-informed, transparent, and traceable.\n* They facilitate collaboration and understanding among team members and stakeholders.\n* They serve as a valuable reference for future maintenance and evolution of the system.\n\n**Implementation:**\n\n* Define a standard ADR template with sections for decision context, problem, solution, alternatives considered, consequences, trade-offs, and decision.\n* Establish a central repository (e.g., a wiki or document management system) to store ADRs.\n* Assign responsibility for creating and maintaining ADRs.\n* Implement a review and approval process for ADRs.\n* Regularly review and update ADRs as the project evolves."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDirect form the [jdbc documentation](http://clojure-doc.org/articles/ecosystem/java_jdbc/home.html)\nAnother common need with SQL is for database migration libraries. Some of the more popular options are:\n* Drift\n* Migratus\n* Ragtime\n\n## Decision\n,"After a quick read of the documentation, and see that all solutions are similar, I select Migratus, by the comodity of have a lein plugin\n",Use Drift for database migrations.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTe console appication it is the more siple app that come to my mind.\n\n## Decision\n,Creates a console app and send the data via parameters\n,Use a console application to create a simple command-line interface (CLI) for the application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn clojure a normal work flow is use the repl. The problem is that when you reload the appliction the states die.\n[mount](https://github.com/tolitius/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.\nDepending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.\n\n## Decision\n","Use mount libray and clojure tools space.\nThe decision of mount over component is made afer review bouth solutions. My feeling is\n* Mount is more clojure dialect oriented\n* Mount use namespace and component\nRecords, this made that the compliler control the dependencies\n* Mount is [less contagious](https://engineering.riotgames.com/news/taxonomy-tech-debt)\n",Use [mount](https://github.com/tolitius/mount) to preserve state during application reloads.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\n\n## Decision\n","We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\n","**Decision:** Invest in team training and cloud infrastructure expertise to build the required high-scalability and resilience.\n\n**Justification:**\n\n* The team needs to acquire in-depth knowledge of Clojure and cloud computing to effectively design and implement a scalable application.\n* Cloud infrastructure expertise is crucial for understanding and leveraging cloud services for autoscaling, resilience, and geographic distribution across multiple cloud providers.\n* By investing in training and expertise, the team can develop the necessary skills to build and maintain a high-performance application that meets the scalability and availability requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the final stack canditate for persistence is [cockroachdb](https://www.cockroachlabs.com/), but the windows version\n\n## Decision\n","We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\n",CockroachDB is not a suitable option for the stack due to its lack of support for Windows.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\n\n## Decision\n","This will be a pet project, the goal is learning clojure ecosystem, not the aplication itself.\n","**Decision:** Adopt a framework for Clojure development, such as Luminus or Compojure, to provide a structured and well-supported approach to common architectural tasks.\n\n**Rationale:**\n\n* Frameworks provide pre-defined components and best practices for building applications, reducing the need for developers to implement complex features from scratch.\n* They offer standardized interfaces for common tasks, reducing duplication of effort and allowing for easier integration with other tools.\n* Frameworks often include support for databases, API routing, authentication and security, streamlining the development process.\n* By adopting a well-established framework, developers can leverage the experience and knowledge of the community, resulting in more robust and maintainable applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need working with command line arguments.\n\n## Decision\n,Use of [tools.cli](https://github.com/clojure/tools.cli)\n,Use the `argparse` package.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are testing how deploy in diferente databases per envirtoment, and don't find the way of do it using the pluggin\n\n## Decision\n",Develop the migration part in the code\n,"**Decision**: Use a database migration tool to manage database changes and deployments across different environments.\n\n**Rationale**: A database migration tool provides a structured and automated way to handle database schema changes and data migrations, ensuring consistency and integrity across different environments. It allows for the safe and controlled deployment of database changes, reducing the risk of data loss or corruption. Additionally, migration tools can help track and manage the history of database changes, making it easier to revert or roll back changes if necessary."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to document and track architectural decisions.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture and communicate architectural decisions.\n* They help ensure that decisions are made in a thoughtful and transparent manner.\n* ADRs serve as a valuable reference for future development and maintenance efforts.\n\n**Implications:**\n\n* A template and process for creating and reviewing ADRs will be developed.\n* Developers will be responsible for creating and maintaining ADRs related to their work.\n* The ADRs will be reviewed by the architectural review board or designated decision-making body.\n* The ADRs will be stored in a central repository and made available to the project team.\n\n**Consequences:**\n\n* Improved clarity and consistency in architectural decision-making.\n* Reduced risk of architectural drift and unintended consequences.\n* Enhanced collaboration and communication among stakeholders.\n* Increased transparency and accountability for architectural decisions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\nread a bit about how vim-plug was pretty good. And it felt like it was time for\na change.\n\n## Decision\n,[vim-plug](https://github.com/junegunn/vim-plug) will be used to manage Vim\nplugins in favour of [Vundle](https://github.com/VundleVim/Vundle.vim).\n,Switch to using vim-plug as the plugin manager for Vim.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Starting with macOS Catalina, zsh will be the default shell](https://support.apple.com/en-us/HT208050).\n\n## Decision\n",zsh should be used in place of bash.\n,Use zsh as the default shell for macOS Catalina and later.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nYCM has been a mostly positive experience over the years of using it, however,\nthere have been many occasions where time has been spent fixing issues. The\ntime has come to look at using an alternative.\n\n## Decision\n",The decision is to use [CoC](https://github.com/neoclide/coc.nvim).\n,Migrate from YCM to an alternative solution that meets YCM's functionality and provides additional features.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n","To record the architectural decisions made on this project, we will use an Architectural Decision Record (ADR). ADRs will be created for each significant architectural decision that is made, and will include the following information:\n\n* The decision that was made\n* The rationale for the decision\n* The alternatives that were considered\n* The consequences of the decision\n* The date the decision was made\n* The person(s) who made the decision\n\nADRs will be stored in a central repository, and will be made available to the project team. This will help to ensure that all stakeholders are aware of the architectural decisions that have been made, and will help to prevent the same decisions from being made multiple times."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/).\nThey are a thing that will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repository which has a very basic use case will\nprovide some exposure to the service.\n\n## Decision\n,The decision is to replace Travis CI with GitHub Actions.\n,Use GitHub Actions for CI/CD pipelines.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want Pomegranate to be a separate application from Figgy, but need some way\nfor Figgy to tell Pomegranate about new resources so that when something is\nmarked Complete in Figgy or taken down that it's reflected in Pomegranate.\n\n## Decision\n",Figgy will send create/update/delete messages to a fanout RabbitMQ Exchange.\nPomegranate will register a durable queue which listens to that exchange and\nprocess messages using [Sneakers](https://github.com/jondot/sneakers).\nThe message will contain the following information:\n* Collection slugs the object is a member of\n* Manifest URL of the object\n* change event (create / update / delete)\n,Implement a webhook endpoint that Figgy can POST to when a resource is completed or taken down.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described at https://adr.github.io/\n",Establish an Architectural Decision Record (ADR) template and process to document all architectural decisions made on the project. This template should include the following information:\n\n* Decision ID\n* Decision Title\n* Decision Statement\n* Decision Rationale\n* Decision Alternatives Considered\n* Decision Consequences\n* Decision Owners\n* Decision Date
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPUDL has [collection landing\npages](http://pudl.princeton.edu/collections/pudl0058) because it's a digital collections site which\nstaff pushed content into. PUDL required all items be in a single collection,\nand couldn't provide good search-across. Figgy is a staff back-end, so we need some sort of\napplication to provide that functionality.\nCurators also had a history of either requesting or creating ad-hoc websites to showcase\ntheir material or accompany on-site exhibits. We wanted instead to provide a CMS for\nthem to create those experiences based on material they curate which wouldn't\ncreate metadata silos and increased maintenance.\nFurther, curators often had different use cases about how metadata should\ndisplay in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain\nway, but be cataloged according to best practices.\nSpotlight is an exhibit building platform that provides controlled CMS\nfunctionality and is built on Blacklight similar to our new catalog. It allows\nfor local overrides of fields.\n\n## Decision\n",We will use Spotlight to fulfill both the requirements of a Collection Landing\nPage as well as exhibits. Curators of collections will generate the collection\npages. Staff who wish to generate exhibits will be able to manage membership in\nFiggy but have the tools to create those exhibits in Pomegranate.\n,Use Spotlight as the platform for creating exhibit-specific curatorial experiences.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFiggy resources may have any of the following visibilities:\n- Open (public)\n- Princeton (netid)\n- On Campus (ip)\n- Reading Room\n- Private\nFor each of these we need a policy regarding whether it will be indexed in DPUL.\nWe used to index only public / complete items. But to support the music reserves\ncollection we need pages that would have a viewer for logged-in institutional\nusers only.\n\n## Decision\n,Resources with the following visibilities should index into DPUL:\n- Open (public)\n- Princeton (netid)\n- On Campus (ip)\nThis is implemented with a token authentication mechanism in `iiif_resource#def\nurl`\n,"**Decision**: DPUL will index **all** figgy resources, regardless of visibility, except for those that have a specific ""noindex"" metadata tag."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSpotlight uses the term ""Exhibits"" because its primary use case is generating\nmultiple exhibit sites. However, as per\n[ADR #2](./0002-build-on-spotlight.md) we want to use it as landing pages for\ncollections.\nThe term ""Collection"" was used everywhere else, including PUDL, so we decided to\nuse that instead. Originally reported in\n[#89](https://github.com/pulibrary/pomegranate/issues/89).\n\n## Decision\n","* Use ""Collection"" in the UI everywhere ""Exhibit"" is mentioned in Spotlight.\n","Resolve any instances of ""Exhibits"" in the codebase with ""Collections"" and update terminology for landing pages accordingly."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to be able to display a resource in more than one collection, because\nin Figgy a resource can be a member of multiple collections.\nAt the time of this decision, one IIIFResource could only be a member of one Exhibit.\n\n## Decision\n",We create one IIIFResource per Collection of which it is a member.\nWe map each IIIFResource to one SolrDocument.\n,"To allow a resource to appear in multiple exhibits, a resource can be instantiated for each exhibit in which it appears."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nObjects in Pomegranate need to get their metadata from Figgy, where they are\nadministered. Spotlight ships with a IIIF-based indexer. Figgy already produces\nIIIF manifests to support viewing the objects. However the metadata bucket\ndoesn't contain rich enough metadata for pomegranate use cases.\nFiggy (plum, at the time) didn't have an API at the time this decision was made. Manifests were the\nonly way to get data out. Today Figgy has a graphql API.\n\n## Decision\n","We will use the IIIF Manifests to pull data from Figgy into Pomegranate. The\nManifest gives us the manifest url (used for presenting a viewer), the thumbnail\niiif image url, and the jsonld metadata location (via seeAlso).\n",Pomegranate should use IIIF manifests (from Figgy) as the source of object metadata.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nI'm currently working on migrating from redux store to re-frame. It's a good point to define data\nstructure and API to access it.\n# Decision\nApplication DB be a hashmap with the following keys:\n- `nodes` to store hashmap where the key is the node's id and value is a node.\n- `root` to store a root node id\n- `loc` to store focused node id\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\nApplication should define the following event handlers to modify nodes:\n- `[:nodes/make node]` create a node at given loc.\n- `[:nodes/append-child child]` adds a child as a bottom child.\n- `[:nodes/insert-child child]` adds a child as a top child.\n- `[:nodes/insert-up child]` adds a sibling to up of loc.\n- `[:nodes/insert-down child]` adds a sibling to down of loc.\n- `[:nodes/remove]` removes node at loc; lot will be moved to the preceding node.\n- `[:nodes/replace node]` replaces node at loc.\nFollowing event handler to modify loc:\n- `[:loc/root]` moves loc to root.\n- `[:loc/down]` moves loc down.\n- `[:loc/up]` moves loc up.\n- `[:loc/bottom]` moves loc to bottom.\n- `[:loc/top]` moves loc to top.\n- `[:loc/right]` moves loc right.\n- `[:loc/left]` moves loc left.\nFollowing event handler to modify root:\n- `[:root/change root]` change root to a new value.\n\n## Decision\n","Application DB be a hashmap with the following keys:\n- `nodes` to store hashmap where the key is the node's id and value is a node.\n- `root` to store a root node id\n- `loc` to store focused node id\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\nApplication should define the following event handlers to modify nodes:\n- `[:nodes/make node]` create a node at given loc.\n- `[:nodes/append-child child]` adds a child as a bottom child.\n- `[:nodes/insert-child child]` adds a child as a top child.\n- `[:nodes/insert-up child]` adds a sibling to up of loc.\n- `[:nodes/insert-down child]` adds a sibling to down of loc.\n- `[:nodes/remove]` removes node at loc; lot will be moved to the preceding node.\n- `[:nodes/replace node]` replaces node at loc.\nFollowing event handler to modify loc:\n- `[:loc/root]` moves loc to root.\n- `[:loc/down]` moves loc down.\n- `[:loc/up]` moves loc up.\n- `[:loc/bottom]` moves loc to bottom.\n- `[:loc/top]` moves loc to top.\n- `[:loc/right]` moves loc right.\n- `[:loc/left]` moves loc left.\nFollowing event handler to modify root:\n- `[:root/change root]` change root to a new value.\n","The application DB should be a hashmap with the following keys:\n- `nodes` to store hashmap where the key is the node's id and value is a node.\n- `root` to store a root node id\n- `loc` to store focused node id\nNode should be represented as a hashmap with `id`, `type`, `value`, `children`, and `parent` keys.\nApplication should define the following event handlers to modify nodes:\n- `[:nodes/make node]` create a node at given loc.\n- `[:nodes/append-child child]` adds a child as a bottom child.\n- `[:nodes/insert-child child]` adds a child as a top child.\n- `[:nodes/insert-up child]` adds a sibling to up of loc.\n- `[:nodes/insert-down child]` adds a sibling to down of loc.\n- `[:nodes/remove]` removes node at loc; lot will be moved to the preceding node.\n- `[:nodes/replace node]` replaces node at loc.\nFollowing event handler to modify loc:\n- `[:loc/root]` moves loc to root.\n- `[:loc/down]` moves loc down.\n- `[:loc/up]` moves loc up.\n- `[:loc/bottom]` moves loc to bottom.\n- `[:loc/top]` moves loc to top.\n- `[:loc/right]` moves loc right.\n- `[:loc/left]` moves loc left.\nFollowing event handler to modify root:\n- `[:root/change root]` change root to a new value."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI want to be able to mix and match different implementations\nof the ddd sample application, without needing to pull in\na large collection of unnecessary dependencies.\nI want all of the code to be together in one place; which\nis to say, I want to treat the entire project as a mono-repo.\nI can't be bothered to maven install/maven deploy each\nlittle piece to propagate the necessary changes between\nisolated libraries.\n\n## Decision\n",Use a maven reactor project to track the dependencies between\ndifferent libraries within the project\n,Create a monorepo with multiple modules.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nI can't be bothered to be installing the citerus library into my\nrepositories all the time.\nI want to keep my copy of that closely synchronized to the common\nproject, so that any progress that is made can be incorporated\ninto my work.\nI want a convenient way to introduce fixes in the existing implementation\nas I discover the need.\nI want to keep the touch on the existing code _light_; I expect\nto be making some fairly drastic changes in approach, and I don't\nwant to be debating my choices with the maintainers.\nI'm not currently comfortable working with maven projects where\nthe elements are not rooted under a common location.\n\n## Decision\n","Add the upstream as a git submodule, so that it's clear precisely\nwhich version of the upstream is in play at any given time.\nChanges that are of general interest can be in that space.\nMost radical changes (for a _gentle_ definition of radical) should\nbe maintained elsewhere.\n",Use `gitsubmodule` to maintain a copy of the library inside your project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To use an architectural decisions record to document and track the architectural decisions made on this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe currently have a ""public sector domains"" table (along with an ""organisations"" table containing the owners of the domains).\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\nEach of these needs to be treated as a separate ""website"" in the context of accessibility monitoring.\nThus, we either change the ""public sector domains"" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\n\n## Decision\n","We will create and populate a new table in the PubSecDomains schema that contains:\n* url\n* name of the service\n* the site's title from its HTML <head><title> element, where given\n* the site's description from its <head><meta name=""description""> element, where given\n* last_updated timestamp\n* the website's sector (foreign key to sectors table)\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\n",Create a new table for public sector websites and initially copy over those domains that we can confirm are definitely websites.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** Implement an Architectural Decision Record (ADR) process.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to document and communicate architectural decisions.\n* They allow stakeholders to understand the rationale behind decisions and their potential implications.\n* ADRs help to avoid inconsistencies and duplication of effort by centralizing decision-making.\n\n**Implementation:**\n\n* Establish a template for ADRs that includes sections for decision statement, context, rationale, and consequences.\n* Designate a team or individual responsible for reviewing and approving ADRs.\n* Implement a process for creating, reviewing, and storing ADRs.\n* Regularly review and update ADRs as the project evolves.\n\n**Expected Benefits:**\n\n* Improved architectural decision-making and communication.\n* Reduced inconsistencies and duplication of effort.\n* Enhanced stakeholder understanding and buy-in.\n* Facilitated decision traceability and revision history."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe choice of tools that will be built and used by the accessibility monitoring team are influenced by:\n* Suitability to the task\n* Cost and ongoing-cost (open-source preference)\n* The [GDS Way](https://gds-way.cloudapps.digital/).\nAs defined in ADR002, we need to meet a number of requirements:\n1. Maintaining an organised list of public sector websites\n1. Picking websites to test from that list\n1. Triaging each site\n1. Prioritising sites for testing\n1. Tracking the progress of testing on each website\n1. Creating a report for the completed test\n1. Sending the report to the site's owner\n1. Managing and recording interaction with the site's owner\n\n## Decision\n","We will...\n### Use Zendesk\nRationale:\n* GDS have a license for Zendesk.\n* It has an extensive, well-documented API.\n* There is a lot of experience in GDS of general usage, and a fair amount in using the API.\n* We have a sandbox Zendesk environment.\nThis will be the driver of testing work. Tickets will be created in Zendesk (manually or automatically) representing websites to test.\nThey be prioritised in Zendesk and then be assigned to / picked up by Accessibility Officers.\nZendesk will also handle communication and follow-up with the site owner.\nThis satisfies items 5, 7 and 8 above.\n### Use Postgres\n* A relational database is best suited to the requirements for both a public sector domains database and the testing records\n* It is open-source\n* It is well supported and documented\n* It is available as a ""plug-and-play"" service on [GOV.UK PaaS](https://www.cloud.service.gov.uk/) (see below)\nThis satisfies item 1 and facilitates items 2, 5 and 6.\n### Use GOV.UK Platform-as-a-Service\n* Meets GDS' [cloud-first policy](https://www.gov.uk/guidance/government-cloud-first-policy)\n* VERY well supported with an extremely well-experienced and skilled team within GDS.\n* Supports all of the chosen technologies\n### Use Deque Axe\n[Deque Axe](https://github.com/dequelabs/axe-core) is a 3rd-party, open-source tool that tests the web page that you give it against a set of predefined rules.\n* It is open-source\n* It can be run from the command-line and so can probably be automated.\n* It produces results in a machine-readable format (JSON)\nThis satisfies item 3.\n### Use the GOV.UK Design System for the front-end\nThe [GOV.UK Design System](https://design-system.service.gov.uk/) is a set of components, styles and patterns created by the GOV.UK team after extensive user-research.\nNot only does this serve as a ready-made template engine, but we know that it scores extremely highly on accessibility.\n### Programming languages\n* Use node.js for Axe-integration as it is well suited to asynchronous http calls.\n* Use node.js (and nunjucks)for front-end code as that's what GOV.UK Design System is written in.\n* Use Python 3 for Zendesk integration as there is a well-maintained open-source Python library.\n### (Under review) Use PowerMapper SortSite\n[SortSite](https://www.powermapper.com/products/sortsite/) by PowerMapper is a 3rd-party commercial product that is widely used in the testing of websites, and has a thorough accessibility checking feature. It also crawls a website and catalogues its pages, a function that Axe does not perform and would be useful to us.\nHowever, it is not open-source and only runs in a Windows or MacOS environment, so would not be easy to integrate into an automated process.\nThere is a web service whereby a domain is submitted to it and it returns an HTML report, but this is not a machine-friendly format and would require screen-scraping and parsing - quite a major task, and one that's prone to breaking if/when SortSite changes its output format.\nThe site-mapping functionality of SortSite could be achieved with an existing open-source python or node library.\n","**Decision:**\n\nThe team will use the following tools for accessibility monitoring:\n\n* **WebAIM WAVE and aXe** for website testing\n* **Google Sheets** for maintaining an organized list of public sector websites, triaging sites, prioritizing sites for testing, tracking the progress of testing, and managing interactions with website owners\n* **Google Forms** for creating and sending reports to website owners\n* **A dedicated email address** for communication with website owners\n\n**Rationale:**\n\n* **Suitability to the task:** WAVE and aXe are industry-standard tools for website accessibility testing. Google Sheets and Forms provide the necessary functionality for managing and tracking the testing process.\n* **Cost and ongoing-cost (open-source preference):** WAVE and aXe are open-source tools, making them free to use. Google Sheets and Forms are part of the G Suite productivity suite, which is available to government organizations at a reduced cost.\n* **The [GDS Way](https://gds-way.cloudapps.digital/):** The tools chosen align with the GDS Way principles of being user-centered, agile, and collaborative. Google Sheets and Forms are widely used within government, facilitating collaboration and knowledge sharing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe accessibility monitoring team have to test ""a diverse, representative and geographically balanced distribution"" of public sector websites.\nIn order that no public sector website is excluded from potential scrutiny, it follows that the team will need to have a full list of websites that are in the public sector, together with the type of service (e.g. education, health, central/local government etc) and, where applicable, geographical location.\nThere are two approaches to creating such a list:\n1. **Domain-led:** Find every domain that has been registered to an organisation that is deemed to be ""public sector"" and determine the organisation that owns it\n2. **Organisation-led:** Find every organisation that is deemed to be ""public sector"" and every service that each such organisation runs online, and find the corresponding website.\n### 1. Domain-led strategy\nThe limitation of (1) is that (with certain exceptions, e.g. .ac.uk and .gov.uk) there is no regulation over what top-level domain should be used by the many flavours of public sector organisations. They could be anywhere.\nAt the time of writing (July 2020) there are approximately 1.7 billion websites in the world, hosted on 409 million domain names. The websites we're looking for could be hosted on \*.uk domains or generic .com/.net/.org (or even .me, .info, .tv or any potentially any other ""generic TLD"").\nOf the 149m country-specific domains registered, 7% (~10.5M domains) are \*.uk, so we can at least exclude the other 93% (139m) non-UK country-specific domains (that is, we are very unlikely to find a UK public sector website hosted on a .tk or .ru domain, for instance\*).\nThat leaves a mere 270m domains that Public Sector websites could potentially be hosted on. Scouring those domains for websites that fall into the definition of ""uk public sector"" would be a lengthy and resource-intensive task.\nNevertheless, a list of the domains that we can be sure are public sector, and that we can retrieve data about, would give the accessibility monitoring team a good range to select from.\nThe domains that fall into this category are:\n* gov.uk\n* nhs.uk\n* nhs.net\n* ac.uk\n* sch.uk\n* police.uk\n* parliament.uk\n* mod.uk\n* gov.wales\n* llyw.cymru\n* gov.scot\n### 2. Organisation-led strategy\nIt should be feasible to compile a list of organisation _types_ that are in the public sector (e.g. ""schools"", ""central government"", ""local authorities"", Universities"", ""NHS"").\nWe can then move onto making lists of the actual _organisations_ in each of those categories by referring to the bodies that regulate them.\nA lot of this information is online in one form or other, but certainly not all of it.\nSome initial work has been done by the accessibility monitoring team already, with the result being a ""list of lists"" that is currently in a Google Sheets spreadsheet. Each of these lists - in various formats - would need to be somehow imported into the database. Where a website for the organisation in the list is specified, this can be added to the domains list.\n\n## Decision\n","Our intention is to use both strategies.\n* We will create a **database of public sector domains**.\n* We will populate it with lists of domains and organisations from official sources, together with, as far as possible, contact details and other useful data such as page ranking, http(s) status.\n* We will also use the data gleaned from domain registers etc to seed a list of **public sector organisations**.\n* We will compile a list of public sector categories which will form a list-of-lists; these lists will then be used to further populate the database of organisations.\n* Organisations will be categorised by location (including ""national"") and by sector (probably using [the definitions of ""organisations"" categorised for the Local Governments Association by ESD](https://standards.esd.org.uk/?uri=list%2ForganisationTypes))\n* Wherever possible, links will be created between the domains and organisations. At some point in the future, organisations that don't have associated domains will need to have their associated websites found and entered into the database so that automated testing can be carried out.\n","The organization-led strategy is the better approach for creating a list of public sector websites. This strategy is more feasible because it involves compiling a list of organization types that are in the public sector and then making lists of the actual organizations in each category by referring to the bodies that regulate them. This information is more likely to be available online and in a more consistent format than the domain-led strategy, which would involve scouring a large number of domains for websites that fall into the definition of ""uk public sector."""
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe accessibility monitoring team require a means of:\n* Picking websites to test (from a random list weighted by a variety of criteria, from complaints passed to us, from previously-tested sites (to test changes) and sites specifically chosen by guiding bodies)\n* Collecting and maintaining a list of public sector websites from which to pick\n* Triaging each site (check it is live, check for accessibility statement(s), measure its size in pages etc)\n* Prioritising sites for testing\n* Tracking the progress of testing on each website\n* Creating a report for the completed test\n* Sending the report to the site's owner\n* Managing and recording interaction with the site's owner\nWhilst there are a number of accessibility testing solutions available, they are all geared towards testing of one's own site, and require (or at least, assume) access to the source code and deployment system. Testing of multiple sites by a 3rd party from outside the sites' domains is not a common requirement for vendors!\nSimilarly, whilst the reports generated by some of the testing tools are very good, they are not up to GDS standards for content and, ironically, accessibility. Furthermore, GDS have no control over the content of the generated reports.\nWe therefore need a combination of database administration, semi-automatic accessibility testing and a Customer Relationship / Support Ticket Management system, all able to move data between them in a usable fashion.\n\n## Decision\n","We will use 3rd-party tools where they provide part of the functionality that is required and can be made to communicate easily with each other.\nWe will write whatever code is necessary to form the ""glue"" that binds them together and to automate whatever can be (and is worth) automating.\n","**Develop a custom application that integrates the following features:**\n\n**Database:**\n\n* Manage and maintain a list of public sector websites\n* Store accessibility testing data and progress\n\n**Semi-Automatic Accessibility Testing:**\n\n* Randomly select websites from the list based on weighted criteria\n* Check websites for accessibility statement(s)\n* Measure website size in pages\n\n**Customer Relationship Management (CRM) System:**\n\n* Track interactions with site owners\n* Send accessibility testing reports\n* Prioritize sites for testing\n\n**Integration:**\n\n* Allow data to flow seamlessly between the database, testing system, and CRM\n* Provide a user-friendly interface for managing website selection, testing, and reporting\n\n**Additional Features:**\n\n* Reports that meet GDS content and accessibility standards\n* Ability to track changes to websites over time\n* Automated reminder system for site owners to address accessibility issues"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrent version of clang-format is aging an becoming unavailable.\n\n## Decision\n,We will move to clang-format 5 and not the bleeding edge version. Visual Studio 2017 will ship with clang-format 5 so it makes sense to standardize\non this version across the project.\n,Migrate to the latest version of clang-format to ensure availability and security updates
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVisual Studio 2017 has been released for roughly a year and includes improvements to C++ standards conformance. It is C++14\nfeature complete and has many C++17 features.\n\n## Decision\n,As part of the maintenance cycle after release 3.13 we will move the Windows compilers forward to\nVisual Studio 2017 in a step towards requiring C++14 as a minimum standard.\n,Upgrade to Visual Studio 2017 to utilize the improved C++ standards conformance and features.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRed Hat 7's default compiler is gcc 4.8. This is not C++14 compliant and prevents access to more modern C++ features.\n\n## Decision\n,As part of the maintenance cycle after release 3.13 we will move Red Hat 7 compilers over to use\ngcc 7 as part of the [devtoolset-7](https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/) tools provided by Red Hat.\n,"**Decision:** Migrate the default compiler to GCC 11 or a newer version that supports C++14.\n\n**Rationale:**\n\n* GCC 11 or newer versions provide full support for C++14 and offer numerous performance and security enhancements over legacy compilers like GCC 4.8.\n* Upgrading the compiler will unlock access to modern C++ features, improving the productivity and efficiency of development efforts.\n* Migrating to a newer compiler will align with industry best practices and enable the efficient adoption of emerging C++ standards.\n\n**Consequences:**\n\n* **Positive:**\n    * Access to modern C++ features and improved code quality\n    * Enhanced performance and security\n    * Reduced development time and improved code maintainability\n* **Negative:**\n    * Potential temporary disruption during the migration process\n    * May require code modifications to support the new compiler\n* **Mitigations:**\n    * Plan the migration carefully and conduct thorough testing to minimize disruptions\n    * Provide support and documentation to assist developers in adapting to the new compiler"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the future architectural decisions made.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Maintain an Architectural Decision Record (ADR)**\n\nEstablish a central repository to document all architectural decisions made during the development process. This record will include:\n\n* The decision itself\n* The context and rationale behind the decision\n* The alternatives considered and why they were rejected\n* The implications and consequences of the decision\n* The impact on other architectural components\n* The date and author of the decision\n* Any relevant supporting documentation
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a ""Hello World"" state with little to no modifications.\n# Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\n# Consequences\nThe lifcorp-init will be able to block users from skipping required parameters.\n# Status\nAccepted\n\n## Decision\n","Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\n# Consequences\nThe lifcorp-init will be able to block users from skipping required parameters.\n# Status\nAccepted\n","Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To use an Architectural Decision Record (ADR) format to document architectural decisions made on this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhat colors shall be used for buttons, statistics and inactive elements considering usability. This concerns first and\nforemost the tool applications.\n\n## Decision\n","Buttons or any usable elements should use the colors #ff9900, #9999ff, #cc6666 and #cc6699 as background and black text.\nDisplayed text should preferably be colored #ff9900 on black background. Other text colors may be used after consideration.\nGraphs, diagrams and other status and statistical displays should use the colors #ffcc99, #9999cc, #9999ff and for warning purposes #ff9966, #ff9900 and #cc6666.\nDesign elements should use the colors #cc99cc, #9999cc and #ff9966. These colors are less ""strong"" and don't take as much attention.\n[1]: http://mrbsdomain.com/gallery/var/albums/repository/lcars_colors.gif\n","**Decision:**\n\nFor buttons, **primary actions** should use the **primary color**, while **secondary actions** should use a **secondary color**.\n\nFor statistics, **positive or significant values** should use a **green color**, while **negative or insignificant values** should use a **red color**.\n\nFor inactive elements, a **grayed-out or disabled color** should be used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\n\n## Decision\n","Chosen option: ""Xlib"", because it was used already for the additional tooling and there's lots of documentations and examples.\n### Positive Consequences <!-- optional -->\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\n### Negative Consequences <!-- optional -->\n* Refactoring code\n* Wayland might be more modern\n* XCB is faster at times due to asynchronous\n",Use XCB.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhat language shall be used for the development of the window manager?\n## Decision Drivers <!-- optional -->\n* I want to learn Kotlin\n* I want to learn Python\n* I don't want to take care of the need to update an API layer between the window manager code and the\n* Libraries for display management and tooling in that area is usually written in C\n\n## Decision\n,"* I want to learn Kotlin\n* I want to learn Python\n* I don't want to take care of the need to update an API layer between the window manager code and the\n* Libraries for display management and tooling in that area is usually written in C\nChosen option: ""Kotlin/Native"", because it compiles to fast native code and C-APIs can be used directly.\n### Positive Consequences <!-- optional -->\n* Can use C-APIs directly (converted automatically by tooling)\n* Compiles to fast native application\n* No extra API layer\n### Negative Consequences <!-- optional -->\n* Needs separate compiling on every machine type\n","Use Kotlin.\n\nKotlin is a modern language that is easy to learn and use. It is also interoperable with C, which is the language that is typically used for display management and tooling in that area. This means that you will not need to take care of the need to update an API layer between the window manager code and the libraries for display management and tooling in that area."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe application menu needs to know the currently active windows, shall display the name and enable the user to select or close them. Therefor it is necessary to exchange the corresponding information with the window manager.\n## Decision Drivers <!-- optional -->\n* Fast, the application menu entry information should be updated ""instantly"" when windows open, close or get renamed\n* No message should be lost\n\n## Decision\n","* Fast, the application menu entry information should be updated ""instantly"" when windows open, close or get renamed\n* No message should be lost\nChosen option: ""Message queues"", because this approach is a faster than shared storage approaches and doesn't need to handle locking. Message queues are asynchronous (the producer doesn't need to wait for the consumer) and messages are queued and not lost.\n### Positive Consequences <!-- optional -->\n* Easy to use and flexible\n* Library support in Python\n### Negative Consequences <!-- optional -->\n* Manual implementation in Kotlin necessary (but straight forward)\n","Implement two-way communication on IPC channels, one for each direction"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\n\n## Decision\n,"Chosen option: ""Client Properties / Atoms"", because it is the most reliable and side-effect free way to identify the tool windows.\n",The window manager should use a special property or flag to mark these tool windows. This property or flag can be set by the application when creating the window. The window manager can then use this property or flag to identify the tool windows and treat them accordingly.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\n# Decision\n[Swiftlint configuration](./../../.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\n\n## Decision\n",[Swiftlint configuration](./../../.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\n,Use Swiftlint configuration to enforce adherence to code style conventions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nFor legibility and discoverability, it is helpful to have a clear ordering of members within each type. Criteria which factor into this include:\n1. Member kind (property, method, subtype)\n2. Access (public/internal/private)\n3. Nature of member (stored or computed property, override or unique method)\nThere are different approaches to how these should be prioritized in C++/Objective-C, whether you're focussing on the needs of the type's consumer or implementer and which slices of behavior you most want to separate.\n# Decision\nWhere possible, members should be organized as follows:\n```\nclass MyClass: BaseClass {\n// MARK: - Constants\npublic static let valueA = 1\nprivate static let valueB = 2\n// MARK: - Types\npublic struct SubTypeA {}\nprivate struct SubTypeB {}\n// MARK: - Stored Properties\npublic var propertyA = 1\nprivate var propertyB = 2\n// MARK: - Computed Properties\npublic var propertyC: Int { return propertyA * 3 }\nprivate var propertyD: Int { return propertyB * 4 }\n// MARK: - Constructors\npublic init() {}\nprivate init(param: Int) {}\n// MARK: - Methods\npublic static func k() {}\npublic func f() {}\nprivate func g() {}\nprivate static func h() {}\n// MARK: - BaseClass overrides\npublic override var propertyL: Int { return propertyA * 3 }\npublic override func base() {}\n}\nextension MyClass: SomeComformance {\npublic var i: Int { return 0 }\npublic func j() {}\n}\n```\nImportant points to note:\n1. public before private\n2. static lifetimes before properties before methods\n3. stored properties before computed properties\n4. constructors before other methods\n5. overrides grouped based on the class they override\n6. protocol conformances in separate extensions (unless auto-synthesis is involved)\nIn most cases, these sections will not all be present... don't use a heading for a section not included\n# Consequences\nThere are a couple points that aren't totally decided.\nThey do not *need* to have ""mark"" headings and when they do, provided the contents themselves are organized, a simple ""Properties"" or ""Methods"" is sufficient to cover all methods or properties (e.g. doesn't need to be broken into ""Stored"" and ""Computed"").\nHowever, overrides sections should have a heading indicating which class' methods they override, otherwise its purpose is difficult to understand.\nStatic methods are all in one section with the other methods, with public static first and private static last (after all non-static methods). However:\n1. Most public static functions are constructors and should go in the constructor section (probably ahead of init functions)\n2. Many private static functions are called from just one location, lifted out for purely syntactic reasons. Sometimes these might appear alongside the function they're lifted out-of, sometimes they might appear at the end of the file since they're mostly an implementation detail that can be ignored.\nThere's a little flexibility here and when reviewing PR's suggestions and requests for improvement may be made prior to approving a PR.\n\n## Decision\n","Where possible, members should be organized as follows:\n```\nclass MyClass: BaseClass {\n// MARK: - Constants\npublic static let valueA = 1\nprivate static let valueB = 2\n// MARK: - Types\npublic struct SubTypeA {}\nprivate struct SubTypeB {}\n// MARK: - Stored Properties\npublic var propertyA = 1\nprivate var propertyB = 2\n// MARK: - Computed Properties\npublic var propertyC: Int { return propertyA * 3 }\nprivate var propertyD: Int { return propertyB * 4 }\n// MARK: - Constructors\npublic init() {}\nprivate init(param: Int) {}\n// MARK: - Methods\npublic static func k() {}\npublic func f() {}\nprivate func g() {}\nprivate static func h() {}\n// MARK: - BaseClass overrides\npublic override var propertyL: Int { return propertyA * 3 }\npublic override func base() {}\n}\nextension MyClass: SomeComformance {\npublic var i: Int { return 0 }\npublic func j() {}\n}\n```\nImportant points to note:\n1. public before private\n2. static lifetimes before properties before methods\n3. stored properties before computed properties\n4. constructors before other methods\n5. overrides grouped based on the class they override\n6. protocol conformances in separate extensions (unless auto-synthesis is involved)\nIn most cases, these sections will not all be present... don't use a heading for a section not included\n# Consequences\nThere are a couple points that aren't totally decided.\nThey do not *need* to have ""mark"" headings and when they do, provided the contents themselves are organized, a simple ""Properties"" or ""Methods"" is sufficient to cover all methods or properties (e.g. doesn't need to be broken into ""Stored"" and ""Computed"").\nHowever, overrides sections should have a heading indicating which class' methods they override, otherwise its purpose is difficult to understand.\nStatic methods are all in one section with the other methods, with public static first and private static last (after all non-static methods). However:\n1. Most public static functions are constructors and should go in the constructor section (probably ahead of init functions)\n2. Many private static functions are called from just one location, lifted out for purely syntactic reasons. Sometimes these might appear alongside the function they're lifted out-of, sometimes they might appear at the end of the file since they're mostly an implementation detail that can be ignored.\nThere's a little flexibility here and when reviewing PR's suggestions and requests for improvement may be made prior to approving a PR.\n","Members should be ordered as follows:\n\n1. Constants\n2. Types\n3. Stored properties\n4. Computed properties\n5. Constructors\n6. Methods\n7. Base class overrides\n8. Protocol conformances in separate extensions\n\nImportant points to note:\n\n- Public members should come before private members.\n- Static lifetimes should come before properties and methods.\n- Stored properties should come before computed properties.\n- Constructors should come before other methods.\n- Overrides should be grouped based on the class they override.\n- Protocol conformances should be in separate extensions.\n\nIn most cases, not all of these sections will be present. Do not use a heading for a section that is not included.\n\nOverrides sections should have a heading indicating which class' methods they override. Otherwise, their purpose is difficult to understand.\n\nStatic methods are all in one section with the other methods, with public static first and private static last (after all non-static methods). However:\n\n- Most public static functions are constructors and should go in the constructor section (probably ahead of init functions).\n- Many private static functions are called from just one location, lifted out for purely syntactic reasons. Sometimes these might appear alongside the function they're lifted out of, sometimes they might appear at the end of the file since they're mostly an implementation detail that can be ignored."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\niOS applications can be written in Objective-C or Swift. Objective-C offers greater interaction with C++ code but is considered a legacy language choice in the iOS developer community. The `pact-consumer-swift` framework was built to support Objective-C as well, but it's proven to become a bigger challenge supporting both with newer Xcode and Swift versions.\n# Decision\nThe framework is written in Swift.\n# Consequences\n\n## Decision\n",The framework is written in Swift.\n# Consequences\n,The framework will be written in Swift.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nAlmost all software we write depends on some other code, library or development tool which allows us to build what we want faster. Although this project attempts to avoid bringing in 3rd party dependencies, there are is functionality already written that is critical to this projects success.\n# Decision\nThe main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation/pact-reference/rust](https://github.com/pact-foundation/pact-reference/tree/main/rust/pact_mock_server_ffi).\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and/or interact with different services (interacting with Pact Mock Server, etc.).\nMatt's [CwlPreconditionTesting](https://github.com/mattgallagher/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `./Carthage/Checkouts/CwlPreconditionTesting/*` added into the project itself). For SPM it is defined as a dependency in `./PactSwiftServices/Package.swift`.\n# Consequences\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\n# Follow-up (September 30, 2020)\nAll 3rd party dependencies have been successfully removed from this project/framework.\n\n## Decision\n","The main dependency is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation/pact-reference/rust](https://github.com/pact-foundation/pact-reference/tree/main/rust/pact_mock_server_ffi).\nThe binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.\nUnfortunately SPM doesn't handle the binary dependencies well at the time of this writing. Therefore a SPM package is required\nThere will be a separation of responsibilities between PactSwift framework and PactSwiftServices in a separate (yet embedded) project which will provide extra functionality by reaching out to and/or interact with different services (interacting with Pact Mock Server, etc.).\nMatt's [CwlPreconditionTesting](https://github.com/mattgallagher/CwlPreconditionTesting) is a dependency this project can't really exist without. To support distributon of PactSwift using both Carthage and SPM, the dependency CwlPreconditionTesting is brougt into the PactSwiftServices project (files `./Carthage/Checkouts/CwlPreconditionTesting/*` added into the project itself). For SPM it is defined as a dependency in `./PactSwiftServices/Package.swift`.\n# Consequences\nDue to SPM not handling binary dependencies well. When linking and embedding a binary framework while building and running in Xcode everything works fine, `xcodebuild` command in command line builds the project and dependencies just fine.\nYet, when running `swift build` in terminal, SPM doesn't know where to find it. That's why a separate SPM package to provide the binary framework as a dependency is required and unfortunately the binary framework is duplicated in the codebase - once in `PactSwiftServices` project and once in `PactMockServer` swift package.\n# Follow-up (September 30, 2020)\nAll 3rd party dependencies have been successfully removed from this project/framework.\n",The main dependency of this project is the programmable in-process mock server that can receive network requests and respond with the response we define. This dependency is written in rust and is available at [pact-foundation/pact-reference/rust](https://github.com/pact-foundation/pact-reference/tree/main/rust/pact_mock_server_ffi). The binary framework(s) that are built using `cargo lipo --release` command are added into the Xcode project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`PactSwift` takes advantage of Mock Server FFI binaries built from shared Rust code. These are generally large binary files when it comes to iOS and macOS platforms and we are limited with hosting them in the GitHub repo. The FFI also follows it's own source and changes are available independently to changes to `PactSwift`'s functionality. Separating the responsibilities would be welcomed.\nFurthermore, the pain of managing multiple binaries with the same name but each with its specific architecture slice could be reduced by generating an `XCFramework` using an automated script and kept from the framework user. These can blow up to more than 100Mb each (the fat binary with all slices for iOS platform blew up to more than 300MB). Using `XCFramework` we can shed off a lot of the statically linked code. Mock Server FFI (`MockServer.swift`) is the only part of `PactSwift` package that depends on binaries being built for specific architectures and run platforms. With removal of binaries from the main `PactSwift` project, we should be able to avoid managing them, mixing them up (as they are all named the same), discarding them at `git add` and `commit` steps and rebuilding them at next `PactSwift` build/test cycle.\n\n## Decision\n",- Mock Server FFI interface and implementation to be split into it's own Swift Package called `PactSwiftMockServer` and distributed as a binary (`XCFramework`) when on Apple platforms and as a source package when used on Linux platforms.\n- Utilities used by both the main `PactSwift` and `PactSwiftMockServer` packages are split into one package called `PactSwiftToolbox`.\n- Where it makes sense the dependencies' versions should be exact. If exact version is not set for a valid reason then `.upToMinor()` must be used to avoid breaking changes when releasing packages in isolation.\n- Scripts to automate the release processes will be provided within the projects' scripts folders.\n,**Separate the Mock Server FFI into a separate repository and build XCFrameworks for each architecture.**\n\nThis will address the following concerns:\n\n* Reduce the size of the PactSwift repository by moving the large Mock Server FFI binaries to a separate repository.\n* Improve the management of the FFI binaries by using an automated script to generate XCFrameworks for each architecture.\n* Eliminate the need for framework users to manage multiple binaries with the same name.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIstanbul version <1.x.x has been deprecated and is no longer receiving updates.\nnyc is one of the suggested replacements. Additional information is available\non [npm](https://www.npmjs.com/package/istanbul). This leaves the package open\nto security flaws that will not be patched. Features available in the latest\nversions of node will not be supported.\n\n## Decision\n,The decision is to migrate from istanbul to nyc.\n,Migrate to nyc or another supported package.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/).\nGitHub Actions will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repo, which has a basic use case will provide some\nexposure to the service.\n\n## Decision\n",The decision is to replace Travis CI with GitHub Actions.\n,Use GitHub Actions as the primary CI mechanism for this project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLearning Machine handles a Blockcerts verifier in multiple repository and with different ways of deploying. This is costly and hard to maintain.\nDecision has been made to unify the verifier into one sole component and repository, with scalibility and maintainability at heart.\n\n## Decision\n","#### JS\nWe decided to use Polymer 3.0 as previous versions of the verifier were already written with Polymer. Also because Web Components seem like a promising technology that could open a interesting future for the usage of the component.\nState of the application will be handled by Redux, as demonstrated in the [example project of Polymer](https://github.com/Polymer/pwa-starter-kit).\n#### CSS\nWe decided to use ITCSS for its interesting way to handle CSS scalability and maintainability.\nMore information about this methodology can be found here:\nhttps://github.com/sky-uk/css\nhttps://www.xfive.co/blog/itcss-scalable-maintainable-css-architecture/\n#### Testing\nWe are using the tools provided by the Polymer Project, hence [WCT](https://github.com/Polymer/tools/tree/master/packages/web-component-tester). We also test for accessibility.\n#### Accessibility\nThe Web Component needs to be WCAG2.0 AA compliant.\n","Migrate the multiple Blockcerts verifiers into a single component and repository, using a scalable and maintainable architecture. The new component should provide a consistent interface for verifying Blockcerts, and should be easy to deploy and update."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[pwa-starter-kit](https://github.com/Polymer/pwa-starter-kit) is an example of a Redux application working with Polymer 3. While it serves its purpose, the separation of concerns is not entirely clear between the state and the views, which means that in the example views have too much knowledge of what provides and modfies the state.\nThis is potentially dangerous for scalability, and bloats the view code with decisions that shouldn't be of its concerns.\nReact provides an architectural pattern to handle this abstraction, in the shape of `Containers`. Containers are wrapper around the view component, which connect (via the `react-redux` helper function of the same name) the view and the state (to read it, and to action it).\nThis pattern is interesting because from an architectural standpoint every responsibility is properly distributed. The view receives props, may they be data or callback functions, and utilizes them, the container selects where to retrieve the data or callback functions, and the state solely stores the data and provide mechanisms to modify it.\n\n## Decision\n","We decided to keep this approach for a Polymer project too. Because the out-of-the-box tools do not provide that abstraction, we implemented our own architectural approach to fit this need.\nA visual representation of the intent is as follows:\n![Chain of connection: State > Connector > Container > Component](https://user-images.githubusercontent.com/12797962/41294972-a254d432-6e59-11e8-8e08-214c43772173.png)\n- The state handles the live data of the application.\n- The connector has an instantiation of the store, but is also a metaview, which enables communication with the methods of the store, without exposing it elsewhere.\n- The container is an instance of the connector, which allows defining which part of the state and which actions we would like to bind to its wrapped component.\n- And the wrapped component is just a regular component which expects some props, but does not know nor care where they come from.\n","We should implement a `Container` pattern for Redux applications using Polymer 3. This pattern will help to separate the concerns between the state and the views, making the applications more scalable and easier to maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRedux provides a good mechanism to handle the state and its mutations, but not so much to handle the business logic.\nHowever, like always, to keep a good segregation of concerns, we want to handle all business related logic, as well as all external communication of the application isolated.\n\n## Decision\n","To do so, we introduce the concept of domain.\nThe domain is the part responsible for everything that's not the view concern, nor the state concern.\nThe view concern is to render data.\nThe state concern is to store and modify data.\nThe rest belongs to the domain.\nA visual representation is as follows:\n![Domain connection: outer world > domain > state > view](https://user-images.githubusercontent.com/12797962/41419432-4c70fd4e-6ff2-11e8-8acd-a18367087193.png)\nEach method of the next inner layer only knows about the layer above it.\nThis domain is only inspired from Domain Driven Design and aims at being a simpler implementation of the approach.\n","**Decision:** Create a separate layer for business logic.\n\n**Rationale:**\n\n* Redux is a good mechanism for managing state and mutations, but it is not well-suited for handling business logic.\n* Separating business logic from the state management layer will help to improve the application's architecture and maintainability.\n* An isolated business logic layer will be easier to test and debug.\n* This approach will allow for better reuse of business logic across different parts of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSometimes labels need to handle pluralization. While it could be just as easy as adding a `s` at the end of the word in English (it is not), French or Spanish, other languages have a variation of their plural form that require a better handling.\n\n## Decision\n",We chose to follow the second approach which adds less overhead to the bundle.\nIt comes at the cost of having the contributor to add the plural version of the string they want translated. But this explicit approach also reduces the risk of error and inconsistency.\n,Use the framework provided by the ICU4J library to generate the correct plural form (as a string) for the specified number of resource entries.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\n\n## Decision\n","Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\nShared styles will be hosted under `src/shared-styles` folder and organized following the ITCSS nomenclature.\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\n","We use a combination of CSS custom properties, CSS variables, and CSS mixins to achieve our goal of maintaining component-level styles while enabling shared styles.\n\n**CSS custom properties** allow us to define custom properties that can be used by any CSS rule. This allows us to create a central location for shared styles, and then use those shared styles in our component-level styles.\n\n**CSS variables** are similar to CSS custom properties, but they are scoped to a specific element or component. This allows us to create shared styles that are only used within a specific component or group of components.\n\n**CSS mixins** are a way to group CSS rules together and then reuse them in multiple places. This allows us to create reusable code snippets that can be used to create consistent styles across our components.\n\nBy combining these three techniques, we can create a scalable and maintainable style system that meets our needs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\nAnimating proves convenient on `max-height`\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\n\n## Decision\n","We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\nWe thus decided to trick the component:\n- we only calculate the `totalHeight` once, since content will not change once rendered.\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https://github.com/Polymer/lit-html/blob/master/src/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\n","**Decision:** Use `max-height: 100vh` for the substeps list to ensure that it can accommodate indeterminate heights while still allowing for animation.\n\n**Rationale:**\n\n* Using `max-height: 100vh` ensures that the substeps list can accommodate indeterminate heights, such as those caused by multi-line error messages.\n* Setting the maximum height to the viewport height allows the list to expand to the full height of the viewport, providing ample space for all substeps and any associated error messages.\n* This approach maintains the functionality of the progress bar, which is dependent on the height of the substeps list. By setting the maximum height to be consistent with the progress bar's height, we ensure that the progress bar continues to work as expected.\n* This solution provides flexibility and adaptability, allowing for the display of substeps with varying heights while still enabling animation and maintaining the functionality of the progress bar."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\ndon't choose and use both, but i want to make it consistent.\n## Decision Drivers\n* New empty project\n* Kotlin now fully supported for Android by Google\n\n## Decision\n","* New empty project\n* Kotlin now fully supported for Android by Google\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\nGoogle now, which eliminates the risk of being dropped any time soon.\nPositive Consequences:\n* Less code, more fun!\n* Having to learn a new language\n* Great interoperability with Java if needed\nNegative consequences:\n* Having to learn a new language\n","Use Kotlin as the primary language for new Android projects.\n\n**Rationale:**\n\n* Kotlin is fully supported by Google for Android development.\n* Kotlin offers several advantages over Java, including:\n    * Concise and expressive syntax\n    * Improved type safety\n    * Null safety\n* Adopting Kotlin as the primary language will ensure consistency across new projects."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nI need a way to document design decisions.\n\n## Decision\n,"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\nPositive Consequences:\n* Choices and reasoning are persisted and versioned\nNegative consequences:\n* Having to document\n",Use Architectural Decision Records (ADRs) to document the rationale behind architectural decisions and to ensure that these decisions remain aligned with business goals and objectives.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThis app shows how i would build an Android app. Therefore the app architecture is the goal itself,\nthe features this app has, are just a way to show an app architecture.\n## Decision Drivers\n* Availability\n* Realism\n* Content creation\n\n## Decision\n","* Availability\n* Realism\n* Content creation\nChosen option: ""The Movie Db"", because it is more realistic when the app consumes a real service.\nPositive Consequences:\n* It is realistic\nNegative consequences:\n* When it's down the app cannot consume it\n",**Decision:** Choose an Android app architecture that meets the following requirements:\n\n* **Availability:** The architecture should use well-established libraries and frameworks to ensure wide industry support.\n* **Realism:** The architecture should reflect best practices and design patterns commonly used in real-world Android app development.\n* **Content creation:** The architecture should facilitate the creation of high-quality content that showcases the app's features and functionality.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide how to deploy our Vue app in Heroku. We can achieve this either\nby using a Node.js web server, or by deploying the app as a static site with\nthe [Heroku static buildpack](https://github.com/heroku/heroku-buildpack-static).\n\n## Decision\n","We will use the static buildpack to deploy, as our app will essentially be a\nstatic frontend backed by TIMDEX. This is consistent with [Vue's deployment\nguidelines](https://cli.vuejs.org/guide/deployment.html#general-guidelines)\non deployments.\n","The decision is to deploy the Vue app as a static site using the Heroku static buildpack.\n\n**Rationale:**\n\n* The Heroku static buildpack is specifically designed for deploying static sites, which makes it a more suitable option for a Vue app that primarily serves static content.\n* Using the Heroku static buildpack eliminates the need for a Node.js web server, which simplifies the deployment process and reduces the runtime overhead.\n* The Heroku static buildpack supports various caching mechanisms, which can improve the app's performance and reduce load times.\n* As a static site, the Vue app will be served directly from Heroku's CDN, providing better scalability and reliability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNPM is causing confusion as to why lock files are changing in local\nenvironments when no changes have been made. We have found explanations and\nworkarounds, but it feels like the type of unexpected default behavior that will\nlead to frustration as new developers join the project.\nYarn is an alternative package manager that seems to have a more expected set\nof default behaviors while maintaining compatibility in case we need to revert.\n\n## Decision\n",We will use Yarn instead of NPM for this project.\n,"**Decision:** Migrate to Yarn as the package manager for this project.\n\n**Rationale:**\n\n* NPM's unexpected default behavior of modifying local lock files without clear reasons can lead to confusion and frustration among developers.\n* Yarn exhibits more expected default behaviors, reducing the risk of unexpected changes and simplifying package management.\n* Maintaining compatibility with NPM allows for a smooth transition and the option to revert if necessary."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe reviewed various options for building our front end and decided Vue.js was\nthe best fit. However, Vue is preparing for a major version upgrade. We could\nchoose to use the current stable version (2) and migrate to the new version (3)\nwhen it is released or start with the release candidates for the new version.\n\n## Decision\n",We will use Vue.js 3 releases candidates.\n,"**Decision:** Use the release candidates for Vue.js version 3.\n\n**Rationale:**\n\nWhile Vue.js version 2 is stable and battle-tested, version 3 offers significant performance and architectural improvements. By using the release candidates, we can:\n\n* Take advantage of the new features and optimizations in version 3 without the risk of major breaking changes.\n* Gain early experience with the new version and identify any potential migration issues.\n* Position ourselves to seamlessly upgrade to the stable version of Vue.js 3 when it is released.\n\n**Consequences:**\n\n* There is a slight risk that release candidates may contain bugs or unexpected behavior.\n* The team will need to be prepared to invest time in migrating to the stable version once it is available."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement Architectural Decision Records (ADRs) as a structured way to document and track architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe GOV.UK PaaS billing system receives a series of events from Cloud Foundry notifying, for each tenant, whether services or resources have been created, renamed, or deleted.\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\nIf there is a problem with a tenant's bill it was very difficult to find the source of the problem.\n\n## Decision\n","We need to have a persistent record of the services or resources each tenant has provisioned with a dates indicating when the service or resource was/is being used. This persistent record is in the new `resources` table.\nThe reason for this is that there is no need to regenerate historical records of services or resources provisioned for tenants each time billing is run each month since this information does not change. Furthermore, recording this information for each month makes it difficult for us to calculate bills between any two dates and times.\nThe `resources` table also acts as an audit point within GOV.UK PaaS billing. It makes investigation of discrepancies in tenant bills easier to investigate. Anyone supporting GOV.UK PaaS billing can first look at the contents of `resources` and see whether the discrepancy arose in the population of `resources` or afterwards in the actual calculation of the bill.\n","The new GOV.UK PaaS billing system will store a permanent record of the results of each stage of billing consolidation, including what services or resources tenants have provisioned and on which specific day."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe needed to decide where to terminate TLS connections for public and tenant\nfacing endpoints and how to manage the corresponding private keys.\nWe had previously decided to only support HTTPS to both deployed applications\nand Cloud Foundry endpoints.\nAt the time of writing there were 4 endpoints to consider:\n- Deployed applications (gorouter). Accessed by the public.\n- CF API. Accessed by tenants.\n- UAA. Accessed by tenants.\n- Loggregator. Accessed by tenants.\n- SSH proxy. In theory accessed by tenants, but not working in our environment.\nWe had an existing credentials store suitable for storing the private keys at\nrest. Only a small number of engineers within the team can access; the same\nones that can make IAM changes using our account-wide terraform config.\nPlacing ELBs in front of public-facing services is an architectural pattern\nadvised by Amazon [in order to reduce attack\nsurface](https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf).\nSpecifically they advise that it helps withstand volumetric Denial of Service\nattacks; the ELB handles TCP connections and therefore the responsibility for\nhandling DDOS at Layer 4 and below resides with the ELB team.\nWe did a spike, where we attempted to place everything public-facing or\ntenant-facing behind ELBs. We found that:\n- In HTTP mode the ELBs do not support web sockets. This is known to break\nloggregator, which relies on them for log streaming. It would also prevent\ntenants from using web sockets within their applications.\n- When the ELB is in TCP mode, we have no way of communicating the client IP\naddress to the downstream service. Practical consequences of this would be\nthat tenants would be unable to see in their logs who is using their service or\ndo any access control based on client IP address.\nIn attempting to solve the second problem, we explored some options:\n- ELB has support for the [Proxy\nProtocol](http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt), but\nunfortunately none of the downstream services, such as gorouter, support it. It\nseemed simple to add support to gorouter.\n- We could introduce another intermediary proxy such as HAProxy, which\nunderstands the proxy protocol and adds or appends to an `X-Forwarded-For`\nheader with the client IP address as provided via the proxy protocol.\nDecision\n========\nWe decided to:\n- use the ELB to terminate TLS\n- use the ELB in TCP mode\n- submit proxy protocol support to gorouter\n- use S3 logging to ensure we have the IP addresses of clients using the CF\nendpoint\nStatus\n======\nAccepted\nConsequences\n============\n- We played a [spike to investigate setting X-Forwarded-For\ncorrectly](https://www.pivotaltracker.com/projects/1275640/stories/116619465)\nwhich produced an [upstream PR to add proxy protocol support to\ngorouter](https://github.com/cloudfoundry/gorouter/pull/126) and [another to introduce\nX-Forwarded-Proto headers](https://github.com/cloudfoundry/gorouter/pull/127)\n- As an interim measure until gorouter gained support, [we added an\nintermediate HAProxy to introduce `X-Forwarded-For` and `X-Forwarded-Proto`\nheaders](https://www.pivotaltracker.com/story/show/116309951).\n\n## Decision\n","========\nWe decided to:\n- use the ELB to terminate TLS\n- use the ELB in TCP mode\n- submit proxy protocol support to gorouter\n- use S3 logging to ensure we have the IP addresses of clients using the CF\nendpoint\nStatus\n======\nAccepted\nConsequences\n============\n- We played a [spike to investigate setting X-Forwarded-For\ncorrectly](https://www.pivotaltracker.com/projects/1275640/stories/116619465)\nwhich produced an [upstream PR to add proxy protocol support to\ngorouter](https://github.com/cloudfoundry/gorouter/pull/126) and [another to introduce\nX-Forwarded-Proto headers](https://github.com/cloudfoundry/gorouter/pull/127)\n- As an interim measure until gorouter gained support, [we added an\nintermediate HAProxy to introduce `X-Forwarded-For` and `X-Forwarded-Proto`\nheaders](https://www.pivotaltracker.com/story/show/116309951).\n",- Terminate TLS using ELB.\n- Use ELB in TCP mode.\n- Submit proxy protocol support to gorouter.\n- Use S3 logging to capture client IP addresses for the CF endpoint.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo comply with [Payment Card Industry (PCI) Requirements](https://www.pcisecuritystandards.org/pci_security/maintaining_payment_security) we will remove the use of shared credentials anywhere in the platform.\nWe will use Google Single Sign On (SSO) to avoid credential sharing.\nTo enable SSO we will expose some of the APIs on the BOSH instance without using a SOCKS5 proxy. This is due to the SOCKS5 proxy not being compatible with our VPN, which in turn creates a support risk.\nWe discussed proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https://docs.google.com/document/d/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps/edit#heading=h.xscqoqxlc072)\n\n## Decision\n",We will remove the reliance on SOCKS5 or SSH tunnels for the User Account and Authentication Service (UAA) API endpoint so that we can remove the use of shared credentials.\nWith these in place we are unable to complete an SSO journey from one of our existing IDPs as the BOSH instance is not browser accessible.\nThe current method will be replaced with SSO in order to obtain a UAA token. This token is then used to interact with the rest of the APIs on the BOSH instance.\n,"To enable Google Single Sign On (SSO) and comply with PCI Requirements, the following decision was made:\n\n- Expose some of the APIs on the BOSH instance without using a SOCKS5 proxy."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe need to provide tenants with the ability to provision databases for use in\ntheir applications. Our first iteration of this will be using RDS.\nWe investigated some implementations of a service broker which supported RDS\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\n- [18F](https://github.com/18F/rds-service-broker)\nDecision\n========\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\nStatus\n======\nAccepted\nConsequences\n============\nWe will be maintaining a new service broker, but have a head start on creating\nit by basing it on an existing service broker.\n\n## Decision\n","========\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\nStatus\n======\nAccepted\nConsequences\n============\nWe will be maintaining a new service broker, but have a head start on creating\nit by basing it on an existing service broker.\n","We will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker) RDS broker. As this is not a supported product, we will fork this and maintain this and implement new features ourselves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enable, or provide the option to enable,\n[storage encryption (AKA encryption at rest) for the RDS instances](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)\nof the tenants databases created by our [RDS broker](https://github.com/alphagov/paas-rds-broker). The broker has logic to enable this option on creation.\nBut there are [some limitations](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations):\n* Storage Encryption can only be enabled on creation of the DB. There is no way to update an instance to enable or disable encryption. The only way is by creating a encrypted (or unencrypted) copy of a snapshot, to then restore it to a DB instance.\n* <s>Storage Encryption is only supported [for some instance types](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#d0e47573). Specifically it is not supported for `db.t2.small` and `db.t2.micro.`, used in our Small and Free plans</s>\n* Update 2018-01-24: Amazon have now [enabled support for encryption of `t2` instances](https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-enables-encryption-at-rest-for-additional-t2-instance-types/).\nIn consequence:\n* Users cannot migrate from a plan with encryption (Medium or Large) to a plan without encryption (Small or Free). But this use case is less common.\n* Users can migrate from Free/Small to a plan with encryption (Medium or Large), but the instance will not have encryption enabled.\n* If we enable encryption in the existing plans, the existing databases will remain without encryption.\n* Due the API broker limitations, it is not possible to query the attributes of an existing instance.\nWe have 3 options to proceed:\n1. Enable Encryption in the Medium and Large plans, and document the restrictions.\n* Less effort to implement.\n* We might end having unencrypted database in a plan that is meant to be encrypted, which is confusing for the users and operators.\n* Existing Instances will remain unencrypted.\n2. Change the instance type for the Small plan to `db.m3.medium`.\n* Would allow migrate from Small to Medium or Large.\n* We will still have the problem for the Free plan.\n* Increases the costs for the Small plan (double).\n3. Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption.\n* It would be more explicit and clear, and the plan would match the state of the existing database.\n* Existing instances would still match with the plan description.\n* We will add more plans, which makes it more confusing for the tenants.\n\n## Decision\n","We decided to provide additional explicit plans with Encryption enabled, and keep the old ones.\nWe will add logic in the broker to prevent updates between plans with or without encryption.\nWe have decided only add the option of encryption to the HA plans to minimise the number of new plans added. In most of the cases the tenants would choose HA together with encryption and, although adding more plans is easy, removing them is painful once they are being used.\n","Option 3. Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA service that's onboarding at the moment depends on MySQL, memcached and\nelasticsearch that we don't yet offer, but are likely to in future, and has\nchosen to host them on AWS using RDS MySQL, Elasticache memcached and hosted\nElasticache.\nWe wanted to ensure that only the app instances belonging to that service could\nconnect to those services in order to avoid other services being able to access or modify their data.\n- MySQL requires authentication and allows TLS\n- Elasticache memcached does not provide authentication or TLS\n- AWS Hosted elasticsearch [appears to provide the ability to sign\nelasticsearch requests using an AWS access\nkey](https://aws.amazon.com/blogs/security/how-to-control-access-to-your-amazon-elasticsearch-service-domain/).\nRequiring a custom elasticsearch client.\nThe options for authentication are:\nVia Internet, using network access controls\n-------------------------------------------\nThis is described for\n[elasticache](http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Access.Outside.html)\nHow we would set it up:\n- Tenant binds services to public IP addresses, restricts access to PaaS egress IPs.\n- PaaS team changes default [application security\ngroup](https://docs.cloudfoundry.org/adminguide/app-sec-groups.html) set to\nforbid access to those IP addresses.\n- PaaS team creates new application security group allowing access to those IP addresses.\n- PaaS team binds application security group to space(s) belonging to tenants\nRisks/costs:\n- Access via Internet IP is via NAT instance, introducing a likely single point\nof failure and additional running costs to tenants.\n- Traffic is in plaintext over the Internet for some services.\n- PaaS team need to do work every time a tenant adds/changes service IP addresses.\n- If they forget to request this, other PaaS tenants have access to their\nservice IPs.\n- PaaS team need to do work (assigning application security groups) in every\noccasion a space needs to gain or lose\naccess to the list of service IPs.\nUsing private address space (VPC peering)\n-----------------------------------------\nHow we would set it up:\n- Tenant raises support request to begin setup process\n- PaaS team responds with a unique IP allocation eg. 172.16.0.0/24 for tenant to use\n- Tenant creates VPC using that address space\n- Tenant creates AWS security group(s) restricting access from PaaS VPC to expected services\n- Tenant provides PaaS team with their AWS account id and the VPC id.\n- PaaS team sends VPC peering request\n- PaaS team creates new application security group allowing access to the VPC IP allocation\n- PaaS team binds application security group to space(s) belonging to tenants\n- Tenant accepts VPC peering request\nRisks/costs:\n- Introduces a new network security boundary between VPCs; a risk of\naccidentally introducing security group rules that allow more access from the\npeered VPC than intended.\n- PaaS team need to do work (assigning application security groups) on every\noccasion a space needs to gain or lose\naccess to the peered VPC.\nAlthough this specific example uses VPC peering because the tenant in question\nuses AWS, we could use the same principle (us assigning IP address space and\nchanging application security groups) to a VPN or some other network overlay\ntechnology to allow us to connect to things other than VPCs.\n\n## Decision\n",We will offer VPC peering to tenants in specific cases where it is appropriate.\n,"**Decision:** Use VPC peering to provide private access to the MySQL, memcached, and Elasticsearch services.\n\n**Rationale:**\n\n* VPC peering provides a secure and isolated private network connection between the PaaS platform and the tenant's AWS VPC.\n* This approach eliminates the need for public IP addresses and Internet-based access, reducing the risk of data exposure and data access by unauthorized entities.\n* VPC peering also avoids the need for custom elasticsearch clients, as the tenant can use existing Elasticsearch clients to connect via the peered VPC.\n* While VPC peering requires additional setup and configuration compared to using network access controls via the Internet, it offers a more secure and controlled solution for accessing the required services."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\nThere are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\n\n## Decision\n,"* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\n","**Decision:** Use TLS/SSL encryption for all three network sections.\n\n**Rationale:** TLS/SSL encryption is a widely adopted and well-understood technology that provides strong encryption for data in transit. It is supported by all major browsers and web servers, and it is relatively easy to implement and manage.\n\n**Details:**\n\n* For the User to ELB section, use TLS/SSL termination at the ELB. This will encrypt all traffic between the user and the ELB.\n* For the ELB to router section, use TLS/SSL encryption between the ELB and the router. This will encrypt all traffic between the ELB and the router.\n* For the Router to cells section, use TLS/SSL encryption between the router and the cells. This will encrypt all traffic between the router and the cells."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR032](/architecture_decision_records/ADR032-ssl-only-for-applications-and-cf-endpoints) we decided that\nwe would only support https for applications on the PaaS, and that we would\ndrop plain http connections (port 80).\nSince then, we've observed this causing confusion for users on numerous\noccasions where they think their app isn't working after being pushed.\nThe situation is improved with the inclusion of the `cloudapps.digital` domain\nin the [HSTS preload](https://hstspreload.org/?domain=cloudapps.digital) list,\nbut this only helps users with recent versions of modern browsers.\nAs a result of the continued confusion for users we should revisit the decision\nfrom ADR443.\nThere are a number of things that could be done to address this:\n### Update the CF CLI to include the scheme\nCurrently, the CF CLI outputs the fully-qualified hostname of the app after\npushing, but doesn't include the scheme. This has caused confusion for users\nwhen this is copy/pasted into browsers, and then times out.\nGetting the CLI to include the scheme here will help with the specific case of\nusers getting confused immediately after pushing an app.\nIt's unclear how much work this involves, as currently information about\nwhether a route is http or https doesn't appear to be modeled in CloudFoundry\nanywhere.\nIf this involves changes to the CLI, there's no guarantee that users will\nupgrade their CLI.\nAdditionally, there is some debate about how effective this change would be. It\nwill probably fix some cases, but won't cover everything.\n### Redirect http to https\nAdd an endpoint that listens to all http requests on cloudapps.digital and\nredirects them to the corresponding https URL.\nThere's a risk with this that a service could link to the http version of a\npage by mistake and not notice due to the redirect. We can mitigate this be\nhaving the redirect strip the path and query when redirecting so that it always\nredirects to the base URL.\nThere's another risk that a misconfiguration could allow non-encrypted traffic\nthrough to applications. This would need to be mitigated by having acceptance\ntests to cover this.\n\n## Decision\n",We will redirect http traffic to the corresponding root https endpoint.\nWe will continue to maintain HSTS preload lists for our production domains.\n,We will redirect all http requests to https.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS would like to be able to isolate specific tenant apps and tasks to\ndifferent pools of virtual machines (VMs).\nGOV.UK PaaS would like to be able to prevent specific tenant apps and tasks\nfrom egressing to the internet.\nApps running inside the separate pools of VMs should be able to discover and\naccess other apps running within the platform, providing that the correct Cloud\nFoundry Network Policies have been created.\nApps running in the shared pools of VMs should be able to discover and access\napps running inside an isolation segment, providing that the correct Cloud\nFoundry Network Policies have been created.\n\n## Decision\n","GOV.UK PaaS will implement egress-restricted isolation segments.\nIsolation segments will be configured by a GOV.UK PaaS developer, in a similar\nmanner to VPC peering connections.\nIsolation segments will have the following variable properties:\n- Number of instances (e.g. 1, 2, 3, 6)\n- Instance type (e.g. small/large - maps to an AWS instance type + disk sizing)\n- Whether egress to the internet is restricted\nWe will use IPTables rules to achieve egress restriction.\n### Isolation segments\nCloud Foundry supports separating apps and tasks for specific Organizations\nand Spaces via a feature called\n[Isolation Segments](https://docs.cloudfoundry.org/adminguide/isolation-segments.html).\nAn Isolation Segment is a group of Diego cells with separate placement tags,\nwhich map to the isolation segment name.\nIsolation segments will be implemented as new instance groups defined in the\nBOSH deployment manifest, with additional placement tags. A placement tag\ncorresponds to an isolation group name.\nFor example, an instance group with the placement tags:\n- `fast-cpu`\n- `fast-network`\nenables us to run the following commands successfully:\n- `cf create-isolation-segment fast-cpu`\n- `cf create-isolation-segment fast-network`\nwhich creates two isolation segments.\nThese isolation segments can be shared such that a segment can be:\n- used by only a single organization or space\n- shared by multiple organizations and spaces\n### Egress restrictions\nContainer-to-container networking within Cloud Foundry is implemented via a\nVirtual Extensible Local Area Network\n([VXLAN](https://tools.ietf.org/html/rfc7348)).\nEach container is assigned a virtual IP address inside the subnet 10.255/16\n[Silk](https://github.com/cloudfoundry/silk)\nand VXLAN create/update/delete\n[IPTables](https://linux.die.net/man/8/iptables) rules\nvia the Container Network Interface\n([CNI](https://github.com/containernetworking/cni)),\nto ensure containers can talk to each other.\nIPTables is an interface to control networking within the Linux kernel.\nExisting network traffic restrictions are defined by Silk and VXLAN as\ndescribed above. We can configure extra IPTables rules with higher precedence\nto create tighter restrictions than currently exist.\nIPTables can be used to prevent unauthorised egress via REJECT rules,\ndepending on the destination IP address. This can be done, either:\n- In the global INPUT or FORWARD chains, with source IP qualifier to ensure only container traffic is affected\n- In each container’s “netout” chain\nImplementing such IPTables rules allows us to block traffic from an IP address\nwithin 10.255.0.0/16 (apps) to an address outside 10.0.0.0/8 (outside the VPC).\nThis would have the effect of preventing app traffic egressing from the\nplatform.\nWe would apply these IPTables rules to running apps and tasks, but not staging\napps.  This will allow staging apps to communicate with the outside world (e.g.\nfor downloading dependencies).\n",Implement Network Security Groups (NSGs) across all GOV.UK PaaS cloud regions. Configure NSGs to restrict egress and ingress traffic to/from isolated segments of the platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nThe alpha initially took the approach of starting with a vanilla set of cloud\nfoundry manifests, and merging new values into it using spiff. This became\ndifficult to reason about, and cf-release was forked because it was easier than\noverriding necessary values using spiff. However, the confusing spiff hierarchy\nremained.\nDecision\n========\nWe will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\nStatus\n======\nAccepted\nConsequences\n============\nWe will need to define an upgrade path for our cloud foundry manifests - making\nsure we pull in the required values from the new release manifests into ours.\n\n## Decision\n",========\nWe will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\nStatus\n======\nAccepted\nConsequences\n============\nWe will need to define an upgrade path for our cloud foundry manifests - making\nsure we pull in the required values from the new release manifests into ours.\n,"Create custom manifests based on cf-release, modify them as needed, and use spruce to merge them in the required order."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Audit events\nThe GOV.UK PaaS has components which expose events which can be used for\nauditing. For example:\n- BOSH director kernel audit logs\n- BOSH director Credhub security events\n- BOSH director UAA events\n- Cloud Foundry UAA events\n- Cloud Foundry Cloud Controller security events\nThe BOSH director and BOSH managed instances store these logs in\n`/var/vcap/sys/log/`.\n### Logging service\nThe Cyber Security team run a centralised log ingestion system called the\nCentral Security Logging Service (CSLS).\nThis service runs in AWS and uses [CloudWatch log group subscription\nfilters](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html)\nto ingest logs, which are then sent to Splunk for indexing.\n\n## Decision\n","We will use Terraform to create log groups in our existing pipelines:\n- `create-bosh-concourse`\n- `create-cloudfoundry`\nWe will store audit logs in CloudWatch for 18 months (545 days).\nWe will use Terraform to create log group subscription filters which will send\nlogs to CSLS.\nTerraform failing to create log group subscription filters should not block the\nexecution of the pipeline, so our pipelines are not coupled to CSLS.\nWe will run a fork of the\n[awslogs-boshrelease](https://github.com/alphagov/paas-awslogs-boshrelease),\non all instances that have relevant audit and security event logs, to send logs\nfrom the instances to CloudWatch.\nThe CloudWatch log groups will have subscription filters that will send logs\nto CSLS so that logs are indexed in Splunk.\n","The GOV.UK PaaS will ingest audit events from BOSH director, BOSH managed instances, and Cloud Foundry into the Central Security Logging Service (CSLS).\n\n**Technical Decision:**\n- BOSH director and BOSH managed instances store logs in `/var/vcap/sys/log/`.\n- Use CloudWatch log group subscription filters to ingest logs into CSLS.\n- Route logs from Cloud Foundry components to CSLS using their existing logging mechanisms.\n- Comply with GDS Operations Security Guidance.\n- Enhance security compliance reporting.\n- Improve audit event analysis by consolidating them in a centralized log repository."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[PaaS product pages] have been reviewed and a number of accessibility issues\nhave been identified. To resolve those we, would need to make an upgrade and\nand review if any additional changes are needed to align with the GOV.UK Design System.\nAs those pages are built in Ruby and in [ADR024] we've made the decision\nto develop our user-facing applications on Node, it's a good opportunity to\nlook at rebuilding the product pages.\nWe've discussed user needs and it emerged that:\n* anyone in the team (developer and non-developer) should be able to update pages\nwith less effort\n* pages should be performant for end users\n* pages should be rendered by the server\n* keeping pages up to date with GOV.UK Design System releases should be quicker and easier\n* alignment of technologies for our user-facing web products should provide better\ndeveloper experience and give us the option to have shared component libraries\nWith the above in mind we researched options. Our admin interface is built in React,\nso we narrowed the scope to React-based static site generators.\nWe ended up comparing two: [NextJS] with static page export and [GatsbyJS]\nwhich exports static pages by default.\nFor page content we agreed that writing pages in [Markdown] is a good option,\nso we tested both with [MDX] which can also embed React components inside content pages.\n[NextJS] and [GatsbyJS] have different approaches to development and there are minor\nperformance differences between them.\nOur use case for now is narrow enough, and with the primary need of anyone in the team\nbeing able to update pages, [NextJS] marginally gets more votes as Gatsby cannot be installed and run on\nnon-developer machines.\n\n## Decision\n",We will use [NextJS] together with [MDX] to author PaaS product pages content in\n[Markdown] and deliver them to users as static pages.\n,To build the product pages using NextJS with static page export and MDX for page content.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](../ADR032-ssl-only-for-applications-and-cf-endpoints).\nTo add our domains to [HSTS preload lists](https://hstspreload.appspot.com/), there are these requirements:\n1. Serve a valid certificate.\n2. Redirect from HTTP to HTTPS on the same host.\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\n4. Serve an HSTS header on the base domain for HTTPS requests:\nWe need an endpoint to provide these requirements.\nOur Cloud Foundry app endpoint already [serves the\nright HSTS Security header with HAProxy](../ADR008-haproxy-for-request-rewriting)\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\nWe can implement a second ELB to listening on HTTP and HTTPS and use\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\nBut this increases our dependency on the HAProxy service.\nWe must serve from the root domain (or apex domain), but it is not allowed to\nserve [CNAME records in the root/apex domain](http://serverfault.com/questions/613829/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\nan issue when serving the service using ELB or CloudFront.\n\n## Decision\n","* We will implement a basic [AWS API Gateway](https://aws.amazon.com/api-gateway/)\nwith a default [MOCK response](https://aws.amazon.com/about-aws/whats-new/2015/09/introducing-mock-integration-generate-api-responses-from-api-gateway-directly/)\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\ncontent of the response is irrelevant, it can be a 302.\nA [Custom Domain Name](http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nwhich creates a [AWS Cloud Front distribution](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html),\nwill provide public access to this API.\n* We will use [AWS Route 53 `ALIAS` resource record](http://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html)\nto [serve the IPs of the AWS Cloud Front distribution as A records](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html).\n","Implement a custom endpoint, using Nginx, and configure it to serve the HSTS header on the base domain for HTTPS requests. This endpoint should be served from the root domain (or apex domain) and should listen on both HTTP and HTTPS. Nginx should be configured to redirect from HTTP to HTTPS on the same host and serve all subdomains over HTTPS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nIn https://www.pivotaltracker.com/story/show/115252309 we described a pattern\nfor defining security groups between services and the VMs that connect to\nthem. This involved creating a client security group for each service that\nthe service allowed connections from. This client security group was then\napplied to the VMs that needed to be able to connect.\nThis has the problem that in some cases we would need to add several client\nsecurity groups to a given VM, and we are at risk of running into the AWS\nlimits (by default it's a max of 5 groups per interface).\n# Decision\nWe will use an alternative approach where a security group is defined for each\ncomponent (or component group) in the system. Services then define rules\nallowing connections from the components that need access. There are some\ninstances where a component will be both a client of other services and a\nprovider of a service. We will use the same security group for both of these\nroles.\nThis approach has a number of advantages:\n* The number of security groups applied to a VM is driven by the number of\ncomponents on that VM, not the number of things that they need to connect to.\nThe number of components is likely to be small, and we're better able to\ncontrol this that we can control the number of things a  component talks to.\n* It's easier to see what components connect to a given service by looking at\nthe rules for that service's security group.\n* When moving a component between VMs it's much clearer which security groups\nneed to move with it.\n# Status\nAccepted\n# Consequences\n* This will enable us to avoid hitting the AWS limits for the number of\nsecurity groups applied to a given interface.\n* It will make it clearer which components are allowed to connect to a given\nservice.\n* There will be some work to update our existing groups to use this pattern.\nThere are some potential issues:\n* There's the possibility for cycles in the Terraform dependency graph when we\nhave 2 components that both initiate connections to each other. This can be\navoided by using the `aws_security_group_rule` Terraform resource instead of\ndefining the rules inline in the `aws_security_group`.\n* When looking at a given VM, it's harder to see what services it connects to.\nThis is not seen as a major problem because it should be possible to create\ntooling to visualise this without much effort.\n\n## Decision\n","We will use an alternative approach where a security group is defined for each\ncomponent (or component group) in the system. Services then define rules\nallowing connections from the components that need access. There are some\ninstances where a component will be both a client of other services and a\nprovider of a service. We will use the same security group for both of these\nroles.\nThis approach has a number of advantages:\n* The number of security groups applied to a VM is driven by the number of\ncomponents on that VM, not the number of things that they need to connect to.\nThe number of components is likely to be small, and we're better able to\ncontrol this that we can control the number of things a  component talks to.\n* It's easier to see what components connect to a given service by looking at\nthe rules for that service's security group.\n* When moving a component between VMs it's much clearer which security groups\nneed to move with it.\n# Status\nAccepted\n# Consequences\n* This will enable us to avoid hitting the AWS limits for the number of\nsecurity groups applied to a given interface.\n* It will make it clearer which components are allowed to connect to a given\nservice.\n* There will be some work to update our existing groups to use this pattern.\nThere are some potential issues:\n* There's the possibility for cycles in the Terraform dependency graph when we\nhave 2 components that both initiate connections to each other. This can be\navoided by using the `aws_security_group_rule` Terraform resource instead of\ndefining the rules inline in the `aws_security_group`.\n* When looking at a given VM, it's harder to see what services it connects to.\nThis is not seen as a major problem because it should be possible to create\ntooling to visualise this without much effort.\n",Use an alternative approach where a security group is defined for each component (or component group) in the system. Services then define rules allowing connections from the components that need access.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\nIn Q2 2019 (August/September) we moved all of our platform secrets from AWS S3 to [Credhub](https://docs.cloudfoundry.org/credhub/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https://github.com/pivotal-cf/credhub-release/blob/master/docs/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\n\n## Decision\n","Credhub has the notion of a transitional certificate. As written in [their documentation](https://github.com/pivotal-cf/credhub-release/blob/master/docs/ca-rotation.md), a transitional certificate is\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\nThe new `rotate-certs` job has three tasks:\n- `remove-transitional-flag-for-ca`\n- `move-transitional-flag-for-ca`\n- `set-transitional-flag-for-ca`\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\n",Automate certificate rotation using Credhub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've been running with the cell provisioning policy in ADR017 since February.\nWe haven't ever run out of cell capacity, but we've observed that there's\nexcess capacity that we're paying for.\nAt the time that we wrote ADR017 we had fewer tenants and an individual\ntenant's quota was a much greater proportion of total memory size. In other\nwords a single tenant could conceivably use a greater proportion of our excess\ncapacity.\nCells are still deployed across 3 AZs.\nWe still don't have a way to autoscale the number of cells to meet demand, so\nwe need to ensure that we have surplus capacity for when we're not around.\nCells are almost completely uncontended; we're not experiencing CPU or disk I/O\ncontention and not all the cell memory is being used.\nOver the 3 month period from 1st August - 1st November\n- Usable memory (free + cached + buffered) is running between 77% and 92% of total cell memory\n- The maximum increase in memory usage over an exponentially smoothed average from a week previously was 36%\n- We're running at about 10% of our total container capacity\n- container usage has peaked at about 20% above the previous weeks\n- Average CPU usage is about 10%. We see daily peaks of 80%\n- reps think that about 50% of the capacity of cells is used\n- the largest amount that rep's allocated memory increased week on week was 55%\n\n## Decision\n","Our objectives are:\nState | Expected behaviour\n------|-------------------\n# All cells operational | Enough capacity to allow some but not all tenants to scale up to their full quota. The amount of excess capacity required should be enough to accommodate the fluctuations we can expect over a 3 day period (weekend + reaction time)\n# While CF being deployed | As above: enough capacity to allow some tenants to scale up to their full quota\n# One availability zone failed/degraded | Enough capacity to maintain steady state app usage. Not guaranteed to be able to scale apps up.\n# More than one AZ failed | The system is not expected to have sufficient capacity to host all running apps.\nTo achieve this we need to start basing our capacity planning on current memory\noccupied by processes on cells, rather than the sum of all quotas given to\nusers. We will define alerts for capacity planning purposes, the in-hours\nsupport person is expected to respond by adjusting the number of cells.\nWe want to ensure that cells have some headroom above a smoothed\naverage:\n- to allow some headroom for increases in the memory consumed by apps.\n- to allow buffering and caching to occur and not adversely impact application\nperformance.\nFrom our data analysis (see context) the amount of memory consumed by apps\ncan reach about 36% over a week-ago's smoothed average. We round up to 40% to\ninclude buffering/caching.\nIf an AZ fails, we need enough capacity remaining to host all our apps. The\nfailed AZ's apps are evenly divided amongst the surviving AZs. Because we have\ntwo remaining AZs, each surviving AZ will have 1.5x as many apps running.\nBecause we want 40% headroom, we'll want 1.4 (headroom) x 1.5 (evacuated apps)\ncurrent usage. This is about 2x actual memory consumed by processes on cells.\nTherefore we need to start alerting when the memory occupied by processes on\ncells is above 50%, when suitably smoothed to avoid noise / small spikes\ncausing frequent alarms.\nCPU usage is assumed to be a linear relation of memory usage and we will have a\nsimilar alert defined when it exceeds 50% on cells.\nIn addition to wanting the cells to not run short on memory, we also want\ntenants to be able to scale apps up and down when all AZs are functional. In\norder to ensure this, we need to allow for a ~50% increase in requested memory,\nwhich means alerting when all the reps have a cumulative remaining capacity of\n~33%, when smoothed to avoid false alarms.\nWe also need enough container capacity to allow tenants to scale apps up and\ndown and deploy new apps. We should alert when we're using > 80% of the sum of\nour reps' container capacity. Again, this should be smoothed to ensure that\nshort lived fluctuations in usage don't cause unnecessary alerts.\nIt is likely that patterns such as the fluctuation in memory use over a week\nmay change over time. We should review this decision after 6 months.\n",Reduce the number of cells and reduce the memory allocation for each cell to bring the memory used into line with actual usage patterns of tenant nodes.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe wanted to open up access to tenant applications in our production environment.\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\n* [#116104189 - set up Pingdom](https://www.pivotaltracker.com/story/show/116104189)\n* [#115347323 - allow public access to tenant applications](https://www.pivotaltracker.com/story/show/115347323)\nDecision\n========\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\nStatus\n======\nProposed\nConsequences\n============\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\n### Positive\n* We are now able to test accessibility using the staging environment\n* We are now able to use Pingdom to assert not just application health, but routing as well.\n* We have maintained consistency between staging and production\n### Negative\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\n* We would no longer have healthchecks via Pingdom for our CI environment.\n\n## Decision\n","========\nIt was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\nStatus\n======\nProposed\nConsequences\n============\nA story is now required to remove the Pingdom health check for our CI environment, and the security groups allowing Pingdom probes.\n### Positive\n* We are now able to test accessibility using the staging environment\n* We are now able to use Pingdom to assert not just application health, but routing as well.\n* We have maintained consistency between staging and production\n### Negative\n* Any applications in our staging environment need to be considered for whether they are suitable to be public.\n* We would no longer have healthchecks via Pingdom for our CI environment.\n","Make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn https://www.pivotaltracker.com/story/show/115252309 we described a pattern\nfor defining security groups between services and the VMs that connect to\nthem. This involved creating a client security group for each service that\nthe service allowed connections from. This client security group was then\napplied to the VMs that needed to be able to connect.\nThis has the problem that in some cases we would need to add several client\nsecurity groups to a given VM, and we are at risk of running into the AWS\nlimits (by default it's a max of 5 groups per interface).\n\n## Decision\n","We will use an alternative approach where a security group is defined for each\ncomponent (or component group) in the system. Services then define rules\nallowing connections from the components that need access. There are some\ninstances where a component will be both a client of other services and a\nprovider of a service. We will use the same security group for both of these\nroles.\nThis approach has a number of advantages:\n* The number of security groups applied to a VM is driven by the number of\ncomponents on that VM, not the number of things that they need to connect to.\nThe number of components is likely to be small, and we're better able to\ncontrol this that we can control the number of things a  component talks to.\n* It's easier to see what components connect to a given service by looking at\nthe rules for that service's security group.\n* When moving a component between VMs it's much clearer which security groups\nneed to move with it.\n","**Decision:**\n**Client security groups for service mesh will be defined using AWS Network Firewall. AWS Network Firewall consumes one security group per interface, rather than one per rule.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are starting to develop a number of user-facing applications with web\ninterfaces that need to be GOV.UK branded. In order to keep things consistent\nwe want to pick a single programming language to write these in.\nWe've previously chosen Ruby on Rails as our desired framework in the previous\n[ADR022]. And whether or not it was a right choice, we decided to revisit and\nreconsider that choice.\nThe reason for that, is simply the direction GDS is heading at. It would appear\nthat old components will be soon deprecated and the Frontend Community has no\ndesire to support gems in the future. Saying that, we could have remain with\nRails and delegate the component management to NPM. This would however increase\nthe amount of possible maintenance work we would need to undertake, due to the\nuse of [nunjucks] by the Design System team.\nWe questioned the need of running Rails application for something that\nessentially is a templating system for existing data and API.\nAfter some more research, discussion with the head of the Frontend Community,\nmembers of the team and some others in GDS, the better choice would be Node for\nthe following reasons:\n* It's the way Frontend Community is heading at\n* It will be easier to rotate/onboard Frontend Developers\n* The initial applications are to be simple (thin layer between API calls and\nHTML parsing)\n* It's light and essentially is JavaScript\n* It supports [nunjucks] which will help us in maintenance\n\n## Decision\n",We will use Node to create new user-facing applications that render a web\ninterface for our service but will not be used to implement any significant\n‘application logic’.\n,Use Node.js for developing user-facing applications with web interfaces that need to be GOV.UK branded.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe want to serve [HSTS\nheaders](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security) for all\nHTTPS requests to the apps domains, but it will safeguard existing users from\nbeing MITMed over insecure connections and it will improve the user experience\nwhen they click on a hostname that doesn't have a protocol.\n(Note that without pre-loading in browsers this won't help first time users,\nbut that is out of context)\nWe want to leave open the option of able to override these headers from\nthe tenant application if they wish.\nThis feature requires conditionally process and modify the request headers.\nThere are several possible implementations:\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\nand add the header if required, by:\n* Supporting the specific HSTS headers, and allowing configure some\nsort of behaviour and default value.\n* Allow inject any additional header if they are missing.\nBut [current `gorouter` implementation](https://github.com/cloudfoundry/gorouter/commit/0d475e57b1742c42ba6d98d1ed853edc9f709893)\ndoes not support any of these features, which require being added.\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\nthe go-routers and after the ELB.\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\nAll the commercial CDN have the capacity to add additionally headers\nconditionally.\n4. AWS ELB: They do not support this logic and will not in the short term.\nIn consequence they cannot be used to solve this problem.\nDecision\n========\nWe do not want to add any additional logic in the CDN, as they will\nbe an optional part of the platform and we will try to keep as simple\nas possible.\nWe consider that the optional solution would be implement this logic in\nthe `gorouter`, but that requires some development effort and a PR being merged\nupstream.\nBecause that we will implement, in the short term, the second option: a proxy\nin front of the `gorouter`.\n* We will implement [HAproxy](http://www.haproxy.org/) in front of the go router.\n* Ha-proxy is the default LB solution for the official CF distribution.\n* It is really powerful and has good support.\n* Enough features to cover our needs.\n* It will be setup colocated with the `gorouter`, proxying directly to\nlocalhost.\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\nis OK as the two services are colocated in the same VM.\n* We will reuse the code from [official haproxy job from cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/haproxy),\nalthough we will have to fork it to add additional settings in the\nhaproxy configuration.\nFuture work:\n* We will implement and propose a PR to add logic in go-router to allow\ndefine additional headers.\nStatus\n======\nAccepted\nConsequences\n============\n### Positive\n* We will be able to easily add more logic to rewrite the HTTP communication\nto the applications using HAProxy.\n* HAProxy SSL termination has better performance than `gorouter`, although\nthis has a low impact because ELB is terminating the end user connections\nand using keep alive connections to the gorouter/haproxy.\n* HAProxy supports web-sockets and does HTTP multiplexing.\n* We can implement HTTP => HTTPS redirect in HAProxy.\n### Negative\n* Adds some additional latency to every request.\n* We have to maintain our custom haproxy release.\n* Another moving part to monitor and take into account.\n### See Also\n[ADR012](ADR012-haproxy-healthcheck/)\n\n## Decision\n","========\nWe do not want to add any additional logic in the CDN, as they will\nbe an optional part of the platform and we will try to keep as simple\nas possible.\nWe consider that the optional solution would be implement this logic in\nthe `gorouter`, but that requires some development effort and a PR being merged\nupstream.\nBecause that we will implement, in the short term, the second option: a proxy\nin front of the `gorouter`.\n* We will implement [HAproxy](http://www.haproxy.org/) in front of the go router.\n* Ha-proxy is the default LB solution for the official CF distribution.\n* It is really powerful and has good support.\n* Enough features to cover our needs.\n* It will be setup colocated with the `gorouter`, proxying directly to\nlocalhost.\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\nis OK as the two services are colocated in the same VM.\n* We will reuse the code from [official haproxy job from cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/haproxy),\nalthough we will have to fork it to add additional settings in the\nhaproxy configuration.\nFuture work:\n* We will implement and propose a PR to add logic in go-router to allow\ndefine additional headers.\nStatus\n======\nAccepted\nConsequences\n============\n### Positive\n* We will be able to easily add more logic to rewrite the HTTP communication\nto the applications using HAProxy.\n* HAProxy SSL termination has better performance than `gorouter`, although\nthis has a low impact because ELB is terminating the end user connections\nand using keep alive connections to the gorouter/haproxy.\n* HAProxy supports web-sockets and does HTTP multiplexing.\n* We can implement HTTP => HTTPS redirect in HAProxy.\n### Negative\n* Adds some additional latency to every request.\n* We have to maintain our custom haproxy release.\n* Another moving part to monitor and take into account.\n### See Also\n[ADR012](ADR012-haproxy-healthcheck/)\n","Implement HAProxy as a proxy in front of the gorouter to conditionally process and modify request headers, including HSTS headers."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to provide tenants with the ability to provision databases for use in\ntheir applications. Our first iteration of this will be using RDS.\nWe investigated some implementations of a service broker which supported RDS\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\n- [18F](https://github.com/18F/rds-service-broker)\n\n## Decision\n,"We will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\n",Use the RDS service broker from 18F
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\n\n## Decision\n","We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\n","**Decision:** Simplify the deployment pipeline by removing the Staging environment.\n\n**Rationale:**\n\n* The Staging environment is not providing significant value compared to the time and cost overheads it incurs.\n* Removing the Staging environment will reduce the time it takes for changes to reach Production, improving the overall efficiency of the deployment process.\n* Simplifying the pipeline will also reduce the operational complexity, making it easier to manage and maintain.\n\n**Implementation:**\n\n* Remove the Staging environment from the deployment pipeline configuration.\n* Update the CI/CD process to promote changes directly from CI to Production, with appropriate safeguards in place (e.g., automated testing, manual approvals).\n* Monitor and evaluate the performance of the simplified pipeline to ensure it meets the desired goals of faster deployments and reduced overhead."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS uses [AWS Shield Advanced](https://aws.amazon.com/shield/features/#AWS_Shield_Advanced) as well as AWS WAF to protect from DDoS attacks.\nHowever the mitigations are not automatic and we have access to the AWS DDoS Response Team\n(DRT) who are experts in mitigating these types of attack.\n```\nShield Advanced detects web application layer vectors, like web request floods and\nlow-and-slow bad bots, but does not automatically mitigate them. To mitigate web\napplication layer vectors, you must employ AWS WAF rules or the DRT must employ the\nrules on your behalf.\n```\nIn order to be functional they require access to our AWS WAF logs in order to identify what\nthe attack is and where is is coming from, and API access to the WAF in order to apply the\nmitigating rules.\nTo enagage the AWS DRT team we will set up CloudWatch alarms on our WAF rules in order to trigger\nthe [emergency engagement Lambda](https://s3.amazonaws.com/aws-shield-lambda/ShieldEngagementLambda.pdf)\n\n## Decision\n",We will grant access to the AWS DRT to read from restricted S3 buckets\n,**Decision:** To grant the AWS DDoS Response Team (DRT) read-only access to our AWS WAF logs and API access to AWS WAF to enable them to mitigate web application layer DDoS attacks.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn ADR008 and ADR014 we decided to use HAProxy, for three reasons:\n- Writing HSTS header if they are not present in the upstream request\n- Implementing HTTP -> HTTPS redirect\n- Custom health check for Gorouter\nThese problems have since been fixed:\n- Header rewriting was implemented in [v0.183.0](https://github.com/cloudfoundry/routing-release/releases/tag/0.183.0)\n- HTTP healthchecking was implemented in [v0.139.0](https://github.com/cloudfoundry/routing-release/releases/tag/0.139.0)\n- HTTP -> HTTPS redirect can be done using AWS ALBs\nWe currently use multiple ELBs (classic) which we want to replace with ALBs.\nWe want to use ALBs because:\n- ELBs are deprecated in terraform and cause crashes\n- ALBs can give us more metrics in CloudWatch\n- ALBs have better support for X-Forwarded-For\n- ALBs support fixed-response which can be used for HTTP -> HTTPS rewriting\nHAProxy adds significant complexity to our routing deployment and maintenance:\n- Proxy Protocol is non-standard and hard to understand\n- HTTP -> HTTPS rewriting is hard to understand\n- HAProxy config is rarely touched\n- We have to maintain our own HAProxy BOSH release\n- HAProxy duplicates the number of logs we receive because every platform request is written twice\n- HAProxy adds an extra network hop for every request\n\n## Decision\n",- Replace ELBs with ALBs\n- Use ALB fixed-response to redirect HTTP -> HTTPS\n- Use Gorouter directly for:\n- TLS termination\n- HSTS header rewriting\n- Healthchecking the router instance\n- Remove HAProxy\n,Deprecate HAProxy and use ALBs for routing traffic.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\n![pipeline image](images/pipeline.jpg)\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\n### Naming considerations\nA number of aspects were considered as part of the naming process.\n* Clear sense of purpose\n* Clear distinction between Production and other Environments\n* No overly technical names (for example, hosting/paas/scalable-elastic-public-government-hosting)\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\nDecision\n========\nFor _non_ production environments we will be using the following domains:\n* [environment name].cloudpipeline.digital\n* [app name].[environment name].cloudpipelineapps.digital\nFor our production environment we will be using the following domains:\n* cloud.service.gov.uk\n* [app name].cloudapps.digital\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\nDomain Overview\n===============\n### Development Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.foo.dev.cloudpipeline.digital\nCloud Foundry API | api.foo.dev.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.foo.dev.cloudpipeline.digital\nApplications | bar.foo.dev.cloudpipelineapps.digital\n### Continuous Integration (CI) Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.master.ci.cloudpipeline.digital\nCloud Foundry API | api.master.ci.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.master.ci.cloudpipeline.digital\nApplications | bar.master.ci.cloudpipelineapps.digital\n### Staging Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.staging.cloudpipeline.digital\nCloud Foundry API | api.staging.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.staging.cloudpipeline.digital\nApplications | bar.staging.cloudpipelineapps.digital\n### Production Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.cloud.service.gov.uk\nCloud Foundry API | api.cloud.service.gov.uk\nCloud Foundry User Account and Authentication | uaa.cloud.service.gov.uk\nApplications | bar.cloudapps.digital\nStatus\n======\nAccepted\nConsequences\n============\nCertificates etc. had to be purchased, domains registered, and our automated deployments configured to allow us to specify the domains for each stage of the pipeline.\n\n## Decision\n","========\nFor _non_ production environments we will be using the following domains:\n* [environment name].cloudpipeline.digital\n* [app name].[environment name].cloudpipelineapps.digital\nFor our production environment we will be using the following domains:\n* cloud.service.gov.uk\n* [app name].cloudapps.digital\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\nDomain Overview\n===============\n### Development Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.foo.dev.cloudpipeline.digital\nCloud Foundry API | api.foo.dev.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.foo.dev.cloudpipeline.digital\nApplications | bar.foo.dev.cloudpipelineapps.digital\n### Continuous Integration (CI) Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.master.ci.cloudpipeline.digital\nCloud Foundry API | api.master.ci.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.master.ci.cloudpipeline.digital\nApplications | bar.master.ci.cloudpipelineapps.digital\n### Staging Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.staging.cloudpipeline.digital\nCloud Foundry API | api.staging.cloudpipeline.digital\nCloud Foundry User Account and Authentication | uaa.staging.cloudpipeline.digital\nApplications | bar.staging.cloudpipelineapps.digital\n### Production Domains\nPurpose | URL |\n------------ | -------------\nDeployer Concourse | deployer.cloud.service.gov.uk\nCloud Foundry API | api.cloud.service.gov.uk\nCloud Foundry User Account and Authentication | uaa.cloud.service.gov.uk\nApplications | bar.cloudapps.digital\nStatus\n======\nAccepted\nConsequences\n============\nCertificates etc. had to be purchased, domains registered, and our automated deployments configured to allow us to specify the domains for each stage of the pipeline.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\nThe PaaS has several environments which will need to use Aiven. These\nenvironments should be isolated from each other so that changes made in testing\nand development environments do do not affect production users.\nAiven provide a ""Project"" abstraction where a user can be a member of several\nprojects. API tokens are user specific. By creating one user per project it's\npossible to scope API tokens to a project.\n\n## Decision\n","We'll use separate projects for separate environments, initially using the\nfollowing Aiven projects:\n* ci-testing (for the CI environment for the elasticsearch broker itself)\n* paas-cf-dev\n* paas-cf-staging\n* paas-cf-prod\nFor staging and prod we will use separate API tokens within the same project to\nseparate credentials between the London and Ireland regions.\nWe will have the following per-project users to hold API tokens:\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\nThe credentials for the ci and dev users will be stored in the\n`paas-credentials` passwordstore. staging and prod will be stored in\n`paas-credentials-high`.\nMembers of the PaaS team will each have their own user which will have access\nall of the projects for management purposes.\n","Use Aiven's ""Project"" abstraction to create one user per PaaS environment. Use\nAPI tokens to authenticate with the Elasticsearch backing service, with each API\ntoken scoped to a single project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNote: This has been superceeded. See [Status](#status) below.\nIt is expected for the government websites to be secure and keep the user\ninteractions private. Because that we want to enforce all communications to\nany application and to the platform endpoints to use only and always HTTPS,\nas [it is described in the Gov Service Manual](https://www.gov.uk/service-manual/technology/using-https).\nWhen a user inputs a website name without specifying the\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\nEven if the server always redirect HTTP to HTTPS, an initial\nunprotected request including user information will be transferred\nin clear: full URL with domain, parameter, [cookies without secure flag](https://en.wikipedia.org/wiki/HTTP_cookie#Secure_and_HttpOnly)\nor browser meta-information.\n[HTTP Strict Transport Security](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security)\nmitigates this issue by instructing modern browsers that support it to\nalways connect using HTTPS.\nThis is also a [requirement in the service manual](https://www.gov.uk/service-manual/technology/using-https).\nThere is still a potential initial unprotected HTTP request that might happen\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\nTo solve this issue, the root domain can be added to\n[HSTS preload list](https://hstspreload.appspot.com/) which will be used by most\ncommon browsers.\nCurrently the only way to avoid any clear text HTTP interaction is closing or\ndropping any attempt to connect to the port 80 at TCP level.\nAlthough not all application deployed on the PaaS will be ""services""\nas in the service manual meaning, we must not allow HTTP to make\nit easier to service owners to comply with this requirements.\n\n## Decision\n",We will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\nWe will implement and maintain HSTS preload lists for our production domains.\n,"To make it easier for service owners to comply with the Government Service Manual requirement to encrypt all communication, any TCP connection over port 80 should be closed or dropped at TCP level."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWhen building pipelines using concourse, we investigated using the [pool\nresource](https://github.com/concourse/pool-resource) in order to control flow\nthrough jobs. This was an alternative to the use of the\n[semver resource](https://github.com/concourse/semver-resource).\nThese 2 resources are both workarounds to solve the problem of triggering jobs\nwhen we haven't made changes to a resource.\nThe problem is that the pool resource relies on write access to a github repo,\nwhich means we must pass public keys that allow this access into the pipeline\nand deployed concourse instance - we want to minimise the number of credentials\nwe pass, and the semver resource relies on AWS credentials that are already\npassed.\nDecision\n========\nWe will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\nStatus\n======\nAccepted\nConsequences\n============\nThis was an investigation into a different approach, so no consequences\n\n## Decision\n","========\nWe will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\nStatus\n======\nAccepted\nConsequences\n============\nThis was an investigation into a different approach, so no consequences\n","Use the semver resource for flow between jobs in pipelines, instead of the pool resource."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\nThis is a form of end-to-end encryption.\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\nThis is a form of end-to-end encryption.\nThe [cf-dev mailing list](https://lists.cloudfoundry.org/g/cf-dev/message/9143) alleges\nthat the IPSec release is no longer maintained.\n\n## Decision\n,We will not run IPSec in our BOSH deployments.\n,To remove IPSec from GOV.UK PaaS.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to serve [HSTS\nheaders](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security) for all\nHTTPS requests to the apps domains, but it will safeguard existing users from\nbeing MITMed over insecure connections and it will improve the user experience\nwhen they select a hostname that doesn't have a protocol.\n(Note that without pre-loading in browsers this won't help first time users,\nbut that is out of context)\nWe want to leave open the option of able to override these headers from\nthe tenant application if they wish.\nThis feature requires conditionally process and modify the request headers.\nThere are several possible implementations:\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\nand add the header if required, by:\n* Supporting the specific HSTS headers, and allowing configure some\nsort of behaviour and default value.\n* Allow inject any additional header if they are missing.\nBut [current `gorouter` implementation](https://github.com/cloudfoundry/gorouter/commit/0d475e57b1742c42ba6d98d1ed853edc9f709893)\ndoes not support any of these features, which require being added.\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\nthe go-routers and after the ELB.\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\nAll the commercial CDN have the capacity to add additionally headers\nconditionally.\n4. AWS ELB: They do not support this logic and will not in the short term.\nIn consequence they cannot be used to solve this problem.\n\n## Decision\n","We do not want to add any additional logic in the CDN, as they will\nbe an optional part of the platform and we will try to keep as simple\nas possible.\nWe consider that the optional solution would be implement this logic in\nthe `gorouter`, but that requires some development effort and a PR being merged\nupstream.\nBecause that we will implement, in the short term, the second option: a proxy\nin front of the `gorouter`.\n* We will implement [HAproxy](http://www.haproxy.org/) in front of the go router.\n* Ha-proxy is the default LB solution for the official CF distribution.\n* It is really powerful and has good support.\n* Enough features to cover our needs.\n* It will be setup colocated with the `gorouter`, proxying directly to\nlocalhost.\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\nis OK as the two services are colocated in the same VM.\n* We will reuse the code from [official haproxy job from cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/haproxy),\nalthough we will have to fork it to add additional settings in the\nhaproxy configuration.\nFuture work:\n* We will implement and propose a PR to add logic in go-router to allow\ndefine additional headers.\n",1. Implement the logic in the `gorouter` itself.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGOV.UK PaaS runs a fork of the [RDS Service Broker](https://github.com/alphagov/paas-rds-broker) and uses it to provide Postgres database services\nto its tenants.\nAs of writing, our service plan 'menu' looks like this:\n```\n$ cf marketplace -e postgres\nGetting service plan information for service offering postgres in org gds-tech-ops / space sandbox as 117196824971928474330...\nbroker: rds-broker\nplan                            description                                                                                                                                                                          free or paid   costs   available\ntiny-unencrypted-10             5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.micro. Free for trial orgs. Costs for billable orgs.    free                   yes\nsmall-10                        20GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.                                            paid                   yes\nsmall-ha-10                     20GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.                          paid                   yes\nmedium-10                       100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.                                           paid                   yes\nmedium-ha-10                    100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.                         paid                   yes\nlarge-10                        512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.                                        paid                   yes\nlarge-ha-10                     512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.                      paid                   yes\nxlarge-10                       2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.                                          paid                   yes\nxlarge-ha-10                    2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.                        paid                   yes\ntiny-unencrypted-11             5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.    free                   yes\nsmall-11                        100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-11                     100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-11                       100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-11                    100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-11                        512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.                                        paid                   yes\nlarge-ha-11                     512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.                      paid                   yes\nxlarge-11                       2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                                          paid                   yes\nxlarge-ha-11                    2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                        paid                   yes\ntiny-unencrypted-10-high-iops   25GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.   free                   yes\nsmall-10-high-iops              100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-10-high-iops           100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-ha-10-high-iops          500GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-10-high-iops              2.5TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.2xlarge.                                        paid                   yes\nlarge-ha-10-high-iops           2.5TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.2xlarge.                      paid                   yes\nxlarge-10-high-iops             10TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.4xlarge.                                         paid                   yes\nxlarge-ha-10-high-iops          10TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.4xlarge.                       paid                   yes\ntiny-unencrypted-11-high-iops   25GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.   free                   yes\nsmall-11-high-iops              100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-11-high-iops           100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 11. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-11-high-iops             500GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-11-high-iops          500GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-11-high-iops              2.5TB Storage Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.                                paid                   yes\nlarge-ha-11-high-iops           2.5TB Storage Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.2xlarge.              paid                   yes\nxlarge-11-high-iops             10TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                                         paid                   yes\nxlarge-ha-11-high-iops          10TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 11. DB Instance Class: db.m5.4xlarge.                       paid                   yes\nmedium-10-high-iops             500GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m5.large.                                           paid                   yes\ntiny-unencrypted-12             5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.    free                   yes\nsmall-12                        100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-12                     100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-12                       100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-12                    100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-12                        512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.                                        paid                   yes\nlarge-ha-12                     512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.                      paid                   yes\nxlarge-12                       2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                                          paid                   yes\nxlarge-ha-12                    2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                        paid                   yes\ntiny-unencrypted-12-high-iops   25GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.micro. Free for trial orgs. Costs for billable orgs.   free                   yes\nsmall-12-high-iops              100GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                                           paid                   yes\nsmall-ha-12-high-iops           100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 12. DB Instance Class: db.t3.small.                         paid                   yes\nmedium-12-high-iops             500GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                                           paid                   yes\nmedium-ha-12-high-iops          500GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.large.                         paid                   yes\nlarge-12-high-iops              2.5TB Storage Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.                                paid                   yes\nlarge-ha-12-high-iops           2.5TB Storage Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.2xlarge.              paid                   yes\nxlarge-12-high-iops             10TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                                         paid                   yes\nxlarge-ha-12-high-iops          10TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 12. DB Instance Class: db.m5.4xlarge.                       paid                   yes\n```\nThere's some background information needed to understand some decisions made when setting up our plans:\n* [AWS's gp2 storage](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2) scales at 3 IOPS per GB of volume size, between a base of 100 IOPS at 33.33GB\ndisk size up to 16kIOPS at 5,334GB disk size.\n* AWS does not provide gp3 storage for RDS.\n* [RDS storage](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html) does not allow you to decrease storage of a database instance, or increase storage of\na database instance without adding at least 10%.\nWe have identified several inconsistencies and issues with the current plans when studying them in\norder of volume size.\n## Problems and potential options\nReplacing plans in this section refers to:\n* renaming the existing plan to have a `-deprecated` suffix\n* marking it private\n* scoping it down to only the existing spaces using it\n* creating a new one under its old name, with new configuration\nThis way, we avoid problems with billing and existing service instances using these plans.\n### Problem 1\nThere is no upgrade path available from `medium` `high-iops` plans to `large` plans, because of the\nRDS 10% storage increase rule - 500GB to 512GB would violate it.\n1. We could replace the `medium` `high-iops` plans with ones of smaller disk space, say 465GB (`⌊512/1.1⌋`).\n* This would leave existing users of these plans (6 instances) unable to upgrade (both in terms\nof plan size and database version) without going up to `large`.\n2. We could replace the `large` plans with ones of larger disk space, say 564GB (`⌈512*1.1⌉`).\n### Problem 2\nThe `small` plans are inconsistent - `small-10` (and `small-ha-10`) provide 20GB disk space, but\nthe Postgres 11 and 12 variants provide 100GB disk space.\n1. We could replace the `small-11`, `small-ha-11`, `small-12`, and `small-ha-12` plans with 20GB\neditions.\n* This would leave existing users of these plans (170 instances) unable to upgrade (both in\nterms of plan size and database version) without going up to `small` `high-iops` or `medium`.\n2. We could replace the `small-10` and `small-ha-10` plans with 100GB editions.\n* This would make approximately zero sense as they'd be identical to the `small-10-high-iops`\nand `small-ha-10-high-iops plans`.\n3. We could do nothing and leave it be. It's not technically breaking anything irght now.\n* This might create a dilemma when we come to introduce `small-13` and `small-ha-13`. Should it\nbe 20GB or 100GB? `small-13-high-iops` and `small-ha-13-high-iops` will be 100GB so it would\nbe illogical, however if we went for 20GB, there would be no `small-12` -> `small-13` upgrade path.\n### Problem 3\nThe Postgres engine version is part of the plans themselves. It is not relevant to billing or\nparticularly how the underlying resource is structed on AWS.\n1. We could make it a config option.\n* This might make the broker more complex as we'd have to handle existing instances.\n2. We could do nothing and leave it be.\n* This means a lot of duplication and potential for inconsistency (e.g., as we've seen between\nsmall-10 and small-12).\n### Problem 4\nThe disk space is part of the plans but this leads to massive jumps in plan whenever you just want\na bit more disk space.\n1. We could make it a config option.\n* We'd have to factor this into billing.\n* This might make the broker more complex as we'd have to handle existing instances.\n2. We could do nothing and leave it be.\n* Problem to solve in a future PaaS? Would future PaaS even want to do service plans? Probably,\nbut we shouldn't pre-empt it.\n### Problem 5\nIt's not clear that users know there could be legitimate reasons to want to upgrade from\n`medium` `high-iops` to `large` (non-`high-iops`). If a user doesn't understand the disk space\n<-> IOPS link, they might have `medium` `high-iops`, need more RAM/a bigger CPU/more connections,\nand decide straight to `large` `high-iops` instead of plain `large`.\n1. We could add IOPS information to each plan's description.\n* These descriptions are already quite wordy and this would make things worse.\n2. We could do nothing and leave it be.\n### Problem 6\nThe `xlarge` `high-iops` plans provide a disk larger than that needed to achieve maximum\nAWS gp2 IOPS.\n1. We could make future xlarge `high-iops` plans 5,334GB in disk size.\n* However there would then be no upgrade path for the current user.\n2. We could do nothing and leave it be.\n\n## Decision\n",We will take these actions:\n* Problem 1: Action 2 - replace the `large` plans with 564GB versions\n* Problem 2: Action 3 - do nothing for now\n* Problem 3: Action 2 - do nothing for now\n* Problem 4: Action 2 - do nothing for now\n* Problem 5: Action 2 - do nothing for now\n* Problem 6: Action 2 - do nothing for now\n,"### Decision\nHere are the decisions we have made and are planning to implement:\n- **Problem 1:** Replace the `medium-10-high-iops` and `medium-11-high-iops` plans with\n100GB disk space equivalents (465GB is not a valid disk size in AWS).[1]\n- **Problem 2:** Replace the `small-10` and `small-ha-10` plans with 100GB disk space\nequivalents, and remove the other `small` plans which provide 20GB disk space.[1]\n- **Problem 3:** Make the Postgres engine version a config option.[4]\n- **Problem 4:** Do nothing and leave it be.[2]\n- **Problem 5:** Do nothing and leave it be.[2]\n- **Problem 6:** Do nothing and leave it be.[2]\n\n[1]: This is potentially backwards-incompatible, as a user could be using these plans on the\nbasis of disk size, rather than IOPS. However, as disk space on AWS RDS cannot be reduced\nwithout data loss, the chances of this are low.\n[2]: The current plan structure has been built up over a period of years, and it's not\nclear that any alternative would be better. It would have been better to design them in a\nmore structured way from the beginning, but that is difficult to do in practice (for example,\nthe type and number of RDS instances which can be provisioned using a particular RDS provisioned\nIOPS value changes over time).\n[4]: This will have backwards-compatibility implications as existing service instances which are\nconfigured with a particular Postgres version will need to be migrated to using a config option.\nThis is sub-optimal, but until we can finalise a plan for a future PaaS setup (and whether\nservice plans will exist within it), it is the least disruptive option.\n### Alternatives\nAs mentioned in the options section, here are some alternative decisions which could be made:\n- **Problem 1:** Replace the `large` plans with 564GB equivalents, which would allow the\n`medium` `high-iops` plans to be upgraded directly to `large`.\n- **Problem 2:** Replace the `large` plans with 100GB equivalents, and then create small\nplans with 20GB equivalents.\n- **Problem 4:** Make the disk space a config option.\n- **Problem 6:** Make future large `high-iops` plans 5,334GB in disk size.\n### Reason\nThese decisions have been made based on the following reasoning:\n- **Problem 1:** Replacing the `medium` `high-iops` plans with 100GB disk space equivalents\navoids introducing a new 564GB disk size which would not have an upgrade path from `small`\nplans. It is the lesser inconvenience to existing users.\n- **Problem 2:** Replacing the `small-10` and `small-ha-10` plans with 100GB disk space\nequivalents and removing the other `small` plans will address the inconsistency in plan size\nfor Postgres 10, 11, and 12.\n- **Problem 3:** Making the Postgres engine version a config option will make it more flexible\nto change the Postgres version without requiring a plan replacement. It is the most backward-\ncompatible and future-proof option, and will allow us to remove the current plan\nduplication.\n- **Problem 4:** Doing nothing and leaving it be is the least disruptive option for existing users.\n- **Problem 5:** Doing nothing and leaving it be is the easiest option, and the consequences are\nmild.\n- **Problem 6:** Doing nothing and leaving it be is the least disruptive option for existing users.\n\n### Conclusion\nThese decisions will be implemented over the coming weeks.\n\n### non-functional decisions\n### Performance\nThe performance of the service will be unaffected.\n### Scalability\nThe scalability of the service will be unaffected.\n### Availability\nThe availability of the service will be unaffected.\n### Security\nThe security of the service will be unaffected.\n### Reliability\nThe reliability of the service will be unaffected.\n### Testability\nThe testability of the service will be unaffected.\n### Maintainability\nThe maintainability of the service will be unaffected.\n### Cost\nThe cost of the service will be unaffected."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\n```\nMemory reserved by orgs: 368640 MB (360 GB)\nMemory reserved by apps: 107108 MB (104 GB)\nMemory actually used by apps: 32868 (32 GB)\n```\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\n\n## Decision\n","We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\n",We will provision 2x memory capacity per AZ.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of our deployment we have a pipeline, where changes that are made can move from a development environment through to production illustrated thusly:\n![pipeline image](../images/pipeline.jpg)\nThere are a number of externally available endpoints that are accessed to manage and view information about the platform, as well as issue commands via the Cloud Foundry API. In addition to this, a URL also needs to be available to access Apps hosted on the platform. These need to be accessed via some sort of sensible URL.\nThe reason for splitting system domains from app domains was to prevent applications from stealing traffic to CF components (for example, api.<domain>) or masquerading as official things of the platform (for example, signup.<domain>).\n### Naming considerations\nA number of aspects were considered as part of the naming process.\n* Clear sense of purpose\n* Clear distinction between Production and other Environments\n* No overly technical names (for example, hosting/paas/scalable-elastic-public-government-hosting)\n* Prevent possibility of domains suggesting 'live' service, for example if we allowed [app name].paas.gov.uk it could appear as thought they were live services.\n\n## Decision\n","For _non_ production environments we will be using the following domains:\n* [environment name].cloudpipeline.digital\n* [app name].[environment name].cloudpipelineapps.digital\nFor our production environment we will be using the following domains:\n* cloud.service.gov.uk\n* [app name].cloudapps.digital\nIt is important to note that live services will 'Bring Your Own' domain, apps available at cloudapps.digital are not live 'production' applications.\n","**Use a subdomain for the system domains and a separate subdomain for app domains.**\n\n**Rationale:**\n\n* This approach provides a clear sense of purpose for each subdomain, with system domains handling management and monitoring tasks and app domains hosting application traffic.\n* The distinction between Production and other environments is maintained by using different subdomains for each environment.\n* The names chosen avoid overly technical terms and are easy to understand and remember.\n* The use of a separate subdomain for app domains prevents confusion between system and app domains and eliminates the possibility of domains suggesting live services."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe want to enable, or provide the option to enable,\n[storage encryption (AKA encryption at rest) for the RDS instances](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)\nof the tenants databases created by our [RDS broker](https://github.com/alphagov/paas-rds-broker). The broker has logic to enable this option on creation.\nBut there are [some limitations](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations):\n* Storage Encryption can only be enabled on creation of the DB. There is no way to update an instance to enable or disable encryption. The only way is by creating a encrypted (or unencrypted) copy of a snapshot, to then restore it to a DB instance.\n* Storage Encryption is only supported [for some instance types](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#d0e47573). Specifically it is not supported for `db.t2.small` and `db.t2.micro.`, used in our Small and Free plans.\nIn consequence:\n* Users **can not** migrate from a plan with encryption (Medium or Large) to a plan without encryption (Small or Free). But this use case is less common.\n* Users **can** migrate from Free/Small to a plan with encryption (Medium or Large), but the instance will not have encryption enabled.\n* If we enable encryption in the existing plans, the existing databases will remain without encryption.\n* Due the API broker limitations, it is not possible to query the attributes of an existing instance.\nWe have 3 options to proceed:\n1. Enable Encryption in the Medium and Large plans, and document the restrictions.\n* Less effort to implement.\n* We might end having unencrypted database in a plan that is meant to be encrypted, which is confusing for the users and operators.\n* Existing Instances will remain unencrypted.\n2. Change the instance type for the Small plan to `db.m3.medium`.\n* Would allow migrate from Small to Medium or Large.\n* We will still have the problem for the Free plan.\n* Increases the costs for the Small plan (double).\n3. Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption.\n* It would be more explicit and clear, and the plan would match the state of the existing database.\n* Existing instances would still match with the plan description.\n* We will add more plans, which makes it more confusing for the tenants.\nDecision\n========\nWe decided to provide additional explicit plans with Encryption enabled, and keep the old ones.\nWe will add logic in the broker to prevent updates between plans with or without encryption.\nWe have decided only add the option of encryption to the HA plans to minimise the number of new plans added. In most of the cases the tenants would choose HA together with encryption and, although adding more plans is easy, removing them is painful once they are being used.\nStatus\n======\nAccepted.\nConsequences\n============\n* More plans will be added to the offer, which is more confusing the tenants. Better documentation would be required to help the tenants to pick the right plan.\n* It is not possible to migrate from a Non-Encrypted plan to an Encrypted plan, and vice versa.\n\n## Decision\n","========\nWe decided to provide additional explicit plans with Encryption enabled, and keep the old ones.\nWe will add logic in the broker to prevent updates between plans with or without encryption.\nWe have decided only add the option of encryption to the HA plans to minimise the number of new plans added. In most of the cases the tenants would choose HA together with encryption and, although adding more plans is easy, removing them is painful once they are being used.\nStatus\n======\nAccepted.\nConsequences\n============\n* More plans will be added to the offer, which is more confusing the tenants. Better documentation would be required to help the tenants to pick the right plan.\n* It is not possible to migrate from a Non-Encrypted plan to an Encrypted plan, and vice versa.\n","Provide additional explicit plans with Encryption enabled, and keep the old ones. Add logic to prevent updates between plans with or without encryption."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur service plans have evolved incrementally over the last few years and are in\nneed of some attention. Names are inconsistent, potentially confusing and\nin many cases contain irrelevant redundant information that is of no practical\nuse to the platform operators or to tenants consuming the service.\nAdding additional versions of services has the potential to compound the\nproblem by multiplying plans of different characteristics with different\nversions.\n\n## Decision\n","We have decided to use the following naming convention for naming plans going forward:\n```\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\n```\nWhere:\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\n* `HA` is the string `ha` to indicate highly available if relevent.\n* `LABEL` is a string describing some specific variant of the service if relvent.\n* `VERSION` is the version number of the service plan.\n### For example:\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\n### Example migrations of some existing plans:\n```\nL-HA-enc-dedicated-9.5 => large-ha-9.5\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\ntiny-clustered => tiny-clustered-3.2\ntiny-unclustered => tiny-unclustered-3.2\n```\n### Additionally:\n* We will avoid use of the word ""free"" in names.\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\n* We will avoid use of uppercase characters in names.\n* We will avoid abbriviations where possible (""medium"" instead of ""M"", ""large"" instead of ""L"").\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\n","**Standardize Service Plan Naming**\n\n**Goal:**\nCreate a consistent and meaningful naming convention for service plans to improve clarity and reduce confusion.\n\n**Rationale:**\n* Inconsistent and redundant plan names make it difficult for platform operators and tenants to identify and manage services.\n* Different versions of services exacerbate the problem, leading to a proliferation of plan options with varying characteristics.\n* A standardized naming convention will provide a clear and concise way to differentiate between plans, simplify navigation, and streamline service management.\n\n**Implementation Details:**\n\n* Establish a set of guidelines for plan naming that defines:\n    * Clear and descriptive prefixes and suffixes\n    * Removal of unnecessary or redundant information\n    * Consistent formatting and capitalization\n* Review existing plans and rename them according to the new naming convention.\n* Implement automated validation rules to ensure new plans adhere to the naming standards.\n* Update documentation and communication materials to reflect the new plan names.\n\n**Benefits:**\n\n* Improved clarity and understanding of service plans\n* Reduced confusion and ambiguity for platform operators and tenants\n* Simplified management and navigation of service offerings\n* Consistency and alignment between plans and their corresponding services\n* Reduced risk of misconfigurations due to plan confusion"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTenants provision a wide variety of services and AWS/Aiven resources via GOV.UK PaaS. We need to calculate bills for these services/resources. GOV.UK PaaS billing receives notifications of when services or resources are created, renamed, or deleted from upstream in the form of events from Cloud Foundry.\nThe original GOV.UK PaaS billing system translated the Cloud Foundry events into records of services/resources by calendar month before calculating the final monthly bill for each tenant. This process, called billing consolidation, was done at the start of every month and there was no persistent record of the results of each stage of processing, including what services or resources tenants had provisioned. After each stage of processing database tables were populated but the contents of these tables were impermanent, being refreshed the next time billing consolidation was run.\nIn the GOV.UK PaaS billing rewrite, this has been changed. We want to calculate bills for variable time periods and also to forecast bills for the future (for the web-based billing calculator). However, the method to calculate the actual bill always needs to be the same.\n\n## Decision\n","The code to calculate the bill has been decoupled from the code used to calculate the bill. This is so we can use exactly the same code for calculation of all bills, whether these bills are for tenants or for prospective tenants (using the billing calculator).\nThe approach taken is:\n1. Populate database temporary table with what is being billed (which resources over which time interval, including the future). This can be in a stored function or embedded SQL. This is the code entry point into billing.\n2. Call a stored function to calculate the bill ([`calculate_bill`](https://github.com/alphagov/paas-billing/blob/main/billing-db/sprocs/calculate_bill.sql)) only using the contents of the temporary table populated in step 1 above. No parameters are passed into ([`calculate_bill`](https://github.com/alphagov/paas-billing/blob/main/billing-db/sprocs/calculate_bill.sql)).\n","**Decision:** Introduce and use database tables to store only the latest state of billing data, to avoid garbage collection and provide the most up-to-date data for billing periods."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are starting to develop a number of user-facing applications with web\ninterfaces that need to be styled to look like GOV.UK etc. In order to keep\nthings consistent we want to pick a single programming language and framework\nto write these in.\nWe've previously used [Sinatra][] for this, but ran into\nissues with its default configuration which isn't secure, leading to an XSS\nvulnerability. We therefore want to choose something that comes with secure\ndefaults, and makes it easier to avoid this sort of issue.\nRequirements:\n* Must be well supported by [govuk_template][] and [govuk_frontend_toolkit][] (as\nwell as the future [govuk-frontend][] project)\n* Must be understood broadly by members of the team.\n* Must be understood broadly by members of the frontend developer community within GDS.\nAfter dicsussion with the head of the frontend community and members of the\nteam, the choice seems to be Rails for the following reasons:\n* Most of www.gov.uk is written in Rails, as is the Verify frontend, and\ntherefore is well known within the frontend developer community.\n* It's well supported by the frontend toolkits (both projects are available as\ngems that provide a Rails engine). Given the wide use of Rails with GDS, the\nfuture [govuk-frontend][] project is likely to support it.\n* It's the framework that's most familiar to our team.\n* It is opinionated, and comes with secure defaults making it much easier to\ncreate a secure web app.\n\n## Decision\n",We will use Ruby and Rails to create new user-facing applications with web frontends.\n,Rails is the chosen framework for developing user-facing applications with web interfaces that need to be styled like GOV.UK.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to the release of postgres 13 plans to tenants, the policy for choosing\nthe set of postgres extensions we allow for a certain major version was ad-hoc.\nA mixture of ""allow everything"", simply copying the list from the previous\npostgres version and not actually trying any of these extensions had lead\nto an extensions list (published at https://docs.cloud.service.gov.uk/deploying_services/postgresql/#add-or-remove-extensions-for-a-postgresql-service-instance)\nwith:\n- misnamed entries\n- entries removed from earlier postgres versions\n- missing newly-offered extensions\n- extensions that could never be used without superuser access\n- extensions that could never be used with the VPC restrictions we have\nin place for our RDS instances.\nSome of these extensions had been omitted from the list in the documentation,\nbut it was unclear how these decisions had been made and where we stood on\neach extension.\nThis is confusing for tenants and could lead them down a path of trying to\nuse an extension which will never work, or even start designing a system\nthat relies on functionality in a listed extension only to find it unusable.\n\n## Decision\n","For postgres 13 onwards, offer a selection of extensions limited to those\nwe know can be successfully enabled and think are feasible to use given\nthe limitations of our platform.\nMaintain a document (initially a spreadsheet https://docs.google.com/spreadsheets/d/100qBo3Q2mfY70ek9fNWbpEsS4HzjOarF3q1a_hPR1uU/edit?usp=sharing)\ntracking our conclusions on each extension by postgres major version.\nWhen preparing to offer a new postgres major version to tenants, a new\nsheet should be copied from the previous release's sheet and adjusted\naccording to the new list of extensions in the RDS documentation. New\nentries should be researched and decided upon whether they are feasible\nto use without superuser privileges and from within our VPC.\nUsing a dev environment with the new postgres available and a script\nsuch as the one found in\nhttps://github.com/alphagov/paas-rds-broker/tree/main/scripts (or preferably\nan improved one), each extension remaining in the ""final list"" should be\nchecked in turn to ensure it can be enabled and then disabled. This may\ninvolve discovering (through trial and error) which extensions have\ndependencies on others. It's likely to be the same as the previous release,\nso that's a good starting point. This is good information to maintain as it\nwould come in useful if we ever decided to make the rdsbroker smarter and able\nto auto-load dependencies.\nThis should also reveal whether any new extensions require\n`shared_preload_libraries`. Any that do should have appropriate additions\nmade to https://github.com/alphagov/paas-rds-broker/blob/main/rdsbroker/supported_extensions.go .\nBeyond making sure an extension loads, actually trying out the functionality\nof each extension is beyond the time committment we're willing to give\nthis for now. We may update the documentation to note the semi-supported\nnature of postgres extensions on our platform.\n","**Decision:**\n\nCreate a curated list of allowed extensions for each major Postgres version and publish it in the documentation.\n\n**Rationale:**\n\n* Standardizes the process for selecting allowed extensions, ensuring consistency and transparency.\n* Reduces confusion for tenants by providing a clear and comprehensive list of supported extensions.\n* Eliminates the risk of tenants attempting to use extensions that are not supported or do not work within the service's constraints.\n* Facilitates system design by providing tenants with reliable information about available functionality.\n\n**Implementation:**\n\n* Establish a process to review and evaluate extensions for each new major Postgres version.\n* Create a curated list of approved extensions, excluding those that:\n    * Are misnamed or defunct.\n    * Require superuser access or conflict with VPC restrictions.\n* Regularly update the documentation to reflect the latest list of approved extensions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR040 BOSH access without SOCKS](../ADR040-bosh-access-without-socks/) we removed the requirement for using a SOCKS5 proxy or SSH tunnel to access the User Account and Authentication Service (UAA).\nWe are moving towards a [zero trust network model](https://www.ncsc.gov.uk/blog-post/zero-trust-architecture-design-principles) and as part of this, we are removing the IP allow lists that have been in place.\nWe discussed the proposed methods with IA and Cyber after reviewing the [RFC created as part of #169915408](https://docs.google.com/document/d/1XZsrNp88tOSyC_bjy1mg3Yyv2TkpKgYSjoYResGAbps/edit#heading=h.xscqoqxlc072)\n\n## Decision\n","We will remove the reliance on IP allow lists for all services on the BOSH instance.\nMutual TLS will replace the allow lists.\nCyber prefer this method, as it give a much stronger authentication to the platform. This is due to authenticating both the individual and the machine that are accessing critical services.\n",We will remove all the IP allow lists and add an OAuth-based authentication method.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL/TCP mode ELB is not able to set any `X-Forwarded` header.\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http://serverfault.com/questions/775010/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\nThere are two options to workaround this:\n* Use a more complex configuration of HAProxy with two frontends/listeners chained\n* Disable SSL between ELB and HAProxy\nDecision\n========\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\nWe don't think this has any significant increase in risk because:\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\n* the inner interface of the ELB is on an internal network in our VPC\nStatus\n======\nAccepted\nConsequences\n============\nThe http traffic between ELB and HAProxy will not be encrypted.\n\n## Decision\n",========\nWe have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\nWe don't think this has any significant increase in risk because:\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\n* the inner interface of the ELB is on an internal network in our VPC\nStatus\n======\nAccepted\nConsequences\n============\nThe http traffic between ELB and HAProxy will not be encrypted.\n,Disable SSL encryption between the internal IP of the Elastic Load Balancer (ELB) and HAProxy to allow the use of ProxyProtocol.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use [Bosh](https://bosh.io/) to create and manage our cloudfoundry deployment on AWS.\nTo deploy software, Bosh needs certain binary dependencies available.\nThese are known as bosh [releases](https://bosh.io/docs/release.html).\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https://concourse-ci.org/) pipeline.\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\n### Building Bosh Releases\nWe investigated different approaches to creating bosh releases, in particular\n* Multiple pipelines created dynamically using [branch manager](https://github.com/alphagov/paas-concourse-branch-manager)\n* A single pipeline using [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource)\nThe work on these spikes was recorded in\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/115142265\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/128937731\n\n## Decision\n",We will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\n,We should continue building bosh releases manually and store them on GitHub for use in our Bosh deployment.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](../ADR006-rds-broker).\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\nThe broker must:\n1. Start the restore from snapshot, which can take minutes.\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\n3. Once that is finish, reset the master password of the RDS instance.\n4. Finally reset the passwords of the users previously bind in the original DB.\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\nThe rds-broker must respond accordingly.\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html). This requires a lot of additional work and integration effort.\n* Store the state in some database or k/v store.\n\n## Decision\n","We decided:\n* Implement a state machine using the [AWS tags](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html) of the instance.\nWe will add a list of tags for each pending operations to execute.\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\nWe assume that:\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\n* that update the tags is atomic and synchronous.\n",Store the state in a database or k/v store.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe offer our users the following backing services through [Aiven](https://aiven.io):\n- Elasticsearch\n- InfluxDB (private beta)\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\n- debug and respond to usage and service performance changes\n- understand the operational characteristics of their applications and services\n- make better capacity planning and budgeting decisions\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\n- shipping logs to an Elasticsearch/Rsyslog\n- sending metrics to Datadog\n- sending metrics to Aiven Postgres/InfluxDB\n- exposing metrics in Prometheus exposition format\nWe currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https://github.com/bosh-prometheus/prometheus-boshrelease) and have confidence and experience using it.\nWe will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.\n\n## Decision\n",We will use Prometheus to scrape Aiven-provided services.\nWe will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.\nWe will need to automate the following tasks:\n1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance.\n2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.\n,Use Prometheus with Aiven service integration to send metrics to InfluxDB in order to provide users with visibility into the metrics for their Aiven backing services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn our Concourse pipelines a locking mechanism was required to prevent concurrent deploys. Concurrent deploys cause\nproblems because:\n* one deployment may make changes which break another\n* we have availability tests which tell us if a specific change causes app or API downtime\n* Bosh has its own locking mechanisms, which mean concurrent deploys will fail.\n* concurrent testing puts extra load on the platform and Concourse\n\n## Decision\n","We decided to implement locking using the Concourse pool resource, AWS CodeCommit, and Terraform.\nThe Concourse pool resource was chosen because it is the Concourse-native solution for locking. This created a\ndependency on having a Git repository for the lock, as this is how the pool resource is implemented. AWS CodeCommit was\nchosen over Github for several reasons:\n* client libraries for interacting with the necessary APIs were much more mature for AWS and this particular use case.\nIt saved a lot of work.\n* Github would have required managing users and SSH keys, or tokens. AWS could use the existing instance profile of the\nConcourse VM.\n* Github would have meant the repository containing the locks would have been public.\nTerraform was required to allow our pipelines (the Concourse instance profile) to manage AWS CodeCommit. We used a\npattern whereby we allow the creation of IAM users under a specific name prefix, and allow adding these users to a\npredefined IAM group. The permissions of said group are defined in a private repository in Terraform configuration\nwhich is ran manually. This limits the permissions the Concourse instance profile can grant to users.\n",Implement a semaphore-based locking mechanism in Concourse to prevent concurrent deploys.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen building pipelines using concourse, we investigated using the [pool\nresource](https://github.com/concourse/pool-resource) in order to control flow\nthrough jobs. This was an alternative to the use of the\n[semver resource](https://github.com/concourse/semver-resource).\nThese 2 resources are both workarounds to solve the problem of triggering jobs\nwhen we haven't made changes to a resource.\nThe problem is that the pool resource relies on write access to a github repo,\nwhich means we must pass public keys that allow this access into the pipeline\nand deployed concourse instance - we want to minimise the number of credentials\nwe pass, and the semver resource relies on AWS credentials that are already\npassed.\n\n## Decision\n",We will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\n,Use the semver resource over the pool resource due to the reduced credential exposure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\netc.) use the APIs to manage or access IaaS resources.\nThe most common mechanism for authenticating the API calls is to create an\nIdentify and Access Management (IAM) user with the appropriate permissions,\ngenerate an Access Key ID and Secret Access Key for that user, and export\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\nmost utilities and libraries.\nThe problem with this approach is that it's very easy to accidentally leak\nthe plain text keys. They can appear in output from your shell, which you\nmight copy+paste into a gist or email when debugging a problem. You might\nadd them to your shell configuration or include them in a script, which can\nbe pushed to a public code repository.\nOur team have leaked keys like this on more than one occasion. It's worth\nnoting that even if you realise that you've done this, delete the commit and\nrevoke the keys, they may have already been used maliciously because\nautomated bots monitor sites like GitHub using the [events firehose][] to\ndetect any credentials.\n[events firehose]: https://developer.github.com/v3/activity/events/\nAs an alternative to using pre-generated keys, AWS recommends that you use\n[IAM roles and instance profiles][] when accessing the API from EC2\ninstances. You delegate permissions to the EC2 instance and temporary\ncredentials are made available from the instance metadata service. Most\ntools and libraries automatically support this. The credentials are\nregularly rotated and never need to be stored in configuration files.\n[IAM roles and instance profiles]: http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-roles-with-ec2\n\n## Decision\n","To reduce the likelihood of us leaking AWS keys we will use IAM roles and\ninstance profiles for all operations that run from EC2 instances. This\nincludes everything that happens within Concourse and Cloud Foundry.\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\nan [`aws:SourceIp` condition][condition] to\nenforce that IAM accounts for team members are only used from the office IP\naddresses.\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\nThe IAM roles, profiles, and policies will be managed by our\n[aws-account-wide-terraform][] repo.\n[aws-account-wide-terraform]: https://github.digital.cabinet-office.gov.uk/government-paas/aws-account-wide-terraform\n",Migrate the deployment environment to use IAM roles and instance profiles for authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wanted to open up access to tenant applications in our production environment.\nAs part of an earlier story, Pingdom checks were set up for a healthcheck application in CI, Staging, and Production. At this stage applications were not accessible from non-office IP addresses.\nThe problem we faced was a need to test the code introduced to make our production environment accessible to the world. Initially, we suggested applying the same change to our staging environment. However, this approach means all applications in staging will be accessible from anywhere.\nIf we use Pingdom to assert an application is accessible from the outside world then we need to remove the explicit rules (security groups) allowing Pingdom traffic. This means our CI environment would not be accessible to Pingdom probes.\n* [#116104189 - set up Pingdom](https://www.pivotaltracker.com/story/show/116104189)\n* [#115347323 - allow public access to tenant applications](https://www.pivotaltracker.com/story/show/115347323)\n\n## Decision\n","It was decided we would make the staging environment accessible to the outside world as well as production, and define future work for removing the CI Pingdom check and security groups allowing Pingdom probes, and setting up tests from the pipeline which use the Pingdom API.\nGiven that the advantages relate to the availability of our production environment, they outweigh not having an automated healthcheck on an application in our CI environment. However, we remain open to hearing solutions to providing healthchecks for CI in future.\n","To open up access to tenant applications in production, configure security groups on an application-by-application basis instead of opening up access to all."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to decide where to terminate TLS connections for public and tenant\nfacing endpoints and how to manage the corresponding private keys.\nWe had previously decided to only support HTTPS to both deployed applications\nand Cloud Foundry endpoints.\nAt the time of writing there were 4 endpoints to consider:\n- Deployed applications (gorouter). Accessed by the public.\n- CF API. Accessed by tenants.\n- UAA. Accessed by tenants.\n- Loggregator. Accessed by tenants.\n- SSH proxy. In theory accessed by tenants, but not working in our environment.\nWe had an existing credentials store suitable for storing the private keys at\nrest. Only a small number of engineers within the team can access; the same\nones that can make IAM changes using our account-wide terraform config.\nPlacing ELBs in front of public-facing services is an architectural pattern\nadvised by Amazon [in order to reduce attack\nsurface](https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf).\nSpecifically they advise that it helps withstand volumetric Denial of Service\nattacks; the ELB handles TCP connections and therefore the responsibility for\nhandling DDOS at Layer 4 and below resides with the ELB team.\nWe did a spike, where we attempted to place everything public-facing or\ntenant-facing behind ELBs. We found that:\n- In HTTP mode the ELBs do not support web sockets. This is known to break\nloggregator, which relies on them for log streaming. It would also prevent\ntenants from using web sockets within their applications.\n- When the ELB is in TCP mode, we have no way of communicating the client IP\naddress to the downstream service. Practical consequences of this would be\nthat tenants would be unable to see in their logs who is using their service or\ndo any access control based on client IP address.\nIn attempting to solve the second problem, we explored some options:\n- ELB has support for the [Proxy\nProtocol](http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt), but\nunfortunately none of the downstream services, such as gorouter, support it. It\nseemed simple to add support to gorouter.\n- We could introduce another intermediary proxy such as HAProxy, which\nunderstands the proxy protocol and adds or appends to an `X-Forwarded-For`\nheader with the client IP address as provided via the proxy protocol.\n\n## Decision\n",We decided to:\n- use the ELB to terminate TLS\n- use the ELB in TCP mode\n- submit proxy protocol support to gorouter\n- use S3 logging to ensure we have the IP addresses of clients using the CF\nendpoint\n,"Do not terminate TLS at the ELBs. Instead, terminate TLS at the individual backend applications and Cloud Foundry services. All traffic to the applications and services should be in plain text."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nStories: [#123490171](https://www.pivotaltracker.com/story/show/123490171) & [#121933113](https://www.pivotaltracker.com/story/show/121933113)\nWe were investigating how to avoid downtime to deployed applications when we\nmade changes to the platform. We had discovered that short outages occurred\nwhen the gorouter was taken out of service.\ngorouter has drain functionality to allow upstream load balancers to gracefully\ntake instances of the gorouter out of service before any requests start to\nfail.\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor/1.1`\nstart failing with HTTP 503. User requests are allowed to continue.\nTwo things prevented us from using drain mode:\n- we had the ELB configured to use TCP healthchecks, not HTTP\n- the ELB sends HTTP healthcheck requests with `User-Agent:\nELB-HealthChecker/1.0`, which means they were not recognised as healthcheck\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\ndraining gorouter out of service.\nThe result was a very small amount of downtime for deployed apps that received\nrequests during a short window of < 1 second.\nDecision\n========\nWe decided to:\n- submit a change upstream to [allow the gorouter to recognise ELB\nhealthchecks](https://github.com/cloudfoundry/gorouter/pull/138)\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\nwhich appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects\n- enable HTTP healthchecks on the ELB\nStatus\n======\nAccepted\nConsequences\n============\n- There was less downtime for deployed applications during a deploy.\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\n\n## Decision\n","========\nWe decided to:\n- submit a change upstream to [allow the gorouter to recognise ELB\nhealthchecks](https://github.com/cloudfoundry/gorouter/pull/138)\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\nwhich appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects\n- enable HTTP healthchecks on the ELB\nStatus\n======\nAccepted\nConsequences\n============\n- There was less downtime for deployed applications during a deploy.\n- We have an additional reason to keep the intermediate HAProxy we introduced as a temporary measure.\n","- Submit a change upstream to allow the gorouter to recognise ELB healthchecks.\n- Implement a healthchecking port 82, in the HAProxy, which appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects.\n- Enable HTTP healthchecks on the ELB."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith the implementation of ADR021 we have reduced the number of cells in\nproduction in order to make more efficent use of our budget. This in turn means\nthat we have increased the load on the individual cells. Originally the idle CPU\nmonitor was set in line with the free memory on cells monitor (for alerting on\na need to scale the cell pool), however CPU usage does not appear to grow\nlinearly with allocated memory for tenant applications.\n\n## Decision\n","In order to avoid false positives from triggering due to CPU load spiking rather\nthan being a constant level we will increase the monitoring window to 24 hours.\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\nthresholds to warn at 37% and become critical at 33%.\n",The minimum CPU utilization threshold should be reduced to 40% to ensure that cells are scaled up before they become overloaded.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\nThere are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\nDecision\n========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n\n## Decision\n","========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n","**Decision:**\nTo encrypt traffic between the user and application instances via TLS for the User to ELB and ELB to router sections, and via IPSec for the Router to cells section.\n\n**Rationale:**\n- TLS is used between the user and the ELB to protect data in transit, as it is a widely accepted and secure protocol.\n- The ELB uses a certificate issued by a trusted certificate authority (CA), and access to the private key is strictly controlled.\n- The ELB connects to the router VM via TLS, ensuring that data in transit is protected.\n- The Cloud Foundry platform does not support TLS between the gorouter and application instances, and some application instances may not be able to speak TLS. Therefore, IPSec is used to encrypt traffic between the router and application instances transparently.\n\n**Consequences:**\n- Secure and encrypted traffic between the user and application instances, protecting data from network eavesdropping."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe alpha initially took the approach of starting with a vanilla set of cloud\nfoundry manifests, and merging new values into it using spiff. This became\ndifficult to reason about, and cf-release was forked because it was easier than\noverriding necessary values using spiff. However, the confusing spiff hierarchy\nremained.\n\n## Decision\n",We will create our own set of manifests based on those in cf-release.\nWe will modify these as required.\nWe will use spruce to merge a series of files into the yml required by cloud\nfoundry\nWe will name the files with a numeric prefix and rely on shell globbing to\ndetermine the merge order rather than listing the merge order in the\nbuild-manifest script.\n,Refactor to declarative manifests that are generated from a common set of data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nStories: [#123490171](https://www.pivotaltracker.com/story/show/123490171) & [#121933113](https://www.pivotaltracker.com/story/show/121933113)\nWe were investigating how to avoid downtime to deployed applications when we\nmade changes to the platform. We had discovered that short outages occurred\nwhen the gorouter was taken out of service.\ngorouter has drain functionality to allow upstream load balancers to gracefully\ntake instances of the gorouter out of service before any requests start to\nfail.\nWhen a USR1 signal is sent to instruct the gorouter to start draining,\nhealthchecking requests, identified by the header `User-Agent: HTTP-Monitor/1.1`\nstart failing with HTTP 503. User requests are allowed to continue.\nTwo things prevented us from using drain mode:\n- we had the ELB configured to use TCP healthchecks, not HTTP\n- the ELB sends HTTP healthcheck requests with `User-Agent:\nELB-HealthChecker/1.0`, which means they were not recognised as healthcheck\nrequests by gorouter, they returned HTTP 200 and the ELB did not take the\ndraining gorouter out of service.\nThe result was a very small amount of downtime for deployed apps that received\nrequests during a short window of < 1 second.\n\n## Decision\n","We decided to:\n- submit a change upstream to [allow the gorouter to recognise ELB\nhealthchecks](https://github.com/cloudfoundry/gorouter/pull/138)\n- implement a healhchecking port 82, in the HAProxy we introduced in ADR008,\nwhich appends the `User-Agent: HTTP-Monitor/1.1` that gorouter expects\n- enable HTTP healthchecks on the ELB\n","**Decision:** Re-configure the ELB to use HTTP healthchecks, and update gorouter to handle HTTP healthchecks with `User-Agent: ELB-HealthChecker/1.0`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe use [Bosh](https://bosh.io/) to create and manage our cloudfoundry deployment on AWS.\nTo deploy software, Bosh needs certain binary dependencies available.\nThese are known as bosh [releases](https://bosh.io/docs/release.html).\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https://concourse.ci/) pipeline.\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\n### Building Bosh Releases\nWe investigated different approaches to creating bosh releases, in particular\n* Multiple pipelines created dynamically using [branch manager](https://github.com/alphagov/paas-concourse-branch-manager)\n* A single pipeline using [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource)\nThe work on these spikes was recorded in\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/115142265\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/128937731\nDecision\n========\nWe will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\nStatus\n======\nAccepted\nConsequences\n============\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\nWe may have to add additional config in projects we fork to allow us to create final builds.\n\n## Decision\n",========\nWe will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\nStatus\n======\nAccepted\nConsequences\n============\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\nWe may have to add additional config in projects we fork to allow us to create final builds.\n,We will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe have a single pipeline `create-cloudfoundry` which creates a Cloud Foundry\ndeployment, and also deploys additional services to the platform.\nThese services include:\n- PaaS Accounts\n- PaaS Admin\n- PaaS Billing\n- PaaS Metrics\nWhich are core components to our platform, but not to Cloud Foundry.\nCurrently these services are unnecessarily coupled in a couple of places:\n- The `post-deploy` job\n- The `custom-acceptance-tests` job\nUnnecessarily coupling has resulted in flakey app tests blocking CVE\nremediation from reaching production.\n# Decision\nMove, where possible, PaaS services into their own jobs (within the same\n`create-cloudfoundry` pipeline) such that they do not impede progress of\ndeployment to the core platform.\n# Status\nAccepted\n# Consequences\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\nThe individual jobs in the pipeline will be less mysterious.\n\n## Decision\n","Move, where possible, PaaS services into their own jobs (within the same\n`create-cloudfoundry` pipeline) such that they do not impede progress of\ndeployment to the core platform.\n# Status\nAccepted\n# Consequences\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\nThe individual jobs in the pipeline will be less mysterious.\n",Move PaaS services into their own jobs (within the `create-cloudfoundry` pipeline) such that they do not impede progress of deployment to the core platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe will only [serve HTTPS traffic, keeping TCP port 80 (HTTP) closed and use HSTS preload lists](ADR443-ssl-only-for-applications-and-cf-endpoints.md).\nTo add our domains to [HSTS preload lists](https://hstspreload.appspot.com/), there are these requirements:\n1. Serve a valid certificate.\n2. Redirect from HTTP to HTTPS on the same host.\n3. Serve all subdomains over HTTPS (actually checks for `www.domain.com`)\n4. Serve an HSTS header on the base domain for HTTPS requests:\nWe need an endpoint to provide these requirements.\nOur Cloud Foundry app endpoint already [serves the\nright HSTS Security header with HAProxy](ADR008-haproxy-for-request-rewriting.md)\nand could be configured to serve the additional `preload` and `includeSubDomains` flags,\nbut we cannot use it because we keep port 80 (HTTP) closed for this endpoint.\nWe can implement a second ELB to listening on HTTP and HTTPS and use\nHAProxy to do the HTTP to HTTPS redirect and serve the right header.\nBut this increases our dependency on the HAProxy service.\nWe must serve from the root domain (or apex domain), but it is not allowed to\nserve [CNAME records in the root/apex domain](http://serverfault.com/questions/613829/why-cant-a-cname-record-be-used-at-the-apex-aka-root-of-a-domain). We must configure A records in this domain. This can be\nan issue when serving the service using ELB or CloudFront.\nDecision\n========\n* We will implement a basic [AWS API Gateway](https://aws.amazon.com/api-gateway/)\nwith a default [MOCK response](https://aws.amazon.com/about-aws/whats-new/2015/09/introducing-mock-integration-generate-api-responses-from-api-gateway-directly/)\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\ncontent of the response is irrelevant, it can be a 302.\nA [Custom Domain Name](http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nwhich creates a [AWS Cloud Front distribution](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html),\nwill provide public access to this API.\n* We will use [AWS Route 53 `ALIAS` resource record](http://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html)\nto [serve the IPs of the AWS Cloud Front distribution as A records](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html).\nStatus\n======\nAccepted\nConsequences\n============\nTo setup AWS API Gateway Domain Names, it is required access to the SSL certificates. There is the option of uploading the certificates in a different step and create the AWS Cloud Front distribution manually.\n\n## Decision\n","========\n* We will implement a basic [AWS API Gateway](https://aws.amazon.com/api-gateway/)\nwith a default [MOCK response](https://aws.amazon.com/about-aws/whats-new/2015/09/introducing-mock-integration-generate-api-responses-from-api-gateway-directly/)\nthat returns the right HTTP header `Strict-Transport-Security`. The actual\ncontent of the response is irrelevant, it can be a 302.\nA [Custom Domain Name](http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nwhich creates a [AWS Cloud Front distribution](http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html),\nwill provide public access to this API.\n* We will use [AWS Route 53 `ALIAS` resource record](http://docs.aws.amazon.com/Route53/latest/APIReference/CreateAliasRRSAPI.html)\nto [serve the IPs of the AWS Cloud Front distribution as A records](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html).\nStatus\n======\nAccepted\nConsequences\n============\nTo setup AWS API Gateway Domain Names, it is required access to the SSL certificates. There is the option of uploading the certificates in a different step and create the AWS Cloud Front distribution manually.\n",* Implement a basic AWS API Gateway with a default MOCK response that returns the right HTTP header Strict-Transport-Security.\n* Use AWS Route 53 ALIAS resource record to serve the IPs of the AWS Cloud Front distribution as A records.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.\nIt was paused because some IA issues with Logit were not resolved. At one point\nRE recommended that PaaS should host our own logstash as this part of the\nservice was not widely available by market Elastic SaaS providers. The PaaS\nteam was also considering to use Elasticsearch on AWS with our accounts.\n\n## Decision\n,"An updated conversation with the RE tool team has confirmed that the IA issues\nhad been resolved, and that GDS can continue to use Logit for now.\nIt is a GDS strategy to use a consistent logging solution. Hence, we should\ncontinue our migration of platform logs to logit, including our logstash\nfilters.\nThere is considerably less maintenance work for us if we use Logit's logstash\nfilter rather than hosting the bosh release one. In the future if GDS choose to\nuse another vendor that do not have a hosted logstash solution, they would need\nto provide a migration strategy for all the current logstash users.\n",**Decision:** Continue working on shipping cloud foundry platform logs to Logit.\n\n**Rationale:**\n\n* Logit is a market-leading provider of log management services.\n* Logit has resolved the IA issues that were previously preventing the integration with Cloud Foundry.\n* PaaS is not currently equipped to host its own logstash or Elasticsearch on AWS.\n* Continuing to work on the Logit integration will provide PaaS with a reliable and scalable solution for managing Cloud Foundry platform logs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nIt is expected for the government websites to be secure and keep the user\ninteractions private. Because that we want to enforce all communications to\nany application and to the platform endpoints to use only and always HTTPS,\nas [it is described in the Gov Service Manual](https://www.gov.uk/service-manual/technology/using-https).\nWhen a user inputs a website name without specifying the\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\nEven if the server always redirect HTTP to HTTPS, an initial\nunprotected request including user information will be transferred\nin clear: full URL with domain, parameter, [cookies without secure flag](https://en.wikipedia.org/wiki/HTTP_cookie#Secure_and_HttpOnly)\nor browser meta-information.\n[HTTP Strict Transport Security](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security)\nmitigates this issue by instructing modern browsers that support it to\nalways connect using HTTPS.\nThis is also a [requirement in the service manual](https://www.gov.uk/service-manual/technology/using-https).\nThere is still a potential initial unprotected HTTP request that might happen\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\nTo solve this issue, the root domain can be added to\n[HSTS preload list](https://hstspreload.appspot.com/) which will be used by most\ncommon browsers.\nCurrently the only way to avoid any clear text HTTP interaction is closing or\ndropping any attempt to connect to the port 80 at TCP level.\nAlthough not all application deployed on the PaaS will be ""services""\nas in the service manual meaning, we must not allow HTTP to make\nit easier to service owners to comply with this requirements.\nDecision\n========\nWe will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\nWe will implement and maintain HSTS preload lists for our production domains.\nStatus\n======\nAccepted\nConsequences\n============\nWe must configure and maintain our domain in the HSTS preload lists.\nUsers of browsers which do not support HSTS, or HSTS preload lists, will not\nbe able to connect to the sites without specify the protocol `https://` in\nthe URL. This only happens when the user manually inputs the URL in the\nbrowser.\n\n## Decision\n","========\nWe will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\nWe will implement and maintain HSTS preload lists for our production domains.\nStatus\n======\nAccepted\nConsequences\n============\nWe must configure and maintain our domain in the HSTS preload lists.\nUsers of browsers which do not support HSTS, or HSTS preload lists, will not\nbe able to connect to the sites without specify the protocol `https://` in\nthe URL. This only happens when the user manually inputs the URL in the\nbrowser.\n",We will only open port 443 (HTTPS) and drop/reject any TCP connection to TCP port 80 (HTTP).\n\nWe will implement and maintain HSTS preload lists for our production domains.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n\n## Decision\n","We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\n",Remove the ability for users other than the one that created the database to modify the database schema.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to pass correct client IP and requested protocol to applications deployed to the platform. To achieve this we want to use X-Forwarded-For and X-Forwarded-Proto headers.\nIn the current setup we've got HAProxy behind ELB to allow insert HSTS headers, and ELB is configured in SSL mode (not HTTPS) because it does not support WebSockets. In SSL/TCP mode ELB is not able to set any `X-Forwarded` header.\nThe solution is to use ProxyProtocol to pass information about recorded client IP and protocol to HAProxy which can set required headers for us. Unfortunately [ELB sets ProxyProtocol header inside SSL stream and HAProxy expects it outside](http://serverfault.com/questions/775010/aws-elb-with-ssl-backend-adds-proxy-protocol-inside-ssl-stream).\nThere are two options to workaround this:\n* Use a more complex configuration of HAProxy with two frontends/listeners chained\n* Disable SSL between ELB and HAProxy\n\n## Decision\n",We have decided to disable SSL encryption between internal IP of ELB and HAProxy to allow us to use ProxyProtocol.\nWe don't think this has any significant increase in risk because:\n* gorouter to cell traffic is already HTTP (it has to be because we can't do termination in app containers)\n* the inner interface of the ELB is on an internal network in our VPC\n,Disable SSL between ELB and HAProxy
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n-------\n### Current plans\nAs of 2019-09-03 [GOV.UK PaaS](https://cloud.service.gov.uk) offers the following postgres and mysql\nplans to everyone:\n#### MySql\n| service plan           | description                                                                                                                    | free or paid |\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------|--------------|\n|#  tiny-unencrypted-5.7   | 5GB Storage, NOT BACKED UP, Dedicated Instance. MySQL Version 5.7. DB Instance Class: db.t2.micro.                             | free |\n|#  medium-ha-5.7          | 100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.large.     | paid |\n|#  large-ha-5.7           | 512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.2xlarge.   | paid |\n|#  xlarge-ha-5.7          | 2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.4xlarge.     | paid |\n|#  small-ha-5.7           | 20GB Storage, Dedicated Instance, Highly Available. Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.t2.small.      | paid |\n|#  small-5.7              | 20GB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.t2.small.                        | paid |\n|#  medium-5.7             | 100GB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.large.                       | paid |\n|#  large-5.7              | 512GB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.2xlarge.                     | paid |\n|#  xlarge-5.7             | 2TB Storage, Dedicated Instance, Storage Encrypted. MySQL Version 5.7. DB Instance Class: db.m4.4xlarge.                       | paid |\n#### Postgres\n| service plan           | description                                                                                                                                                        | free or paid |\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n|#  tiny-unencrypted-9.5   | 5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.t2.micro.                               | free |\n|#  medium-ha-9.5          | 100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.large.      | paid |\n|#  large-ha-9.5           | 512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.2xlarge.   | paid |\n|#  xlarge-ha-9.5          | 2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.4xlarge.     | paid |\n|#  small-ha-9.5           | 20GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.t2.small.       | paid |\n|#  small-9.5              | 20GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.t2.small.                         | paid |\n|#  medium-9.5             | 100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.large.                        | paid |\n|#  large-9.5              | 512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.2xlarge.                     | paid |\n|#  xlarge-9.5             | 2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 9.5. DB Instance Class: db.m4.4xlarge.                       | paid |\n|#  tiny-unencrypted-10    | 5GB Storage, NOT BACKED UP, Dedicated Instance, Max 50 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.micro.                                | free |\n|#  small-10               | 20GB Storage, Dedicated Instance, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.                          | paid |\n|#  small-ha-10            | 20GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 200 Concurrent Connections. Postgres Version 10. DB Instance Class: db.t2.small.        | paid |\n|#  medium-10              | 100GB Storage, Dedicated Instance, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.                         | paid |\n|#  medium-ha-10           | 100GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 500 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.large.       | paid |\n|#  large-10               | 512GB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.                      | paid |\n|#  large-ha-10            | 512GB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.2xlarge.    | paid |\n|#  xlarge-10              | 2TB Storage, Dedicated Instance, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.                        | paid |\n|#  xlarge-ha-10           | 2TB Storage, Dedicated Instance, Highly Available, Storage Encrypted, Max 5000 Concurrent Connections. Postgres Version 10. DB Instance Class: db.m4.4xlarge.      | paid |\n### Current analysis\nIn [#167839682](https://www.pivotaltracker.com/n/projects/1275640/stories/167839682) we had a look\nat the usage on our existing databases and concluded that, for the most part, the existing plans are\nappropriate for current usage. We didn't identify any databases that could easily have been on a\nsmaller instance size if only it had higher IOPS (which was one hypothesis).\nThere was some evidence in the form of support tickets that our current small plans don't have\nenough IOPS, causing people to prematurely upgrade to mediums.  AWS warn you against creating DBs\nwith less than 100G disk due to IOPS restrictions, so it seems sensible that we shouldn't provided\nplans smaller than that.\nAt the moment, prices for postgres 10.5 Tiny, Small and Medium instances are:\n| plan      | price / month |\n|-----------|---------------|\n|# tiny      | £12.03        |\n|# small     | £24.50        |\n|# small-ha  | £48.98        |\n|# medium    | £125.94       |\n|# medium-ha | £251.79       |\nIf we were to increase the disk size on small instances from the current 2G to 100G, this would\ncause the small plans to increase roughly as follows (based on Ireland prices):\n| plan      | price / month       |\n|-----------|---------------------|\n|# tiny      | £12.03              |\n|# small     | £34.40 (was £24.50) |\n|# small-ha  | £67.88 (was £48.98) |\n|# medium    | £125.94             |\n|# medium-ha | £251.79             |\nWe have about 100 ""small"" databases, of which 28 are HA and 70 are not. This means if we changed the\ndisk on the existing plans our tenants would have to pay an extra £1,200/month (but on the flip\nside, they might be able to stick with small plans for longer, because of the better IOPS).\nAlternatively we could add a new pair of plans like ""small-high-iops"" and ""small-ha-high-iops"".\nThis would allow an upgrade path of `small -> small-high-iops -> medium`, instead of the current\n`small -> medium`, which would avoid people paying for CPU and memory they don't need.\nSeparately, we should add support for postgres 11 and mysql 8, which are both supported by RDS now.\nFor these new plans we should use the newest available instance types (so t3 / m5 instead of t2 /\nm4).\nDecision\n--------\n### Create new plans for postgres 11\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322288):\n| service plan          | summary                                                     |\n|-----------------------|-------------------------------------------------------------|\n|# tiny-unencrypted-11   | 5GB Storage, NOT BACKED UP. DB Instance Class: db.t3.micro. |\n|# small-11              | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# small-ha-11           | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# medium-11             | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# medium-ha-11          | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# large-11              | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# large-ha-11           | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# xlarge-11             | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n|# xlarge-ha-11          | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new plans for mysql 8\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322475):\n| service plan       | summary                                          |\n|--------------------|--------------------------------------------------|\n|# tiny-unencrypted-8 | 5GB Storage. DB Instance Class: db.t3.micro.     |\n|# small-8            | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# small-ha-8         | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# medium-8           | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# medium-ha-8        | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# large-8            | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# large-ha-8         | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# xlarge-8           | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n|# xlarge-ha-8        | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new ""small-high-iops"" and ""small-ha-high-iops"" plans, for mysql 5.7, postgres 9.5 and 10\nWe should play [a story to add ""High IOPS"" plans for the existing postgres / mysql versions](https://www.pivotaltracker.com/story/show/168322617).\nThese should have 100G of storage and be on the `t3.small` instance type (since there's no reason to\nbe adding new plans on old instance types).\nThis story may turn out not to be a priority.\n### Do not change the instance types for the existing plans\nWe considered whether to upgrade existing databases to new m5 / t3 instances, or to make it so that\nnew databases on old versions (i.e. postgres 9.5 / 10, mysql 5.7) get latest-generation instance\ntypes.\nWe decided not to do this because it would be tricky to administer the change, and it's nice to have\nthe new instance types as an incentive for people to upgrade.\nStatus\n------\nAccepted\nConsequences\n------------\n* Small databases for the newer engine versions (postgres 11 / mysql 8) will be more expensive and\nhave more disk.\n* Some tenants may be able to stick with small databases for longer before they have to upgrade to\nthe medium plan, saving them money.\n* Tenants will be able to get higher performing databases by using the newest engine versions\n(postgres 11 / mysql 8).\n* It will be possible for us to implement ""downgrading"" from `medium` to `small` databases for the\nnew plans in the broker (since they have the same sized disks) - this could save tenants money if\nthey optimise their database usage and discover they could move from a `medium` to a `small`.\n* We will have 9 to 11 more plans for postgres and mysql.\n\n## Decision\n","--------\n### Create new plans for postgres 11\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322288):\n| service plan          | summary                                                     |\n|-----------------------|-------------------------------------------------------------|\n|# tiny-unencrypted-11   | 5GB Storage, NOT BACKED UP. DB Instance Class: db.t3.micro. |\n|# small-11              | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# small-ha-11           | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# medium-11             | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# medium-ha-11          | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# large-11              | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# large-ha-11           | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# xlarge-11             | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n|# xlarge-ha-11          | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new plans for mysql 8\nWe should play [a story to add the following new plans](https://www.pivotaltracker.com/story/show/168322475):\n| service plan       | summary                                          |\n|--------------------|--------------------------------------------------|\n|# tiny-unencrypted-8 | 5GB Storage. DB Instance Class: db.t3.micro.     |\n|# small-8            | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# small-ha-8         | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# medium-8           | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# medium-ha-8        | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# large-8            | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# large-ha-8         | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# xlarge-8           | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n|# xlarge-ha-8        | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n(note: t3 / m5 instances, small plans have 100G storage instead of 20G)\n### Create new ""small-high-iops"" and ""small-ha-high-iops"" plans, for mysql 5.7, postgres 9.5 and 10\nWe should play [a story to add ""High IOPS"" plans for the existing postgres / mysql versions](https://www.pivotaltracker.com/story/show/168322617).\nThese should have 100G of storage and be on the `t3.small` instance type (since there's no reason to\nbe adding new plans on old instance types).\nThis story may turn out not to be a priority.\n### Do not change the instance types for the existing plans\nWe considered whether to upgrade existing databases to new m5 / t3 instances, or to make it so that\nnew databases on old versions (i.e. postgres 9.5 / 10, mysql 5.7) get latest-generation instance\ntypes.\nWe decided not to do this because it would be tricky to administer the change, and it's nice to have\nthe new instance types as an incentive for people to upgrade.\nStatus\n------\nAccepted\nConsequences\n------------\n* Small databases for the newer engine versions (postgres 11 / mysql 8) will be more expensive and\nhave more disk.\n* Some tenants may be able to stick with small databases for longer before they have to upgrade to\nthe medium plan, saving them money.\n* Tenants will be able to get higher performing databases by using the newest engine versions\n(postgres 11 / mysql 8).\n* It will be possible for us to implement ""downgrading"" from `medium` to `small` databases for the\nnew plans in the broker (since they have the same sized disks) - this could save tenants money if\nthey optimise their database usage and discover they could move from a `medium` to a `small`.\n* We will have 9 to 11 more plans for postgres and mysql.\n","We should play a story to add the following new plans:\n| service plan          | summary                                                     |\n|-----------------------|-------------------------------------------------------------|\n|# tiny-unencrypted-11   | 5GB Storage, NOT BACKED UP. DB Instance Class: db.t3.micro. |\n|# small-11              | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# small-ha-11           | 100GB Storage. DB Instance Class: db.t3.small.              |\n|# medium-11             | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# medium-ha-11          | 100GB Storage. DB Instance Class: db.m5.large.              |\n|# large-11              | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# large-ha-11           | 512GB Storage. DB Instance Class: db.m5.2xlarge.            |\n|# xlarge-11             | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n|# xlarge-ha-11          | 2TB Storage. DB Instance Class: db.m5.4xlarge.              |\n<br>\nWe should play a story to add the following new plans:\n| service plan       | summary                                          |\n|--------------------|--------------------------------------------------|\n|# tiny-unencrypted-8 | 5GB Storage. DB Instance Class: db.t3.micro.     |\n|# small-8            | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# small-ha-8         | 100GB Storage. DB Instance Class: db.t3.small.   |\n|# medium-8           | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# medium-ha-8        | 100GB Storage. DB Instance Class: db.m5.large.   |\n|# large-8            | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# large-ha-8         | 512GB Storage. DB Instance Class: db.m5.2xlarge. |\n|# xlarge-8           | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n|# xlarge-ha-8        | 2TB Storage. DB Instance Class: db.m5.4xlarge.   |\n<br>\nWe should play a story to add ""High IOPS"" plans for the existing postgres / mysql versions.\nThese should have 100G of storage and be on the `t3.small` instance type (since there's no reason to\nbe adding new plans on old instance types).\n<br>\nWe considered whether to upgrade existing databases to new m5 / t3 instances, or to make it so that\nnew databases on old versions (i.e. postgres 9.5 / 10, mysql 5.7) get latest-generation instance\ntypes.\n<br>\nWe decided not to do this because it would be tricky to administer the change, and it's nice to have\nthe new instance types as an incentive for people to upgrade."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\netc.) use the APIs to manage or access IaaS resources.\nThe most common mechanism for authenticating the API calls is to create an\nIdentify and Access Management (IAM) user with the appropriate permissions,\ngenerate an Access Key ID and Secret Access Key for that user, and export\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\nmost utilities and libraries.\nThe problem with this approach is that it's very easy to accidentally leak\nthe plain text keys. They can appear in output from your shell, which you\nmight copy+paste into a gist or email when debugging a problem. You might\nadd them to your shell configuration or include them in a script, which can\nbe pushed to a public code repository.\nOur team have leaked keys like this on more than one occasion. It's worth\nnoting that even if you realise that you've done this, delete the commit and\nrevoke the keys, they may have already been used maliciously because\nautomated bots monitor sites like GitHub using the [events firehose][] to\ndetect any credentials.\n[events firehose]: https://developer.github.com/v3/activity/events/\nAs an alternative to using pre-generated keys, AWS recommends that you use\n[IAM roles and instance profiles][] when accessing the API from EC2\ninstances. You delegate permissions to the EC2 instance and temporary\ncredentials are made available from the instance metadata service. Most\ntools and libraries automatically support this. The credentials are\nregularly rotated and never need to be stored in configuration files.\n[IAM roles and instance profiles]: http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-roles-with-ec2\nDecision\n========\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\ninstance profiles for all operations that run from EC2 instances. This\nincludes everything that happens within Concourse and Cloud Foundry.\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\nan [`aws:SourceIp` condition][condition] to\nenforce that IAM accounts for team members are only used from the office IP\naddresses.\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\nThe IAM roles, profiles, and policies will be managed by our\n[aws-account-wide-terraform][] repo.\n[aws-account-wide-terraform]: https://github.gds/government-paas/aws-account-wide-terraform\nStatus\n======\nAccepted\nConsequences\n============\nWe'll still need to use AWS keys for operations that run outside of EC2.\nCare must be taken when storing and handling these credentials. These\noperations include:\n- Creation of Bootstrap Concourse instance\n- Running of `aws-account-wide-terraform`\nUsing IAM profiles has the drawback that any process running on the VM can\nget the same credentials. This model does not play well when it is required\nto assign the credentials to specific processes running in different containers\n(for example concourse, CF apps), as all the containers will have access to\nthe AWS IAM profile.\nWe'll need to maintain our own forks of some standard Concourse resources to\nadd support for IAM roles and instance profiles because the maintainers\ndon't wish to support this feature ([concourse/s3-resource/pull/22][]).\nThese resources include:\n[concourse/s3-resource/pull/22]: https://github.com/concourse/s3-resource/pull/22\n- [alphagov/paas-s3-resource](https://github.com/alphagov/paas-s3-resource)\n- [alphagov/paas-semver-resource](https://github.com/alphagov/paas-semver-resource)\nWe'll need to use the office VPN to administer AWS when outside of the\noffice. This matches what we have to do to administer Concourse or Cloud\nFoundry from outside the office. There are disaster recovery provisions for\nthe VPN if the office has connectivity problems.\n\n## Decision\n","========\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\ninstance profiles for all operations that run from EC2 instances. This\nincludes everything that happens within Concourse and Cloud Foundry.\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\nan [`aws:SourceIp` condition][condition] to\nenforce that IAM accounts for team members are only used from the office IP\naddresses.\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\nThe IAM roles, profiles, and policies will be managed by our\n[aws-account-wide-terraform][] repo.\n[aws-account-wide-terraform]: https://github.gds/government-paas/aws-account-wide-terraform\nStatus\n======\nAccepted\nConsequences\n============\nWe'll still need to use AWS keys for operations that run outside of EC2.\nCare must be taken when storing and handling these credentials. These\noperations include:\n- Creation of Bootstrap Concourse instance\n- Running of `aws-account-wide-terraform`\nUsing IAM profiles has the drawback that any process running on the VM can\nget the same credentials. This model does not play well when it is required\nto assign the credentials to specific processes running in different containers\n(for example concourse, CF apps), as all the containers will have access to\nthe AWS IAM profile.\nWe'll need to maintain our own forks of some standard Concourse resources to\nadd support for IAM roles and instance profiles because the maintainers\ndon't wish to support this feature ([concourse/s3-resource/pull/22][]).\nThese resources include:\n[concourse/s3-resource/pull/22]: https://github.com/concourse/s3-resource/pull/22\n- [alphagov/paas-s3-resource](https://github.com/alphagov/paas-s3-resource)\n- [alphagov/paas-semver-resource](https://github.com/alphagov/paas-semver-resource)\nWe'll need to use the office VPN to administer AWS when outside of the\noffice. This matches what we have to do to administer Concourse or Cloud\nFoundry from outside the office. There are disaster recovery provisions for\nthe VPN if the office has connectivity problems.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of GOV.UK PaaS we have a number of applications that we consider to be\nan essential part of the platform deployed as cloud foundry apps. These include:\n* paas-admin\n* paas-billing\n* paas-accounts\nPrior to July 2019 the versions of these applications were pinned in paas-cf,\nand they were only deployed along with the whole platform. This had a couple of\nnegative consequences:\n* Following a change to an app's source and a passing build an additional\nmanual change to paas-cf was required to deploy it\n* Deploying apps required waiting for unrelated slow bits of the pipeline to\ncomplete (e.g. terraform apply, bosh deploy, cf acceptance tests), despite\nthe fact that a change to an app could not possibly affect or be affected by\nthese steps\nThere were also some advantages of specifying the version of applications\nexplicitly:\n* There's an record of which versions of the applications work with which\nversions of the platform.\n* Merges to paas-cf require a signed merge commit to be deployed, so we can be\nexplicit about the set of people who are allowed to deploy changes\n\n## Decision\n",We will update the pipeline to:\n* Require GPG signatures on git commits before deploying applications in staging and production\n* Automatically deploy every build of the master branch of the applications to staging\n* In staging: tag the application repo when a deployment passes its acceptance tests\n* In production: automatically deploy any build which has been tagged by staging\n,"The decision is to change the deployment process of the essential PaaS applications to allow for individual app deployments. This will remove the need to update the pinned versions in the platform repository, and will allow for faster deployments of app changes. The change will also require additional work to ensure that the versions of the applications and the platform are compatible, and to manage the merge commit process for these applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are moving AWS hosting from Ireland to London. This ADR contains the decisions of the DNS names we will use for apps and system components that will be hosted in London.\n\n## Decision\n,"We will use the following domain patterns for the London hosting:\n* `(system_component).london.(system_domain)`\n* `(app_name).london.(app_domain)`\nWhere:\n* (system_component) -- api, uaa, doppler, ssh, etc.\n* (system_domain) -- _cloud.service.gov.uk_, _staging.cloudpipeline.digital_\n* (app_domain) -- _cloudapps.digital_, _staging.cloudpipelineapps.digital_\nThe reasons are:\n* We should re-use the (system_component) first part to minimise the changes to the Cloud Foundry manifests.\n* We should re-use the (system_domain) and (app_domain) last part, because these domains are assigned to GOV.UK PaaS as the public interface.\n* The domain part `london` is preferrable to `uk`, because AWS may provide multiple-region hosting within the UK in the future.\nThe domain structure for the dev and CI environments won't change. For the dev environments we will create a flag to choose where to create the deployment.\n### Examples\n#### Production\n|Ireland|London|\n|----|------|\n|# _api.cloud.service.gov.uk_|_api.london.cloud.service.gov.uk_|\n|# _sample-app.cloudapps.digital_|_sample-app.london.cloudapps.digital_|\n#### Staging\n|Ireland|London|\n|----|------|\n|# _api.staging.cloudpipeline.digital_|_api.london.staging.cloudpipeline.digital_|\n|# _sample-app.staging.cloudpipelineapps.digital_|_sample-app.london.staging.cloudpipelineapps.digital_|\n",The following DNS names will be used for apps and system components that will be hosted in London:\n\n**Apps:**\n- app1.london.example.com\n- app2.london.example.com\n\n**System Components:**\n- db1.london.example.com\n- cache1.london.example.com
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context\nWe run a Cloud Foundry platform in London (AWS eu-west-2 region) to allow GOV.UK PaaS tenants to host their applications and services in the UK. This is driven by the hosting needs of some of the existing GOV.UK PaaS tenants.\nIn addition we need to manage the use of Amazon Web Service resources to reduce our running costs. Having staging environments in both Ireland and the UK increases the total infrastructural costs, which is not justified by the benefit of the additional tested cases.\nThe London region is newer than the Ireland and currently offers a subset of services compared to the Ireland region. Hence, having the staging environment in London should allow us to capture the cases that may cause failure due to the unavailability of services.\nTherefore, the risks of running a single staging environment are offset by the cost savings and simplification of deployment pipelines.\n# Decision\nWe will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\n# Status\nAccepted\n# Consequences\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\n\n## Decision\n",We will remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).\n# Status\nAccepted\n# Consequences\nWe will have one staging environment for testing both London and Ireland production environment. This ADR does not affect the setup of the dev or production environment.\n,Remove the Ireland (eu-west-1) staging environment resulting in a single staging environment in AWS London (eu-west-2).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\n```\nMemory reserved by orgs: 368640 MB (360 GB)\nMemory reserved by apps: 107108 MB (104 GB)\nMemory actually used by apps: 32868 (32 GB)\n```\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\nDecision\n========\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\nStatus\n======\nAccepted\nConsequences\n============\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\n\n## Decision\n","========\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\nStatus\n======\nAccepted\nConsequences\n============\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\n","We will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\nContext\n=======\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\nDecision\n========\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\nStatus\n======\nAccepted\nConsequences\n============\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\n\n## Decision\n","========\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\nStatus\n======\nAccepted\nConsequences\n============\nWe return the same credentials to all apps bound to the same PostgreSQL database (RDS) instance.\n",Issue the same credentials to all applications bound to the database. The credentials will be stored in the backing database and encrypted using a symmetric key held by the broker.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDrains are an important aspect of any model, they are an essential boundary\ncondition for cases such as biomass drains. Curently rate laws are only specified using\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\nIntroducing drains into Maud requires implementing this into the ODE system which\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\nTo specify drains we create a new class independent of reactions, despite the fact that\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\non varying enzyme concentrations, which we've established that drains do not have.\nDrains are considered as priors to ensure that prior predictive checks are informative\nof the experimental condition.\n\n## Decision\n",Drains will not be considered as a reaction class.\n,"The decision is to introduce a new class of drains that are independent of reactions, despite the fact that they occur in the system of ODEs. This benefits post-processing as some techniques rely on varying enzyme concentrations, which we've established that drains do not have. Drains are considered as priors to ensure that prior predictive checks are informative of the experimental condition."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe try to structure our package in logical sub-units but we want to maintain a\nconsistent public interface.\n\n## Decision\n,We allow for arbitrarily nested sub-packages but export important classes and\nfunctions to the top level thus exposing a public interface. Our unit tests\nshould reflect this package structure.\n,**Decision:**\n\nCreate a module system to encapsulate the internal implementation details of the package while exposing a well-defined public interface.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make a decision on the testing framework for our project.\n\n## Decision\n,We will make use of pytest. It is a de facto standard in the Python community\nand has unrivaled power.\n,**Decision:**\n\nThe testing framework selected for this project is [Insert Testing Framework Name].
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter sampling from the posterior distribution users may want the option\nof validating their results against experimental data or to predict\ncellular behaviour using a trained kinetic model.\n\n## Decision\n,"Posterior predictive samples will be implemented using the posterior\ndraws for kinetic and thermodynamic parameters. The boundary conditions\nfrom the predicted experiments will be sampled from their marginal distributions.\nThe number of samples will be limited to the number of samples from the posterior\ndistribution.\nIn order to define what is part of the training set and prediction set a new\nfile will be introduced called experiment_metadata. A toml file where the training\nprediction split will be defined under the headers [""training""], and [""prediction""]\nrespectively.\nA new stan file will be made where there is no model block (minimal if required).\nIn the generated_quantities (gqs), we will iterate through the draws from a previously\ngenerated set of csvs. the drains, conc_enzymes, and conc_unbalanced_metabolites\nwill be sampled from their priors using the functions normal_rng() and lognormal_rng()\nfor their respective generative distributions.\nlog_probabilities should be calculated if measurements are also included for the\nprediction experiments.\n",**Option 1: Provide a web-based interface for users to submit experimental data and receive predictions from trained kinetic models.**\n\n**Benefits:**\n\n* Allows users to easily validate their results against experimental data.\n* Provides a platform for users to explore cellular behaviour using trained models.\n* User workload is minimised.\n\n**Considerations:**\n\n* Requires development and maintenance of a web-based interface.\n* May require additional security measures to protect user data.\n\n**Option 2: Provide users with a set of tools and instructions for downloading and running kinetic models locally.**\n\n**Benefits:**\n\n* Gives users more control over the modelling process.\n* Allows users to customise models to their specific needs.\n* Less work for developers.\n\n**Considerations:**\n\n* Requires users to have programming skills or access to technical support.\n* Users are responsible for model installation and maintenance.\n\n**Option 3: Provide a combination of both options 1 and 2.**\n\n**Benefits:**\n\n* Provides flexibility for users with different needs and skill levels.\n* Increases the likelihood that users will be able to find a solution that meets their requirements.\n* More work for developers.\n\n**Considerations:**\n\n* Requires development and maintenance of both a web-based interface and a set of tools for local model execution.\n* May require additional security measures to protect user data.\n\n## Decision\n\nOption 3: Provide a combination of both Options 1 and 2.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSetting up different testing environments and configurations can be a painful\nand error prone process.\n\n## Decision\n,"We use tox to define, configure, and run different test scenarios.\n","**Decision:** Use a testing framework such as Selenium or Cypress to automate the testing process.\n\n**Rationale:**\n\n* Selenium and Cypress are popular open-source testing frameworks that simplify the creation and maintenance of automated tests.\n* They provide a variety of features to support different testing scenarios, including cross-browser compatibility, parallel testing, and reporting.\n* Automation eliminates the need for manual testing, reducing the risk of errors and improving the efficiency of the testing process."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\nto natively support f-strings which are sweet.\n\n## Decision\n,We make an early decision to only support Python 3.6 and above.\n,**Decision**: Migrate the codebase to Python 3.6 or later to take advantage of f-strings and other modern Python features.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo determine the steady state conditions of the model, we require an ODE solver.\nWe switched to the adjoint ODE solver for faster gradient evaluations,\nhowever, recent use suggests that the solver is unstable and fails silently.\n\n## Decision\n","Reverting to the ode_bdf_tol() solver implemented in Stan still solves\nstiff ODE problems but slightly slower. This is overlooked as the current\nadjoint solver is too unstable to use and fails on larger systems. The current\nexample files in the `tests/data/` folder work appropriately, however, larger\nsystems fail with the step size approaching 0.\nAn attempt was made to make the flux calculations in the ode more stable by\nusing built in functions and converting products to sums of logs etc.\nThis did not help the adjoint solver and hence this revertion was made.\nFuture versions of Stan can easily be tested by reverting to the previous\nadjoint solver specification and the input files will still accept the tolerances\nfor the solver.\n",Switch back to the original standard ODE solver.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPhosphorylation is the process which deactivates metabolic enzymes.\nThe process is often conducted using kinases, which phosphorylate enzymes,\nand phosphotases, which dephosphorylate the enzyme. This is a major metabolic\nregulator and is essential for the accurate simulation of metabolic networks.\n\n## Decision\n","To mininimise the number of parameters and reducing sampling time,\nthe linear rate law was selected (see [1] for a review). The linear mechanism\nis approximately correct when the following assumptions are satisfied:\n* [Kinase] and [Phosphotase] << [Metabolic Enzyme],\n* Rapid equilibrium binding between phosphorylation enzymes and metabolic enzyme,\n* [Kinase] << dissociation constant for Kinase,\n* [Phosphotase] << dissociation constant for Phosphotse,\n* Competitive binding of phosphotases and kinases is negligable,\n* The ATP/ADP ratio remains approximately constant,\n* Phosphorylation and Dephosphorylation is an irreversible process.\nUsing these assumptions the steady state phosphorylated concentration is defined as:\n`fraction_phosphorylated = alpha / (alpha + beta)`.\nWhere, alpha and beta correspond to the phosphorylation and dephosphorylation\nrates, which are linear with respect to the kinase and phosphotase concentrations.\n```\nalpha = kcat * [Kinase]\nbeta = kcat * [Phosphatase]\n```\nThe activity of the metabolic enzyme is proportional to the dephosphorylated\namount. To avoid situations where the kinase and phosphatase have the opposite\nimpact on the target enzymem, Maud will only refer to them as activating\nand inhibiting enzymes.\n","Use Kinases and Phosphatases to phosphorylate and dephosphorylate enzymes, respectively, as the mechanism for regulating metabolic pathways."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a simple way to manage our package version.\n\n## Decision\n,We use versioneer to do this for us.\n,Use [semantic versioning](https://semver.org/).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDefining measurements for independent fluxes isn't always clear.\nThis can occur when you measure more fluxes than there are degrees\nof freedom in a network.\nAn example would be this simplified network:\nA -> B -> C\nwhere reaction 1 and reaction 2 are dependent, implying that\nno additional information is achieved by including both.\nAnother issue is knowing when you do not have enough fluxes\nmeasured, resulting in an underdetermined system. Due to the\nBayesian implementation of Maud, these systems are still theoretically\nresolvable. However, supplementing as much information\nas possible will likely be beneficial.\n\n## Decision\n","Identifying underdetermined systems is acomplished by first calculating\nthe null space of the matrix. This gives the number of degrees of freedom\nof the system as well. Then we calculate the reduced row echelon form of\nthe transpose of the null space. The resulting matrix represents the\nindependent flux pathways through the network as rows. If you take the\nmeasured subset of reactions and there is a row containing no non-zero\nentries then the system is not fully described using the current measurements.\nDetermining if the system is overspecified is achieved by comparing the\nnumber of measurements to the degrees of freedom. If the number of measurements\nis larger than the degrees of freedom then the system is overdetermined.\nIt is possible to both have an underdetermined system which is overspecified\nby having multiple measurements on dependent paths. It is also possible to\nrecieve the warning that the system is overspecified by independent measurements.\nFor instance, a linear pathway where the influx and efflux are both measured.\nThis is still valid as they are independent measurements.\n","Define only the fluxes that are independent, and thus provide unique information about the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\nmodel. Experimental conditions are defined with respect to these values and drains,\nwhich are already defined as priors. Therefore, our prior knowledge about the\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\nthe likelihood evaluations to the prior information.\nThe benefit of treating priors in this way is that we define a prior on the phenotype\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\nare still considered using weakly informative priors (read: within biologically relevant\nboundaries).\n\n## Decision\n",Unbalanced metabolites and enzyme concentrations can also be considered as prior distributions\nrather than likelihood evaluations.\n,**Decision:** Shift the measurements of enzymes and unbalanced metabolites from likelihood evaluations to prior information.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose an appropriate license before making Maud public. The main\ncontenders are the Apache license, which would allow others to use Maud in\nproprietary software, and the GPL v3 license, which would not.\n\n## Decision\n","We will use the GPL license but may change to a more permissive license in a\nfuture release, depending on the circumstances at the time.\n","We should choose the Apache license.\n\n**Rationale:**\n\nThe Apache license is a permissive license that allows others to use Maud in proprietary software. This would allow a wider range of users to benefit from Maud, including those who develop proprietary software. The GPL v3 license is a copyleft license that requires any derived works to be released under the same license. This could be a barrier to some users, as it may not be compatible with their business models."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n\n## Decision\n,We use git for version control and GitHub for collaboration.\n,Use Git as a version control system and GitHub as a collaboration platform.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to run simulation studies in order to verify that our models work\nand to answer questions about how informative particular measurements and\npriors are.\n\n## Decision\n,"We will initially make a python script that implements a single simulation\nstudy, targeting the model at `ecoli_small_experiments.toml`. Based on how\nthat goes, we can then think about how to make a more general system for\nrunning arbitrary simulation studies.\nIt's better to creep towards a simulation study system like this rather than\njump straight to the general tool because it's quite a big job, and we don't\nreally know exactly what the general tool will look like yet.\n",Use python's `scipy.stats` package
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe `code_generation.py` module is probably the ugliest part of Maud,\nparticularly the function `create_fluxes_function`. It could be cleaned up a\nbit by removing non-modular enzyme mechanisms.\nNon-modular mechanisms aren't used in any of the models we are currently\ntesting, but for historical reasons the logic in `code_generation.py` basically\nassumes that they are the default. The modular rate law is kind of an\nadd-on. This change would let us move towards switching this around, having the\nmodular rate law as the default and working in other mechanisms as and when\nnecessary.\n\n## Decision\n",We remove non-modular rate laws from the `code_generation.py` module.\n,Remove all the non-modular enzyme mechanisms from `code_generation.py`.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n\n## Decision\n,"We will use isort, black, and flake8.\n",We will use the following tools to ensure code quality:\n\n* [ESLint](https://eslint.org/) for linting\n* [Prettier](https://prettier.io/) for formatting\n* [Husky](https://github.com/typicode/husky) for pre-commit hooks
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to document architectural decisions.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhich latex compiler to use?\n* pdflatex\n* xelatex\n* lualatex\n\n## Decision\n,"Chosen option: ""pdflatex"", because compiles faster and correct ligatures are required at the final typesetting step only.\n### Positive Consequences\n* Faster compile time\n### Negative Consequences\n* Using lualatex just before publication might lead to a different layout and additional effort\n* lualatex's power might not be used at all\n","Use xelatex.\n\n## Evidence and Rationale\nxelatex is the most modern and feature-rich of the three LaTeX compilers. It supports Unicode, OpenType fonts, and advanced typographic features like micro-typography and ligatures. It is also the fastest of the three compilers.\n\npdflatex is the oldest and most widely used LaTeX compiler. It is reliable and produces high-quality output. However, it does not support Unicode or OpenType fonts, and its typographic features are more limited than xelatex.\n\nlualatex is a newer LaTeX compiler that is based on the Lua programming language. It is similar to xelatex in terms of features and performance, but it is still under development and may not be as reliable as xelatex.\n\n## Consequence\nUsing xelatex will allow us to take advantage of its advanced features and produce high-quality documents that are visually appealing and typographically correct."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor high-quality documents, the [ligatures](https://en.wikipedia.org/wiki/Typographic_ligature) have to be right.\nSee [english.stackexchange.com](https://english.stackexchange.com/q/50660/66058) for a long discusisson.\nSee a German rant on wrong ligatures: <https://web.archive.org/web/20150425155310/http://www.mengensatz.de/blog/?p=79>.\nFor instance, in the English word ""offline"", the letters ""f"" and ""l"" must not be joined.\nIn the German word ""Auflage"", the letters ""f"" and ""l"" must not be joined.\nSee also the last lines in <https://tex.stackexchange.com/a/64457/9075>.\n### More readings\n* <https://www.typolexikon.de/ligatur/>\n* Questions on ligatures: <https://tex.stackexchange.com/questions/tagged/ligatures>\n* Disable ligatures in headings: <https://tex.stackexchange.com/q/198743/9075>\n\n## Decision\n","Chosen option: ""lualatex"", because comes out best (see below).\n","**Decision:** Use the the `microtype` package with `Ligatures=TeX` for `\XeLaTeX` or `\LuaLaTeX` (or an equivalent package for other engines).\n\n**Rationale:**\n\n* By default, LaTeX uses the ligatures of the underlying font engine, which may not be appropriate for all languages or typographic styles.\n* The `microtype` package provides control over ligatures, allowing us to specify which ligatures should be enabled or disabled.\n* The `Ligatures=TeX` setting in `microtype` approximates the ligature behavior of TeX, which is considered typographically correct for many languages.\n* This setting ensures that the letters ""f"" and ""l"" are not joined in the English word ""offline"" and the German word ""Auflage"", as required by the problem statement.\n\n**Consequences:**\n\n* This decision will alter the appearance of ligatures in the document by enabling or disabling certain ligatures based on the specified settings.\n* It is important to ensure that the chosen settings are appropriate for the target language and typographic style.\n* Using a package like `microtype` adds an additional dependency to the document."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSource code needs to be highlighted\n\n## Decision\n,"Chosen option: ""minted"", because\n1. Listings is inferior because it doesn't utilize a full lexer\n2. Minted offers integration with [pygments](http://pygments.org/) and is actively maintained.\n3. The person who took over minted development, [evaluated all alternatives](https://tex.stackexchange.com/a/103471/9075) and came to the conclusion that minted is the most promising solution.\nPositive Consequences:\n* Source is highlighted properly\nNegative consequences:\n* Users need to install python and pygments\n","Use a syntax highlighter library, such as Highlight.js or Prism.js, to highlight the source code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAllocation of Prison Offender Managers is a new process being introduced by\nOMiC, and it's different from Offender Supervisor allocation (data for which\nis mostly stored in per-prison spreadsheets). The data which will be created\nby this process does not live anywhere yet.\nWe've heard that some prisons are starting to use the new terminology and have\nstarted to store staff names as POMs against prisoners in NOMIS (although they\nare not actually using the OMiC model yet). There is no support in NOMIS for\nthe tiering data or anything else needed to support the allocation process.\nWe have learned from our allocation discovery that staff around the prison\nneed to know who is responsible for a prisoner, but there's no indication that\nstoring that data in NOMIS is a good way of meeting that need.\nWe don't know much yet about the user needs around handover of responsibility\nfrom prison to probation. It's possible that there will be a stronger need to\nstore some allocation data in Delius (for example) so we may need to revisit\nthis decision in future.\nHMPPS needs to reduce its dependence on NOMIS over time, and building an\necosystem of APIs around new datastores will enable data to be used in other\nservices without it all needing to be in one central database. The keyworker\nAPI is an existing example of this pattern.\n\n## Decision\n",The allocation API will store all data created by the allocation process in its\nown database.\nWe will not also write allocation data back to NOMIS or any other system.\n,"The data required for Allocation of Prison Offender Managers will be stored in a new datastore. This datastore will be separate from NOMIS, and will be accessible via an API. This will allow other services to use the data without having to store it in their own databases."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to run the smallest number of pre-production environments which give us\nuseful feedback on changes to our applications before we deploy them to\nproduction.\nWe think that as we integrate with more systems (NOMIS, Delius, OASys etc) we\nare likely to need more pre-production environments of our own in order to test\nagainst pre-production environments of those systems. It's unlikely that data\nwill match up well across systems in those environments so we may need to\ncreate matching data in them ourselves in order to test our applications.\nWe don't know yet how many pre-production environments would be useful for us\nto have in this context.\nWe suspect that challenges around data quality and how quickly records are\nmatched will only be clear in production data, so we're keen to start working\nwith production systems as soon as we can. However only NOMIS has available\nAPIs in production, and working with only one system is less likely to reveal\nthe scale of those challenges.\nSo far we've set up [one environment for our new applications](https://github.com/ministryofjustice/cloud-platform-environments/tree/1afcd91536201415b868ccebcaf1aeb8ecc2d339/namespaces/cloud-platform-live-0.k8s.integration.dsd.io/offender-management-staging)\non the new cloud platform, called Staging.\nIt's straightforward and quick to set up further environments as we need them,\nbut there's no way at the moment to share config between cloud platform\nenvironments so there is a code maintenance cost to having many environments.\nWe still need to finish setting up authentication on our applications as a\nminimum before we start using production APIs to other systems. There's little\nbenefit in us having a Production environment ourselves until we can do that -\nwe're a long way from having real users.\nSetting up authentication means that we need to pick a NOMIS pre-production\nenvironment to use from our Staging environment, because we're going to use the\nNOMIS OAuth2 server (see [ADR 0011](0011-use-nomis-oauth-server2-for-allocation-api-authentication.md)).\nOur team already have comprehensive access to the T3 NOMIS environment from our\nwork on Visit someone in prison. That environment contains anonymised data,\nwhich is sufficient for our needs at this stage. All the NOMIS-based services\nwe need are running there. It's commonly used as a development environment\n(rather than staging) by other services.\n\n## Decision\n",We will start out with one shared Staging environment for our new applications.\nWe will use the T3 NOMIS environment from that environment to start with.\n,Set up an Authentication environment and use it to make our first attempts to use production APIs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur main source of data on prisoners and prison staff is NOMIS. The\nallocation tool currently uses the Custody API, as decided in [ADR 0006](https://github.com/ministryofjustice/offender-management-architecture-decisions/blob/master/decisions/0006-use-the-custody-api-to-access-nomis-data.md), to retrieve\ndata on both offenders and staff.\nThere are still four APIs into NOMIS providing general data access, with varying\napproaches to presenting the data and authentication. We still do not want to add\nto this duplication.\nAlthough it has been agreed by the HMPPS technical community that we would\nlike to move all clients to use the Custody API in preference to the other APIs,\ninitial use of the Custody API has raised some issues. Problems exist\nwith the locality of data, N+1 API requests, and data that is unavailable\nthrough the currently published endpoints. These issues are intrinsic to\nthe current design of the Custody API, and it is unlikely that they will\nbe resolved or mitigated in the short to medium term. The scale of the work\nrequired makes it unrealistic that we would be able to deliver this in a\nrealistic timescale.\nThe Elite2 API, is under more active development, is entirely owned by the\nSheffield team and provides endpoints more focused on the need of clients.\nMost of the endpoints available work on the caseload of the connecting\nclient token, but also support the provision of more specific parameters\nto handle alternate needs. Although we will not be authenticating with\nthe token of the eventual end user, enough flexibility exists for us to\nobtain the data we require from Elite2.\nElite2 provides functionality closer to what we currently require\nand its design encompasses the need for extra API endpoints for specific\nservices. Moving to Elite2 is a pragmatic, and tactical approach to\nresolving the issues around the Custody API to allow us to deliver the\nallocation component of MOiC. This decision doesn't invalidate the\noverall agreed strategy of moving HMPPS services to the Custody API\nover time, but it highlights that more coordinated work is needed to\nachieve that than we are able to take on ourselves.\n\n## Decision\n","We will use the Elite2 API for access to the data we require from NOMIS.\nWe will work with the team in Sheffield on development of the Elite2 API to\nadd support for accessing the data we need, in the structure that we need it.\n",The tactical decision is to use the Elite2 API to access NOMIS data for the allocation component of MOiC.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Users\nWe need to authenticate users in order for them to access the service. We will\nhave users from prisons and probation services, both public and private in both\ncases.\nWe expect our earliest users to be prison-based Senior Probation Officers and\nOffender Management Unit administrative staff, followed by Prison Offender\nManagers and community-based Senior Probation Officers. It's possible that we\nwill also need to enable Community Offender Managers to log in in future, but\nwe don't yet know what we might build for them so that's not certain.\n### Existing options\nWe do not want to build user authentication from scratch ourselves. There are\ntwo shared approaches for authenticating users which are currently in use with\nHMPPS digital services, both of which are based on OAuth2 and support\nrole-based access control:\n- MOJ Signon\n- NOMIS OAuth server, including new SSO functionality\nBoth of these originally emerged from the needs around one or two services, and\nwere subsequently adopted by a few other services. A discovery aiming to\ndevelop a clearer strategic direction in this space has recently concluded, and\nfound that neither of the two existing approaches meet the needs of users and\nteams well across the range of HMPPS services and their user groups. For now,\nthough, these are the two available options to consider.\nIf we use MOJ Signon, we would use the authorization code grant type. The team\nare familiar with MOJ Signon from their work on Visit someone in prison.\nIf we use NOMIS Auth, we would need to decide whether to use the new SSO\nfunctionality with the authorization code grant type (which has been developed\nrecently and is not yet available in production) from the beginning, or whether\nto have our own login page and use the password grant type initially, only\nswitching over to use the SSO approach when it's available in production.\nThe SSO approach means that we would have less to build ourselves, and wouldn't\nneed to handle users' passwords, and those advantages mean more generally that\nit's intended to replace existing use of the password grant type by other\nservices.\n### Service\nThe services we're working on for Manage Offenders in Custody are seen as being\nfairly closely related to a group of services which already use NOMIS Auth.\nThe users of our service who are working in prisons will need to have NOMIS\naccounts anyway. Probation staff have so far not had NOMIS accounts, but\nanother service team (Home Detention Curfew) is using NOMIS Auth and intending\nto create accounts for probation users for that service. The new web-based\nservices being built around NOMIS mean that users won't have to be on the\nQuantum network to access the services, and in future it will be possible to\nmanage a NOMIS account without access to that network as well (for password\nresets, updating profile info, managing roles etc).\nSome of our users (OMU admins in some prisons) will already have MOJ Signon\naccounts in order to use Visit someone in prison, but many won't be doing that\nalready (and they would all have NOMIS accounts anyway).\n### Route to production\nWe expect that matching up data between NOMIS and Delius will be the main\nchallenge for us, and that we'll only be able to see how well that works in\nproduction because there aren't any pre-production environments with related\ndata across the two systems. We're keen to get into production as soon as we\ncan for that reason.\nHowever we don't currently expect to have any way of getting access to the\nDelius API for some months, which reduces the urgency of this consideration.\nOur priority now is to make decisions which enable us to make progress with\ndevelopment.\nThe NOMIS Auth team expect the new SSO to be in production within the next\ncouple of months, which is very likely to be before we get Delius API access.\n\n## Decision\n","We will use NOMIS accounts for authenticating users of the allocation manager.\nWe will use NOMIS SSO from the start, rather than building our own login page.\n","Use the NOMIS Auth server with the new SSO functionality, using the authorization code grant type."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to protect the allocation API with authentication, but we'd rather not\nhave to come up with an approach to do that ourselves from scratch.\nThe new [NOMIS OAuth2 server](https://github.com/ministryofjustice/nomis-oauth2-server)\nis already being used in production for authentication on almost all of the\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\nauthenticate with the Custody API, and the other services which may need to use\nthe allocation API are very likely to already be using this authentication\nmethod for the other APIs they use.\nClients can use one token (of a particular grant type) to authenticate with all\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\nthose services - they don't have to work with multiple different authentication\napproaches.\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\nservices can verify the integrity and authenticity of tokens presented by\nclients using the corresponding public key.\nWe've decided that the allocation manager will be entirely responsible for user\naccess control and will call other APIs directly, and the allocation API will\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\nThat means that the allocation API doesn't need to know which user it's\nreturning data for, and we can use a system-to-system approach to\nauthentication.\nWe don't know of any other shared approaches to API authentication which are\nused in the prison space.\n\n## Decision\n",We will use the NOMIS OAuth2 server for authentication on the allocation API.\nWe will use the client credentials OAuth2 grant type for authentication on the\nallocation API.\nWe will verify signatures on presented tokens in the allocation API.\nWe will respect expiration times on presented tokens in the allocation API.\n,"The allocation API will use NOMIS OAuth2 for authentication.\n\n**Consequences**\n\n* Reduces risk by using a shared approach to authentication which is already in production and used for other APIs.\n* Simplifies development by not having to design and implement our own authentication approach.\n* Reduces time by not having to design, implement and test our own authentication approach.\n* Increases maintainability by using a well-supported and maintained authentication server."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have reached a point where we are going to start reading and writing data to our Allocation API application database and will need to manage database migrations when adding tables, columns and so forth.  This ADR decision outlines the agreed upon approach that the team will take for managing these migrations.\n\n## Decision\n","We will use blue-green deployments split into the following steps (using adding column as an example):\n* Add a database migration that inserts the new column\n* Update the application so that all new data gets written to new column\n* Run a task to copy all the data from the old column to the new column\n* Update the application so that it reads from the new column\n* Add a database migration that removes the old column\nWe will also ensure that any migrations include 'up' and 'down' methods, rather than just 'change' to avoid any situations where Rails doesn't know how to handle the inverse of the up or down.\n",Use Flyway to manage database migrations.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe recommended way of managing application config needed at runtime on the\ncloud platform is to use Kubernetes secrets. These can be created manually\nusing kubectl on the command line to apply config files, or as part of a\ndeployment process.\nWe want to manage as much of our application config as possible in\nversion-controlled code, so that we can see when config was changed and so\nthat we can easily reproduce changes and rebuild environments.\nThe team are used to using [git-crypt](https://www.agwa.name/projects/git-crypt/)\nto encrypt files containing secrets from their work on Visit someone in prison,\nbut that service is hosted on a different platform which means that the secrets\nare in a private repo, separate from the application. It's easy to forget to\nupdate config at the right time when it's managed separately from the\napplication, and that has resulted in production outages in the past on that\nservice.\nIf we use git-crypt to encrypt secrets files in public repos rather than\nprivate ones, the risk associated with accidentally pushing secrets in\nunencrypted files is increased. If we take this approach, we would want to make\nit easier to avoid doing that rather than relying only on people being careful.\nWe are using kubectl for deploying. Some other teams are using helm, which has\nsupport for templating so that values for config files can be kept separately\nfrom the structure of the configuration. Unless we start using templating of\nsome kind in our config files, we will need to keep the whole files containing\nKubernetes secrets definitions private instead of just the secret values.\n### Other options\nAlternatives to using git-crypt in public repos include:\n- separating out secrets into a private repo, using git-crypt there instead.\nThis would increase the likelihood of us not updating the config when it's\nneeded, leading to production outages.\n- creating Kubernetes secrets manually from the command line, using config\nfiles committed in the application repo without secret values or encryption -\nso the secret values would be generated as needed and only stored in the\nrelevant namespace. This would mean that our Kubernetes secrets could not be\ndeployed as part of our automated pipeline but would need to be manually\nupdated, and the manual workflow for updating a single value in a secret\nwith multiple data values would be more complex. We would need to be careful\nto not accidentally commit the values in the secrets files after deploying\nthem.\n- storing secret values in CircleCI envvars, to be used when creating\nKubernetes secrets during deployment, using templating in our secrets files.\nCircleCI has no concept of environments (all envvars for a project are\navailable to all jobs) so all of our test and build steps would have access\nto production secrets, and we would need to define an approach to namespacing\nCircleCI envvars for different environments. Even so, envvars needed for\nrunning tests, building and pushing images, running deployments and running\napplications in different environments would all live together in one list,\nwhich would make that list more confusing than it already is.\n- using another secrets store to manage our secrets, probably instead of using\nKubernetes secrets. This would add another external dependency for our\napplications and would have the same challenges around either managing\nsecrets values manually (thereby losing the ability to automate processes) or\nfinding somewhere to manage version-controlled secrets from, and having\nanother place to coordinate changes to required config with application\ndeploys. It could also introduce another system for the team to authenticate\nwith in order to manage secrets.\n\n## Decision\n","We will keep the Kubernetes secrets definitions which are related to our\napplications in the public application repos, encrypted with git-crypt.\nWe will only add team members who need access to production secrets to the\ngit-crypt setup. The only exception to this is our CircleCI deployment jobs\nwhich need to decrypt the secrets to deploy them.\nWe will define a standard for creating and managing GPG keys and only use keys\nwith the git-crypt setup which meet that standard.\nWe will set up pre-commit hooks on our repos which contain encrypted secrets\nto help us use git-crypt well.\nWe will agree processes for revoking credentials with other relevant teams in\ncase they are accidentally exposed.\nIf another standard way of managing secrets for applications running on the\ncloud platform is developed, we will consider using that instead.\n",Use git-crypt to encrypt secrets files in public repos and implement a pre-commit hook to prevent pushing unencrypted secrets.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we decided to start off with separate API and user-facing applications\n(see [ADR 0004](0004-separate-api-and-user-facing-applications.md)) we thought\nthat we would need to build more than one service as part of this work, to\nmeet different needs of different users. That may still be true in general, but\nfrom what we've learned in the Handover discovery it's looking more likely at\nthe moment that the Handover phase of our work will involve expanding on\nAllocations rather than being an entirely separate journey and service.\nWe were also intending that the allocation manager would be a fairly minimal\nuser-facing application, which would fetch all the data it needed from the\nallocation API and then render pages based directly on it.\nWe've been looking at how we should set up authentication on our two\napplications and have realised that that split of responsibilities between the\nallocation manager and API could lead to some confusions:\n- Calling the Custody API from the allocation API means that the allocation API\ncan be seen as sitting in two different layers depending on the context:\neither above the Custody API from the point of view of the allocation manager,\nbut on the same level as the Custody API from the point of view of other\nservices which need to use allocation data. This lack of clarity raises\nquestions around which OAuth2 grant type to use for authentication on the\nallocation API.\n- Responsibility for access control may be split between the allocation\nmanager and API. The user-facing application would need to know what the user\nshould have access to in order to show the right options, but the API may\nalso need to know what the user should have access to in order to fetch the\nright data from other APIs.\nWe still think there's value in having a separate allocation API, but it would\nbe easier to understand if it were a more RESTful interface to the data it\nstores.\n\n## Decision\n","The allocation API will be a simpler interface to the data held in its database.\nThe allocation API will not call other APIs to return data from them to the\nallocation manager.\nThe allocation manager will call the allocation API and other APIs (Custody,\nDelius etc) directly.\nThe allocation manager will be entirely responsible for access control for its\nusers.\n","Move the minimal user-facing application into the allocation API (either as a separate component, or using a microframework that runs in the same process as the allocation API), and use the API as a very thin wrapper around the data that Allocations handles."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are moving into beta on allocations, which is the first area of our work on\nManage Offenders in Custody. The team is now also in discovery on handover, and\nthere are more areas which we know very little about at this stage which we\nintend to look at in future.\nWe anticipate that we will build more than one product as part of this work, to\nmeet different needs of different users. That means that we're very likely to\nbuild more than one application as part of this larger service area in future.\nWe need to decide now whether we should start out by building one application\nfor allocations, or follow the common pattern of separating an API layer from\npresentation.\n### Microservices\nThere are many advantages to a microservices architecture, but also some costs.\nIf we start off with more than one application, the decisions we make early on\nabout their responsibilities may take more work to change later on, as we learn\nmore.\nOn the other hand, if we start off with one application and decide later that\nwe want to split it into several, that work is likely to be substantial. Given\nthe timescales involved in OMiC, it may be hard to prioritise paying off that\ntechnical debt when it becomes a burden.\nWe anticipate that other services will need to use allocation data, and so we\nare likely to need to make it accessible via APIs at some stage anyway. Those\nservices may need to access allocation data in different ways from our\nfrontend, so more work may be needed then to support them even if we do have\nan API from the start.\n### Operating microservices\nMicroservices can introduce more operational complexity by moving function\ncalls to network calls and by introducing more moving parts which need to be\ndeployed. However we expect the number of microservices to remain relatively\nsmall so do not anticipate this complexity to be significant for us.\nOur choice of the Kubernetes-based Cloud Platform for hosting (see [ADR 0002](0002-use-cloud-platform-for-hosting.md))\nshould make it easier to scale small services efficiently and independently.\nIt also gives us easy access to tooling designed to support many small\nservices.\nWe expect our decision on how far we should follow a microservices approach to\nhave minimal impact on our hosting costs.\n\n## Decision\n","We will start off with two applications for allocation:\n- an allocation API which will call other APIs (to read data from NOMIS, and\nin future Delius and OASys)\n- an allocation frontend which will call the allocation API and serve\nprogressively enhanced HTML to users (see [ADR 0003](0003-use-progressive-enhancement.md))\n","Start off with one application for allocations, but build it in a way that makes it easy to split into many applications later on.\n\nThis decision is based on the following factors:\n\n* The benefits of a microservices architecture outweigh the costs for our use case.\n* We anticipate that we will need to make allocation data accessible via APIs at some stage anyway.\n* The operational complexity of microservices is not a major concern for us.\n\nBy starting off with one application, we can avoid the costs of splitting it into multiple applications later on. However, we will need to make sure that the application is designed in a way that makes it easy to split later on. This will require us to use a modular design and to avoid creating tight coupling between different parts of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs documented in [ADR 0014](0014-access-the-delius-api-via-ndh.md) we are\ngoing to access the Delius API via the NOMIS data hub using TLS mutual auth,\nmanaged on our side by a sidecar container.\nWe need to find a way of testing our auth setup - in particular that it only\nallows access to the Delius API from our allocation manager. The NDH only\nexists in production, so we need to decide how to test this setup and\ndeployment of the sidecar in our other environments (at the moment we only\nhave staging and production). We want staging to be as similar as possible to\nproduction so that we're testing changes in a realistic environment before\npushing them to production.\n\n## Decision\n","We will set up a separate namespace on the cloud platform for a Delius staging\nenvironment.\nWe will deploy the NDH-side mutual auth container into that environment, along\nwith the Delius API with a dataset constructed to match T3 NOMIS data.\nInitially our main concern is testing the mutual auth setup so instead of the\nDelius API we could deploy a simple HTTP server if that's easier in the short\nterm.\nWe will deploy the sidecar in our staging environment and connect to our Delius\nstaging environment from it.\n","Create a ""mock NDH"" in staging for testing purposes only. This mock NDH should:\n- Be as similar as possible to the real NDH in terms of its API and behaviour, so that we can test our auth setup and sidecar deployment in a realistic environment.\n- Be isolated from the real NDH so that it cannot be used to access the real Delius API.\n- Be easy to set up and tear down, so that it can be used for testing purposes only."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe service manual clearly states that teams should use progressive enhancement\nwhen building services: https://www.gov.uk/service-manual/technology/using-progressive-enhancement\nThis supports [point 12 of the service standard](https://www.gov.uk/service-manual/service-standard/create-a-service-thats-simple),\nwhich is about ensuring that all users can use the service successfully the\nfirst time they try - including users with accessibility needs.\nThe service manual is also clear that [internal services should be held to the\nsame standard as public-facing services](https://www.gov.uk/service-manual/design/services-for-government-users).\nSome of the services for prison and probation staff which have been built over\nthe last couple of years are not progressively enhanced. Without JavaScript\nthey display no content.\nSince these services are in a similar space to our work and have overlapping\nuser bases with ours (although they are not the only existing services in this\nspace), we have considered whether we should take a similar approach to them.\n\n## Decision\n",We will use progressive enhancement for all our user-facing applications.\n,Adhere to the service manual's guidance on progressive enhancement and ensure that all internal services are progressively enhanced.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Data and policy\nWe've known since very early on in our work on Manage Offenders in Custody that\nwe have a critical dependency on being able to use current or very recent data\nfrom Delius to inform the allocation process, in line with the new OMiC policy.\nThe policy defines two pieces of data which determine the type of Prison\nOffender Manager who can be allocated, and which staff in prisons have no other\naccess to:\n- whether the case will be handled by the National Probation Service or a\nCommunity Rehabilitation Company\n- the tier (A/B/C/D), which is based on the scores for three risk assessments\nOur user research has indicated that other information held in Delius is also\nlikely to be useful in informing the allocation decision, and is important for\nPOMs and Senior Probation Officers in carrying out their responsibilities under\nOMiC.\nOne of the main aims of the policy is to enable prison and probation services\nto work together more effectively, for which better sharing of information is\ncrucial.\n### Existing Delius API\nAn API for Delius already exists and is deployed alongside Delius, but is only\naccessible from inside that environment. Providing secure access to the API for\nother services has long been thought to depend on the Delius migration to AWS\nbeing completed.\nHowever that migration has taken longer than originally hoped, and we need to\nfind a viable approach to get access to the data we need so that we can provide\na service which has some value for our users.\nWe know that some work on the API itself will also be needed to make it return\nthe data we need, but getting access to the API at all is the more significant\nchallenge.\nThe API itself does not have authentication built in at the moment, so all\naccess to it needs to be secured in other ways. It will use the NOMIS OAuth\nserver in future.\n### Overnight extract\nIn the absence of API access, the Home Detention Curfew team have set up an\novernight batch process to write the names of responsible officers into NOMIS\nfrom a Delius extract.\nWe could use this approach for the data we need for MOiC, but it's far from\nideal for us:\n- We wouldn't have live data available, which could lead to allocation\ndecisions being made on the basis of outdated risk information\n- It would introduce more dependencies on an outdated architectural pattern\n- Duplicating data between systems means that the ownership of the data is\nunclear\n- We would need to support this approach until the API is accessible, and then\nswitch to using the API - this would take more work and redesigning interfaces\nthan using the API from the start\nWe want to avoid this approach if we possibly can, but will consider it as a\nlast resort.\n### Other services which might provide risk assessment data\nWe have also heard that another team's work around risk assessments might make\nthe data we need available from their new service, but we have just learned\nthat the scope of their work doesn't extend that far in the timescales we're\nworking to.\n### Delius API access via NDH\nAnother option was proposed recently: to give us access to the API via the\nNOMIS Data Hub, which already has secured access to the API. We could do this\nwith mutual TLS auth between the NDH and our allocation manager, using the\nsidecar pattern with our application. This approach is estimated to need less\nthan a week of work and isn't dependent on the Delius migration being\ncompleted.\nWe would need help from people working on the Delius migration to take this\napproach, particularly in setting up mutual auth on the NDH end, but given the\nextent to which this approach unblocks our work on MOiC that is a reasonable\ntradeoff. We would need help from the same people for any of the approaches\nwe've considered.\nThe NDH only exists in production, so if we take this approach we will need to\ndecide what to do in other environments. We know that Delius and NOMIS don't\nhave any shared pre-production environments anyway, so we don't expect this to\ncause any extra problems for us.\n\n## Decision\n","We will set up access to the Delius API via the NDH route.\nWhen the API is accessible through a more standard route, with authentication,\nwe will switch to use that.\nWe will keep our deployment pipeline consistent by using the\nmutual-auth-via-sidecar pattern in our pre-production environments as well as\nin production, if possible.\n",Access the Delius API via the NOMIS Data Hub using mutual TLS authentication between the NDH and the allocation manager.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have implemented the VCR gem as part of our testing suite; this gem allows you to ""record your test suite's HTTP interactions and replay them during future test runs for fast, deterministic, accurate tests."". The interactions are saved into yaml files (cassettes) as part of the spec/fixtures folder and committed to the repository.\nVCR allows you to filter out sensitive information, but this does not cover any data returned in the body.  We get a short-lived access token returned in the body as part of the OAuth flow and need to find a way of ensuring that these are not exposed and used to get access to sensitive data.  Whilst the tests are run against T3, which is an anonymised dataset, we don't want to risk the possibility of someone being able to engineer a way of undoing that and seeing real data.\nThe main options:\n- making the github repository private\n- git-crypting the VCR cassettes folder\n- adding the VCR cassettes folder to .gitignore\n- not using VCR and instead using other ways of mocking external calls in tests\nMaking the github repository private contradicts our commitment to coding in the open, and eventually opening a previously closed repository is often risky and hard to prioritise.\nEncrypting the VCR cassettes folder would effectively mean making a portion of the repository private.  There are three criteria for keeping code closed and they are: not publishing keys and credentials; algorithms related to detecting fraud; and anything around unreleased policy.  This situation does not meet any of these criteria.  Full documentation related to open source code guidance can be found [here](https://www.gov.uk/government/publications/open-source-guidance/when-code-should-be-open-or-closed).\nAdding the VCR cassettes folder to gitignore would mean that we could continue to make use of the gem in local development and during the CircleCi workflow.  The APIs that we are using are being actively developed against and it's important that we catch any changes as early as possible. Therefore a developer can re-record all cassettes at the start of a feature to check for changes, and then just reuse them to speed up the tests.  The cassettes then don't get committed to Github and access tokens are not exposed. CircleCi can continue to run the tests as normal and whilst anyone can access the project, cannot see the contents of the cassettes.\nLastly, there is an option to not use the VCR gem at all and use something like WebMock to mock every API call in the test suite. However, if there were any issues with API documentation being out of date we wouldn't necessarily know what to expect in the response.  Also, if we mock the calls there might have been changes we weren't aware of and find out late in to the development process, whereas actually making the calls will cause the tests to fail and let us investigate.\n\n## Decision\n","We will add the VCR cassettes folder to gitignore.\nWe will change the VCR config to allow a recording mode to be added i.e. ""all"" -> re-record all cassettes, ""new_episodes"" -> append the cassette(s) with any new calls made, ""once"" -> just record them once and reuse.\n",Add the VCR cassettes folder to .gitignore
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have already decided to use Ruby for our new applications (see [ADR 0007](0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md)).\nThe team are already very familiar with Rails and it is widely used within MOJ.\n\n## Decision\n,We will use Rails as our web framework for our new applications.\n,**Decision:** Use Rails as the web framework for our new Ruby applications.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur main source of data on prisoners and prison staff which we need for\nallocations is NOMIS.\nThere are now four APIs into NOMIS providing general data access, with varying\napproaches to presenting the data and authentication. We do not want to add to\nthis duplication.\nThe APIs which have been developed more recently are more under the control of\nHMPPS than the earlier ones (which were developed by a supplier). That gives us\nmore flexibility around how we work with them and makes it possible to get\nchanges into production more quickly. Using one of the newer APIs should mean\nthat we are less blocked by delays around API changes than we have been on our\nwork on Visit someone in prison.\nIt has been agreed by the HMPPS technical community that we would like to move\nall clients to use the Custody API in preference to the other APIs over time.\nAlthough that work has not yet been prioritised, using the Custody API for new\napplications will reduce the work needed in future to align our API usage.\nThe Custody API has been designed to give a more direct view of the data in\nNOMIS than the previous APIs have been - earlier approaches have favoured\nimplementing specific endpoints to meet the needs of service teams rather than\ngiving a more comprehensive view of all the data.\n\n## Decision\n",We will use the Custody API to access the NOMIS data we need.\nWe will work with the team in Sheffield on development of the Custody API to\nadd support for accessing the data we need.\n,We will use the Custody API for all new applications that require data from NOMIS.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Language use across HMPPS\nHMPPS has active development in four languages, including services with\nsignificant prison-staff-facing components in all four: Ruby, Python,\nJavaScript and Java.\nHMPPS has live services (which have passed service assessments) built in Ruby\nand Python. Ensuring that we support our existing users should be our top\npriority, so it is essential to maintain our skills in the languages used in\nour live services.\nOne of the advantages of a microservices approach is that teams can work on\nseparate services in different languages, using HTTP APIs to share data and\nfunctionality. There is no need for all services to be built in the same\nlanguage. We are already using this approach across HMPPS.\nThere is no clear vision or strategy at the moment for changing the number of\nlanguages in use across HMPPS. We are not in a position to decide that for all\nof HMPPS.\n### Team skills\nAll four languages in active use across HMPPS are represented to varying\ndegrees in the skill sets of the current members of the team, but only Ruby is\ncommon to all of them. The team have worked together on a live service built in\nRuby for all of their time at MOJ/HMPPS. We still own that service and\ncontinuously improve it alongside our work on Manage Offenders in Custody,\nalthough we are spending the majority of our time on the latter.\nThe primary language skills of HMPPS's civil servant developers and technical\narchitect (a significant proportion of whom are on this team) are in Ruby and\nPython. It is unrealistic to expect people to be equally proficient in many\nlanguages at the same time.\nThe team have already committed to learning about Kubernetes for the new Cloud\nPlatform (see [ADR 0002](0002-use-cloud-platform-for-hosting.md)) and to\nlearning Java so that we can collaborate on the APIs which are being built in\nSheffield (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)).\nThis is already a significant proportion of unfamiliar technologies for the\nteam to learn and use.\nDeveloping applications involves much more than using the standard library of\na language. The ecosystem of libraries tools around that language often takes\nmuch more work and time to become familiar with than the basics of the language\nitself. Although all of the team know some JavaScript, we do not all have\nexperience of using it for building server-side applications. We would therefore\nhave a lot more to learn if we were to choose to use JavaScript, as some\nrelated but separate services do.\n### Time constraints\nThe Offender Management in Custody programme has fixed timelines for its\nnational rollout in the next year. Although we are not committing to delivering\nparticular services at set dates months in advance, we will reduce our\nopportunity for learning from a smaller set of real users before the national\nrollout if we are not ready to take advantage of the Wales pilot which begins\nin January.\nWe know that allocations is only the first of several areas of the programme\nwhich are likely to need support from us, so timescales are tight for us.\nWe anticipate that the complexity of building this service lies in managing the\nquality of the data available across NOMIS, Delius and OASys, rather than in\nrepresenting that data to users.\nChoosing to use a less familar language for developing our applications, on top\nof what we already need to learn, would put us at significant risk of not\ndelivering working software until several months after our first users need it.\n### Code reuse\nUsing the same language for groups of similar services can make it easier to\nprovide a coherent experience for users by allowing presentation code to be\nshared more easily between services. However, the same HTML structure of pages\ncan be produced by services written in different languages. Since we are\ncommitted to progressive enhancement (see [ADR 0003](0003-use-progressive-enhancement.md)),\nwe will use client-side JavaScript solely to enhance the functionality of those\npages, and that JavaScript can be reused across services regardless of the\nlanguage used on the server.\nAs an example of this approach, there is a strong and active cross-government\ncommunity which develops, researches and supports design patterns, styles and\ncomponents which are used on services built in many different languages:\nhttps://design-system.service.gov.uk/\nWe will base our user-facing applications on this established design system in\nany case. There is already a variety of design approaches in use across the\nprison-staff-facing services we have, and our best chance of standardising that\nwell is to align ourselves with the cross-government approach.\nThat approach is supported by extensive user research over several years and\nacross many services and departments. Using it as our starting point reduces\nthe need for us to undertake duplicate research ourselves to understand the\neffectiveness of alternatives to those existing patterns. We expect that we\nwill need to extend those patterns and develop others inspired by them to meet\nour users' needs, and we will contribute what we learn back to the HMPPS and\ncross-government communities.\nSince it has been agreed that all services which need to use a NOMIS API should\nmigrate to the Custody API (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)),\nany API client library which we build in Ruby can be reused by other Ruby\nservices to ease their migration.\n### Operational considerations\nThe team has considerable experience of operating live services built in Ruby\nat scale.\nWe do not anticipate scaling to be a significant concern for allocation - we\nexpect to have a couple of hundred users a day at most for it.\nThe new Cloud Platform makes it easy, quick and cheap for us to scale up if we\nneed to.\n\n## Decision\n",We will use Ruby for the new applications we build in London as part of Manage\nOffenders in Custody.\n,"Based on the context provided, the decision is to continue using Ruby as the primary language for developing new services in the team. This decision is made based on the following considerations:\n\n* **Existing skills:** Ruby is common to all team members, and the team has significant experience in developing and operating live services in Ruby.\n* **Time constraints:** The team has tight timelines for delivery, and learning a new language would introduce additional risk and delay.\n* **Code reuse:** While it is possible to share presentation code between services written in different languages, the team believes that using Ruby will facilitate code reuse within the team's own services.\n* **Operational considerations:** The team has experience operating Ruby services at scale, and the new Cloud Platform makes it easy to scale up if needed.\n\nThe team acknowledges that other languages may have advantages, but believes that the benefits of using Ruby outweigh the potential benefits of switching languages at this time. The team will continue to monitor language trends and may consider using other languages in the future if it becomes necessary or advantageous."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose tooling for various continuous integration and deployment\ntasks:\n- running automated tests on branches/pull requests\n- running security and code quality checks on branches/pull requests\n- building Docker images\n- deploying applications to our environments\nJenkins and CircleCI are commonly used at MOJ for these tasks, and the team\nhave experience with both of them, as well as other options such as Travis.\nThe Cloud Platform team operate a Jenkins server but are keen to move services\naway from it, especially anything which doesn't use Template Deploy.\nThe LAA fee calculator is built and deployed using CircleCI with the new cloud\nplatform, so we'd have an example to follow if we also chose CircleCI. That\nteam didn't evaluate a lot of options when they made that decision, but it's\nworking well so far for them. Our applications are a little more complex than\ntheirs since we have a database, but we don't expect to be doing anything\nunusual in our build and deploy pipelines.\nThere's also some [documentation](https://ministryofjustice.github.io/cloud-platform-user-docs/02-deploying-an-app/004-use-circleci-to-upgrade-app/)\non using CircleCI with Helm for continuous deployment to the new cloud\nplatform - we may not want to follow it exactly, but it covers some useful\ntopics.\nOur team haven't been practising continuous deployment (that is, deploying\nevery change automatically to production) on Visit someone in prison - deploys\nto production for that service need to be triggered by a human. We'd like to\nkeep that option open for this service, though.\nIt's important to us that our build and deployment configuration is managed in\ncode. We've decided to start out with two applications (see [ADR 0004](0004-separate-api-and-user-facing-applications.md))\nand want to be able to easily make and reproduce changes to builds and\ndeployments for both.\nWe're keen to get started quickly - we'd rather not spend time assessing lots\nof tooling options at this stage. Our needs will evolve anyway, so it's fine\nto pick something that works for us now and revisit that decision later on if\nwe need to.\n\n## Decision\n",We will use CircleCI for continuous integration.\nWe will use CircleCI for deploying.\n,"We will use CircleCI for our continuous integration and deployment tasks.\n\nCircleCI is a well-supported and widely used tool that meets our needs for:\n\n- Running automated tests on branches/pull requests\n- Running security and code quality checks on branches/pull requests\n- Building Docker images\n- Deploying applications to our environments\n\nAdditionally, CircleCI is already in use by other teams at MOJ, which gives us access to shared knowledge and support."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPreviously, it was decided in [ADR 0004](https://github.com/ministryofjustice/offender-management-architecture-decisions/blob/master/decisions/0004-separate-api-and-user-facing-applications) that we would separate data and presentation concerns.  This however was reversed by [ADR 0010](https://github.com/ministryofjustice/offender-management-architecture-decisions/blob/master/decisions/0010-allocation-api-has-less-responsibility) which meant there was an overlap of data concerns with some data access from the presentation layer, and some via the Allocation API.\nIt was envisioned that starting off with more than one application would mean\nthat we would be able to structure the responsibilities early in the process and\nreduce later efforts, but in practice this has not happened. With more exposure\nto some of the APIs we are dependent on which service should access them has\nbecome less clear over time.  For instance, it was discovered that it was better\nfor the Allocation API to retrieve staff data from Elite2, rather than the\npresentation layer.\nThere was concern that later migration from a monolith to separate services would\nbe technical debt that we would be unable to pay off in future, due to other\ncompeting pressures. The cost of managing two different services, sharing contexts\nand overlapping boundaries has however increased the development complexity and\ncognitive load.\nThere was a requirement that other services are likely to require access to the\nallocation information that we have stored. This made sense when there was a\nclean separation of concerns (with all data access via the Allocation API) but\ncurrently provides little benefit. Whether the API is a separate service or\na modular component of a monolith is currently a deployment strategy as\narchitecturally it provides few benefits over a modular application. It is\nentirely possible to provide an API via a modular monolith.\nAs we have progressed with development, we have encountered issues with\nour approach of enriching API sourced data with locally acquired data. Processes\nwhere we retrieve data, enrich it with data from external APIs and then enrich it\nwith data from local APIs result in the movement of lots of data which has\nperformance costs.  Direct access to the database for 'local' data would\nremove issues with both performance and moving data across boundaries containing\nisolated (but related) logic.\n\n## Decision\n","We will integrate the existing Allocation API into the Allocation Manager, and make\nthe public api available at /api.\nWe will work in a single unified codebase in a well-designed modules to\nreduce some future effort in separating concerns.\n",We should migrate to a monolith architecture and deprecate the current Allocation API.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe team are familiar with the [Template Deploy](https://github.com/ministryofjustice/template-deploy)\nstack from their work on Visit someone in prison. This approach was developed\nseveral years ago as a temporary solution, but it doesn't meet our needs for\nmany reasons, including:\n- It defines the initial state of the stack, but does not reliably manage the\nstate of resources after that, so manual changes can be made to those\nresources which are not easily visible or managed in code.\n- It's expensive to run: each application has multiple dedicated EC2 instances,\nwhich typically run at very low load.\n- Deploys for some applications take 10-15 minutes, and if they're cancelled\nthe stack is left in an inconsistent state.\n- The monitoring configuration isn't specific to each application, and the\nstandard alert limits aren't appropriate for everything.\n- Few people understand how it works.\nThe Cloud Platform team are working on a new\n[Kubernetes-based hosting platform](https://ministryofjustice.github.io/cloud-platform-user-docs/#cloud-platform-user-guide),\nto replace Template Deploy. The LAA fee calculator is already live on that\nplatform, and other teams are using it for development and pre-live services.\nThis platform is MOJ D&T's strategic hosting choice.\nThe Sheffield ops team also run services built in the Digital Studio on AWS and\nAzure, including some in related areas to our work such as New NOMIS, Keyworker\nand Licences. There is an intent to move those applications to the new Cloud\nPlatform. Our team aren't familiar with those stacks.\n\n## Decision\n",We will use the Cloud Platform for hosting our applications.\n,The team will use the new Kubernetes-based hosting platform for all new services and will work with the Cloud Platform team and the Sheffield ops team to migrate existing services to the new platform as soon as possible.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDuring our work on Manage Offenders in Custody, we will have to make\narchitectural decisions around both design and scope of the work.\nWhen making decisions, we should record them somewhere for future reference,\nto help us remember why we made them, and to help teams working in related areas\nunderstand why we made them.\nWe should make our decisions public so that other teams can find them more\neasily, and because [making things open makes things better](https://www.gov.uk/guidance/government-design-principles#make-things-open-it-makes-things-better).\nWork done as a result of decisions we need to record is likely to be split\nacross multiple repositories.\n\n## Decision\n","We will use Architecture Decision Records, as described by Michael Nygard in\nthis article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\nAn architecture decision record is a short text file describing a single\ndecision.\nWe will keep ADRs in this public repository under decisions/[number]-[title].md\nWe should use a lightweight text formatting language like Markdown.\nADRs will be numbered sequentially and monotonically. Numbers will not be\nreused.\nIf a decision is reversed, we will keep the old one around, but mark it as\nsuperseded. (It's still relevant to know that it was the decision, but is no\nlonger the decision.)\nWe will use a format with just a few parts, so each document is easy to digest:\n**Title** These documents have names that are short noun phrases. For example,\n""ADR 1: Record architectural decisions"" or ""ADR 9: Use Docker for deployment""\n**Status** A decision may be ""proposed"" if it's still under discussion, or\n""accepted"" once it is agreed. If a later ADR changes or reverses a decision, it\nmay be marked as ""deprecated"" or ""superseded"" with a reference to its\nreplacement.\n**Context** This section describes the forces at play, including technological,\npolitical, social, and local to the service. These forces are probably in\ntension, and should be called out as such. The language in this section is\nvalue-neutral. It is simply describing facts.\n**Decision** This section describes our response to these forces. It is stated\nin full sentences, with active voice. ""We will ...""\n**Consequences** This section describes the resulting context, after applying\nthe decision. All consequences should be listed here, not just the ""positive""\nones. A particular decision may have positive, negative, and neutral\nconsequences, but all of them affect the team and service in the future.\nThe whole document should be one or two pages long. We will write each ADR as\nif it is a conversation with a future person joining the team. This requires\ngood writing style, with full sentences organised into paragraphs. Bullets are\nacceptable only for visual style, not as an excuse for writing sentence\nfragments.\n[adr-tools](https://github.com/npryce/adr-tools) can help us work with our ADRs\nconsistently.\nWe will link to these ADRs from other documentation where relevant.\n",Not provided in the given context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow to convert image files properly to pdf?\nSince there are thousands of different image formats, there will never\nbe support for all. The most common containers should be supported,\nthough:\n- jpeg (jfif, exif)\n- png\n- tiff (baseline, single page)\nThe focus is on document images, maybe from digital cameras or\nscanners.\n# Considered Options\n* [pdfbox](https://pdfbox.apache.org/) library\n* [imagemagick](https://www.imagemagick.org/) external command\n* [img2pdf](https://github.com/josch/img2pdf) external command\n* [tesseract](https://github.com/tesseract-ocr/tesseract) external command\nThere are no screenshots here, because it doesn't make sense since\nthey all look the same on the screen. Instead we look at the files\nproperties.\n**Input File**\nThe input files are:\n```\n$ identify input/*\ninput/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\ninput/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\ninput/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\ninput/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\n```\nSize:\n- jfif.jpg 240k\n- letter-en.jpg 467k\n- letter-en.png 191k\n- letter-en.tiff 4.0M\n\n## Decision\n","Tesseract.\nTo not use more external tools, imagemagick and img2pdf are not\nchosen, even though img2pdf shows the best results and is fastest.\nPdfbox library would be the favorite, because results are good and\nwith the [twelvemonkeys](https://github.com/haraldk/TwelveMonkeys)\nlibrary there is support for many images. The priority is to avoid\nmore external commands if possible.\nBut since there already is a dependency to tesseract and it can create\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\nwith images can be converted to searchable PDFs with images. And text\nextraction is required anyways.\n","Based on the information available, the best option for the use case is the `pdfbox` library. This library is specifically designed for working with PDF documents and has support for converting common image formats such as JPEG, PNG, and TIFF to PDF."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nThere are multiple web applications with their rest servers and there\nare multiple document processors. These processes must communicate:\n- once a new job is added to the queue the rest server must somehow\nnotify processors to wake up\n- once a processor takes a job, it must propagate the progress and\noutcome to all rest servers only that the rest server can notify the\nuser that is currently logged in. Since it's not known which\nrest-server the user is using right now, all must be notified.\n# Considered Options\n1. JMS (ActiveMQ or similiar): Message Broker as another active\ncomponent\n2. Akka: using a cluster\n3. DB: Register with ""call back urls""\n# Decision Outcome\nChoosing option 3: DB as central synchronisation point.\nThe reason is that this is the simplest solution and doesn't require\nexternal libraries or more processes. The other options seem too big\nof a weapon for the task at hand. They are both large components\nitself and require more knowledge to use them efficiently.\nIt works roughly like this:\n- rest servers and processors register at the database on startup each\nwith a unique call-back url\n- and deregister on shutdown\n- each component has db access\n- rest servers can list all processors and vice versa\n\n## Decision\n",Choosing option 3: DB as central synchronisation point.\nThe reason is that this is the simplest solution and doesn't require\nexternal libraries or more processes. The other options seem too big\nof a weapon for the task at hand. They are both large components\nitself and require more knowledge to use them efficiently.\nIt works roughly like this:\n- rest servers and processors register at the database on startup each\nwith a unique call-back url\n- and deregister on shutdown\n- each component has db access\n- rest servers can list all processors and vice versa\n,Use a database as a central synchronization point for communication between web applications and document processors.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nTo allow searching the documents contents efficiently, a separate\nindex is necessary. The ""defacto standard"" for fulltext search on the\nJVM is something backed by [Lucene](https://lucene.apache.org).\nAnother option is to use a RDBMS that supports fulltext search.\nThis adds another component to the mix, which increases the complexity\nof the setup and the software. Since docspell works great without this\nfeature, it shouldn't have a huge impact on the application, i.e. if\nthe fulltext search component is down or broken, docspell should still\nwork (just the fulltext search is then not working).\n# Considered Options\n* [Apache SOLR](https://solr.apache.org)\n* [ElasticSearch](https://www.elastic.co/elasticsearch/)\n* [PostgreSQL](https://www.postgresql.org/docs/12/textsearch.html)\n* All of them or a subset\n# Decision Outcome\nIf docspell is running on PostgreSQL, it would be nice to also use it\nfor fulltext search to save the cost of running another component. But\nI don't want to lock the database to PostgreSQL *only* because of the\nfulltext search feature.\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\npart of Lucene and therefore lives in the Apache ecosystem. I would\nchoose SOLR over ElasticSearch, because I used it before.\nThe last option (supporting all) is interesting, since it would enable\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\nthe database for docspell.\nIn a first step, identify what docspell needs from a fulltext search\ncomponent and create this interface and an implementation for Apache\nSOLR. This enables all users to use the fulltext search feature. As a\nlater step, an implementation based on PostgreSQL and/or ElasticSearch\ncould be provided, too.\n\n## Decision\n","If docspell is running on PostgreSQL, it would be nice to also use it\nfor fulltext search to save the cost of running another component. But\nI don't want to lock the database to PostgreSQL *only* because of the\nfulltext search feature.\nElasticSearch and Apache SOLR are quite similiar in features. SOLR is\npart of Lucene and therefore lives in the Apache ecosystem. I would\nchoose SOLR over ElasticSearch, because I used it before.\nThe last option (supporting all) is interesting, since it would enable\nto use PostgreSQL for fulltext search for those that use PostgreSQL as\nthe database for docspell.\nIn a first step, identify what docspell needs from a fulltext search\ncomponent and create this interface and an implementation for Apache\nSOLR. This enables all users to use the fulltext search feature. As a\nlater step, an implementation based on PostgreSQL and/or ElasticSearch\ncould be provided, too.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow should the application be structured into its main components? The\ngoal is to be able to have multiple rest servers/webapps and multiple\ndocument processor components working togehter.\n# Decision Outcome\nThe following are the ""main"" modules. There may be more helper modules\nand libraries that support implementing a feature.\n\n## Decision\n","The following are the ""main"" modules. There may be more helper modules\nand libraries that support implementing a feature.\n","The application will be structured into the following main components:\n\n- **REST servers/webapps**: These components will be responsible for handling incoming HTTP requests and returning responses.\n- **Document processor components**: These components will be responsible for processing documents and performing other business logic.\n\nThese components will be deployed to separate servers or containers, and they will communicate with each other over a network using a message bus or other communication mechanism."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nSome PDFs contain only images (when coming from a scanner) and\ntherefore one is not able to click into the pdf and select text for\ncopy&paste. Also it is not searchable in a PDF viewer. These are\nreally shortcomings that can be fixed, especially when there is\nalready OCR build in.\nFor images, this works already as tesseract is used to create the PDF\nfiles. Tesseract creates the files with an additional text layer\ncontaining the OCRed text.\n# Considered Options\n* [ocrmypdf](https://github.com/jbarlow83/OCRmyPDF) OCRmyPDF adds an\nOCR text layer to scanned PDF files, allowing them to be searched\n\n## Decision\n",Add ocrmypdf as an optional conversion from PDF to PDF. Ocrmypdf is\ndistributed under the GPL-3 license.\n,"Integrate OCRmyPDF to automatically add an OCR text layer to scanned PDF files, enabling searching and text selection."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nSince docspell may store important documents, it should be possible to\nencrypt them on the server. It should be (almost) transparent to the\nuser, for example, a user must be able to login and download a file in\nclear form. That is, the server must also decrypt them.\nThen all users of a collective should have access to the files. This\nrequires to share the key among users of a collective.\nBut, even when files are encrypted, the associated meta data is not!\nSo especially access to the database would allow to see tags,\nassociated persons and correspondents of documents.\nSo in short, encryption means:\n- file contents (the blobs and extracted text) is encrypted\n- metadata is not\n- secret keys are stored at the server (protected by a passphrase),\nsuch that files can be downloaded in clear form\n# Decision Drivers\n* major driver is to provide most possible privacy for users\n* even at the expense of less features; currently I think that the\nassociated meta data is enough for finding documents (i.e. full text\nsearch is not needed)\n# Considered Options\nIt is clear, that only blobs (file contents) can be encrypted, but not\nthe associated metadata. And the extracted text must be encrypted,\ntoo, obviously.\n\n## Decision\n","* major driver is to provide most possible privacy for users\n* even at the expense of less features; currently I think that the\nassociated meta data is enough for finding documents (i.e. full text\nsearch is not needed)\n# Considered Options\nIt is clear, that only blobs (file contents) can be encrypted, but not\nthe associated metadata. And the extracted text must be encrypted,\ntoo, obviously.\nNo encryption, because of its complexity.\nFor now, this tool is only meant for ""self deployment"" and personal\nuse. If this changes or there is enough time, this decision should be\nreconsidered.\n",Decision: Encrypt only the contents of private documents.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nCurrently there is a `Scheduler` that consumes tasks off a queue in\nthe database. This allows multiple job executors running in parallel\nracing for the next job to execute. This is for executing tasks\nimmediately – as long as there are enough resource.\nWhat is missing, is a component that maintains periodic tasks. The\nreason for this is to have house keeping tasks that run regularily and\nclean up stale or unused data. Later, users should be able to create\nperiodic tasks, for example to read e-mails from an inbox or to be\nnotified of due items.\nThe problem is again, that it must work with multiple job executor\ninstances running at the same time. This is the same pattern as with\nthe `Scheduler`: it must be ensured that only one task is used at a\ntime. Multiple job exectuors must not schedule a perdiodic task more\nthan once. If a periodic tasks takes longer than the time between\nruns, it must wait for the next interval.\n# Considered Options\n1. Adding a `timer` and `nextrun` field to the current `job` table\n2. Creating a separate table for periodic tasks\n\n## Decision\n","The 2. option.\nFor internal housekeeping tasks, it may suffice to reuse the existing\n`job` queue by adding more fields such that a job may be considered\nperiodic. But this conflates with what the `Scheduler` is doing now\n(executing tasks as soon as possible while being bound to some\nresource limits) with a completely different subject.\nThere will be a new `PeriodicScheduler` that works on a new table in\nthe database that is representing periodic tasks. This table will\nshare fields with the `job` table to be able to create `RJob` records.\nThis new component is only taking care of periodically submitting jobs\nto the job queue such that the `Scheduler` will eventually pick it up\nand run it. If the tasks cannot run (for example due to resource\nlimitation), the periodic scheduler can't do nothing but wait and try\nnext time.\n```sql\nCREATE TABLE ""periodic_task"" (\n""id"" varchar(254) not null primary key,\n""enabled"" boolean not null,\n""task"" varchar(254) not null,\n""group_"" varchar(254) not null,\n""args"" text not null,\n""subject"" varchar(254) not null,\n""submitter"" varchar(254) not null,\n""priority"" int not null,\n""worker"" varchar(254),\n""marked"" timestamp,\n""timer"" varchar(254) not null,\n""nextrun"" timestamp not null,\n""created"" timestamp not null\n);\n```\nPreparing for other features, at some point periodic tasks will be\ncreated by users. It should be possible to disable/enable them. The\nnext 6 properties are needed to insert jobs into the `job` table. The\n`worker` field (and `marked`) are used to mark a periodic job as\n""being worked on by a job executor"".\nThe `timer` is the schedule, which is a\n[systemd-like](https://man7.org/linux/man-pages/man7/systemd.time.7.html#CALENDAR_EVENTS)\ncalendar event string. This is parsed by [this\nlibrary](https://github.com/eikek/calev). The `nextrun` field will\nstore the timestamp of the next time the task would need to be\nexecuted. This is needed to query this table for the newest task.\nThe `PeriodicScheduler` works roughly like this:\nOn startup:\n- Remove stale worker values. If the process has been killed, there\nmay be marked tasks which must be cleared now.\nMain-Loop:\n0. Cancel current scheduled notify (see 4. below)\n1. get next (= earliest & enabled) periodic job\n2. if none: stop\n3. if triggered (= `nextrun <= 'now'`):\n- Mark periodic task. On fail: goto 1.\n- Submit new job into the jobqueue:\n- Update `nextrun` field\n- Check for non-final jobs of that name. This is required to not\nrun the same periodic task multiple times concurrently.\n- if exist: goto 4.\n- if not exist: submit job\n- Unmark periodic task\n4. if future\n- schedule notify: notify self to run again next time the task\nschedule triggers\n","Option 2: Create a separate table for periodic tasks.\n\nThis option is preferred because it provides better isolation and scalability for periodic tasks. Having a separate table allows for more efficient querying and indexing of periodic tasks, and it also makes it easier to add new features to the periodic task system in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nThe question is whether the REST Api should return an ISO8601\nformatted string in UTC timezone, or the unix time (number of\nmilliseconds since 1970-01-01).\nThere is quite some controversy about it.\n- <https://stackoverflow.com/questions/47426786/epoch-or-iso8601-date-format>\n- <https://nbsoftsolutions.com/blog/designing-a-rest-api-unix-time-vs-iso-8601>\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\nis the better readability. But elm folks are on the other side:\n- <https://package.elm-lang.org/packages/elm/time/1.0.0#iso-8601>\n- <https://package.elm-lang.org/packages/rtfeldman/elm-iso8601-date-strings/latest/>\nOne can convert from an ISO8601 date-time string in UTC time into the\nepoch millis and vice versa. So it is the same to me. There is no less\ninformation in a ISO8601 string than in the epoch millis.\nTo avoid confusion, all date/time values should use the same encoding.\n# Decision Outcome\nI go with the epoch time. Every timestamp/date-time values is\ntransfered as Unix timestamp.\nReasons:\n- the Elm application needs to frequently calculate with these values\nto render the current waiting time etc. This is better if there are\nnumbers without requiring to parse dates first\n- Since the UI is written with Elm, it's probably good to adopt their\nstyle\n\n## Decision\n","I go with the epoch time. Every timestamp/date-time values is\ntransfered as Unix timestamp.\nReasons:\n- the Elm application needs to frequently calculate with these values\nto render the current waiting time etc. This is better if there are\nnumbers without requiring to parse dates first\n- Since the UI is written with Elm, it's probably good to adopt their\nstyle\n",Use epoch time for timestamp/date-time values.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nDocspell currently only supports PDF files. This has simplified early\ndevelopment and design a lot and so helped with starting the project.\nHandling pdf files is usually easy (to view, to extract text, print\netc).\nThe pdf format has been chosen, because PDFs files are very common and\ncan be viewed with many tools on many systems (i.e. non-proprietary\ntools). Docspell also is a document archive and from this perspective,\nit is important that documents can be viewed in 10 years and more. The\nhope is, that the PDF format is best suited for this. Therefore all\ndocuments in Docspell must be accessible as PDF. The trivial solution\nto this requirement is to only allow PDF files.\nSupport for more document types, must then take care of the following:\n- extracting text\n- converting into pdf\n- access original file\nText should be extracted from the source file, in case conversion is\nnot lossless. Since Docspell can already extract text from PDF files\nusing OCR, text can also be extracted from the converted file as a\nfallback.\nThe original file must always be accessible. The main reason is that\nall uploaded data should be accessible without any modification. And\nsince the conversion may not always create best results, the original\nfile should be kept.\n# Decision Drivers\nPeople expect that software like Docspell support the most common\ndocument types, like all the “office documents” (`docx`, `rtf`, `odt`,\n`xlsx`, …) and images. For many people it is more common to create\nthose files instead of PDF. Some (older) scanners may not be able to\nscan into PDF files but only to image files.\n# Considered Options\nThis ADR does not evaluate different options. It rather documents why\nthis feature is realized and the thoughts that lead to how it is\nimplemented.\n# Realization\n\n## Decision\n","People expect that software like Docspell support the most common\ndocument types, like all the “office documents” (`docx`, `rtf`, `odt`,\n`xlsx`, …) and images. For many people it is more common to create\nthose files instead of PDF. Some (older) scanners may not be able to\nscan into PDF files but only to image files.\n# Considered Options\nThis ADR does not evaluate different options. It rather documents why\nthis feature is realized and the thoughts that lead to how it is\nimplemented.\n# Realization\n","Docspell will support the most common document types (like `docx`, `rtf`, `odt`, `xlsx`, ...) and images in addition to PDF files."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nWith support for more file types there must be a way to extract text\nfrom all of them. It is better to extract text from the source files,\nin contrast to extracting the text from the converted pdf file.\nThere are multiple options and multiple file types. Again, most\npriority is to use a java/scala library to reduce external\ndependencies.\n# Considered Options\n\n## Decision\n","- MS Office files: POI library\n- Open Document files: Tika, but integrating the few source files that\nmake up the open document parser. Due to its huge dependency tree,\nthe library is not added.\n- PDF: Apache PDFBox. I know this library better than itext.\n","**Option 1: Tika**\n\n**Advantages:**\n\n* Open source\n* Supports a wide range of file types, including PDF, DOCX, and XLSX\n* Java library, reducing external dependencies\n\n**Option 2: Apache PDFBox**\n\n**Advantages:**\n\n* Open source\n* Specifically designed for handling PDF files\n* Provides fine-grained control over PDF processing\n\n**Option 3: JODConverter**\n\n**Advantages:**\n\n* Open source\n* Java library\n* Can convert a wide range of file types to PDF, including DOCX, XLSX, and PPTX\n\n**Decision:**\n\n**Option 1: Tika**\n\nTika is the recommended option due to its wide file type support, Java library, and open source nature. It provides a comprehensive solution for extracting text from a variety of file formats."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nDocspell should have support for files that contain the actual files\nthat matter, like zip files and other such things. It should extract\nits contents automatcially.\nSince docspell should never drop or modify user data, the archive file\nmust be present in the database. And it must be possible to download\nthe file unmodified.\nOn the other hand, files in there need to be text analysed and\nconverted to pdf files.\n# Decision Outcome\nThere is currently a table `attachment_source` which holds references\nto ""original"" files. These are the files as uploaded by the user,\nbefore converted to pdf. Archive files add a subtlety to this: in case\nof an archive, an `attachment_source` is the original (non-archive)\nfile inside an archive.\nThe archive file itself will be stored in a separate table `attachment_archive`.\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\n- `attachment_source`: report.jpg\n- `attachment`: report.pdf\n- `attachment_archive`: files.zip\nArchive may contain other archives. Then the inner archives will not\nbe saved. The archive file is extracted recursively, until there is no\nknown archive file found.\n# Initial Support\nInitial support is implemented for ZIP and EML (e-mail files) files.\n\n## Decision\n","There is currently a table `attachment_source` which holds references\nto ""original"" files. These are the files as uploaded by the user,\nbefore converted to pdf. Archive files add a subtlety to this: in case\nof an archive, an `attachment_source` is the original (non-archive)\nfile inside an archive.\nThe archive file itself will be stored in a separate table `attachment_archive`.\nExample: uploading a `files.zip` ZIP file containing `report.jpg`:\n- `attachment_source`: report.jpg\n- `attachment`: report.pdf\n- `attachment_archive`: files.zip\nArchive may contain other archives. Then the inner archives will not\nbe saved. The archive file is extracted recursively, until there is no\nknown archive file found.\n# Initial Support\nInitial support is implemented for ZIP and EML (e-mail files) files.\n",Store archive files in a separate `attachment_archive` table. Each archive file can contain multiple `attachment_source` files. The `attachment_source` table will contain references to the original (non-archive) files inside an archive.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow can office documents, like `docx` or `odt` be converted into a PDF\nfile that looks as much as possible like the original?\nIt would be nice to have a java-only solution. But if an external tool\nhas a better outcome, then an external tool is fine, too.\nSince Docspell is free software, the tools must also be free.\n# Considered Options\n* [Apache POI](https://poi.apache.org) together with\n[this](https://central.sonatype.com/artifact/fr.opensagres.xdocreport/org.apache.poi.xwpf.converter.pdf/1.0.6)\nlibrary\n* [pandoc](https://pandoc.org/) external command\n* [abiword](https://www.abisource.com/) external command\n* [Unoconv](https://github.com/unoconv/unoconv) external command\nTo choose an option, some documents are converted to pdf and compared.\nOnly the formats `docx` and `odt` are considered here. These are the\nmost used formats. They have to look well, if a `xlsx` or `pptx`\ndoesn't look so great, that is ok.\nHere is the native view to compare with:\nODT:\n{{ figure(file=""example-odt-native.jpg"") }}\n\n## Decision\n","Unoconv.\nThe results from `unoconv` are really good.\nAbiword also is not that bad, it didn't convert the chart, but all\nfont markup is there. It would be great to not depend on something as\nbig as libreoffice, but the results are so much better.\nAlso pandoc deals very well with DOCX files (using the `context`\nengine). The only thing that was not rendered was the embedded chart\n(like abiword). But all images and font styling was present.\nIt will be a configurable external command anyways, so users can\nexchange it at any time with a different one.\n",Use pandoc
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nUsers want to add custom metadata to items. For example, for invoices\nfields like `invoice-number` or `total`/`amount` make sense. When\nusing a pagination stamp, every item gets a pagination number.\nThis is currently not possible to realize in docspell. But it is an\nessential part when organizing and archiving documents. It should be\nsupported.\n# Considered Options\n\n## Decision\n","- values are strings at the database\n- values are strings when transported from/to server\n- client must provide the correct formatted strings per type\n- numeric: some decimal number\n- money: decimal number\n- text: no restrictions\n- date: a local date as iso string, e.g. `2011-10-09`\n- bool: either `""true""` or `""false""`, case insensitive\n","**Decision:**\n\nImplement an extension mechanism for custom metadata.\n\n**Rationale:**\n\nThis decision allows for a flexible and extensible solution to the problem of adding custom metadata to items. It avoids having to hardcode specific fields into the system, and instead allows users to define their own custom fields as needed. This flexibility is essential for supporting a wide range of use cases, including the organization and archiving of documents."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow can plain text and markdown documents be converted into a PDF\nfiles?\nRendering images is not important here, since the files must be self\ncontained when uploaded to Docspell.\nThe test file is the current documentation page of Docspell, found in\n`microsite/docs/doc.md`.\n```\n---\nlayout: docs\nposition: 4\ntitle: Documentation\n---\n# {page .title}\nDocspell assists in organizing large amounts of PDF files that are\n...\n\n## Decision\n","Java library ""flexmark"".\nI think all results are great. It depends on the type of document and\nwhat one expects to see. I guess that most people expect something\nlike pandoc-html produces for the kind of files docspell is for (it is\nnot for newspaper articles, where pandoc-latex would be best fit).\nBut choosing pandoc means yet another external command to depend on.\nAnd the results from flexmark are really good, too. One can fiddle\nwith options and css to make it look better.\nTo not introduce another external command, decision is to use flexmark\nand then the already existing html->pdf conversion.\n","**Decision**: Integrate headless Puppeteer as a lambda function.\n\n**Rationale**: Headless Puppeteer is a library that allows you to control a headless Chrome instance and generate PDFs from web pages. It is well-suited for this task because it:\n\n* **Is self-contained**: Headless Puppeteer does not require any additional dependencies or software to run, making it easy to deploy as a lambda function.\n* **Supports markdown**: Headless Puppeteer can convert HTML and markdown documents into PDFs.\n* **Is fast and efficient**: Headless Puppeteer is known for its speed and efficiency, making it suitable for processing large numbers of documents.\n\n**Alternatives Considered**:\n\n* **Third-party PDF converter APIs**\n    * **Pros:** Easy to integrate, no need to manage infrastructure.\n    * **Cons:** Can be expensive, lack of control over the conversion process, potential privacy concerns.\n* **Server-side rendering**\n    * **Pros:** Can render dynamic content, more control over the conversion process.\n    * **Cons:** Requires setting up and maintaining a server, can be more complex to implement."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Context and Problem Statement\nHow can HTML documents be converted into a PDF file that looks as much\nas possible like the original?\nIt would be nice to have a java-only solution. But if an external tool\nhas a better outcome, then an external tool is fine, too.\nSince Docspell is free software, the tools must also be free.\n# Considered Options\n* [pandoc](https://pandoc.org/) external command\n* [wkhtmltopdf](https://wkhtmltopdf.org/) external command\n* [Unoconv](https://github.com/unoconv/unoconv) external command\nNative (firefox) view:\n{{ figure(file=""example-html-native.jpg"") }}\nI downloaded the HTML file to disk together with its resources (using\n*Save as...* in the browser).\n\n## Decision\n",wkhtmltopdf.\nIt shows the best results.\n,Use the `wkhtmltopdf` external command.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nProject Thoth itself consists of many components all having their own release cycles and delivery artifacts such as\ncontainer image or Python libraries.\n\n## Decision\n,"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\nproject/product level while maintianing freedom of others to update.\n### Positive Consequences <!-- optional -->\n* users have a clear base line of versions, these versions have been tested with each other and have\nundergone integration testing.\n* a release can be referenced from documents, so that operational procedures have a clear relationship with component\nversions being used\n* we can maintain sets of different versions for different deployment environments\n* we can provide a version string with each API provided by the project\n### Negative Consequences <!-- optional -->\n* A release might not contain the latest versions of components\n<!-- markdownlint-disable-file MD013 -->\n",**Decision:** Establish a standard release cadence for all Project Thoth components.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n[Qeb-Hwt](https://github.com/marketplace/qeb-hwt) is a GitHub Application which adds `thamos advise` based output to\nPull Requests as a check. This functionality could be integrated into https://github.com/marketplace/khebhut and\ncomplexity and maintain costs.\n## Decision Drivers <!-- optional -->\n* cost of maintaining Qeb-Hwt code and app\n* redundancy of infrastructure\n\n## Decision\n,"* cost of maintaining Qeb-Hwt code and app\n* redundancy of infrastructure\nChosen option: ""merge function into Khebhut"", because we can reduce the cost of maintaining our software infrastructure\nby reducing redundancy.\n","**Decision:** Integrate the functionality of Qeb-Hwt into Khebhut.\n\n**Rationale:**\n\n* **Reduced maintenance costs:** Eliminating the need to maintain a separate codebase and application for Qeb-Hwt will significantly reduce maintenance costs.\n* **Simplified infrastructure:** Integrating the functionality into Khebhut will eliminate the need for separate infrastructure and setup for Qeb-Hwt, reducing complexity and potential points of failure.\n* **Increased efficiency:** By consolidating the functionality into a single application, the development and deployment process will be streamlined, resulting in increased efficiency and reduced lead times."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nYetibot currently resides across two primary repos:\n- github.com/yetibot/yetibot\n- github.com/yetibot/yetibot-core\nThese code bases continue to grow in size, and consist of a diverse range of\nfeatures, many of which many users won't care to use.\n\n## Decision\n","Switching to a plugin system allows us to split up the code base into much more\nfine grained, logical units. For example, we may split the `github` command into\nits own plugin.\nThe first plugin is [yetibot-kroki](https://github.com/yetibot/yetibot-kroki).\nWe will continue to extract plugins from both of the above code bases.\n","Migrate Yetibot codebase into a single repo, with submodules for each component."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nImage names are important for branding and let others identify easily a specific image they need. For example ""I want to work on computer vision project with Tensorflow, what stack and image should I use?"" Having a trusted well maintained source of images with clean naming convention can help on that.\n\n## Decision\n","Selected option: `ps-{application}` as it shows what our intention is: we want to provide a curated/predictable software stack, it might be used by ODH or RHODS or others, it might use S2I or other technology. Moreover helps from pipeline creation point of view, because the length of repo name on quay can crate issues.\n### Positive Consequences <!-- optional -->\n* users can immediately select an image based on the application they want.\n* using overlays we can have a variety of combination, not just for ml_framework\n### Negative Consequences <!-- optional -->\n* N/A\n<!-- markdownlint-disable-file MD013 -->\n","Establish a naming convention for images that include information about the image's content, purpose, and context. This convention should be documented and enforced to ensure consistency and ease of identification. The naming convention can may include:\n\n- Stack (TensorFlow, PyTorch, etc.)\n- Model (EfficientNet, ResNet, etc.)\n- Dataset (ImageNet, CIFAR-10, etc.)\n- Task (Classification, Object Detection, etc.)\n- Size (Small, Medium, Large)\n- Version (1.0, 2.0, etc.)"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Use an Architectural Decision Record (ADR) template to document architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## **Context**:\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\n\n## Decision\n","In order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\nWe did this because we don't know if our consumers rely on the previous behavior of meta/health checking dependencies if they were of unspecified importance.\n","**Decision:** Replace the primary health checks with health checks on the Kubernetes Service created for the primary pods in a deployment.\n\n**Rationale:**\n\n* Ensures that the health check endpoint only checks the primary dependencies of a project, as intended.\n* Leverages the built-in health check capabilities of Kubernetes, ensuring consistency and reliability.\n* Simplifies the primary health check implementation and reduces maintenance overhead.\n* Facilitates easier configuration and management of health checks for primary dependencies."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Decision:** To implement an ADR (Architectural Decision Record) process.\n\n**Rationale:**\n\n* Ensures that architectural decisions are documented, reviewed, and approved.\n* Provides a single source of truth for architectural knowledge.\n* Facilitates communication and collaboration among stakeholders.\n* Helps to maintain consistency and architectural integrity throughout the project.\n\n**Implementation Details:**\n\n* Establish a template for ADRs.\n* Define a process for creating, reviewing, and approving ADRs.\n* Identify stakeholders responsible for participating in the ADR process.\n* Store ADRs in a version-controlled repository.\n* Review ADRs regularly to ensure they remain relevant and up-to-date."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\n\n## Decision\n","We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\nTable names                                       |\n--------------------------------------------------|\nrefunds-sessions-front-<opg_stackname>            |\nrefunds-sessions-caseworker-front-<opg_stackname> |\n","**Implement a centralized key management service to manage encryption keys for frontend user sessions.**\n\n**Rationale:**\n\n* **Improved Security:** A centralized key management service provides a secure and centralized location to manage and control encryption keys, reducing the risk of unauthorized access.\n* **Simplified Key Management:** By centralizing key management, operations such as key rotation, generation, and disposal become more efficient and auditable.\n* **Prevention of Synchronization Issues:** A centralized service ensures that all services have the most up-to-date encryption keys, eliminating synchronization issues and preventing errors during scaling operations.\n* **Control and Governance:** A centralized service allows for better control and governance over encryption key usage and access, ensuring compliance with security regulations and best practices.\n* **Supports Scaling:** A centralized key management service can efficiently handle key management requirements for scaling systems, reducing administrative overhead and ensuring secure key management even as the system grows."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe app primarily displays information for medical staff working in hospitals.  The information needs to come from somewhere and there are a few options:\n### Download content from a CMS/S3 bucket/Website\nWe could pull the information in the app from somewhere on the web.  Some kind of CMS, S3 bucket or website where the files are hosted would be the most obvious choices.\nThis does make it easier to quickly the information in the app, but would require the build out of more infrastructure and architecture to support all this.\nIt also could mean that if the internet connection goes down to a hospital in  a crisis any information that isn't already cached in the app would be unavailable to users.\n### Embed the content in the app\nThe alternative method is to embed all of the information in the app so that all of the information is included, after downloading the app.\nThis is architecturally much simpler and means all the content will be available offline if the internet should go down.\nSubsequently, a new version of the app needs to be released to update the information.\n\n## Decision\n","For the initial versions of the application, we will embed the content in the app.\nOnce the situation stabilises, and we can provide support to more hospitals, we can re-visit the decision and decide if we want to download content.\n","**Embed the content in the app.**\n\n**Rationale:**\n\n* Offline availability: Internet outages could prevent access to information from external sources, which is critical for medical staff. Embedding the content ensures information remains accessible even offline.\n* Architectural simplicity: Embedding the content reduces the complexity and infrastructure requirements compared to downloading from external sources.\n* Maintainability: App updates are the primary means of updating content, which is simpler compared to managing external infrastructure."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFlutter is a framework for building apps provided by Google.  Google describes it as a:\n> ... UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\nIt emphasizes fast development, expressive and flexible UI, and native performance.\nGregg Miller, a Senior Anesthesiologist at Western Health hospitals, reached out to us with a mobile app he had been developing to provide COVID-19 information to frontline medical staff.  The app that Gregg had developed in was with Flutter.  Gregg sought assistance and guidance from the GDG community to help him complete the app.  Considering time constraints, we needed to put together and ship an app to the hospital before the crisis swamped the staff, which at that time as predicted to occur in about two weeks.\nLuke immediately put out the call and assembled a team of Flutter developers from the GDG community.  We performed an initial evaluation on the codebase and decided that it would be best to deliver a re-write rather than building on the existing codebase.\nAt this stage it would have been possible to drop Flutter and build out native applications, or built the app using some other cross platform toolkit such as react native.\n\n## Decision\n",Time was of the essence - as we already had a team of Flutter developers ready to go we decided to continue on with using Flutter to build the app.  It was also likely that Flutters emphasis on fast development and an expressive UI would be helpful in quickly building out the app.\n,"**Decision:** Use Flutter to build the COVID-19 information application for frontline medical staff.\n\n**Rationale:**\n\n* Flutter is a framework that emphasizes fast development, expressive and flexible UI, and native performance.\n* The app needs to be shipped quickly to the hospital before the crisis swamped the staff.\n* A re-write of the existing codebase was necessary.\n* Dropping Flutter and building native applications or using a different cross-platform toolkit would have taken more time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\n\n## Decision\n","We are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\n","Use the Beta version of Flutter to build iOS correctly, given the missed stable release."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDo we need a MarketPlace component?\n\n## Decision\n,It doesn't look like there is a requirement for a need of a MarketPlace component.  It's decided we will not have one for now.\n,"**Decision:** Yes, a Marketplace component is required.\n\n**Justification:**\n\nThe Marketplace component will provide a centralized platform for users to discover, acquire, and manage software and services. This will improve the overall user experience by simplifying the process of finding and using software and services. It will also enable us to better monetize our offerings by providing a platform for developers to sell their products."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and/or SMS.\n\n## Decision\n",A Notifier component is needed to send email and/SMS to customers.\n,"Implement a notification system using a microservices architecture with a message broker (such as Kafka or RabbitMQ) to handle communication between services. The system should include a service for sending emails, a service for sending SMS messages, and a central service for managing notifications and routing them to the appropriate service."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDo we need a Loyalty Program component?\n\n## Decision\n,It doesn't look like there is a requirement for a need of a Loyalty Program component.  It's decided we will not have one for now.\n,"**Decision:** Yes, we need a Loyalty Program component.\n\n**Rationale:**\n\n* The Loyalty Program component will provide customers with personalized rewards and incentives to increase customer loyalty and retention.\n* The program will track customer purchases, preferences, and interactions, enabling targeted campaigns and promotions.\n* By providing value and recognition to customers, the program can drive repeat business and positive word-of-mouth.\n* The data collected from the Loyalty Program will also provide valuable insights into customer behavior and preferences, which can be used to improve product offerings and marketing strategies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo ensure scalability, reliability and performance, microservice architecture should be used\n\n## Decision\n",All agreed with this decision\n,"**Decision:**\n\nAdopt a microservice architecture for the system to achieve scalability, reliability, and high performance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe state and data are required for the following components: Inventory, Customer, Order Management, Rating Manager and Recommendation Manager.  We had a database for each of the component initially.  However, it can save on the number of distrubted transactions by storing state and data in one database.\n\n## Decision\n",It was agreed that one database would be sufficient.\n,Store the state and data in a single database to reduce the number of distributed transactions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe purpose of Segmentation engine are:\n- grouping of customers' goals\n- targeted groups\n- history\n- purchasing behaviour\n- eating habit, location\n- managing coupons and promotions\nWhat is the difference between Recommendation and Segmentation engines?\n\n## Decision\n",It was decided to not have it.\n,"**Recommendation engines:**\n\n* Focus on personalizing content and product recommendations for individual users\n* Use machine learning and artificial intelligence to analyze user behavior and preferences\n* Provide tailored recommendations based on past purchases, browsing history, and similar user profiles\n\n**Segmentation engines:**\n\n* Divide customers into distinct groups based on shared attributes\n* Create targeted campaigns and marketing strategies for each segment\n* Use segmentation criteria such as demographics, psychographics, and purchasing behavior\n* Help businesses understand their customer base better and optimize marketing efforts"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nShould we have an Inventory Manager and a Fridge Manager where the Inventory Manager would hold the food items and Fridge would manage the physical fridges?\n\n## Decision\n,Inventory Manager can manage the following:\n- manage fridges\n- manage food items including their pricing\n- manage the number of items in each fridge\n,"**Decision:** Yes, we will have an Inventory Manager and a Fridge Manager.\n\n**Rationale:**\n\n* **Separation of concerns:** Separating the management of food items from the management of physical fridges allows for better organization and testability.\n* **Scalability:** The Inventory Manager can manage a large number of food items, while the Fridge Manager can handle numerous physical fridges.\n* **Flexibility:** The Inventory Manager can be updated independently of the Fridge Manager, allowing for changes to the food inventory without affecting fridge operations.\n* **Extensibility:** Additional features can be added to either the Inventory Manager or Fridge Manager without compromising the integrity of the system.\n* **Improved data management:** The Inventory Manager maintains a centralized record of all food items, while the Fridge Manager tracks the location and availability of food within each fridge."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\n\n## Decision\n,The decision is to introduce another queue for Order Management\n,**Prioritize messages from Order Management over messages from Rating Manager and Recommendation Manager in the queuing system.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\n\n## Decision\n",Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\n,"**Decision:** Yes, the messages sent from Order Management, Rating Manager, and Recommendation Manager can be asynchronous.\n\n**Rationale:**\n\n* Asynchronous messaging decouples the senders and receivers of messages, allowing them to operate independently and reducing the risk of system failures.\n* It enables scalability by allowing multiple instances of the message handlers to be deployed, distributing the load and improving throughput.\n* It provides flexibility in message delivery, allowing for different message delivery semantics (e.g., at least once, best effort) to be used based on the business requirements.\n* It supports eventual consistency between the sending and receiving systems, reducing the impact of temporary message failures on the overall system functionality."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\n\n## Decision\n",The decision is to introduce a queue for all these messages.\n,Aggregate and send notifications in batches to avoid overwhelming the Notifier.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a polling mechanism for getting Smartfridges' inventory.  We would have its last known inventory even if it goes down.\n\n## Decision\n,The decision is to have the polling mechanism to regularly obtain Smartfridges' state/inventory.\n,"Implement a cache of the Smartfridges' inventory. The cache will be updated regularly by the polling mechanism. This way, we can still have the last known inventory even if a Smartfridge goes down."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDo we need a Kiosks-Fridge-customer-vendor-management to manage kiosks, fridge, customer and vendor?\n\n## Decision\n","We will have separate components to manage kiosks (along with point of sale systems), fridges, customers and vendors\n","Yes, we need a Kiosks-Fridge-customer-vendor-management system to manage kiosks, fridges, customers, and vendors. This system will help us to track the status of our kiosks and fridges, manage customer and vendor relationships, and ensure that our operations are running smoothly. This system will allow us to track key performance indicators (KPIs), such as sales, inventory, and customer satisfaction. It will also help us to identify trends and opportunities for improvement. By providing a centralized view of our operations, this system will help us to make better decisions and improve our overall performance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPeople are very used to traditional layered architectures and request/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\n\n## Decision\n",We will use a docker-compose.yml (v.2) as the primary entry point of the project\n,Use Docker to create a simple multi-container application
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\n\n## Decision\n,Use CircleCI for automated ETE tests\n,Use CircleCI for automated ETE tests on build.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDue to WDQS having a lag on catching up with edits to Wikidata, and that lag being affected\nby the size of the entities being edited not just the rate of those edits, high volume edits can\ncreate strain on WDQS due to update processing of the dispatched update jobs,\nmore than WDQS update capacity can handle in a timely manner, resulting in sluggish WDQS.\nExample incident can be found here:\nhttps://www.wikidata.org/w/index.php?title=Wikidata:Administrators%27_noticeboard&oldid=963260218#Edoderoobot_edit_rate\n\n## Decision\n","As part of [T221774](https://phabricator.wikimedia.org/T221774), we want to\nretrieve WDQS lag, from relevant clusters, and add it to maxlag logic provided\nby mediawiki, in order to block edits when WDQS is lagging too much.\nWe also decided that querying the relevant WDQS endpoints lag should happen\nasynchronously, with lag info being cached with a reasonable ttl. That cached\nlag information then can be accessed, in a hook handler for\n[ApiMaxLagInfo](https://www.mediawiki.org/wiki/Manual:Hooks/ApiMaxLagInfo)\nthat will update lag info when they are available.\nThe following visual diagram shows the elements of the design\non conceptual level:\n![](./assets/QueryServiceLagHook_ConceptualDesign.png)\n","Set up a maintenance job that limits the rate of updates sent to the WDQS dispatch queue, based on the size of the entity being updated."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\n\n## Decision\n","The structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer/context approach in favour of `redux` complexity.\n","Use [Redux Modules](https://redux.js.org/tutorials/advanced/beyond-the-basics#using-multiple-reducers) to organize the application's state into separate, self-contained modules. Each module should contain its own reducer, actions, selectors, and sagas. This will make the application more modular, easier to understand, and less error-prone."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\n- Docker registry URLs\n- URLs of API endpoints, machine learning service, Authz service and map server\n- Nginx configuration\n- HTML `<title />`\n- [`Sentry` Package](https://www.npmjs.com/package/@sentry/browser) dependency and configuration\n- [`Matomo` Package](https://www.npmjs.com/package/@datapunt/matomo-tracker-js) dependency and configuration\n- PWA manifest\n- Logo\n- Favicon\n- PWA icons\n- Hardcoded strings\n- Main menu items\n- Theme configuration\n- Maps settings\n- [`amsterdam-stijl` Package](https://www.npmjs.com/package/amsterdam-stijl) Package dependency\n- Package URL and description\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\n\n## Decision\n","Taking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side / container](#Server-side / container) option.\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\nConfiguration is injected in the `index.html` file so it can be read at run-time.\n",**Decision:** Introduce a municipality-agnostic architecture.\n\n**Rationale:**\n\n* Allows the application to be easily customized for different municipalities.\n* Eliminates the need for hardcoding municipality-specific information.\n* Provides a framework for adding new municipalities and managing their configurations.\n\n**Consequences:**\n\n* Requires modifications to the existing codebase.\n* May introduce additional complexity to the application.\n* Requires careful coordination with stakeholders to ensure configuration settings are accurate.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDatabase schema migrations are often considered as part of an application change -\n""in order to do X we will need a new index Y""\nHowever, this does not work well with a zero-downtime approach, when we have\na single replicated database - we can't deploy version 0.2 of the application\nto some servers simultaneously with version 0.2 of the schema to their databases;\nwe need to deploy changes to the database either before or after the\ncorresponding application changes\n\n## Decision\n","Database schema changes should be made independently of application changes.\nWhere an application needs a change to the database, this may entail extra releases\nto make sure the application and database changes can be safely applied and\nsafely rolled back without compatibility problems.\nFor example, if there's a need to change a column name from ""foo"" to ""bar""\nyou may need:\n1. An application release that detects and will work with either a ""foo"" or a ""bar""\ncolumn\n2. A database schema release that renames the column from ""foo"" to ""bar""\n3. An application release that removes the logic in release 1, and just works with ""bar""\nAlternatively, the database change could be made first, using a view or triggers or other\nmechanisms so writes and reads to both ""foo"" and ""bar"" change the same data. This\nis highly dependant on the change needed and the database features available.\nThe database schema migration change should be treated as a first class release:\n1. The change should be run and tested in conjunction with the application\n2. The change should be reviewed and approved by an appropriate approver\n3. The change should be deployed to the Staging environment, and normal acceptance/smoke tests run\n4. The change should be deployed to Production/DR\n### A note on backups\nDepending on the complexity of the change, you may wish to coordinate a backup snapshot\nof the database before running a migration.  This will never be perfect, as with\na zero-downtime system there will still be data being written in the interval between\nthe backup being started (on a replica) and the migration running, so this data\nwould be at risk of being lost.\nMany database migrations however are totally safe and should not need a backup -\nfor instance, adding an index or adding a column is a very low risk change.  Nightly\nbackups should be enough to mitigate against any risk with this sort of change.\n### Alternative approach for Event Store using queues to avoid downtime\n*Only if really necessary* we could perform coordinated releases using queues\nto avoid downtime, for the special case of the Event Store which can be temporarily\nsuspended while messages get queued.\nThe approach would be similar to:\n1. Modify the event recorder Lambda timing so the event recorder does not run, or point it to a test queue instead\n2. Make a new database backup snapshot - it's assumed that for a change this complex, you need a backup\n3. Deploy the database change\n4. Test the change\n5. Re-enable the Lambda\nNote that the SQS queue has a limit of 100,000 queued messages - at peak we have\nhistorically received around 75,000 messages an hour.  So this technique is quite time constrained;\nif anything goes wrong you only have a small amount of time to fix it.\n","Implement database migrations as a separate deployable artifact, parallel to the application. For MySQL, use Flyway as a library embedded in the database migration artifact; For PostgreSQL, use Liquibase, a standalone command-line utility."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSee also ADR 0002 ""Database migrations are standalone releases""\nAs our system is designed for zero downtime, we have to be careful that\nwe don't change the database in a way that causes production issues\n\n## Decision\n","Where possible, we should avoid database migrations that will lock the database\nfor any significant amount of time.  This is hard to enforce, but we will\nmake sure there is documentation in the project README (and here!) on ways\nto achieve this.\nThis mostly affects index creation and changes - we have several years of data\nin our database, and adding or changing indexes can be slow.  In general,\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\ncreated in a non-blocking way.  See https://www.postgresql.org/docs/current/static/sql-createindex.html\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\nto queue up while the index change is made.  *BEWARE* however that SQS queues\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\nIf you have a very complex change, you should consider:\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\naltering indexes - generally our reports run intermittently, so it is safe to have\nno indexes for a period of time, data will still be appended with no problems\n- Performance testing the change - we have a large fake dataset available that\ncan be used to simulate a production database in a test environment\n- Duplicating the database - you could apply the change to a new database containing\na copy of production data, then switch databases, and migrate any missed changes\nfrom the old database to the new.\n### Transactional DDL changes\nMost Postgresql schema changes can be made transactionally - this is\na great feature, as it allows for making multiple changes and having them\nall roll back if something goes wrong.  For example:\n```\nBEGIN;\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\nUPDATE TABLE fizzbuzz set foo = 'splat';\nCOMMIT;\n```\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\nit depends on being able to change the table incrementally, which doesn't fit\nthe transaction model.  If the index creation fails, you are recommended to\ndrop the index and re-create it, as it won't be rolled back and may be\npartially created.\n### Avoiding blocking changes\nThere is a useful table in [this article](https://www.citusdata.com/blog/2018/02/15/when-postgresql-blocks/) which I've reproduced below (as the article may disappear):\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\nOur reports should only use SELECT so most operations won't block them.\nThe Event Recorder however needs to insert data, so you can't run\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\nNote however that some changes may be fast enough despite blocking - adding a column\nfor example.  However you should performance test these changes against a\nproduction-sized database to be sure!\n","To avoid database changes causing production issues, we will make all database migrations standalone releases. This will ensure that any changes to the database are thoroughly tested before being deployed to production."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way of running database migrations to the verify event store database.\n\n## Decision\n,We have chosen to use [Flyway](https://flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\n,"We decided to use Flyway to manage database migrations for the verify event store database.\n\n**Rationale**\n\nFlyway is a popular database migration tool that is well-suited for managing migrations in a continuous integration and deployment (CI/CD) pipeline. It is easy to use and configure, and it supports a wide range of database platforms.\n\n**Consequences**\n\n* **Benefits:**\n    * Flyway will help us to ensure that the verify event store database is always up-to-date with the latest migrations.\n    * Flyway will make it easier to manage database migrations in a CI/CD pipeline.\n* **Drawbacks:**\n    * Flyway will require some additional setup and configuration.\n    * Flyway may not be compatible with all database platforms."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe aim of this project is to allow teams to quickly create their own Jenkins\nplatform, running on AWS.\nJenkins has a master/agent architecture, where a single master node can trigger\njobs running on multiple agents. This decision concerns how we deploy, configure\nand manage both the master and the agents.\n\n## Decision\n","We will make both the master and the agents Docker containers, running on\nseparate managed EC2 instances. We will provision and manage these directly for\nnow, without using an orchestration service like ECS or Kubernetes.\n### Use of separate instances\nHaving separate instances for the master and the workers increases security by\nmaking it impossible for code running in worker jobs to affect the master.\nIn addition, this allows for teams to increase capacity by adding extra worker\nboxes as required.\n### Use of Docker for master\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\nteams' abilities to quickly respond when new Jenkins versions are released in\nresponse to security vulnerabilities, for example.\nIn addition, using Docker means that the configuration can be kept in the git\nrepository along with the rest of the code, rather than managed via the Jenkins\nUI.\n### Use of Docker for agents\nRunning the workers as Docker containers allows isolation of each job, ensuring\nthat each job starts from a known state, making it possible to target a specific\nconfiguration via the Jenkinsfile, and increasing security by making it\nimpossible for the job to affect the underlying VM.\n### Not using orchestration frameworks\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\nsimple enough that it is not worth introducing the added complexity for this\nproject.\n",The master and agents will be deployed using a combination of Ansible and AWS CLI\ncommands.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed to implement HTTPS and TLS termination as part of the template we offer in our MVP.\nWe identified 4 ways to get that done.\n### Technique #1 - using existing nginx inside the Jenkins container\nOur Jenkins master container has nginx installed in it. We can install the certificates and keys\nthere. That is probably the simplest solution without the need of introducing other\ninfrastructure components. Also, that allows us to benefit from having an EIP, which is\nstatic, so it could be used for IP whitelisting, if necessary.\nHowever, the installation and renewal of certificates has to be done manually, which is a major downside.\n### Technique #2 - using ELB\nWe introduce an ELB in front of the existing infrastructure, like so:\nDNS resolution -> ELB -> Jenkins master EC2\nCompared to #1, the major benefit is the ability to manage the provisioning of the certificate fully within AWS using [AWS Certificate Manager](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-server-cert.html).\nThe downsides is not having an EIP (for whitelisting) and that automatic renewal of the certificate meaning higher operational overhead.\n### Technique #3 - using ELB with ASG\nThis is like Technique #2 but we introduce an ASG (auto-scaling group) for the EC2 master instance.\nBy using ACM, ELB and ASG together, it allows AWS to manage the certificate provisioning and renewal, thus removing any operational overhead.\n### Technique #4 - using a NLB\nLike #2 but with a Network Load Balancer (NLB), like so:\nDNS resolution -> EIP -> NLB -> ELB -> Jenkins master EC2\nThe advantage is that we can have an EIP. However, we probably need Cloudwatch\nto detect changes in the EIP which would trigger a Lambda to register the new EIP with the NLB, this is a suggested solution from AWS, but the downsides is that it is complex, expensive and difficult to manage.\n\n## Decision\n","After exploring and understanding all of the techniques and discussion within the team, option 3 was chosen.\nWe draw upon experience of the solution from others within GDS who had successfully implemented it in the past.\nWe also liked that it reduces the operational overhead of managing certificates completely.\nWe were concerned that since does not offer static IPs (via EIP) but we decided that they were not necessary for the MVP as they had not been identified as important during our user research sessions.\n",Technique #3 - using ELB with ASG
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need the user to be able to define Jenkins jobs in code and be able to import them into Jenkins.\nWe have identified a number of ways to do this:\n1. Define jobs with Groovy and inject the script, as we do for the Jenkins configuration\n2. Automatically create the jobs by scanning a Github account\n3. [Jenkins Job Builder]\n4. [Job DSL plugin]\n### Option 1\nThis is the easiest for us as we don't need to implement anything new. We can use the mechanism of injecting Groovy script which is already available. It is also relatively easy for the user to use. The code for the jobs and configuration can't exceed 16 KB, which is a limitation, but we believe that is enough for compact Jenkins installations (a Jenkins with hundreds of jobs is an anti-pattern). [The limit] is because we use [user data] to implement this option.\n### Option 2\nJenkins provides a way to scan a Github organisation or accounts for repositories containing pipeline configurations in a Jenkinsfile. This should be quite easy for the user but the implementation can be quite complex. This option would require the user to pass extra parameters to the Jenkins Terraform module: at least one regular expression to filter the repositories to match, and a Github personal token. The token is needed because scanning Github as an unauthenticated user is extremely slow but only takes a few minutes for a user with authentication. As the module needs a token as an input, there is extra complexity around managing that secret. This would be relatively straightforward to do using the UI but providing this solution as code would be quite involved.\n### Option 3\n(Jenkins Job Builder) is probably the most commonly used at GDS (it's used by Notify, Digital Marketplace) - people generally like it but some issues were pointed out like a difficulty in upgrading to a newer version or in escaping quotes correctly. [GOV.UK] and Pay use a more ad-hoc, homebrewed approach. Both groups rely on Puppet or Chef to inject their jobs into Jenkins. We allow users to install their configuration management tool via cloud-init, so the user is still free to override any mechanism we provide.\n### Option 4\nThis hasn't been explored in great detail as we felt we'd already found a good solution. However, if we revisit the decision made in this PR, this tool should be evaluated more thoroughly.\n\n## Decision\n","We decided to implement solution 1 to keep things simple and because of time constraints. Options two and three can still be used but will not be supported. In the future, we may consider to change to another solution if we feel there is the user need.\n",Option 1: Define jobs with Groovy and inject the script
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo guide our work, we need to decide whether we use AWS’ Elastic File System\n(EFS) or Elastic Block Store (EBS) as the cloud storage service for our Jenkins\nbuild.\nThe decision is needed as it will impact on the team’s work and may be affected\nby wider GDS or Government strategy.\n\n## Decision\n","Based on the below pros and cons, we have decided to remain with EFS as the\ncloud storage service, until such a time that there is a strategic need to\nchange.\nBecause of the way in which the team has compartmentalised the architecture\nof and approach to this Jenkins build, it is eminently possible to move to EBS\nat any point. It also means that further development of the system will not make\nit more complicated to do so.\nIt is for this reason that the decision is not to carry out this work in\nanticipation of it being needed, but rather focus on existing required work and\nonly carry out the work of converting to EBS should the need arise, at which\npoint that work would need to be prioritised.\n","**Decision:** Use Elastic File System (EFS) for the cloud storage service for the Jenkins build.\n\n**Rationale:**\n\n* **Shared access:** EFS allows for multiple EC2 instances to access the same file system, enabling a team to work on the same build.\n* **High availability:** EFS is a distributed file system that provides redundancy and high availability, ensuring that builds can continue even if individual EC2 instances fail.\n* **Elasticity:** EFS automatically adjusts its capacity based on demand, ensuring that it can handle the varying storage needs of the build process.\n* **Cost:** EFS offers pay-as-you-go pricing without the need for upfront capacity purchases, making it more cost-effective for the team's current needs.\n* **Alignment with GDS strategy:** EFS is a recommended cloud storage service by GDS for use in production environments, indicating its alignment with wider GDS strategy."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo guide our work, we need to decide on our approach to supporting (or not)\na build system that we may provide. This document outlines the pros and cons of\npossible options and recommends an approach.\nOnce decided, this decision will be used to hold further workshops to design the\nprocesses and technology required to put that approach into practice.\n### The options\nWe discussed 3 options that affect our support approach, plus a fourth that\ninvolves a third party and does not necessarily affect the way we support our\nproduct. As such, the fourth does not have pros and cons articulated below.\n1. Release a working and stable infrastructure as code. Unpinned versions, so\neach fresh installation includes the latest versions of everything.\n2. Host it (essentially become a SaaS provider) and maintain a limited suite of\nplugins.\n3. Periodically release stable versions of Jenkins (as code), with a suite of\nplugins with their versions pinned. Provide guidance on upgrading with each\nrelease.\n4. Pay for an externally hosted (and therefore maintained) version of Jenkins.\nDon’t provide anything ourselves except recommendation and perhaps a little\nguidance\nHybrids of some of these are possible. For example, we could have Jenkins as\ncode (1 or 3) for those who need more configurability whilst also\nrecommending/paying for an externally hosted thing (4). Or we could both provide\ncode (1 / 3) and also host something (2).\n### Option 1: Infrastructure as code only\n**Pros**\n* Supposedly more secure (because latest versions include security updates)\n* Encourages good behaviour (not providing a black box solution forces teams to\nengage with the solution)\n**Cons**\n* Implicit trust up-stream means security vulnerabilities could be introduced\n* Everything could break each time a new version is released (we can’t ensure\ncompatibility)\n* The build becomes unreproducible\n* Build versions can’t be tracked\n* Latest versions would only be installed for fresh installs, so potentially\nincreasing divergence from secure versions\n### Option 2: Host Jenkins ourselves\n**Pros**\n* Teams transfer all of the ‘cons’ to us\n* Teams don’t need certain expertises\n* We can enforce upgrades (security)\n**Cons**\n* Expensive (people, infrastructure, etc.)\n* Cost recovery has to be dealt with\n* Cost might be a barrier to adoption\n* Less configurable for teams\n* Residual unused instances have to be managed\n* Have to manage OLAs/SLAs\n* Need MTM/SMT agreement\n* Service becomes a single point of failure for all service teams using it\n(up-time, etc.)\n### Option 3: Release stable pinned versions periodically\n**Pros**\n* Users are reassured that it will work\n* ^ therefore lower barrier to adoption\n* Central point for monitoring security fixes\n* Automated testing (integration/acceptance) of version compatibility, etc. is\npossible\n* Guidance and optimised workflow = easier for teams to do things properly\n**Cons**\n* Someone has to stay current (security and updates)\n* Someone has to maintain and update it\n* Limited plugins: the fewer we support the more teams have to support (but may\nnot)\n* Limited plugins: if teams use a lot around GDS then maybe puts pressure on us\nto provide more in our 'limited' suite\n\n## Decision\n","Based on the above pros and cons, we recommend that option 3 (perhaps with\noption 4 as well, depending on user needs that are uncovered) is the best\noption.\n","Option 3 should be implemented, which includes periodically releasing stable versions of Jenkins (as code), with a suite of plugins with their versions pinned. It will provide guidance on upgrading with each release."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\n## Technical Problem\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\n\n## Decision\n","Buildah addresses the problems of docker container privilege escalation, is feature rich to support typical use cases (as it is feature compatible with docker), and provides us a means in which to provide docker image artifacts.\n",To use Buildah to build docker images.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in\n[documenting architecture decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\n",Create an Architectural Decision Record (ADR).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAny service activator pattern will have a message pump, which reads from a queue.\nThere are different strategies we could use, a common one for example is to use a BlockingCollection to hold messages read from the queue, and then use threads from the threadpool to process those messages.\nHowever, a multi-threaded pump has the issue that it will de-order an otherwise ordered queue, as the threads will pull items from the blocking collection in parallel, not sequentially.\nIn addition, where we have multiple threads it becomes difficult to create resources used by the pump without protecting them from race conditions.\nThe alternative is to use a single-threaded message pump that reads from the queue, processes the message, and only when it has processed that message, processes the next item. This prevents de-ordering of the queue, because items are read in sequence.\nIf a higher throughput is desired with a single threaded pump, then you can create multiple pumps. In essence, this is the competing consumers pattern, each performer is its own message pump.\nThe message pump performs the usual sequence of actions:\n- GetMessage. Read Message From Queue\n- Translate Message. Translate Message from Wire Format to Type\n- Dispatch Message. Dispatch Message based on Type\nPrior art for this is the Windows Event Loop which uses this approach, and is used by COM for integration via the Single-Threaded Apartment model.\n\n## Decision\n",Use a single-threaded message pump to preserve ordering and ensure sequential access to shared resources. Allow multiple pump instances for throughput.\n,Use a single-threaded message pump.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement a formal Architectural Decision Record (ADR) process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https://www.terraform.io/docs/enterprise/guides/recommended-practices/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\n\n## Decision\n",Use workspaces to manage multiple environments.\n,**Decision:** Use Terraform workspaces to manage multiple environments.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\n\n## Decision\n,Use Terraform Cloud for teams\n,**Decision:** Use Terraform Cloud for state management.\n\n**Rationale:**\n\n* Terraform Cloud stores state encryption at rest.\n* Terraform Cloud provides collaboration features for workspaces.\n* Terraform Cloud reduces the burden of managing AWS/GC storage and DynamoDB locking.\n* Terraform Cloud is suitable for small teams.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement Architectural Decision Records (ADRs) using a lightweight template and a version-controlled repository.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA couple of client projects need a simple authentication system. One of the\nprojects already uses Flask and Postgres, while another is in the design phase.\nIn the short term we want to have a minimal and functional authentication system\nimplemented as soon as possible.\nIn the very long term we hope that this implementation would be reused many\ntimes and to have easily customized drop-in libraries for Flask projects.\n\n## Decision\n","Create a minimal reference authentication implementation that uses Flask and\nPostgres. Include unit tests (hopefully strive for very high code coverage), and\ndatabase migrations.\nOrganize the database logic into a simplified CQRS-inspired style code structure:\n* app/models.py contain all sql models.\n* app/services.py contain all db commands that modify database state.\n* app/queries.py contain all db queries to the database.\nDelay any features that aren't in our current project requirements. The kinds of\nfeatures that may be addressed in future versions, but not initially:\n* Implement this reference as a [Flask extension](https://flask.palletsprojects.com/en/1.1.x/extensiondev/), and possibly add to the PyPi public repositories.\n* Create a React reference implementation.\n* password resets\n* email account verification\n* permissions\n* multi factor authentication\n* OAuth\n* Expiring tokens\n* Revoking/blacklisting tokens\n* prevent re-use of old passwords\n","**Decision:** Utilize Flask-Login as the authentication system for both immediate and long-term needs.\n\n**Rationale:**\n\n* **Short-term suitability:** Flask-Login is a lightweight and straightforward authentication library for Flask applications, meeting the immediate requirement for a functional system.\n* **Long-term viability:** Flask-Login has a proven track record and a large community, supporting its potential for future reuse and customization.\n* **Flexibility:** Flask-Login allows for easy customization, enabling adaptation to specific project requirements.\n* **Ease of implementation:** Flask-Login's straightforward integration with Flask simplifies implementation, reducing development time.\n* **Community support:** The active community provides resources and support for troubleshooting and feature enhancements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe plan to use AWS as a development/staging environment and as a production environment, on ECS.\n\n## Decision\n","Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\nCircleCI will need to be configured with the following environmental variables\nin order to deploy docker images (using [CircleCI's aws-ecr\norb](https://circleci.com/developer/orbs/orb/circleci/aws-ecr)):\n- AWS_ACCESS_KEY_ID\n- AWS_SECRET_ACCESS_KEY\n- AWS_REGION\n- AWS_ECR_ACCOUNT_URL\nThis project's Dockerfile requires access to a privately hosted github project\n(openfido-utils). Use [Buildkit](https://docs.docker.com/develop/develop-images/build_enhancements/) on CircleCI to grant SSH access to the docker build processes.\n","**Decision:** Use the same account for staging/development and production environments on AWS ECS.\n\n**Rationale:**\n\n* **Cost-effectiveness:** Using a single account eliminates the need for cross-account data transfer charges.\n* **Simplified management:** It simplifies user management, resource allocation, and monitoring across environments.\n* **Improved security:** By using a single account, access control and security measures can be applied consistently to all environments.\n* **Streamlined deployment:** Automated deployment processes can be optimized for a single account, ensuring faster and more reliable deployments.\n* **Consistency:** Maintaining a single account ensures that configurations, tools, and practices are consistent across environments, reducing potential errors and facilitating cross-team collaboration."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** Implement an Architectural Decision Record (ADR) process.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent method for capturing and documenting architectural decisions.\n* They improve transparency and traceability by providing a single source of truth for architectural decisions.\n* ADRs enable the tracking of decisions over time, facilitating audits and future decision-making.\n* They promote collaboration by facilitating the sharing and discussion of architectural options among team members.\n\n**Details:**\n\n* Establish a template or framework for ADRs, including fields for decision identifier, date, decision context, options considered, decision rationale, and impact.\n* Define a process for creating, reviewing, and approving ADRs.\n* Establish a central repository or knowledge base for storing ADRs and ensuring accessibility to all stakeholders.\n* Encourage team members to contribute to and maintain the ADR repository.\n* Regularly review and update ADRs as the project evolves to ensure they remain current and accurate."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Migrate to React Query/Context API/Hooks from Redux\n## Context\nCurrently, HospitalRun Frontend uses [redux](https://react-redux.js.org/) to manage async data\nfetching through [redux thunk](https://github.com/reduxjs/redux-thunk).\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\nsince it always goes back to PouchDB/CouchDB for the most recent data. Redux code is often verbose and\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https://redux-toolkit.js.org/),\nthe application still has a lot of boilerplate. Due to the limited amount of global application\nstate, the desire to reduce boilerplate redux code, and new/other libraries available,\nRedux is no longer the correct tool.\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\ndifficult to isolate testing just to the specific component. For components that save data, it\nis difficult to mock the actions that are dispatched.\n[react-query](https://github.com/tannerlinsley/react-query) is a library for ""handling fetching,\ncaching, and updating asynchronous data in React"". This library has become a popular replacement\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\nstore.\nFor the few uses cases that require global application state (i.e. session information),\nthe [React Context API](https://reactjs.org/docs/context.html) is an alternative from the\nReact library. The context API is designed for the use case of sharing state across\ncomponents.\n\n## Decision\n","HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\ncontext api to manage shared application state such as user information, and hooks for sharing\ncode and business logic.\n",Migrate to React Query and the React Context API to replace Redux for managing async data fetching and global application state.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nWe want to give support to any new Froala major version. For this reason, we must decide whether to create a new plugin for the new version of Froala, or use an existing one.\nBetween major versions of Froala, there can be *differences*, or not, in the way to integrate external plugins. This will be the reason to create a new MathType plugin or not:\nWe Currently have one package for each major Froala version, even though that's not the way Froala works: they have a unique package that stores all the versions. Even if we decided to follow the conventions Froala does, we would have to rename or delete our current modules, and npm wouldn't allow it, meaning that once a name is used, it can't be used never again for a new package.\n\n## Decision\n","We will create and give the Froala MathType packages for any new major version, the following name: *@wiris/mathtype-froalaX*, where X equals the oldest Froala major version. Then, depending on whether the way to integrate in external plugin is the same or different:\n- If the integration is different &rightarrow; Create a new MathType package for the major Froala version.\n- If the integration is the same &rightarrow; Use the same package that already exists for the previous major version. A good example is the situation we are in with the versions 3 and 4, where the integration is the same, so the package for both versions is the mathtype-froala3.\n### Pros and Cons of the Options\n#### A same MathType package to support more than one Froala major versions\nGood, because you'll only maintain one package for multiple identical major versions.\nGood, because it'll reduce DRI.\nBad, because the package name is not self-explanatory.\nBad, because the package name can create confusion on our users\nBad, because our current package convention name includes the version number on it\n#### Create a new package for every new major Froala version using symlinks when the versions external plugin integrations are identical\nGood, because it'll reduce DRI.\nGood, because we know it works, for previous experience with TinyMCE v4 and v5.\nGood, because the name is self-explanatory, since includes the Froala major version.\nBad, because, we'll have to maintain more than one package, at least.\nBad, because the infrastructure is more complex.\n#### Create a common package to store the identical code between Froala major versions that have the same external plugins integration, and create a new package for each version that uses the common one\nGood, because it'll reduce DRI.\nGood, because the name is-self explanatory, since includes the froala major version.\nBad, because, we'll have to maintain more than one package, at least thee.\n","Decision: Create a new plugin for the new version of Froala.\n\n*Reasoning:*\n\n* Between major versions of Froala, there can be differences in the way to integrate external plugins. This could require significant changes to our existing MathType plugin, or even make it incompatible with the new version of Froala.\n\n* Creating a new plugin for the new version of Froala would allow us to take advantage of any new features or improvements in the Froala API, and ensure that our plugin is fully compatible with the new version.\n\n* While it would require some additional development effort, we believe that creating a new plugin is the best long-term solution for supporting multiple versions of Froala."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nWhen writing custom scripts in the package.json files, one must take into account that those commands\nshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages that\nact as an API to mimic these tasks in both environments.\nOne of the most common tasks we do in these scripts is file manipulation (deleting, copying, moving).\n### Pros and Cons of the Options:\n#### Difference Between ShellJS and shx\n**ShellJS:**\nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).\n**shx:**\nGood for writing one-off commands in npm package scripts (e.g. ""clean"": ""shx rm -rf out/"").\n\n## Decision\n","It is proposed to use [shx][shx-url], a wrapper for sh commands that works on Windows too, and behaves as the corresponding sh commands.\nIt has more than 150K downloads per week, is well-maintaned by the community and it occupies less space than our current past alternatives: rimfaf and copyfiles.\n","Use shx for writing one-off commands in npm package scripts, since it's more convenient and easy to read."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nIn the context of the Telemetry project, we want to generate\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https://tools.ietf.org/html/rfc4122) on ietf.org.\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\n\n## Decision\n","We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris/mathtype-html-integration-devkit'.\n* [uuid project](https://github.com/uuidjs/uuid)\n* [uuid package](https://www.npmjs.com/package/uuid)\n### Pros and Cons of the Options\n#### Implement our own Javascript library for that\n- Bad, because Javascript Math.random function is not very good.\n- Bad, because we'll need to maintain it.\n- Bad, because more work to the backlog.\n- Bad, because we're reinventing the wheel.\n#### Using a third party library like github.com/uuidjs/uuid\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads/week).\n- Good, because solves our problem immediately.\n- Good, because it's secure, small and cross-platform.\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\n","Use the UUID library as it is small, secure and provides good support for generating valid and consistent UUIDs as per RFC-4122 standard."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context (Discussion)\nReading the Telemetry documentation proposed by the Data Science team, we have\nencountered that the sender and session ids are not concretely defined for each\nproduct.\nIn the case of MathType web, there could be various interpretations, e.g.\n- senderId: changes on page load.\n- sessionId: changes on opening MathType.\n\n## Decision\n","We asked the Data Science team and finally settled on the following interpretation:\n- senderId: ideally lasts for ever. In practice, should at least last in a same web session, across page loads.\n- sessionId: changes on page load.\n### Pros and Cons of the Options\n#### Change senderId on page load and sessionId on opening MathType\n- Bad, because it does not truly identify individual users.\n#### Keep senderId and change sessionId on page load\n- Good, because it better represents individual users.\n- Bad, because it's hard to keep permanent data on the client's browser.\n","The senderId should change on page load, and the sessionId should change on opening MathType."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA method to authorize against the youtube API to get the access token needs to be added to the application.\n\n## Decision\n,I decided to use the launchWebAuthFlow instead of the getAuthToken api to do the oauth2 authentication. This is\nbecause getAuthToken can only authorize the google signed in in chrome. This prevents me from using a different account\nfor youtube from my chrome account.\n,"The application will use OAuth 2.0 to authorize against the YouTube API. OAuth 2.0 is an industry-standard protocol for authorization. It is simple to implement and provides a secure way to authorize applications.\n\nThe application will use the following steps to authorize against the YouTube API:\n\n1. The user will be redirected to the YouTube authorization page.\n2. The user will be asked to grant the application access to their YouTube account.\n3. If the user grants access, the application will be redirected back to the application with an authorization code.\n4. The application will use the authorization code to request an access token from the YouTube API.\n5. The application will use the access token to make requests to the YouTube API."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","To record architectural decisions, we will use the Architectural Decision Record (ADR) template. ADRs will be stored in a central repository and will be reviewed and approved by the architecture team before being implemented. The ADR template will include the following fields:\n\n- Title: A brief summary of the decision\n- Decision: The decision that was made\n- Context: The background and rationale for the decision\n- Consequences: The potential consequences of the decision\n- Related Decisions: Links to other ADRs that are related to this decision\n\nThis process will ensure that architectural decisions are made in a thoughtful and deliberate manner, and that they are documented and communicated to the team."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to create a riglet for our new bookit project so that we practice what we preach.\n\n## Decision\n,"We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: Travis\n","**Implement a riglet for the bookit project.**\n\n**Rationale:**\n\n* **Alignment with best practices:** Practicing ""what we preach"" demonstrates our commitment to the principles we advocate for.\n* **Educational value:** Building a riglet for the bookit project provides a hands-on learning experience for team members, allowing them to apply design principles in a real-world context.\n* **Documentation:** The riglet can serve as a living example of our architectural decisions and best practices, enabling future teams to learn from our experiences.\n* **Collaboration:** The riglet development process fosters collaboration between architects, developers, and other stakeholders, promoting a shared understanding of the system architecture.\n* **Continuous improvement:** The riglet can be used as a testbed for new technologies or design patterns, allowing us to continuously refine our architectural approach.\n\n**Consequences:**\n\n* **Time and resources:** Implementing the riglet will require time and effort from the development team.\n* **Complexity:** The riglet should be designed carefully to avoid introducing unnecessary complexity into the bookit project.\n* **Maintenance:** The riglet will need to be maintained and updated as the bookit project evolves.\n\n**Alternatives considered and rejected:**\n\n* **Not implementing a riglet:** This would miss out on the educational and documentation benefits of building a riglet.\n* **Using an existing riglet:** This would limit our ability to tailor the riglet to the specific needs of the bookit project."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n\n## Decision\n,* Use AWS CodePipeline and CodeBuild instead of Travis\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n,Use Azure Pipelines as the CI/CD solution.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContext here...\n\n## Decision\n,Decision here...\n,Decision is missing in the provided context.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Log architectural decisions in an Architectural Decision Record (ADR).**\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to document architectural decisions.\n* They clearly state the decision, its context, and the rationale behind it.\n* ADRs facilitate transparency and collaboration by making architectural decisions accessible to all stakeholders.\n* They serve as a valuable reference for future maintenance and enhancements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR details the behavior for `actions/checkout@v2`.\nThe new action will be written in typescript. We are moving away from runner-plugin actions.\nWe want to take this opportunity to make behavioral changes, from v1. This document is scoped to those differences.\n\n## Decision\n","### Inputs\n```yaml\nrepository:\ndescription: 'Repository name with owner. For example, actions/checkout'\ndefault: ${{ github.repository }}\nref:\ndescription: >\nThe branch, tag or SHA to checkout. When checking out the repository that\ntriggered a workflow, this defaults to the reference or SHA for that\nevent.  Otherwise, uses the default branch.\ntoken:\ndescription: >\nPersonal access token (PAT) used to fetch the repository. The PAT is configured\nwith the local git config, which enables your scripts to run authenticated git\ncommands. The post-job step removes the PAT.\nWe recommend using a service account with the least permissions necessary.\nAlso when generating a new PAT, select the least scopes necessary.\n[Learn more about creating and using encrypted secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets)\ndefault: ${{ github.token }}\nssh-key:\ndescription: >\nSSH key used to fetch the repository. The SSH key is configured with the local\ngit config, which enables your scripts to run authenticated git commands.\nThe post-job step removes the SSH key.\nWe recommend using a service account with the least permissions necessary.\n[Learn more about creating and using\nencrypted secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets)\nssh-known-hosts:\ndescription: >\nKnown hosts in addition to the user and global host key database. The public\nSSH keys for a host may be obtained using the utility `ssh-keyscan`. For example,\n`ssh-keyscan github.com`. The public key for github.com is always implicitly added.\nssh-strict:\ndescription: >\nWhether to perform strict host key checking. When true, adds the options `StrictHostKeyChecking=yes`\nand `CheckHostIP=no` to the SSH command line. Use the input `ssh-known-hosts` to\nconfigure additional hosts.\ndefault: true\npersist-credentials:\ndescription: 'Whether to configure the token or SSH key with the local git config'\ndefault: true\npath:\ndescription: 'Relative path under $GITHUB_WORKSPACE to place the repository'\nclean:\ndescription: 'Whether to execute `git clean -ffdx && git reset --hard HEAD` before fetching'\ndefault: true\nfetch-depth:\ndescription: 'Number of commits to fetch. 0 indicates all history for all tags and branches.'\ndefault: 1\nlfs:\ndescription: 'Whether to download Git-LFS files'\ndefault: false\nsubmodules:\ndescription: >\nWhether to checkout submodules: `true` to checkout submodules or `recursive` to\nrecursively checkout submodules.\nWhen the `ssh-key` input is not provided, SSH URLs beginning with `git@github.com:` are\nconverted to HTTPS.\ndefault: false\n```\nNote:\n- SSH support is new\n- `persist-credentials` is new\n- `path` behavior is different (refer [below](#path) for details)\n### Fallback to GitHub API\nWhen a sufficient version of git is not in the PATH, fallback to the [web API](https://developer.github.com/v3/repos/contents/#get-archive-link) to download a tarball/zipball.\nNote:\n- LFS files are not included in the archive. Therefore fail if LFS is set to true.\n- Submodules are also not included in the archive.\n### Persist credentials\nThe credentials will be persisted on disk. This will allow users to script authenticated git commands, like `git fetch`.\nA post script will remove the credentials (cleanup for self-hosted).\nUsers may opt-out by specifying `persist-credentials: false`\nNote:\n- Users scripting `git commit` may need to set the username and email. The service does not provide any reasonable default value. Users can add `git config user.name <NAME>` and `git config user.email <EMAIL>`. We will document this guidance.\n#### PAT\nWhen using the `${{github.token}}` or a PAT, the token will be persisted in the local git config. The config key `http.https://github.com/.extraheader` enables an auth header to be specified on all authenticated commands `AUTHORIZATION: basic <BASE64_U:P>`.\nNote:\n- The auth header is scoped to all of github `http.https://github.com/.extraheader`\n- Additional public remotes also just work.\n- If users want to authenticate to an additional private remote, they should provide the `token` input.\n#### SSH key\nThe SSH key will be written to disk under the `$RUNNER_TEMP` directory. The SSH key will\nbe removed by the action's post-job hook. Additionally, RUNNER_TEMP is cleared by the\nrunner between jobs.\nThe SSH key must be written with strict file permissions. The SSH client requires the file\nto be read/write for the user, and not accessible by others.\nThe user host key database (`~/.ssh/known_hosts`) will be copied to a unique file under\n`$RUNNER_TEMP`. And values from the input `ssh-known-hosts` will be added to the file.\nThe SSH command will be overridden for the local git config:\n```sh\ngit config core.sshCommand 'ssh -i ""$RUNNER_TEMP/path-to-ssh-key"" -o StrictHostKeyChecking=yes -o CheckHostIP=no -o ""UserKnownHostsFile=$RUNNER_TEMP/path-to-known-hosts""'\n```\nWhen the input `ssh-strict` is set to `false`, the options `CheckHostIP` and `StrictHostKeyChecking` will not be overridden.\nNote:\n- When `ssh-strict` is set to `true` (default), the SSH option `CheckHostIP` can safely be disabled.\nStrict host checking verifies the server's public key. Therefore, IP verification is unnecessary\nand noisy. For example:\n> Warning: Permanently added the RSA host key for IP address '140.82.113.4' to the list of known hosts.\n- Since GIT_SSH_COMMAND overrides core.sshCommand, temporarily set the env var when fetching the repo. When creds\nare persisted, core.sshCommand is leveraged to avoid multiple checkout steps stomping over each other.\n- Modify actions/runner to mount RUNNER_TEMP to enable scripting authenticated git commands from a container action.\n- Refer [here](https://linux.die.net/man/5/ssh_config) for SSH config details.\n### Fetch behavior\nFetch only the SHA being built and set depth=1. This significantly reduces the fetch time for large repos.\nIf a SHA isn't available (e.g. multi repo), then fetch only the specified ref with depth=1.\nThe input `fetch-depth` can be used to control the depth.\nNote:\n- Fetching a single commit is supported by Git wire protocol version 2. The git client uses protocol version 0 by default. The desired protocol version can be overridden in the git config or on the fetch command line invocation (`-c protocol.version=2`). We will override on the fetch command line, for transparency.\n- Git client version 2.18+ (released June 2018) is required for wire protocol version 2.\n### Checkout behavior\nFor CI, checkout will create a local ref with the upstream set. This allows users to script git as they normally would.\nFor PR, continue to checkout detached head. The PR branch is special - the branch and merge commit are created by the server. It doesn't match a users' local workflow.\nNote:\n- Consider deleting all local refs during cleanup if that helps avoid collisions. More testing required.\n### Path\nFor the mainline scenario, the disk-layout behavior remains the same.\nRemember, given the repo `johndoe/foo`, the mainline disk layout looks like:\n```\nGITHUB_WORKSPACE=/home/runner/work/foo/foo\nRUNNER_WORKSPACE=/home/runner/work/foo\n```\nV2 introduces a new contraint on the checkout path. The location must now be under `github.workspace`. Whereas the checkout@v1 constraint was one level up, under `runner.workspace`.\nV2 no longer changes `github.workspace` to follow wherever the self repo is checked-out.\nThese behavioral changes align better with container actions. The [documented filesystem contract](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/virtual-environments-for-github-hosted-runners#docker-container-filesystem) is:\n- `/github/home`\n- `/github/workspace` - Note: GitHub Actions must be run by the default Docker user (root). Ensure your Dockerfile does not set the USER instruction, otherwise you will not be able to access `GITHUB_WORKSPACE`.\n- `/github/workflow`\nNote:\n- The tracking config will not be updated to reflect the path of the workflow repo.\n- Any existing workflow repo will not be moved when the checkout path changes. In fact some customers want to checkout the workflow repo twice, side by side against different branches.\n- Actions that need to operate only against the root of the self repo, should expose a `path` input.\n#### Default value for `path` input\nThe `path` input will default to `./` which is rooted against `github.workspace`.\nThis default fits the mainline scenario well: single checkout\nFor multi-checkout, users must specify the `path` input for at least one of the repositories.\nNote:\n- An alternative is for the self repo to default to `./` and other repos default to `<REPO_NAME>`. However nested layout is an atypical git layout and therefore is not a good default. Users should supply the path info.\n#### Example - Nested layout\nThe following example checks-out two repositories and creates a nested layout.\n```yaml\n# Self repo - Checkout to $GITHUB_WORKSPACE\n- uses: checkout@v2\n# Other repo - Checkout to $GITHUB_WORKSPACE/myscripts\n- uses: checkout@v2\nwith:\nrepository: myorg/myscripts\npath: myscripts\n```\n#### Example - Side by side layout\nThe following example checks-out two repositories and creates a side-by-side layout.\n```yaml\n# Self repo - Checkout to $GITHUB_WORKSPACE/foo\n- uses: checkout@v2\nwith:\npath: foo\n# Other repo - Checkout to $GITHUB_WORKSPACE/myscripts\n- uses: checkout@v2\nwith:\nrepository: myorg/myscripts\npath: myscripts\n```\n#### Path impact to problem matchers\nProblem matchers associate the source files with annotations.\nToday the runner verifies the source file is under the `github.workspace`. Otherwise the source file property is dropped.\nMulti-checkout complicates the matter. However even today submodules may cause this heuristic to be inaccurate.\nA better solution is:\nGiven a source file path, walk up the directories until the first `.git/config` is found. Check if it matches the self repo (`url = https://github.com/OWNER/REPO`). If not, drop the source file path.\n### Submodules\nWith both PAT and SSH key support, we should be able to provide frictionless support for\nsubmodules scenarios: recursive, non-recursive, relative submodule paths.\nWhen fetching submodules, follow the `fetch-depth` settings.\nAlso when fetching submodules, if the `ssh-key` input is not provided then convert SSH URLs to HTTPS: `-c url.""https://github.com/"".insteadOf ""git@github.com:""`\nCredentials will be persisted in the submodules local git config too.\n### Port to typescript\nThe checkout action should be a typescript action on the GitHub graph, for the following reasons:\n- Enables customers to fork the checkout repo and modify\n- Serves as an example for customers\n- Demystifies the checkout action manifest\n- Simplifies the runner\n- Reduce the amount of runner code to port (if we ever do)\nNote:\n- This means job-container images will need git in the PATH, for checkout.\n### Branching strategy and release tags\n- Create a servicing branch for V1: `releases/v1`\n- Merge the changes into the default branch\n- Release using a new tag `preview`\n- When stable, release using a new tag `v2`\n","The `actions/checkout@v2` action will use the new typescript runner, and will make the following behavioral changes from v1:\n\n- Improved performance by leveraging the new runner's optimized resource utilization.\n- Enhanced security by implementing best practices and following industry security standards.\n- Simplified configuration by providing a more user-friendly and intuitive interface.\n- Expanded functionality by offering additional features and integrations with other tools and services.\n- Improved documentation and support resources to ensure seamless adoption and troubleshooting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo add/modify content on our patternpediacontent repo 'on behalf of the user', we need to use OAuth Authentification in our client-side application.\nBut requesting https://github.com/login/oauth/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https://github.com/isaacs/github/issues/330)).\nUnfortunately, Github prevents you from implementing the OAuth Web Application Flow on a client-side only application (Reason for it: security-related limitations).\n\n## Decision\n","Chosen option: ""Use our own server-side component"", because\n* we don't want to provide our their client id and secret to an (possibly untrusted) open reverse proxy (comment from the stackoverflow-article: If the owner of the proxy wants to log credentials from the requests they can)\n",**Implement the Github App OAuth flow**\n\n**Pros:**\n- Github Apps does support CORS for requesting an OAuth access token\n- You can implement on a client-side application\n\n**Cons:**\n- Requires creating a GitHub App and installing it on your repositories\n- You will need to ask for additional permissions from your repo owners\n- You will need to manage app installation and permissions\n- Might not be suitable if you need to use multiple GitHub accounts\n- The GitHub App OAuth flow might be more complex to implement than the web application flow
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\n## Decision Drivers\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\n\n## Decision\n","* Pattern languages can be very different, so we should restrict their properties as little as possible.\nA pattern consists of\n* name (string), e.g. ""Elastic Infrastructure""\n* type (string), e.g. <https://purl.org/patternpedia/cloudcomputingpatterns#CloudComputingPattern>\n* uri (string, e.g. <https://purl.org/patternpedia/cloudcomputingpatterns/elasticinfrastructure#ElasticInfrastructure>)\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\ne.g. for the section  https://purl.org/patternpedia/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  ""https://www.cloudcomputingpatterns.org/img/book.png"" or an array of strings\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like ""https://www.cloudcomputingpatterns.org/img/book.png"" as links/pictures and not only as text.\n",Define a JSON schema for the pattern language and pattern structures.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\n\n## Decision\n,"For the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\n",Use EC2 hosting for compute.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to create a riglet for our new twig project so that we practice what we preach.\n\n## Decision\n,"We will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project/repo.\nTechnologies:\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: CodePipeline, with Jenkins as an eventual target\n",Create a simple twig riglet and document the process.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer/simpler riglet flavor\nand put newer approaches to the test.\n\n## Decision\n,"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\n* This will be manual at first and later could be automated via webhooks and lambda functions\n",Migrate Twig to a newer riglet flavor and experiment with newer approaches.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\n\n## Decision\n","Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\n",Leverage a container-based continuous integration/continuous delivery (CI/CD) solution to streamline deployment processes and reduce the need for full rebuilds for configuration changes.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\n\n## Decision\n,We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\n,"Implement a central logging solution, such as Elasticsearch, Logstash, and Kibana (ELK), or a managed service such as Cloud Logging."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","**Architectural Decision:** Implement an architectural decision record (ADR) template.\n\n**Rationale:** To ensure consistency and transparency in documenting architectural decisions, facilitating collaboration and knowledge sharing within the team."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHistorically, our automated deployment scripts have tended to include a large amount of common boilerplate. This is partly because they are self-contained—they can typically be run on their own to perform a deployment. And it is partly just because they originate from what the Visual Studio tooling creates.\nThe problem with this is that it makes the project-specific details hard to spot. When looking at a sea of code that's almost identical to every other project, it's hard to see what it's doing that is in any way different from everything else.\n\n## Decision\n","With `Marain.Instance`, deployment scripts in individual services do not communicate directly with either ARM or Azure AD. (They should not even be aware of what mechanisms are being used to perform this work—they should not need to know whether we are using the PowerShell Az module, the az CLI or even custom library code to talk to Azure, for example.)\nAnything that needs to be done either in Azure or AAD must be done through operations provided by the shared `Marain.Instance` code. It passes in an object that provides various methods that provide the necessary services.\n",Standardize deployment scripts.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMarain services need to be able to deliver diagnostic information somewhere.\n\n## Decision\n,`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\n,"**Decision:** Marain services should deliver diagnostic information to an instance of Splunk.\n\n**Rationale:**\n\n* Splunk is a leading provider of machine data analytics software.\n* Splunk is used by many large organizations to collect and analyze data from a variety of sources, including servers, applications, and networks.\n* Splunk has a proven track record of reliability and scalability.\n* Splunk offers a wide range of features for data collection, analysis, and visualization.\n\n**Consequences:**\n\n* Marain services will be able to deliver diagnostic information to a central location.\n* This information will be available to Marain developers and administrators for troubleshooting and performance analysis.\n* The use of Splunk will provide Marain with a scalable and reliable solution for diagnostic information management."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeployment requirements need to be expressed somehow—either declaratively or in a programming language.\n\n## Decision\n,Individual Marain services express their deployment requirements in the form of a set of PowerShell scripts. These will be run in PowerShell core v6.\n,"Declarative deployment is a method of expressing deployment requirements in a declarative language. This allows for a more concise and expressive way to define deployments, as well as making it easier to reason about the deployment."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\n\n## Decision\n","The `Marain.Instance` repo (this repo) includes a master service list, `Solutions/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer—the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\n","Establish a Marain instance taxonomy, with the following categories:\n\n* Core services\n* Optional services\n* Plug-ins\n* Agents"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have defined (in [ADR 0005](0005-multitenancy-approach-for-marain.md)) the way in which we intend to implement Tenancy in Marain instances using the `Marain.Tenancy` service. As noted in that ADR, managing the desired model by hand would be excessively error prone and as such, we need to design tooling that will allow us to create and manage new tenants, and to allow them to use the Marain services they are licenced for.\nBefore we can build that tooling we need to design the underlying process by which tenant onboarding, enrollment and offboarding will work. This needs to allow new Client Tenants to be onboarded into Marain services without tightly coupling the services so some central thing that knows everything about them.\n\n## Decision\n","We are envisaging a central control-plane API (referred to for the remainder of this document as the ""Management API"") for Marain which primarily builds on top of the Tenancy service. This will provide the standard operations such as creating new tenants and enrolling them to use Marain services.\nIt will also need to allow us to manage concerns such as licensing, billing, metering and so on, but these are out of the scope of this ADR and will be covered by additional ADRs, work items and documentation when required.\n### Onboarding\nOnboarding is a relatively simple part of the process where we create a new Tenant for the client. We will need to determine how we intend licensing to work and what part, if any, the Management API plays.\n### Enrollment\nService enrollment is a more interesting aspect of the process. In order to avoid tightly coupling Marain services to the Management API, we need two things:\n- A means of discovering the available services.\n- A means of determining the configuration that's needed to enroll for a service, receiving and attaching that configuration to tenants being enrolled, and a defined way of creating the required sub-tenants for services to use when making calls to dependent services on behalf of clients.\nAs described in [ADR 0005](0005-multitenancy-approach-for-marain.md), we are envisaging that each service has a Tenant created for it, under a single parent for all Service Tenants. These tenants can then underpin the discovery mechanism that allows the management API to enumerate services that tenants can be enrolled into.\nOnce we have provided a discovery mechanism, we need to define a way in which we can gather the necessary information needed to enroll a tenant to use a service. We are intending to make this work by defining a common schema through which a service can communicate both the configuration it requires as well as the services upon which it depends. Services can then attach a manifest file containing this information to their Service Tenant via a well known property key, allowing the Management API to obtain the manifest as part of the discovery process.\nSince the process of enrollment and unenrollment is standard across tenants, the actual implementation of this can form part of the Management API, driven by the data in the manifests. If we ever encounter a situation where services need to perform non-standard actions as part of tenant enrollment, we can extend the process to support a way in which services can be notified of new enrollments - this could be a simple callback URL, or potentially a broadcast-type system using something like Azure Event Grid. Since we don't yet have any services that would need this, we will not attempt to define that mechanism at this time.\nEnrolling a tenant to use a service does two things:\n- Firstly, it will attach the relevant configuration for the service to the tenant that's being enrolled.\n- Secondly, if the service that's being enrolled in has dependencies on other services, it will create a new sub-tenant of the Service Tenant for the service being enrolled that the service will use when accessing dependencies on behalf of the client. This new subtenant will then be enrolled for each of the depended-upon services, with any further levels of dependency dealt with in the same way.\n### Example\nConsider a scenario when we have two clients and three services:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|\n+-> OPERATIONS\n|\n+-> FOOBAR\n```\nThe dependency tree for the services looks like this:\n```\n+------------+\n|            |\n+-------> WORKFLOW   +------+-----------------+\n+---------+       |       |            |      |                 |\n|         +-------+       +-^----------+      |                 |\n| Contoso |                 |                 |                 |\n|         |                 |                 |                 |\n+----+----+                 |           +-----v------+          |\n|                      |           |            |          |\n|                      |     +-----> OPERATIONS +----+     |\n|      +---------+     |     |     |            |    |     |\n|      |         +-----+     |     +------------+    |     |\n|      | Litware |           |                       |     |\n|      |         +-----------+                       |     |\n|      +---------+                               +---v-----v--+\n|                                                |            |\n+------------------------------------------------> FOOBAR     |\n|            |\n+------------+\n```\nAs can be seen from this diagram:\n- Contoso is licenced to use WORKFLOW and FOOBAR\n- Litware is licenced to use WORKFLOW and OPERATIONS\n- WORKFLOW has dependencies on OPERATIONS and FOOBAR\n- OPERATIONS has a dependency on FOOBAR\nLet's assume that each of the three services require storage configuration, and have a look at what happens when Litware is enrolled to use Workflow.\nFirstly, we will use the manifest attached to the Workflow Service Tenant to obtain the list of required configuration to enroll a tenant for use with Workflow. The Workflow manifest states that Workflow requires CosmosDB storage configuration, and also that it is dependent on Operations and FooBar. We can then use the manifests on the Operations and FooBar Service Tenants to determine what configuration is required for them. Operations is dependent on FooBar, so the process is repeated there. From this process, we can determine that the list of required configuration for Workflow is the sum of four things: Workflow storage config, Operations storage config, FooBar storage config when invoked from Workflow, and FooBar storage config when invoked from Operations.\nThen, we will assemble this information (most likely via a Marain ""management UI"") and begin the process of enrollment. First we enroll the Litware tenant to use workflow by attaching the workflow storage configuration to the Litware tenant. Then, because Workflow has dependencies, we create a sub-tenant of the Workflow Service Tenant that will be used to call these dependencies on behalf of Litware and we attach the ID of the new tenant to the Litware tenant using a well known property name specific to Workflow:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|\n+-> OPERATIONS\n|\n+-> FOOBAR\n```\nNext, we need to enroll the new WORKFLOW+Litware tenant with the Operations and FooBar services.\nWe start with Operations, which repeats the process we have just carried out for Litware and Workflow, resulting in a similar outcome: the WORKFLOW+Litware tenant has configuration attached to it for the Operations service, and the dependency of Operations on FooBar results in a sub tenant being created for Operations to use when calling FooBar on behalf of WORKFLOW+Litware, with the ID of the new tenant being attached to WORKFLOW+Litware using an Operations-specific well-known key:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|\n+-> FOOBAR\n```\nNext, the new OPERATIONS+WORKFLOW+Litware tenant is enrolled for the FooBar service. The FooBar service has no dependencies so we do not need to create any further tenants; we simply attach the storage configuration for FooBar to the tenant being enrolled and returns. This also completes the WORKFLOW+Litware tenant's enrollment for Operations.\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nNext, we continue the Workflow enrollment for the Litware tenant by enrolling the new WORKFLOW+Litware tenant for Workflow's other dependency, FooBar. As with the Operations service enrolling OPERATIONS+WORKFLOW+Litware with FooBar, this does not result in any further tenants being created, just the FooBar config being attached to WORKFLOW+Litware:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nThis completes Litware's enrollment for the Workflow service. As can be seen, this has resulted in multiple service-specific Litware tenants being created but Litware is never explicitly made aware of the existence of these tenants, nor is it able to use them directly. They are used by their parent services to make calls to their dependencies _on behalf_ of the Litware tenant.\nHowever, there is a further step: Litware also needs to be enrolled in the Operations service. At present, Workflow is able to use Operations on Litware's behalf using the WORKFLOW+Litware tenant. However, this is an implementation detail of Workflow and something that should be able to change without impacting Litware - as long as it does not result in a change to the public Workflow API. So, in order to allow Litware to use the Operations service directly, the process we went through for Workflow is repeated. The storage configuration for Operations is attached to the Litware tenant, and then a further sub-tenant of Operations will be created for it to use when accessing FooBar on behalf of Litware:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+Litware\n|\n+-> FOOBAR\n```\nThen, the new OPERATIONS+Litware tenant will be enrolled for the FooBar service, which results in FooBar storage configuration being attached to the OPERATIONS+Litware service:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nThis completes the enrollment of Litware to the Workflow and Operations services. As can be seen from the above, there are three different paths through which Litware makes indirect use of the FooBar service, and it's possible for the client to use separate storage for each. In fact, this will be the default; even if the client is using Marain storage, the data for their three different usage scenarios for FooBar will be stored in different containers.\nIt should be noted that the client does not get to configure these new sub-tenants directly. In fact, they will be unaware of them - they are essentially implementation details of our approach to multi-tenancy in Marain. They will not be able to retrieve the sub-tenants from the tenancy service or update them directly. That said, it's likely that the management API will allow the configuration to be changed - but without exposing the fact that these sub-tenants exist.\n### Default configuration\nWhilst we want to allow users to ""bring their own storage"" for the Marain services, this may not be the most likely scenario. There are effectively four main ways in which Marain can be used:\n- Fully hosted, using the default storage for each service (this storage is deployed alongside the service)\n- Fully hosted, using managed but non-standard storage (we deploy separate storage accounts per client)\n- Hosted, but using client-provided storage (the ""bring your own storage"" model)\n- Self-hosted (i.e. deployed into a client's own Azure subscription), in which case we would expect the storage deployed with the service to be used - essentially the same as the fully hosted option.\nIn order to make the first and last options simpler to use, we will make the configuration for the default storage available to the enrollment process so it can simply be copied to tenants as they are enrolled, rather than requiring it to be explicitly stated for every enrollment. As such, we need the manifest file schema to allow marking configuration as optional, indicating that defaults should be used if that configuration is not provided. The most sensible location to store this default configuration is on the Service Tenant itself.\n### Offboarding\nOffboarding needs to be considered further; there are many questions about what happens to client data if they stop using Marain, and these will likely depend on the licensing agreements we put in place. As a result this will be considered at a later date.\n","We will use a two-step process for provisioning new tenants:\n\n1. A Tenant is onboarded into the Marain platform, a tenancy record is created in the database, an Identity Provider is configured, SCIM2 tokens are configured and the relevant API keys are created.\n2. The new tenant is enrolled into the services they are licenced for. This process may create new projects, datasets and other resources depending on the services to be enabled and the licencing model.\n\nTenants will be offboarded by first removing them from the services they are enrolled into, and then deleting the Tenant record from the database, removing the Identity Provider configuration and deleting the SCIM2 tokens."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain ""world"".\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\n\n## Decision\n","To support this, we have made the following decisions\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as ""Client Tenants"".\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as ""Service Tenants"".\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called ""Client Tenants"" which parents all of the Client Tenants, and an equivalent one called ""Service Tenants"" that parents the Service Tenants (this is shown in the diagram below).\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|\n+-> OPERATIONS\n```\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|     |     +-> (Operations storage configuration)\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|\n+-> OPERATIONS\n```\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\n```\n+------------+\n|            |\n+-------> WORKFLOW   +------+-----------------+\n+---------+       |       |            |      |                 |\n|         +-------+       +-^----------+      |                 |\n| Contoso |                 |                 |                 |\n|         |                 |                 |                 |\n+----+----+                 |           +-----v------+          |\n|                      |           |            |          |\n|                      |     +-----> OPERATIONS +----+     |\n|      +---------+     |     |     |            |    |     |\n|      |         +-----+     |     +------------+    |     |\n|      | Litware |           |                       |     |\n|      |         +-----------+                       |     |\n|      +---------+                               +---v-----v--+\n|                                                |            |\n+------------------------------------------------> FOOBAR     |\n|            |\n+------------+\n```\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|\n+-> OPERATIONS\n|\n+-> FOOBAR\n```\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\nThis leaves the tenant hierarchy looking like this:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> Litware\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nWe now repeat the process of enrolling Litware for the Workflow service:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|     |     +-> (Operations storage configuration)\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\n```\nRoot tenant\n|\n+-> Client Tenants\n|     |\n|     +-> Contoso\n|     |     +-> (Workflow storage configuration)\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> Litware\n|           +-> (Workflow storage configuration)\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\n|\n+-> Service Tenants\n|\n+-> WORKFLOW\n|     |\n|     +-> WORKFLOW+Contoso\n|     |     +-> (Operations storage configuration)\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> WORKFLOW+Litware\n|           +-> (Operations storage configuration)\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\n|           +-> (FooBar storage configuration)\n|\n+-> OPERATIONS\n|     |\n|     +-> OPERATIONS+WORKFLOW+Contoso\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+WORKFLOW+Litware\n|     |     +-> (FooBar storage configuration)\n|     |\n|     +-> OPERATIONS+Litware\n|           +-> (FooBar storage configuration)\n|\n+-> FOOBAR\n```\n","**Decision:**\n\nWe will implement a tenancy model that supports the following scenarios:\n\n* **Managed service:** Marain is deployed and managed by us, and clients are licensed to use it as a PaaS offering.\n* **Private instance:** Clients deploy private instances of Marain into their own cloud subscriptions.\n* **Data isolation:** Clients can store their data in their own storage accounts or databases, while we run the compute aspects of the platform on their behalf.\n* **Multi-tenanted services:** Clients can use Marain to implement their own multi-tenanted services and isolate their clients' storage.\n* **Service dependencies:** Clients can use services indirectly as dependencies of other services they are using.\n\nThe tenancy model will be implemented using the `Marain.Tenancy` service. Tenants will be created and managed by Marain, and each tenant will have its own isolated storage and database. Clients will be assigned to tenants, and they will only be able to access the resources within their own tenant.\n\n**Rationale:**\n\nThis tenancy model meets the requirements of the context and provides a flexible and extensible solution for managing tenants within the Marain ""world"". It supports both managed and private deployments, data isolation, multi-tenanted services, and service dependencies. Additionally, it can be implemented using the existing `Marain.Tenancy` service, which reduces the development effort and allows us to leverage the existing functionality of the service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR defines an approach for a pipeline security management process.  Itt enables application deployment pipelines to perform all tasks associated with the deployment of typical Azure PaaS-based solutions, in a way that adheres to the principle of least privilege and supports oversight and audit requirements.\nImportantly, the process for making such security changes utilises a standard development workflow that allows teams to manage their own configuration with minimal friction, whilst still facilitating centralised oversight when needed (e.g. by the security team).\nBefore describing the process in detail, here are the sorts of tasks that an application deployment pipeline typically needs to be able to perform:\n1. Create resources in the target subscription\n1. Perform ARM role assignments in the target subscription\n1. Query Azure Active Directory (AAD) for existing applications & service principals\n1. Create new AAD applications & service principals\n1. Manage those AAD applications & service principals (e.g. application role assignment, deletion etc.)\nLet's start by outlining how these requirements are commonly achieved today.\n### Create resources in the target subscription\nBy default, the 'Service Connections' that Azure Pipelines use to authenticate with Azure already provide this, with the option of constraining its access to a given resource group.\n### Perform ARM role assignments in the target subscription\nIf enabled for automated approaches this is typically achieved by one of the following:\n* granting the identity behind the service connection the 'owner' role in the target subscription\n* a separate identity with the required permissions, whose credentials are available to pipelines\nFor manual scenarios:\n* raising a service desk ticket\n* asking someone who has permissions to do it for you\n### Query AAD for existing applications & service principals\nThe main pipeline is restructured to expect the required information to be provided as manually-maintained configuration.\n### Create new AAD applications & service principals\nIf enabled for automated scenarios, this is commonly handled in one of three ways:\n* an entirely separate pipeline running under a different identity which does have the necessary permissions\n* this can only be considered fully-automated if the dependent pipeline is able to trigger this pipeline on-demand\n* running a subset of the main pipeline, under a different identity which does have the necessary permissions, such that only the tasks that require the elevated privileges are run\nFor manual scenarios:\n* an out-of-band process whereby an individual with the appropriate permissions executes the necessary steps (either manually or using a script)\n### Manage AAD applications & service principals\nThis is largely equivalent to the preceding 'Create' scenario, with the addition of:\n* a dependency on the 'Query AAD' requirement\n* having permissions to update applications & principals owned by others (where they were created separately)\n>NOTE: All of this functionality may have to run multiple times in the case of an evolving application, particularly where scoping of permissions is being applied.\n\n## Decision\n","In the context of pipelines running in Azure DevOps, an executing pipeline uses an Azure Resource Manager type of [service connection](https://docs.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints) to authenticate to Azure for any tasks it needs to perform.\nBelow is a configuration model that defines the security requirements for each such service connection, which is maintained in a Git repository and has a separate pipeline that applies that configuration in Azure.\n>NOTE: These YAML fragments would exist as individual files in the Git repo, with a logical folder structure to help organise and reflect ownership as required (e.g. across development teams, products etc.).\n```\n---\n#\n# This fragment maintains the shared configuration that defines the permissions used by\n# Azure DevOps service connections managed by this system.\n#\nazurerm:\ncustom_role:\nname: acme-azure-deployer\nallowed_actions:\n- Microsoft.Authorization/roleAssignments/read\n- Microsoft.Authorization/roleAssignments/write\n- Microsoft.Authorization/roleAssignments/delete\nrequired_roles:\n- contributor\n- acme-azure-deployer\nazuread:\nrequired_graph_permissions:\n- 5778995a-e1bf-45b8-affa-663a9f3f4d04      # readDirDataPermissionId\n- 824c81eb-e3f8-4ee6-8f6d-de7f50d565b7      # manageOwnAppsPermissionId\n---\n#\n# Defines the logical environments used as part of application release promotion and their\n# associated Azure subscription.\n#\nenvironments:\n- name: dev\nsubscription: <subscription-id>\n- name: test\nsubscription: <subscription-id>\n- name: prod\nsubscription: <subscription-id>\n---\n#\n# Defines an application/system that deploys resources across 2 resource groups and requires\n# service connections for 3 environments\n#\nname: product_a\nenvironments:\n- dev\n- test\n- prod\nresource_groups:\n- producta-services                       # NOTE: this option relies on a naming convention-based expansion: e.g. 'acme-<environment>-producta-services-rg'\n- acme-${ENVIRONMENT}-producta-data-rg    # NOTE: this option uses a simpler token replacement approach\n---\n#\n# Defines an application/system that deploys resources to a single resource groups and requires\n# service connections for 2 environments\n#\nname: product_b\nenvironments:\n- test\n- prod\nresource_groups:\n- acme-${ENVIRONMENT}-productb-rg\n```\nA security management pipeline, running in Azure DevOps, is able to read this configuration model and performs the following tasks to ensure that a least privilege Azure DevOps service connection is maintained per application, per environment:\n* Ensures the required resource groups exist\n* Maintains the custom role that grants the elevated permissions and maintains its set of assignable scopes (i.e. the resource groups)\n* Maintains the required AAD service principals\n* Assigns the specified roles to the AAD service principals with the required resource group scoping constraints\n* Assigns the required AAD permissions to the service principals\nThis pipeline needs to run with high-level permissions, therefore it must be tightly controlled to ensure that its elevated rights are not exploited:\n* Use of an Azure DevOps project with no user access outside the responsible team\n* The privileged service connection is only available within this restricted project and requires explicit authorisation before it can be used by a new pipeline\n* The pipeline only triggers for changes on the `master` branch\n* The `master` branch is protected and cannot be directly committed to\n* Anyone wishing to make changes to the configuration must raise a Pull Request that is subject to a review/approval policy by at least the responsible team\n","**Decision**\n\nThe pipeline security management process will be implemented using a combination of automated and manual tasks, with the following key principles:\n\n* **Principle of least privilege:** The pipeline will only be granted the minimum permissions necessary to perform its tasks.\n* **Centralized oversight:** The security team will have oversight of all pipeline security changes.\n* **Standard development workflow:** The pipeline security management process will use a standard development workflow that allows teams to manage their own configuration with minimal friction.\n\n**Automated tasks**\n\nThe following tasks will be automated:\n\n* Creating resources in the target subscription\n* Performing ARM role assignments in the target subscription\n* Querying Azure Active Directory (AAD) for existing applications & service principals\n\n**Manual tasks**\n\nThe following tasks will be manual:\n\n* Creating new AAD applications & service principals\n* Managing those AAD applications & service principals (e.g. application role assignment, deletion etc.)\n\n**Rationale**\n\nThe decision to automate some tasks and manual others was made based on the following factors:\n\n* **Complexity:** The automated tasks are relatively simple and can be easily automated. The manual tasks are more complex and require human judgment.\n* **Security:** The automated tasks can be performed without compromising security. The manual tasks require elevated privileges, so they must be performed with care.\n* **Efficiency:** The automated tasks can be performed more efficiently than the manual tasks.\n\n**Implementation**\n\nThe pipeline security management process will be implemented using the following tools:\n\n* **Azure Pipelines:** Azure Pipelines will be used to automate the tasks that can be automated.\n* **Azure AD Service Principal:** The pipeline will use an Azure AD service principal to authenticate with Azure and AAD. The service principal will be granted the minimum permissions necessary to perform the tasks that the pipeline needs to perform.\n* **Terraform:** Terraform will be used to provision the resources in the target subscription.\n* **ARM Template:** ARM Templates will be used to define the role assignments in the target subscription.\n* **AAD PowerShell Module:** The AAD PowerShell module will be used to query AAD for existing applications & service principals, and to create and manage AAD applications & service principals.\n\n**Monitoring**\n\nThe pipeline security management process will be monitored by the security team. The security team will review the pipeline logs and audit the pipeline configuration on a regular basis."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n[Handlebars](https://www.npmjs.com/package/mustache), an extension of [Mustache](https://www.npmjs.com/package/mustache) ([see the differences](https://github.com/handlebars-lang/handlebars.js#differences-between-handlebarsjs-and-mustache)) used in `fxa-auth-server` email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system [began here](https://github.com/mozilla/fxa/issues/4627). While converting our emails to a more modern templating solution, there is an opportunity to evaluate what stack would be the most ideal for FxA emails beyond a proposed templating solution. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.\n\n## Decision\n","### Templating and Styling\n- Option A - Continue to use Mustache and ad-hoc inline styles\n- Option B - Use React server-side to generate static HTML templates with TailwindCSS\n- Option C - Use EJS and MJML, using CSS options offered by MJML\n### Email previewing\n- Option A - Continue to use the `write-emails` command\n- Option B - Use Storybook\n- Option C - Use Mailtrap\n---\nFurthermore, there are a few **other decisions** worth noting that won’t necessarily have pros/cons lists:\n- How to handle generating plaintext versions\n- Transitioning to Fluent over GetText for localization\n- Plans around integration involving feature flagging and the QA process\n### Templating and Styling\nChosen option: ""Option C - Use EJS and MJML, using CSS options offered by MJML"", because:\n- HTML email has a lot of quirks - MJML shifts the burden of maintaining solutions for these off of FxA engineers now and in the future\n- While we use React and Tailwind in other parts of FxA, React is heavy for an email solution since no state is involved, component reuse across FxA would likely be very minimal, and it involves a more complex build setup than EJS\n- MJML helps significantly with responsiveness in emails, reducing time spent developing templates and making future email redesigns easier\n### Email Previewing\nChosen option: ""Option B - Use Storybook"", because:\n- It provides us the flexibility to preview all of the different email states together\n- We have a Storybook deployment already in place so hooking it up for `fxa-auth-server` will ensure consistency\n- Easy to test and perform QA without needing to touch the codebase\n- Support for CSS (whether it's plain old CSS, TailwindCSS or something else)\nNote: Along with using storybook to view the various states of the templates, we are also planning on continued support of [Maildev](https://www.npmjs.com/package/maildev) since it has applications beyond just previewing emails.\n### Other\n**Plaintext files**: We'll use [html-to-text](https://github.com/html-to-text/node-html-to-text) to automatically generate plaintext versions of templates rendered to HTML. There may be a scenario in which the automatically-generated plaintext version does not look exactly how we'd like, in which case we can look into exporting both an MJML _and_ plaintext version of the email from the template file.\n**Localization**: We will upgrade the localization tool from GetText to [Fluent](https://github.com/projectfluent/fluent.js) since it's preferred by the l10n team and other FxA front-ends are using it. With our chosen templating option we can make use of Fluent's [`@fluent/dom`](https://github.com/projectfluent/fluent.js/tree/master/fluent-dom) package.\n**Integration & QA**: During development templates will be marked as being part of a release group. This could be in the form of a mapped list of template names or some variable associated with the template file. Each group will have a corresponding environment variable flag. When this release group’s flag is enabled any templates that fall under it will be used when generating an email template in that environment; until a template's corresponding release group is enabled it is not used and the old/current template will continue to be served. Release groups will be initially enabled in staging until QA has had an opportunity to thoroughly test and evaluate each template in the group, after which they can be incrementally enabled in production. To give us the most flexibiity we can add a new auth-server configuration value that can control which users email are supported and which email templates are supported for mjml. The auth-server can then expose a feature flag method to check this value and then render the correct template.\n### Positive Consequences\n- Using MJML abstracts HTML email peculiarities away and handles responsiveness for us\n- Allows us to move away from inline CSS during development, and improves style reusability and consistency in template files\n- Using EJS allows us to write templates using JavaScript, removing the complexities of custom syntaxes like JSX and Mustache\n### Negative Consequences\n- The Storybook setup will be much more complex than with a React and Tailwind solution\n- Introduces new dependencies, and MJML introduces a small learning curve\n- Our email templates have been battle-tested over the years, and this change could introduce potential new bugs across various clients\n","**Migrate to a more modern templating solution that provides greater flexibility and functionality.**\n\n**Considerations:**\n\n* Evaluate different templating solutions, such as Handlebars, Mustache, and Nunjucks.\n* Consider the trade-offs between flexibility, performance, and ease of use.\n* Explore options for integrating with existing infrastructure and tools.\n\n**Alternatives:**\n\n* Continue using Handlebars, despite its limitations.\n* Use multiple templating solutions for different purposes.\n* Develop a custom templating solution.\n\n**Consequences:**\n\n* Improved flexibility and functionality in email templates.\n* Reduced technical debt and maintenance costs.\n* Potential for increased developer productivity."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\n## Decision Drivers\n- Reduce complexity\n- Improve performance\n- Share code\n\n## Decision\n","- Reduce complexity\n- Improve performance\n- Share code\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\n",Move the database service to a separate physical layer with its own IP address and more restricted permissions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nExternal OAuth Relying Parties (RPs) that Firefox Account users authenticate\nto need to be kept up to date about whether the user still has an active\nsubscription as well as knowing when to delete the user data. For internal\nMozilla use, Mozilla provides [Firefox Service Notifications][] over SQS\nqueues. These events are intended for internal trusted use and not suitable\nfor external RPs.\nA solution for these concerns is a new system referred to as the FxA Event\nBroker which will store FxA related events and distribute them via webhooks\nto relevant RPs that the user has accessed. The initial version described here applies primarily to external RPs which will only receive subscription status\nchanges and account deletion events. This solution is being built with future\nexpansion in mind which will require event stream storage per user and\nnotification of internal Mozilla RPs.\nDifficulties inherent in this solution lie in where to store the source of\ntruth for what RPs a user has authenticated to, where the delivery functionality\nshould reside, where the event streams for a user should reside, what set of\ndata should be included in [Firefox Service Notifications][], and how this\ndata should be communicated to the delivery system to avoid additional API\nquery load on existing FxA services.\n## Decision Drivers\n- Subscription services deadlines\n- Effort required and experience available for FxA changes\n- Separation of concerns from existing FxA microservices\n- Difficulty of schema migrations in existing FxA microservices\n- Suitability of existing FxA databases for large-scale event storage\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\nin-progress)\n\n## Decision\n","- Subscription services deadlines\n- Effort required and experience available for FxA changes\n- Separation of concerns from existing FxA microservices\n- Difficulty of schema migrations in existing FxA microservices\n- Suitability of existing FxA databases for large-scale event storage\n- Architectural desire to treat FxA Auth and OAuth as one (Merging\nin-progress)\nChosen Option: C. Implementing RP notification as a new FxA Event Broker service\nwith webhook and login activity stored in FxA Event Broker, because\n- Less subscription platform timeline risk to store new data in new database vs.\nmodify existing FxA OAuth database.\n- Storing events at scale has database requirements that don't fit in well with\nthe limitations with MySQL encountered in FxA Auth/OAuth.\n- Having FxA Auth be the only store of which RP to notify would require each\nnotification to also include what RPs to notify, increasing the load on the\nFxA Auth database.\n","The decision is to decouple the FxA Event Broker from the FxA backend by creating a new microservice that will store the source of truth for what RPs a user has authenticated to, and will be responsible for delivering events to those RPs via webhooks.\n\nThis decision was made in order to address the following concerns:\n\n- **Subscription services deadlines:** The FxA Event Broker needs to be able to deliver events to RPs in a timely manner in order to meet subscription service deadlines. Decoupling the FxA Event Broker from the FxA backend will allow us to scale the FxA Event Broker independently of the FxA backend, which will help us to meet these deadlines.\n- **Effort required and experience available for FxA changes:** The FxA backend is a complex system, and making changes to it can be time-consuming and error-prone. Decoupling the FxA Event Broker from the FxA backend will allow us to make changes to the FxA Event Broker without affecting the FxA backend, which will save us time and effort.\n- **Separation of concerns from existing FxA microservices:** The FxA Event Broker has a different purpose than the existing FxA microservices, and it should be decoupled from them in order to avoid unnecessary complexity.\n- **Difficulty of schema migrations in existing FxA microservices:** The FxA backend's schema is complex, and it can be difficult to migrate the schema without causing downtime or data loss. Decoupling the FxA Event Broker from the FxA backend will allow us to make changes to the FxA Event Broker's schema without affecting the FxA backend's schema, which will reduce the risk of downtime or data loss.\n- **Suitability of existing FxA databases for large-scale event storage:** The FxA backend's databases are not designed for large-scale event storage. Decoupling the FxA Event Broker from the FxA backend will allow us to use a different database that is better suited for large-scale event storage.\n- **Architectural desire to treat FxA Auth and OAuth as one (Merging in-progress):** The FxA Event Broker will be responsible for delivering events to both FxA Auth and OAuth RPs. Decoupling the FxA Event Broker from the FxA backend will allow us to treat FxA Auth and OAuth as one system, which will make it easier to manage and maintain the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince becoming a monorepo we haven't found a way to share code across packages that works well in local development, CI, and production without fragile, ad-hoc workarounds.\n## Decision Drivers\n- Simpler code sharing\n- Minimal overhead in setup and deployment\n- Works well for js and ts packages\n- Low maintenance\n- Long term viability\n\n## Decision\n","- Simpler code sharing\n- Minimal overhead in setup and deployment\n- Works well for js and ts packages\n- Low maintenance\n- Long term viability\nThe [workspace:\*](https://yarnpkg.com/features/workspaces) dependency type along with typescript [project references](https://www.typescriptlang.org/docs/handbook/project-references.html) provide a good internal dependency solution that addresses the shortcomings of our current situation.\nWorkspaces are not yet supported by npm but are on the [v7 roadmap](https://blog.npmjs.org/post/617484925547986944/npm-v7-series-introduction) so it's pretty safe to assume that they will be widely supported for the foreseeable future. In order to start using workspaces today we'll need to use yarn. This is a slight risk because although it is well supported it doesn't have the level of backing that npm does, however once workspaces are supported by npm there is nothing locking us in to yarn.\n### Advantages\n- No manual linking of dependencies\n- Simpler builds\n- Single lock file\n- Hoisted dependencies\n### Disadvantages\n- In the short term this requires us to switch from npm to yarn as our package manager. This will change dev workflows and is another tool to understand.\n- Some code changes are required to be compatible with yarn, however they're also backward compatible with npm.\n- Yarn doesn't currently support auditing dependencies like npm does.\n","**Use git subtree instead of yarn workspaces for code sharing across monorepo packages**\n\n**Justification:**\n\n* Git subtree meets all the decision drivers:\n    * It provides simple and straightforward code sharing.\n    * It has minimal setup and deployment overhead.\n    * It works well for both JS and TS packages.\n    * It is relatively low-maintenance.\n    * It offers long-term viability as it is a well-established and supported version control mechanism.\n* Compared to yarn workspaces, git subtree is more flexible and less prone to breakage.\n* Git subtree is more suitable for our current development and deployment setup, where packages are managed individually and not all dependencies are managed through yarn workspaces."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have a variety in tools used for unit & functional tests. This variety means developers need to learn & know many different tools. It would be nice to reduce this mental workload, while still using appropriate tools for testing.\n## Decision Drivers\n- Ensuring we're using the right tool for the job.\n- Reducing developer mental workload for working with tests.\n\n## Decision\n",- Ensuring we're using the right tool for the job.\n- Reducing developer mental workload for working with tests.\nIdentify a consistent stack of testing tools per aspect of the project\n,**Use a testing framework that supports multiple testing tools.** This allows developers to choose the right tool for the job without having to learn a new tool. It also reduces the mental workload of working with tests by providing a consistent interface to all of the tools.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFirefox Account originally implemented multi-factor authentication (MFA) support in Q1 of 2018.\nThis feature used TOTP based codes and was based on RFC 6238.\nAdditionally, if users lost their MFA device, they could use one time backup authentication codes to regain access to the account.\nHaving MFA support has helped secure our users' accounts and given them more security flexibility.\nHowever, over time it has become more obvious that users that lose their MFA device (usually phone) are at risk of getting locked out of their account because they don't save their backup authentication codes.\nThere are a non-trivial amount of users that don’t save or download their backup authentication codes, which is currently the only way they can regain access.\nIn 2019 Q4, FxA started requiring that users confirm backup authentication codes before enabling MFA.\nWhile this did help reduce lockouts, we still want to reduce it further.\nWe believe that adding a new MFA method to Firefox Accounts that has the similar security properties as TOTP would allow users to have another method to recover their account.\n## Decision Drivers\n- Improve user's account security\n- Reduce the risk of account lockout because of lost device/backup authentication code\n- Could be completed in roughly a quarter\n\n## Decision\n","- Improve user's account security\n- Reduce the risk of account lockout because of lost device/backup authentication code\n- Could be completed in roughly a quarter\nChosen option: Option A, because this is more inline with Mozilla's security and privacy principles than option B.\nThere is less security risk to users if this feature is added.\n","**Decision:** Implement a second MFA method for Firefox Accounts.\n\n**Rationale:**\n\n* **Improved user security:** Adding a second MFA method increases the security of user accounts by providing an additional layer of authentication.\n* **Reduced risk of lockouts:** A second MFA method reduces the risk of users getting locked out of their accounts due to lost devices or backup authentication codes.\n* **Feasibility:** The implementation of a second MFA method is estimated to take about a quarter, which is a reasonable timeframe."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Subscription Platform uses [Stripe Metadata][stripe-metadata] for numerous\npurposes, including product checkout content, legal document download URLs,\nand product support form options. The metadata can be updated by product\nmanagers through the Stripe dashboard, enabling product managers to manage\ncertain aspects of their subscription products without involving Subscription\nPlatform engineers. However, Stripe metadata has a limit on the number of\nvalues that we will reach in the near future. In order to continue to use\nmetadata in the same fashion in the Subscription Platform, we need to overcome\nthe limit placed by Stripe.\n[stripe-metadata]: https://stripe.com/docs/api/metadata\n## Decision Drivers\n- A higher storage limit than Stripe Metadata\n- A UI for the managers of products on the Subscription Platform to edit the\nmetadata\n- Access management for the UI\n- Updated data accessible in their respective environments without deployment\n\n## Decision\n","- A higher storage limit than Stripe Metadata\n- A UI for the managers of products on the Subscription Platform to edit the\nmetadata\n- Access management for the UI\n- Updated data accessible in their respective environments without deployment\nChosen option: Google Cloud Firestore, because\n- Data management UI and user access control provided through the Google Cloud\nPlatform (GCP) console.\n- It does not require deployments for data updates.\n- It is not novel tech; FxA already uses it elsewhere.\n","Integrate [Celesta][celesta], a metadata store, with Subscription Platform to overcome the limitation of Stripe Metadata. Celesta will provide a higher storage limit and a UI for managing metadata. Celesta also supports access management for the UI and provides updated data in respective environments without deployment.\n\n[celesta]: https://celesta.dev"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [Settings Redesign project](https://github.com/mozilla/fxa/issues/3740) offers an ideal opportunity to review CSS conventions and architecture for FxA to not only use while building new components for this project, but also for FxA moving forward.\nThis [part 1](https://github.com/mozilla/fxa/issues/4808) of [2](https://github.com/mozilla/fxa/issues/5087) ADR concerns the use of CSS variables, how they come into play with SCSS (vs using SCSS variables), CSS-in-JS considerations, and CSS modules.\nNote that 1) ""CSS variables"" are technically called ""CSS custom properties"" but will be referred to as the common ""variable"" nomenclature throughout this ADR and that 2) SASS is a CSS preprocessor while SCSS is our preferred syntax of SASS (explained [here](https://stackoverflow.com/questions/5654447/whats-the-difference-between-scss-and-sass/5654471#5654471)).\n## Decision Drivers\n- Simplicity and clarity\n- DRY and reusable styles\n- Easy integration and use\n- Current engineering experience and preferences\n\n## Decision\n","- Simplicity and clarity\n- DRY and reusable styles\n- Easy integration and use\n- Current engineering experience and preferences\nChosen options: ""CSS variables"" and ""SCSS"", because:\n- While using CSS variables with SASS will prevent using mixins (or SASS functions like `darken`) that rely on variable values, they can be used without impeding core functionality FxA uses SASS for like nested selectors, extends, and functions and mixins that don't rely on variable values like the generation and use of our media queries.\n- CSS variables are native to the browser and decision to use them now is a prudent one. They have good browser support and can be used without a preprocessor and changed after preprocessor compilation. They are targetable by JavaScript, easily themed, and can be scoped globally or on an element, providing options for usage.\n- Scoped component solutions (CSS-in-JS, CSS modules) encourage self-contained components rather than building the UI as a whole. While one goal of FxA is to reuse components across the ecosystem where possible (e.g. the `fxa-react` package), FxA will likely reap more benefits from a class-based approach. This allows for globally shared styles and a loosely coupled stylesheet rather than a tightly coupled CSS-in-JS solution with conditional styles based on props and how the component is used. Classes promote a DRYer and more consistent approach.\n- CSS-in-JS would add one additional layer of tech to learn (like `styled-components` syntax and best practices) while the Settings Redesign project is already introducing other novel tech, and members of the FxA engineering team also as a whole personally prefer not to use CSS-in-JS.\n- This decision doesn't preclude the option of implementing CSS modules at a later time if it's determined that we would prefer component scoped styles with SCSS, and CSS variables could still be used if we later switch to CSS-in-JS.\n",**Option 1: Use CSS variables exclusively**\n* **Benefits:**\n    * Simple and clear to use\n    * DRY and reusable styles\n    * Easy integration and use\n* **Drawbacks:**\n    * Not as well supported as SCSS variables\n    * Can be less efficient than SCSS variables\n* **Recommendation:** Use CSS variables for simple and reusable styles. Use SCSS variables for more complex styles or when efficiency is a concern.\n\n**Option 2: Use SCSS variables exclusively**\n* **Benefits:**\n    * Well supported\n    * Efficient\n    * Can be used for more complex styles\n* **Drawbacks:**\n    * Can be more difficult to use than CSS variables\n    * Not as DRY as CSS variables\n* **Recommendation:** Use SCSS variables for complex styles or when efficiency is a concern. Use CSS variables for simple and reusable styles.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nPayment data used in Stripe, PayPal, Apple, and Google IAP are handled as individual documents. We typically need to access different portions of these at different times. In addition, their schema occasionally changes (API updates). Stripe and PayPal have API limits, which means we typically need to duplicate/cache data we retrieve from them. At the moment we cache this data in Redis. This mostly works, but restricts our ability to run queries across payment data sets without pulling it all into BigQuery, or making substantial amounts of Stripe/PayPal requests.\nTo handle IAP, we need to store Apple receipts and Google subscription objects. These should be organized with the FxA user they belong to. They're arbitrary JSON documents that we also need copied to BigQuery for ETL jobs. To future-proof our payment data storage needs we should have the flexibility needed to store Stripe/Paypal payment data in the same data store with the same ETL capability.\n## Decision Drivers\n- Engineering resources\n- Arbitrary JSON document querying ability\n- Ease of loading data for ETL jobs\n- Capable of storing arbitrary JSON data\n- Integration with Stripe and other payment systems\n\n## Decision\n","- Engineering resources\n- Arbitrary JSON document querying ability\n- Ease of loading data for ETL jobs\n- Capable of storing arbitrary JSON data\n- Integration with Stripe and other payment systems\nGoogle Firestore. We've used Firestore in the event-broker, it has a rich API, and extensions that overlap with our requirements that reduces engineering and operations resources.\n","**Store all payment data in a document store with flexible querying capabilities.**\n\nThis decision is driven by the following considerations:\n\n* **Engineering resources:** A document store will require less engineering effort to implement and maintain than a relational database.\n* **Arbitrary JSON document querying ability:** A document store will allow us to query arbitrary JSON documents, which is essential for our use case.\n* **Ease of loading data for ETL jobs:** A document store will make it easy to load data for ETL jobs, as it can handle large volumes of data and does not require us to define a schema upfront.\n* **Capable of storing arbitrary JSON data:** A document store can store arbitrary JSON data, which is necessary for our use case.\n* **Integration with Stripe and other payment systems:** There are existing integrations available for document stores that make it easy to integrate with Stripe and other payment systems."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the implementation of payment features for subscription services, we've\ndecided to use third-party JavaScript for payment widgets.\nBest practices established by our security team indicate that third-party JS\nshould not be included on the highly-sensitive pages - i.e. such as those used\nfor general account management on Firefox Accounts.\nSo, we need a way to isolate the pages responsible for subscription sign-up and\nmanagement from the rest of Firefox Accounts.\n## Decision Drivers\n* Security when dealing with financial transactions.\n* Security when including with third-party JS code for payment widgets.\n* Simplicity in user experience flows.\n* Delivering against the subscription services deadline.\n\n## Decision\n","* Security when dealing with financial transactions.\n* Security when including with third-party JS code for payment widgets.\n* Simplicity in user experience flows.\n* Delivering against the subscription services deadline.\nChosen option: ""Option A - Payment pages as separate app supplied with\npre-generated access token"", because\n* Further refinements to access token delivery mechanism in Option A do not\nsignificantly affect the rest of the payments app.\n* Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue\n#640](https://github.com/mozilla/fxa/issues/640) is resolved.\n* Doesn't preclude Option C as a future option - e.g. offering embedded\nsubscription widgets to third-parties.\n* Fastest practical option given existing record of reviews by security & UX and\nwork completed so far.\n* Fresh start with a more modern web stack (i.e. React).\n","Create a ""payment kernel"" - a restricted set of pages that perform the sign-up and management functions for subscription services. Isolate the ""payment kernel"" from all other pages on the Firefox Accounts website. This ensures that third-party JavaScript for payment widgets is not included on the highly-sensitive pages."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Subscription Platform was implemented with Stripe as the payment processor and driver of subscription logic. Stripe is used as the source of truth for whether an account has subscriptions associated with it. We would like to add PayPal as a payment option for subscriptions, and integrate it appropriately with the existing system in a least-effort manner that is ideally flexible enough to support future expansions of payment processors, such as IAP.\n## Decision Drivers\n- Engineering resources\n- Future effort needed to integrate IAP\n- Code complexity as it relates to existing Subscription Platform infrastructure\n\n## Decision\n","- Engineering resources\n- Future effort needed to integrate IAP\n- Code complexity as it relates to existing Subscription Platform infrastructure\nStripe-driven out-of-band invoice processing. This was originally considered to be the slower of the approaches, but using PayPal subscriptions ended up being approximately the same engineering effort. Given additional user experience benefits of the Stripe-driven approach, it is now the recommend implementation path.\n","**Decision:** Implement PayPal integration as an extension of the existing Stripe integration, using the same API and data structures as much as possible.\n\n**Rationale:**\n\n* **Engineering resources:** This approach minimizes the engineering effort required, as it leverages the existing Stripe infrastructure and codebase.\n* **Future effort for IAP:** The design is extensible to support IAP integration in the future, as it provides a framework for integrating additional payment processors.\n* **Code complexity:** By using the same API and data structures, the code complexity is reduced compared to creating a separate integration for PayPal.\n\n**Details:**\n\n* Create a new payment method in Stripe called ""PayPal"".\n* Allow users to subscribe using PayPal as the payment method.\n* Handle all subscription-related logic (e.g., cancellations, upgrades) through the existing Stripe API.\n* Store PayPal-specific information (e.g., client ID, secret key) in a secure location.\n* When integrating with IAP in the future, follow a similar approach to extend the payment method functionality to IAP."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nA solution for isolating third-party payment widgets from the rest of account\nmanagement can include building a separate web app on its own dedicated domain.\nAs a side effect in building that app, we have the opportunity to choose\ntechnologies for building it that don't necessarily follow the rest of FxA.\n## Decision Drivers\n* Opportunity for a fresh start with tech stack without rewriting\n* Security in dealing with payment transactions\n* Developer ergonomics\n* Code quality & testing\n* Subscription services deadlines\n\n## Decision\n","* Opportunity for a fresh start with tech stack without rewriting\n* Security in dealing with payment transactions\n* Developer ergonomics\n* Code quality & testing\n* Subscription services deadlines\nChosen option: ""React, Redux, Typescript"", because\n* Chance to start with a fresh stack\n* More vibrant ecosystem\n* Better tooling & developer ergonomics\n","Build a separate web app on its own dedicated domain for isolating third-party payment widgets from the rest of account management. This will allow for a fresh start with the tech stack without rewriting, improve security in dealing with payment transactions, and enhance developer ergonomics, code quality, and testing. Additionally, it will help meet subscription services deadlines."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFirefox Accounts sends and receives messages in JSON utilizing queues. This ADR addresses this\nuse-case only and does not apply to JSON used in RESTful APIs.\nTracking and minimizing drift between the accuracy of message format documentation and\nthe underlying implementation is an error-prone process. In addition, JSON messages are\nfrequently produced and consumed from multiple systems, and in multiple langauges. Ensuring\nvalidity of these messages frequently requires implementing validation logic in each\nlanguage that consumes these JSON messages to ensure correctness for message handling.\nAdding a field to a message format is non-trivial, as every consumer must also update\nits own validation logic. Currently there are a variety of places where messages are consumed\nand produced, most producers do not validate the message they send as the consumer code is not\nalways in the same service where the validation code typically resides.\nThere is currently no versioning for our message formats, which also creates difficulties in\nknowing when validation logic must change and when existing flexibility is sufficient.\n## Decision Drivers\n- Accurate documentation via JSON-Schema doc generation\n- Validation logic generated from JSON-Schema to reduce manually written/maintained code\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\n\n## Decision\n","- Accurate documentation via JSON-Schema doc generation\n- Validation logic generated from JSON-Schema to reduce manually written/maintained code\n- [Semantic Versioning][schemaver] to effectively communicate breaking vs. non-breaking message format changes\nChosen option: ""option 3"", because our existing messages can be represented without any\nmigration needed and external consumers already work with JSON.\n### [option 1] Keep existing approach\nCurrently our documentation drifts if a message format is tweaked and the docs are not updated. Ideally\nour code review process would catch this but sometimes code subleties are not caught that alter the\nmessage format. Validation changes can also fail to appear in the documentation, even though what is\nconsidered a valid message key/value should be documented.\nWe have to occasionally duplicate `joi` schemas between repos, which should likely be moved to `fxa-shared`\nto benefit re-use, but this does not help non-Node projects consume our messages.\n- Pros\n- No additional work is needed that we don't already do.\n- Cons\n- Our documentation frequently drifts.\n- It's additional work to maintain accurate `joi` schemas spread throughout the code-base _or_\n- It's additional work to consolidate our `joi` schemas in `fxa-shared`.\n- `joi` schemas can't be used by consumers not written in JS.\n### [option 2] Use protobuf\nUsing protobuf would allow us to validate and document valid message formats that would also consume\nless space and be more performant to serialize/deserialize. Validation is built-in as invalid messages\nwill not deserialize, and documenation can be generated based on the protobuf spec for a message.\nProtobuf tooling is widely available for most langauges.\n- Pros\n- High throughput messaging could benefit from higher efficiency of serializing/deserializing of messages\n- Documentation stays in sync with message spec\n- Validation is built-in to the message serialization/deserialization process\n- Cons\n- Messages are in binary and not easily introspectable\n- Existing message consumers will need to be updated to handle protobuf\n- Migration procedure will need to be created/implemented to shift from existing JSON format\n### [option 3] Use JSON-Schemas\nJSON-Schemas can define existing FxA JSON messages used in our queue system. There are libraries in\nmost languages to consume these schemas for validation purposes, and libraries exist to generate\ndocumenation from a JSON-Schema.\n[SchemaVer] can be introduced to existing FxA JSON messages as consumers ignore unknown keys, while\nconsumers using the JSON-Schema can verify they can handle new message formats or if they're incompatible.\nSome of this logic is manual, as [SchemaVer] is not an official part of the JSON-Schema specification. There\nis a recommendation of [SchemaVer] in [Best Web Practices for Data](https://www.w3.org/TR/dwbp/#dataVersioning).\n- Pros\n- No changes needed for existing consumers\n- Validation can be done based on the JSON-Schema without custom code\n- Documentation can be generated to stay in sync with message spec\n- JSON is human readable without further processing.\n- JSON is widely used on the web and our increasing use of JWTs also result in JSON.\n- JSON-Schema is widely supported in a variety of langauges.\n- Cons\n- [SchemaVer] is a draft that isn't officially used by the JSON-Schema committees, but no other versioning\nscheme for JSON-Schema appear to exist as of this writing.\n[schemaver]: https://snowplowanalytics.com/blog/2014/05/13/introducing-schemaver-for-semantic-versioning-of-schemas/\n",Use JSON Schema with Semantic Versioning to describe message formats and enable automated validation across multiple languages.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nA mechanism is needed to inform service providers (SP) that a user has paid for a given subscription. A SP could fetch the user's profile information, however, we prevent some 3rd party SPs from doing so, and instead give them all the information they need in a JWT format access token. Adding subscription information in JWT access tokens will give SPs the information they need to verify users have paid for a subscription.\n## Decision Drivers\n- Security - Users should not be able to get access to subscriptions they have not paid for.\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\n\n## Decision\n",- Security - Users should not be able to get access to subscriptions they have not paid for.\n- Extensibility - Adding subscription info should not inhibit future extensions to JWT access tokens.\n- Standards - The [JWT access token draft spec][#jwt-draft-spec] format should be followed as closely as possible.\nChosen option: Adding subscription info into its own claim was chosen because FxA's lax scope checking means bad acting users could grant themselves access to subscriptions they have not paid for. See [this bug regarding FxA's lax scope checking][#lax-scope-checking].\n### Positive Consequences\n- The JWT access token contains all the information an SP needs to verify the user has paid for a subscription.\n- Users are unable to grant themselves access to subscriptions they have not paid for.\n### Negative Consequences\n- SPs must now check two claims from the JWT to ensure a user is able to access a protected resource.\n- An additional claim is added that is not defined in [the JWT access token draft spec][#jwt-draft-spec].\n,"**Add a new `subscription` claim to the JWT access token, containing a list of subscription IDs that the user has paid for.**\n\nThis decision meets the following criteria:\n\n* **Security:** The subscription information will be included in the JWT access token, which is signed by the authorization server. This ensures that only authorized SPs can access the information.\n* **Extensibility:** The new `subscription` claim is added to the JWT access token without modifying the existing claims. This allows for future extensions to the JWT access token without breaking compatibility with existing SPs.\n* **Standards:** The new `subscription` claim follows the format of the JWT access token draft spec."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen Firefox Accounts was created some 6-7 years ago, the front-end Javascript framework and HTML templating options were limited significantly to those available at the time. This ADR details the pros and cons of various FE libraries and frameworks, and why it is desirable for FxA to move to a different technology. It does not cover _how_ we will begin the conversion which can be [read about here](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md).\nWhile Backbone and Mustache have served their purposes for FxA thus far, other modern libraries and frameworks not only offer more appealing architecture and developer conveniences, but also offer better data predictability, potential performance boosts, and a strong community of online support.\n## Decision Drivers\n- The desire for a more modern FE architecture and a more efficient UI\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\n\n## Decision\n","- The desire for a more modern FE architecture and a more efficient UI\n- As Backbone declines in popularity, the desire to move to a library or framework actively being improved with great community support is prevalent\n- A modern framework will likely feel more familiar to new engineers to the team as well as open source contributors\nChosen option: ""Option B - Move to client-side React"", because\n- The team desires the benefits we will gain from moving to a more modern library (excluding option A)\n- The team has experience with (and likes) React\n- We're already using it in other parts of the ecosystem, i.e. the payments server\n- FxA does not need server-side rendering (option C)\n- Angular (option D) is heavy, opinionated with a steep learning curve, and offers at least some features we won't need\n- Web components and libraries around them (option F) are quite new, yet a distant potential architecture shift from React to web components appears to be far less dramatic than from Backbone to React\nIn the end, it is recognized that Vue _may_ have been a good option for FxA, but React was chosen over Vue due to:\n- existing FxA engineering team knowledge\n- the FxA payments server is already using React\n- React has already been reviewed by our security team and ""passed"" the trial run when it was novel tech in our stack\n- As one of our desires is to best protect against such a major refactor proposal in the future, we have to allow package age, online support, popularity and backing, and stability to be factors in our decision.\n","The decision is to move to a modern FE library or framework such as React, Angular, or Vue.js. This will provide a number of benefits, including:\n\n* A more modern and efficient UI\n* Improved data predictability\n* Potential performance boosts\n* A strong community of online support\n* Familiarity to new engineers and open source contributors"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe new FxA settings page introduces a new service in the form of a GQL API that will provide account and profile data to the browser. This service needs to make api requests to the auth-server on behalf of logged in users. The auth-server uses hawk authentication so adding another service between the browser and auth-server is not straightforward. In [ADR-0017](0017-switch-settings-auth-to-sessiontoken.md) we decided to allow the browser to share the session token with the GQL server so that it can make hawk authenticated requests. This leads us to question whether we should deprecate our use of hawk altogether.\nLong ago, the precursor to onepw had 2-phase signup/login that would establish a secret key that would be used to encrypt/decrypt the sessionToken so that it was never in the clear on the wire. It was secure enough that you could login over HTTP and still be safe. We'd still use TLS, but - defense in depth. Once we had the sessionToken we used Hawk for API requests to maintain that level of security. We implemented this protocol and it worked as designed. The problem at the time was we wanted to support low-end phones on slow networks and the extra round trips meant we were too slow; bad UX. Instead of scrapping the whole thing we modified the protocol to the one we use today. The sessionToken is now sent over TLS in the signup/login response, but the ""secure even over HTTP"" dream is gone.\nHawk is designed for authentication over HTTP. Quoting from the design goals of Hawk:\n- simplify and improve HTTP authentication for services that are unwilling or unable to deploy TLS for all resources,\n- secure credentials against leakage (e.g., when the client uses some form of dynamic configuration to determine where to send an authenticated request), and\n- avoid the exposure of credentials sent to a malicious server over an unauthenticated secure channel due to client failure to validate the server's identity as part of its TLS handshake.\nNone of those scenarios apply to auth-server.\nDefining the replacement for hawk is out of scope, but let's consider the properties of the interim replacement: using the sessionToken as a bearer token.\nThe new surface area to which the token is exposed is in the HTTP header of API requests, sent over TLS. Both the client and server storage of the token remains unchanged. A compromised TLS connection would expose the token on more API calls where it would not have been with hawk. The sessionToken is in the body of the login response, so even with hawk that endpoint could leak the token if the TLS connection is compromised.\nA leaked sessionToken would allow access to personal information, email addresses and device metadata, but not the FxA password, secret keys, or Firefox Sync data.\n## Decision Drivers\n- Security\n\n## Decision\n",- Security\nWe will stop using hawk in future work that requires authentication. Selecting a preferred scheme is out of scope of this ADR. Sharing the session token with trusted services is an acceptable interim solution.\n,"**Deprecate hawk authentication in favor of using the session token as a bearer token.**\n\n**Rationale:**\n\n* Hawk authentication is designed for use over HTTP, and the auth-server is not exposed via HTTP.\n* The security benefits of hawk authentication are not applicable to the use case of making authenticated requests to the auth-server on behalf of logged in users.\n* Using the session token as a bearer token exposes the token to a slightly larger surface area, but the risk of compromise is still low.\n* A leaked session token would not grant access to sensitive data such as the FxA password, secret keys, or Firefox Sync data.\n\n**Consequences:**\n\n* The Hawk authentication code will need to be removed from the GQL API service.\n* The auth-server will need to be updated to accept session tokens as bearer tokens.\n* The browser will need to be updated to share the session token with the GQL API service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\n## Decision Drivers\n- User happiness via fewer keystrokes, less confusion\n- Improved signin rates\n\n## Decision\n","- User happiness via fewer keystrokes, less confusion\n- Improved signin rates\nChosen option: ""option 2"", because it minimizes the number of places the user must enter their password.\n### Positive Consequences\n- User will need to type their password in fewer places.\n- Signin completion rates should increase.\n### Negative Consequences\n- There may be user confusion around what it means to sign out.\n### [option 1] Keep the existing flow\nIf a user signs in to Sync first and is not signing into an OAuth\nRP that requires encryption keys, then no password is required.\nIf a user does not sign into Sync and instead signs into an\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\nrequire encryption keys, e.g., Monitor, then they must enter their password.\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\n- Good, because we already have it and no effort is required to keep it.\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\n- Bad, because users need to enter their password more than they need to.\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\n### [option 2] Only ask authenticated users for a password if encryption keys are required\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\n","**Do not prompt for passwords when the user is already authenticated.**\n\nThis decision is based on the following:\n* **User happiness:** Entering passwords is a pain point for users, especially on mobile devices. Eliminating unnecessary password prompts will improve user satisfaction.\n* **Improved signin rates:** Reducing the number of password prompts will make it easier for users to complete authorization flows, leading to higher signin rates.\n* **Technical feasibility:** It is technically feasible to use the existing sessionToken to request new OAuth tokens for RPs that do not require encryption keys."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\nIn [ADR-0014](0014-auth-for-settings-redesign.md), we'd decided to use OAuth2 as the mechanism. But, upon attempting to implement, we discovered that [context missing from OAuth2 tokens][missing-oauth2-context] rendered the option unusable.\n[missing-oauth2-context]: https://github.com/mozilla/fxa/pull/4931#discussion_r411828476\n## Decision Drivers\nBasically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\n- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\n\n## Decision\n","Basically, same as [ADR-0014](0014-auth-for-settings-redesign.md):\n- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\nChosen option: ""Reuse existing sessionToken on content-server with auth-server APIs"", because it's the least novel option requiring fewest changes to auth-server.\n### Positive Consequences\n- Easiest path to authenticating on auth-server for API requests, works almost exactly like the settings client does already.\n### Negative Consequences\n- We defeat the security promises of Hawk, but we're not really relying on them. We'd also like to move away from Hawk in the future anyway.\n",Implement OAuth2 Token Exchange Grant to pass context headers between content-server and downstream services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor over the past 5 years (at least), we have been using [InternJS](https://github.com/theintern/intern) for our functional tests in the content-server. While this has worked well and caught several bugs, we should reevaluate it since we are migrating to react and have an opportunityto use newer/faster/better supported testing frameworks.\n## Decision Drivers\n- Ease of writing tests\n- Good community support\n- Learning curve of settings V2 development\n\n## Decision\n","- Ease of writing tests\n- Good community support\n- Learning curve of settings V2 development\nUse InternJS for functional testing in the settings v2 app, but also update the tests to support async/await. The biggest factor was not having to introduce another testing framework for developers to learn.\n","**Decision:** Migrate to Cypress.io for functional testing.\n\n**Rationale:**\n\n* **Ease of writing tests:** Cypress.io has an intuitive and user-friendly API that makes it easy to write and maintain tests. It also provides a visual testing tool that allows developers to quickly identify UI inconsistencies.\n* **Good community support:** Cypress.io has a large and active community that provides support, resources, and best practices. This ensures that developers will have access to help when needed.\n* **Learning curve of settings V2 development:** While migrating to Cypress.io may require some learning, it is considered to be a modern and well-supported testing framework that will provide significant benefits in the long run. Additionally, the documentation and tutorials provided by Cypress.io make the onboarding process relatively straightforward."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe goal of fxa-email-service was to spin off the email sending responsibilities of FxA into a shared service that multiple Mozilla projects could use. From its readme:\n> The FxA team had an OKR for Q2 2018 about decoupling the auth server from SES and making it possible to send email via different providers. Subsequently, some other teams expressed an interest in depending on a standalone email service too.\n> This repo started as our experiment to see what a decoupled email-sending service would look like, written in Rust. It is now handling all FxA email traffic in production, and we are gradually separating it from the FxA stack to run as a standalone service in its own right.\nHad it achieved the goal of fully decoupling from FxA and being more widely used we'd likely continue using it. However, in the approximately 4 years of its existance it has only really been used as an intermediate step between auth-server and SES. Fortunately in that time it hasn't required much maintenance so its ""weight"", being a fairly large codebase for what it does and our only Rust service, has never been a concern. Recent work to eliminate fxa-auth-db-mysql meant we either needed make changes to it or rethink how FxA sends email. It turned out the work to update email-service was larger than eliminating it and replacing it by sending email directly from auth-server via SES or SMTP.\n## Decision Drivers\n- Future maintenance\n- Email provider flexability\n\n## Decision\n",- Future maintenance\n- Email provider flexability\nWe will eliminate email-service.\n,**Decision:** Deprecate and remove fxa-email-service.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [Settings Redesign project](https://github.com/mozilla/fxa/issues/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https://github.com/mozilla/fxa/issues/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https://github.com/mozilla/fxa/blob/main/docs/adr/0015-use-css-variables-and-scss.md).\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https://protocol.mozilla.org/fundamentals/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\n## Decision Drivers\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\n\n## Decision\n","- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\nChosen options: ""Option B"" with Tailwind CSS for majority styling, and implementation details from ""Option D"" when utility classes don't meet the entire need, because:\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\n","We will use CSS-in-JS (CIJS) and BEM for the Settings Redesign project. CSS-in-JS is a technique where CSS is written in JavaScript files, rather than in separate CSS files. This allows for more dynamic and interactive CSS, and it makes it easier to manage CSS for complex components. BEM is a CSS methodology that uses a consistent naming convention for CSS classes, which makes it easier to read and maintain CSS code.\n\nThis decision was made after considering the following factors:\n\n* **Reusability:** CIJS and BEM both promote reusability by making it easier to create and share CSS components.\n* **Longevity:** CIJS and BEM are both well-supported and have a large community of developers. This means that they are likely to be supported for years to come.\n* **Developer experience:** Some team members are already familiar with CIJS and BEM, which will make the transition to these technologies easier.\n* **Ease of use:** CIJS and BEM are both relatively easy to learn and use. This will make it easier for engineers to pick up these technologies and start using them in their work."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\n## Decision Drivers\n- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\n\n## Decision\n,"- Smooth UX\n- Security\n- Development velocity\n- Ease of integration\nChosen option: ""Authenticate via OAuth2 to use auth-server APIs"", because:\n- It requires minimal changes to existing auth-server implementation and infrastructure.\n- It relies on an authentication mechanism with relatively well-known security properties.\n### Positive Consequences\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\n### Negative Consequences\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\n- We don't have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\n","**Implement a custom authentication and authorization mechanism specific to the Settings Redesign app that leverages the existing infrastructure of the content-server app.**\n\nThis decision aligns with the following decision drivers:\n\n**Smooth UX**: By using a custom mechanism, we can tailor the authentication and authorization process to the specific needs of the Settings Redesign app, ensuring a smooth user experience.\n\n**Security**: The custom mechanism will allow us to implement additional security measures that are specific to the Settings Redesign app, ensuring the protection of sensitive user data.\n\n**Development velocity**: By leveraging the existing infrastructure of the content-server app, we can reduce the development time and effort required to implement the authentication and authorization mechanism.\n\n**Ease of integration**: Integrating the custom mechanism with the existing infrastructure will be relatively straightforward, minimizing disruption to the overall system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFirefox Accounts is introducing a new visual design for Settings (the interface that exists at the address `https://accounts.firefox.com/settings`). The changes involved are significant enough such that it [has been decided](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md) that an entirely new React App will be built more or less independent of the existing Settings web app. With this we can beta test and roll out the new design with minimal disruption.\nAs there are a multitude of ways a React App can be set up and configured, this decision has introduced the question, ""What [toolchain](https://en.wikipedia.org/wiki/Toolchain) should we set up a new React App with?"". This ADR serves to answer that question by going over the various approaches we can take to set up a new React App and how it might integrate with our existing FxA ecosystem.\nIt's important to note that we are not deciding the languages, libraries, or other tools that we will use within the React development environment, but rather, the system that will be used to develop the React App with.\nFinally, we are not making decisions about the App's production server here. We'll be using [fxa-content-server](https://github.com/mozilla/fxa/tree/main/packages/fxa-content-server/server)'s Express server for the production build.\n## Decision Drivers\n- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\n\n## Decision\n","- **Familiarity** - unless there is an overwhelming reason, we don't really want to introduce a completely foreign build process for everyone to learn.\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\n- **Maintainability** - how much work do we set up for ourselves if we're maintaining all upgrades and configuration?\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\nChosen option: ""B3 - Create React App and use Rescripts"", because:\n- Our team has sufficient knowledge in working with CRA over the other options.\n- We don't need to re-invent the wheel. Our needs for the Settings project are not so great that it would be worth spending time creating a ground-up toolchain. To some extent this option allows us to do what we do best without needing to worry about the specifics of the development workflow.\n- Rescripts allows us to layer additional configuration overtop of the existing configuration without jeopardizing the integrity or trading away any of the powers of CRA. Specifically, we can use Rescripts to modify CRA to allow external React components to be imported, as demonstrated in [this repo](https://github.com/jodyheavener/rescripts-react-app) (technical notes [here](https://github.com/jodyheavener/rescripts-react-app/blob/master/.rescriptsrc.js)). This capability could be implemented in our other React Apps.\n- If, down the road, we end up in a place where we absolutely need access to the internals we can always eject.\n","The team decided to use [Nx](https://nx.dev/), a monorepo toolkit that provides a more structured approach to monorepo management, with pre-built configurations for common tools and plugins, such as:\n- Linting\n- Testing\n- Code generation\n- Build caching\n- Documentation generation\n- Code formatting"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared/redis`.\n## Decision Drivers\n- Need for robust redis transactions\n- Increased use of redis for oauth\n- Updating module dependencies\n- move from ""redis"" (last updated 2 years ago) to ""ioredis"" (actively maintained)\n\n## Decision\n","- Need for robust redis transactions\n- Increased use of redis for oauth\n- Updating module dependencies\n- move from ""redis"" (last updated 2 years ago) to ""ioredis"" (actively maintained)\nUse lua scripts. The proof-of-concept [PR](https://github.com/mozilla/fxa/pull/3278) showed lua as viable and an overall simpler option.\n### Positive Consequences\n- Future use cases should be simpler to implement\n- Less custom Redis glue code and simpler stack traces\n- Improved maintainability\n### Negative Consequences\n- Additional developer cost of understanding lua\n- Changes to lua scripts require additional consideration with regard to performance. Performance implications should be a checklist item on any lua script change PR, similar to SQL stored procedures.\n","Investigate the ""ioredis"" module as a replacement for our current redis client in `fxa-shared/redis`. If ""ioredis"" meets our needs, we should migrate to it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nOur functional test suite currently has a 7% success rate when run against our stage environment after a deployment and a 44% success rate when run in CI for pull requests. These low rates are more from flaky tests and a finicky testing stack than legitimate bugs in the code being tested.\nIn stage this rate is too low to be able to confidently move to a continuous delivery pipeline. In CI it slows down development and decreases morale.\nBecause of our low success rate for pull requests each PR needs two runs of a relatively expensive task on average. In the last 90 days we used ~1.2M CircleCI credits for PRs. Ideally we could cut that in half.\nWe should evaluate other testing stack options to improve reliability.\n\n## Decision\n,"Playwright performs better than Intern in all goals. We should prefer it for new tests and begin migrating old tests when they need maintenance and on a case-by-case basis to improve our CI pass rate.\nTo reduce our CircleCI bill and speed up CI runs for pull requests we should segment our functional tests into two categories: P1 and P2 (for lack of a better name). P1 tests for critical functionality run on every pull request. P2 tests should run periodically (daily) and send results to Slack. The difference between a P1 and P2 test suites is that a failed P1 means some ""happy path"" is broken, an S1 or S2 level bug, while P2 tests would represent S3 or S4 bugs.\nP1 tests are the first priority for converting to Playwright.\n",**Decision:**\n\nMigrate to a more reliable testing stack that can improve the success rate of functional tests in both stage and CI environments.\n\n**Rationale:**\n\n* The current testing stack is unreliable and results in a high rate of flaky tests.\n* This low success rate hinders continuous delivery and slows down development.\n* The high number of flaky tests in PRs is also a costly overhead.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor subscription services, we need a way to inform a Relying Party about the\nrelevant subscription status to enable features for a given user.\n## Decision Drivers\n* Engineering simplicity\n* Security\n* Business Operations\n* Marketing\n\n## Decision\n","* Engineering simplicity\n* Security\n* Business Operations\n* Marketing\nChosen option A: Single profile assertions consisting of a list of subscription capability\nstrings based on requesting client ID.\n* Subscription capabilities are conveyed as a single additional profile assertion.\n* FxA maintains a mapping of client-to-capabilities, capabilities-to-product\n* Relying Parties can provide multiple subscription-gated capabilities.\n* Example:\n* **RP-A** provides `goldBadge` and `silverBadge`\n* **RP-B** provides `goldBadge` and `unlimitedStorage`\n* **RP-C** provides `freePuppies`\n* FxA can bundle capabilities into cross-RP products as needed for marketing &\nbusiness purposes.\n* Example:\n* **Product A** bundles `goldBadge` & `unlimitedStorage`\n* RPs are not granted visibility into the user's entire subscription status.\n* Example:\n* User subscribes to **Product A**.\n* **RP-A** will see `goldBadge` but not `unlimitedStorage` in User's `subscriptions` claim in profile.\n* **RP-B** will see both `goldBadge` and `unlimitedStorage`\n* **RP-C** sees no capabilities listed\n","**Decision:** Implement a webhook system to notify Relying Parties of subscription status changes.\n\n**Rationale:**\n\n* **Engineering simplicity:** Webhooks are a lightweight and straightforward way to send notifications without requiring complex integrations.\n* **Security:** Webhooks can be secured using HTTPS and authentication mechanisms, ensuring the confidentiality and integrity of data.\n* **Business Operations:** Webhooks allow for real-time updates on subscription status, enabling timely actions and efficient business processes.\n* **Marketing:** Webhooks can trigger automated marketing campaigns based on subscription status, providing personalized messaging and targeted promotions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [Settings Redesign project](https://jira.mozilla.com/browse/FXA-840) will be created [as a new React application](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\n[GraphQL](https://graphql.org/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\n""Apollo"" in this document refers to the ""Apollo client"" and ""Apollo server"" pieces of the Apollo platform¹, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\n¹Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https://www.apollographql.com/docs/intro/platform/). We do not need to use these to use GQL with Apollo server or Apollo client.\n## Decision Drivers\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\n\n## Decision\n","- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\nChosen option: ""B - Layer GraphQL on top of our REST architecture"", because:\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more ""declarative"" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\n","**Use GraphQL with Apollo for the Settings Redesign project.**\n\n**Rationale:**\n\n* **Performance:** GraphQL allows for more efficient data fetching by reducing the number of network requests and data transferred. It can also prioritize data fetching based on the client's needs, improving performance.\n* **Ease of setup:** Apollo provides a convenient setup for client-side API calls, simplifying the development process.\n* **Clarity:** GraphQL provides a clear and structured way to define data requirements, enhancing code readability and reducing potential errors.\n* **Development speed:** Both the initial setup and ongoing feature development can be accelerated using GraphQL and Apollo's tools, such as caching and React integration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n""Settings"" refers widely to the `accounts.firefox.com/settings` page where users can manage their Firefox account. The ""Settings Redesign Project"" refers to a 2019/2020 project centered around giving this page a fresh user interface. The changes are fairly significant and will require a lot of component shuffling and some functionality refactoring. [See the PRD](https://docs.google.com/document/d/18zu7JCYIsUp8tUMJqb2uErNlzL9f6CQvefLy9HFZ4UY/edit?pli=1#heading=h.cf57dt1i8634).\nThe FxA Engineering team desires to migrate from [Backbone and Mustache to React](https://github.com/mozilla/fxa/blob/main/docs/adr/0010-transition-fxa-from-backbone-to-react.md). This ADR addresses questions around what level of action we should take regarding this desire and the Settings Redesign Project - should a conversion to React be done at a later time, or are the changes significant enough to justify propelling us into React now?\n\n## Decision\n","Chosen option: ""Option C - Take the 'ground up' approach and create a new settings React application"", because\n- The overall goal of the Settings Redesign Project is to offer more flexibility to expand Settings functionality. New features have already been planned. It could be argued that heavy refactoring later (which would be needed for option A or B) would take just as much time as recreating this page with React now.\n- This approach allows us an opportunity to set up Storybook for the content server and review our tests, CSS, and a11y implementations as we implement the UI changes.\n- This approach also simplifies our GH workflow and A/B testing the entire redesign.\nWe can mitigate risks by avoiding a ""big bang"" surprise replacement by implementing, deploying, and launching smaller pieces of MVP functionality along the way under a new front-end route serving the React application while keeping the Backbone Settings live. While this may be tedious, it can be a safety net against losing integrity and wisdom earned in the original system.\n","The decision is to migrate to React during the Settings Redesign Project.\n\nThe team decided that the significant changes required for the Settings Redesign Project justify migrating to React during the project. This approach provides the following benefits:\n- The migration will be completed earlier, allowing us to take advantage of React's benefits sooner.\n- The migration will be more comprehensive and less disruptive if it is done during a time when the codebase is already undergoing significant changes.\n- The team will have more experience with React by the time the project is completed, which will make it easier to maintain the codebase in the future."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\n- Not clear where/how to add new components and takes time to study/understand how things are currently setup in an attempt to mimic the structure for the new component.\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\n## Decision Drivers\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger/OpenAPI a bonus)\n- GraphQL\n- Use of components/services easily in a script\n\n## Decision\n","- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger/OpenAPI a bonus)\n- GraphQL\n- Use of components/services easily in a script\nChosen Option: Use an existing framework: NestJS\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory/filename conventions from NestJS and use `typedi` as a DI system to ease configuration/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\n### Positive Consequences\n- Less documentation to create, as we only document differences for legacy services.\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\n### Negative Consequences\n- There will be some effort involved to migrate the 4 packages to NestJS.\n- We will need to document the differences between NestJS and other legacy services.\n","**Decision:** Adopt the ""Flask"" micro-framework as the application framework for FxA back-end services.\n\n**Rationale:**\n\nFlask provides a lightweight, easy-to-use framework that aligns with the decision drivers:\n\n* **Documented architecture:** Flask has a clear and well-documented architecture, making it easy for new developers to understand and contribute.\n* **Tooling and reduced boilerplate:** Flask plugins and extensions streamline component setup and boilerplate code generation.\n* **Modern paradigms (e.g., DI):** Flask supports DI through its `flask-dependency-injector` extension.\n* **Training materials and ease of development:** Flask is widely used and has extensive documentation and tutorials, making it accessible for new developers.\n* **Legacy migration support:** Flask provides a clear path for migrating legacy applications to its conventions.\n* **Handles common scenarios:** Flask offers built-in support for exception handling, validation, service instantiation, authentication, RESTful APIs, and GraphQL.\n* **Script compatibility:** Flask components and services can be easily consumed in scripts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGitHub Pages is a perfectly good static site host, however, not having the\nability for preview builds is problematic when wishing to test changes prior to\nthem being merged into the main branch. Netlify has the ability to create\npreview builds for each PR or even each branch along with a number of other\nbenefits over GitHub Pages as can be seen in this (totally unbiased!)\n[comparison](https://www.netlify.com/github-pages-vs-netlify/).\n\n## Decision\n",The decision is to move hosting and deployment onto Netlify\n,"**Decision:** Migrate the static site from GitHub Pages to Netlify.\n\n**Rationale:**\n\n* Netlify offers preview builds, enabling testing of changes before merging them into the main branch.\n* Netlify provides additional benefits over GitHub Pages, as highlighted in the provided comparison.\n* Migrating to Netlify will enhance the development workflow by allowing more efficient testing and deployment of changes."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as\n[described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) template to capture and document the architectural decisions made throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis design is based on the observations and lessons learned over the years from various approaches taken\nto solve problems in AWS/Zuora domain. The core idea is minimalism in tools, abstractions and indirections.\n### Small Scala AWS Lambda package size of less than 10 MB\n* There is no need for custom lambda libraries. The following vanilla Scala snippet is all that is necessary to\ndefine Scala lambda\n```scala\nobject Lambda {\ndef handleRequest(input: InputStream, output: OutputStream): Unit = {\n// deserialise stream\n// run program\n// serialise to stream\n}\n}\n```\n* There is no need for importing web frameworks/libraries within lambdas as AWS provides necessary facilities\nout-of-the-box such as logging, concurrency, error handling, etc.\n* Many custom libraries are not necessary as Scala is a very expressive language which can express the same\nconcept using vanilla facilities\n* If something can be done with couple of lines of code, there is no need to import whole library just to do that\n### Prefer direct inlined business logic over abstracted indirections\n* Generally speaking there is not much algorithmically complicated logic in our business domain.\n* Essentially we need an HTTP client, JSON deserialiser, and some request orchestration code.\n* We should be able to inline directly the whole orchestration in a single file, instead of spreading it\nover many files, libraries, and abstractions.\n* Advanced features should be used in few places where needed, instead of across the whole system\n### Use off-the-shelf infrastructural facilities instead of re-inventing the wheel\n* AWS has good documentation, many examples across open source GitHub, and is GDPR compliant.\n* Setting up infrastructure is difficult, but it is largely a one-time affair. After that it rarely changes,\nand when we have to then we should be able to look up documentation\n* For example, if our stack is API Gateway + Lambda, there is no need to import web frameworks, implement custom\nrouters, etc.\n### Testing in production via preconditions-program-postconditions instead of mocked test\n* Favour runtime preconditions and postcondition testing in production instead of unit test with mocked HTTP/JSON.\n```\ninput\n.tap  { preconditions }\n.pipe { program }\n.tap  { postconditions }\n```\n* Reserve unit tests for complicated business logic algorithms, not plumbing\n* Issues such as malformed JSON or wrong HTTP request orchestration are likely to surface quickly after deployment\n(as long as alarming is setup) thus ROI for unit testing these aspects is likely not worth it\n* Runtime preconditions/postconditions is just code alongside main business logic and as such harder to ignore and\nhas lower maintenance cost relative to unit tests\n### Prevent silent failures\n* There have been multiple cases of silent failures introduced by handling errors with techniques that swallow\nerrors without error logging or wiring them to non-200 responses. Failing fast on unrecoverable errors hooks into AWS\nout-of-the-box error logging and alarming at minimal cost.\n* If there is no automatic error recovery possible and system cannot proceed, why fail slow?\n* Automatic error recovery, self-healing, etc., are difficult to achieve ideals, usually not possible in Zuora.\nDue to nature of Zuora outages which are hours long, error handling cannot be meaningfully addressed\nby techniques on a unit level. Instead, they should be addressed on a much higher infrastructural level such as using\nSQS, Step Functions, etc.\n* If error recovery is possible, then it should **not** actually be modelled as an error but simply an alternative path\nthe system tries.\n### Minimal custom abstractions of Zuora\n* Work on whitelisting as opposed to blacklisting principle\n* If system detects a scenario it cannot handle then immediately notify developers to make adjustment\ninstead of trying to predict the Zuora model up-front to handle all imaginable scenarios\n* Modelling of Zuora has not been very successful over the last five+ years due to the way Guardian uses it with\ncountless exceptions in the model\n* Instead use Zuora directly the way they document it which means essentially on the REST API level.\n* Favor preconditions and postconditions to check if we have done the right thing\n* ""Does the invoice look correct after production mutation?"", as opposed to\n* ""Here is the predicted model capturing all the ways invoice can be generated, and the corresponding (mocked) unit test.""\n* Zuora is hard enough to understand without developers putting another layer on top of it\n### Usage of lightweight HTTP and JSON libraries and vanilla Scala\n* Scala is an expressive language, and many concepts can be directly implemented without depending on external libraries\n* JSON and HTTP is boring, there is no need for sophisticated libraries, or even worse,\nre-inventing yet another custom implementation\n* There is no need for special logging or config libraries. AWS provides this out-of-the-box.\n* Most scenarios do not require advanced techniques/libraries. Such techniques should be used only where necessary\nin isolated segments of the codebase.\n\n## Decision\n",Pending\n,"Use direct, inlined business logic over abstracted indirections. For example, use a single file with a few lines of code for lambda function instead of spreading the code over several files and multiple abstractions."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe try to structure our package in logical sub-units but we want to maintain a\nconsistent public interface.\n\n## Decision\n,We allow for arbitrarily nested sub-packages but export important classes and\nfunctions to the top level thus exposing a public interface. Our unit tests\nshould reflect this package structure.\n,"**Decision:**\n\nUse a [facade](https://refactoring.guru/design-patterns/facade) design pattern to expose a simplified and unified interface for a complex subsystem.\n\n**Benefits:**\n\n* **Simplified public interface:** The facade hides the complexity of the underlying subsystem, providing a clear and concise entry point for clients.\n* **Increased flexibility:** The facade can be modified independently of the underlying subsystem, allowing for changes to the subsystem without affecting clients.\n* **Improved maintainability:** The facade centralizes the public interface, making it easier to maintain and update.\n\n**Implementation:**\n\n* Create a facade class that implements the desired public interface.\n* The facade class should delegate method calls to the appropriate classes in the underlying subsystem.\n* Use dependency injection or other mechanisms to provide the subsystem objects to the facade.\n\n**Example:**\n\nConsider a system with multiple classes representing different aspects of an order processing system.\n\n```\npublic interface OrderFacade {\n  Order createOrder(Customer customer, List<Product> products);\n  void updateOrder(Order order, Customer customer, List<Product> products);\n}\n\npublic class OrderFacadeImpl implements OrderFacade {\n  private OrderService orderService;\n  private CustomerService customerService;\n  private ProductService productService;\n\n  @Override\n  public Order createOrder(Customer customer, List<Product> products) {\n    Customer customerFromDB = customerService.getCustomer(customer.getId());\n    List<Product> productsFromDB = productService.getProducts(productIds);\n    Order order = orderService.createOrder(customerFromDB, productsFromDB);\n    return order;\n  }\n\n  @Override\n  public void updateOrder(Order order, Customer customer, List<Product> products) {\n    Customer customerFromDB = customerService.getCustomer(customer.getId());\n    List<Product> productsFromDB = productService.getProducts(productIds);\n    OrderService.updateOrder(order, customerFromDB, productsFromDB);\n  }\n}\n```\n\nIn this example, the `OrderFacade` class provides a simplified public interface for creating and updating orders, while the underlying subsystem (`OrderService`, `CustomerService`, and `ProductService`) handles the complex logic."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make a decision on the testing framework for our project.\n\n## Decision\n,We will make use of pytest. It is a de facto standard in the Python community\nand has unrivaled power.\n,We will use the Robot Framework.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSetting up different testing environments and configurations can be a painful\nand error prone process.\n\n## Decision\n,"We use tox to define, configure, and run different test scenarios.\n",**Decision:** Adopt a Continuous Delivery (CD) tool \n\n**Rationale:**  A CD tool automates the deployment process and allows for the creation of different environments and configurations with ease. This reduces the risk of errors and saves time and effort. \n\n**Consequences:** \n\n* Increased efficiency and consistency in the deployment process\n* Reduced risk of errors\n* Improved collaboration between development and operations teams\n* Faster time to market for new features and updates
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\nto natively support f-strings which are sweet.\n\n## Decision\n,We make an early decision to only support Python 3.6 and above.\n,Migrate to Python 3.6+ to utilize f-strings.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a simple way to manage our package version.\n\n## Decision\n,We use versioneer to do this for us.\n,"We will use semver, a widely used version numbering system that is easy to understand and use."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n\n## Decision\n,We use git for version control and GitHub for collaboration.\n,Use Git and GitHub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n\n## Decision\n","In order to perform these operations we choose\n[pydantic](https://pydantic-docs.helpmanual.io/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n",Serialize and deserialize all data structures as JSON.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n\n## Decision\n,"We will use isort, black, and flake8.\n",Use linters and formatters to ensure code quality.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use the Architectural Decision Record (ADR) template to document all architectural decisions made on this project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSelf-encoded dialects define dialect instances which share the same ID between the instance document and the dialect\ndomain element encoded in such document. This allows the document and the encoded dialect domain element to be treated\nas the same resource.\nOn the other hand Flattened JSON-LD emission only renders one node for each ID.\n\n## Decision\n,We merge both nodes (the document and the encoded domain element) and emit the single merged node with the shared ID.\nThe merged node contains both the properties from the document and the encoded domain element.\nWhen parsing the resulting flattened JSON-LD we parse the merged node twice: first as a domain element and then as a\ndocument. Parsing the properties from the domain element ignores the properties from the document and vice-versa.\n,Use the dialect ID as the resource ID when flattening JSON-LD to improve performance.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo adopt the ScalaJSTypings plugin, usages of Scala types that were not exported to ScalaJS were removed from the scala interface.\nThe Api Designer product uses the `Content.stream` field and calls `toString()` on it. As this field is of type CharStream we hid\nit from export.\n\n## Decision\n",- Rollback the interface change for the `amf.client.remote.Content` class so that the `toString()` method can be called on the `stream` field.\n- Add the `toString()` method in `Content` that returns the content in `stream`\n,"To revert the decision to hide this field from export, allowing the Api Designer to use it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to start deprecating old fields and have no mechanism to do so.\n\n## Decision\n,"We will start deprecating fields. Getter/setter methods in Scala and Platform classes will be deprecated also. Deprecated fields will still be set alongside new fields until these are removed in the next major version.\nHow to deprecate a field:\n1. Deprecate field definition in model class\n1. Deprecate field definition with `@deprecated` annotation\n2. Deprecate field definition with the `deprecated=true` paramter from the `Field` class\n3. Annotate the `fields` value assignment with `@silent(""deprecated"")` annotation to avoid compilation errors from deprecated fields\n2. Deprecate getter/setter methods in Scala and Platform classes with `@deprecated` annotation\n3. Update usages of getter/setter methods to use both legacy and new fields (with the `@silent(""deprecated"")` annotation)\n",We will use a mechanism called field deprecation to start deprecating old fields. Field deprecation allows us to mark a field as deprecated and specify a replacement field. This will allow us to gradually deprecate old fields while still providing a way for users to access the data in the old fields.\n\n**Benefits:**\n\n- Provides a way to gradually deprecate old fields\n- Allows users to access data in old fields while they are being deprecated\n- Prevents breaking changes in the API\n\n**Drawbacks:**\n\n- Requires additional code to handle deprecated fields\n- Can be confusing for users if not documented properly\n\n**Alternatives:**\n\n- Remove old fields immediately: This would be a breaking change and would require users to update their code immediately.\n- Keep old fields indefinitely: This would not allow us to deprecate old fields and would eventually lead to a large number of deprecated fields in the API.\n\n**Decision:**\n\nWe will use field deprecation to start deprecating old fields.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe custom AMF validator needs to show the location of the file from which each error was generated.\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\n\n## Decision\n","A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\n","Add an identifier to each element of the amf model that relates it to the source file and line, within the EMF model and the jsonld representation."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nLinking nodes between different graphs is a feature not provided by AMF/AML. This feature is required by some adopters to:\n* Link nodes from a parsed API specification graph with nodes from a parsed dialect instance graph (RestSDK)\n* Link nodes between different parsed dialect instances (ANG)\n\n## Decision\n,Add a boolean field to DomainElementModel called IsExternalLink that marks that a domain element is a reference to a domain element defined in another graph.\n,"**Decision:** Create a mechanism to allow linking nodes between different graphs.\n\n**Rationale:**\n\n* **Addresses adopter need:** The new mechanism will enable adopters to link nodes from different graphs, meeting their specific requirements.\n* **Extensibility:** The mechanism should be designed to be extensible, allowing for future enhancements and integrations with other tools or frameworks.\n* **Performance:** The mechanism should be optimized for performance, ensuring that linking nodes does not significantly impact the performance of the graph-processing workflow.\n* **Ease of use:** The mechanism should be user-friendly and easy to use, with a clear and well-documented interface.\n\n**Options:**\n\n* **Extend AMF/AML:** Modify the existing AMF/AML framework to include the ability to link nodes between different graphs.\n* **Create a separate tool or framework:** Develop a new tool or framework specifically designed for linking nodes between different graphs.\n* **Use an existing tool or framework:** Explore if there are any existing tools or frameworks that can be used to facilitate node linking.\n\n**Recommendation:**\n\nBased on the rationale and options provided, the recommended decision is to create a separate tool or framework specifically designed for linking nodes between different graphs. This approach provides the most flexibility and control over the linking mechanism, while also allowing for future enhancements and integrations. The tool should be designed with extensibility, performance, and ease of use in mind."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\nmay or may not include their own semantic information (when used with semantic extensions).\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\nmodel.\n\n## Decision\n","We will introduce a hack when rendering Custom Domain properties in JSON-LD that will make the graph look like it was\ngenerated by a dynamic model while still representing the model statically.\nConcretely, the `DomainElementModel` has a `CustomDomainProperties` field which is an `Array(DomainExtensionModel)`.\nEach `DomainExtensionModel` has a `DefinedBy: CustomDomainPropertyModel` and `Extension: DataNode` fields that\nrepresent the definition and value of the domain extension.\nThis in the ""traditional & static"" parts of the model would be rendered like this (summary):\n```json\n{\n""@id"": ""myDomainElement"",\n""customDomainProperties"": [\n{\n""@id"": ""myDomainExtension"",\n""definedBy"": {\n""@id"": ""myCustomDomainProperty""\n},\n""extension"": {\n""@id"": ""myDataNode""\n}\n}\n]\n}\n```\nInstead a ""dynamic"" model is simulated\n```json\n{\n""@id"": ""myDomainElement"",\n""myCustomDomainProperty"": {\n""@id"": ""myDataNode""\n},\n""customDomainProperties"": [\n{\n""@id"": ""myCustomDomainProperty""\n}\n]\n}\n```\nAs you can see a new property was generated using the `@id` of the `CustomDomainProperty` from the original `definedBy`\nfield which does not exist in the static model. The `customDomainProperties` property now points to an array of custom\ndomain property nodes rather than an array of domain extension nodes. The new `customDomainProperties` value indicates\nwhich properties of the serialized node ""are dynamic"".\n","Custom Domain Properties should be rendered as properties in the JSON-LD context, with the prefix `cp:`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn AMF 5 Beta's first iteration ""domain+syntax"" mediatypes like ""application/swagger20+yaml"" were\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\nThis is controversial as:\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\ncouldn't be parsed.\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\nalthough the configuration they used already specified their intended domain and syntax.\n\n## Decision\n",Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\n,"**Decision:** Use a single mediatype per configuration, specifying both domain and syntax in the configuration itself."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\nSome other useful links:\n[https://github.blog/2020-08-13-why-write-adrs/](https://github.blog/2020-08-13-why-write-adrs/)\n[https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/](https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/)\n","**Decision:** To use an ADR (Architectural Decision Record) template to document and track architectural decisions throughout the project.\n\n**Rationale:**\n\n* ADRs provide a structured and consistent way to capture, communicate, and track architectural decisions.\n* They allow stakeholders to understand the reasons behind decisions and the implications for the project.\n* They facilitate discussions, reduce ambiguity, and ensure that decisions are well-informed and aligned with the project's goals.\n* They serve as a valuable resource for future reference and maintenance efforts.\n\n**Consequences:**\n\n* The project team will need to adopt the ADR template and follow its guidelines for recording decisions.\n* Regular reviews and updates of ADRs will be necessary to ensure their accuracy and relevance.\n* ADRs may need to be revised as the project evolves and new information becomes available.\n* By using a standardized template, ADRs will be easier to search, compare, and aggregate for future analysis and reporting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nExpanded JSON-LD emission embeds node links. This means that whenever a node references another node, instead of using a\nJSON-LD link we render the referenced node in place. This is because the advantage of the JSON-LD expanded emission is\nto allow consumer applications (e.g. API Console) to consume JSON-LD as regular JSON; and for that we cannot use JSON-LD\nlinks.\n**The problem**: Embedding nodes causes the resulting JSON-LD to be very big (because the same node is rendered many times).\n_Note: Flattened JSON-LD emission does not suffer this problem because it only renders each node once and then uses\nJSON-LD links for every node reference._\n\n## Decision\n","`extractToLink` is a logic we developed for the expanded JSON-LD emission that introduces _AMF links_ (as opposed to\nJSON-LD links) for some node references. Specifically, it introduces an AMF link and extracts the link target to a\n_declaration_.\n_AMF links_ are only available for `Linkable` domain elements, so we use it in only for certain elements. These links\nare part of the model as opposed to JSON-LD links which are simple graph references.\n",**Decision**: Expand only the first level of references.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the json data model.\n[ADR-0002](0002-use-sha256-with-base64url-encoding.md) describes how to create a hash of the json.\nThe hash must be transferred to from the authorization server to the WebSocket API secure.\nThe validity of the hash must be verified.\n## Decision Drivers <!-- optional -->\n* JWT should be used\n\n## Decision\n,"* JWT should be used\nChosen option: ""Transfer hash in JWT Claim"", because it's the only option when using JWT.\n### Positive Consequences <!-- optional -->\n* multiple hashes for different json documents can be added in one JWT\n",Use JWT (JSON Web Token) to securely transfer the JSON data hash from the authorization server to the WebSocket API and to verify the hash validity.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nExternal services must authorize web clients to the WebSocket API.\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\nOnly little data should be stored for an open WebSocket connection.\n## Decision Drivers\n* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\n\n## Decision\n,"* decoupling of the authorization service and the WebSocket API\n* flexible and well supported on many platforms\nChosen option: ""JWT from pre-shared keys"", because the WebSocket API is loosely coupled and it is well supported on many platforms.\n### Positive Consequences\n* Simple to implement\n* Authorization data can be send in a portable and verifiable way\n### Negative Consequences\n* The shared keys must be handled\n","**Decision:** Use a JSON Web Token (JWT) for authentication.\n\n**Rationale:**\n\nJWTs are:\n\n* **Decoupled:** The JWT contains all the necessary information to authorize the client, so the WebSocket API does not need to maintain a session.\n* **Flexible:** JWTs can be signed with a variety of algorithms and are supported by many platforms.\n* **Efficient:** JWTs are compact and can be easily stored and transmitted.\n\n**Implementation:**\n\n1. The authorization service will generate a JWT for each client.\n2. The client will send the JWT to the WebSocket API as part of the handshake request.\n3. The WebSocket API will verify the JWT and grant access if it is valid."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to transfer json data and verify the integrity of the data.\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\nThe WebSocket API must able to verify the integrity of the json data.\n## Decision Drivers <!-- optional -->\n* Use standard encodings\n\n## Decision\n","* Use standard encodings\nChosen option: ""Send SHA256 hash of Base64Url encoded json"", because this method is platform independent and not much session state is required.\n### Positive Consequences <!-- optional -->\n* The JWT really function as a verification token for the other requests.\n* Can be applied to all json data that must be verified.\n### Negative Consequences <!-- optional -->\n* The json must be transferred in Base64Url encoding\n","**Decision:** Use JSON Web Signature (JWS) to transfer JSON data securely and verify its integrity.\n\n**Rationale:**\n\n* JWS is a standard encoding that provides integrity and tamper-proof mechanisms for JSON data.\n* It uses a digital signature to ensure that the data has not been modified since it was created and signed by the Authorization server.\n* The WebSocket API can verify the signature using the public key of the Authorization server, ensuring that the data received is authentic and unaltered.\n* JSON Web Signature is widely supported and well-established, making it a reliable and interoperable solution for data transfer and verification."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\nJWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.\nWhen implementing JWTs one must decide which method to use.\n## Decision Drivers\n* Multi tenant support with own keys for each tenant\n\n## Decision\n,"* Multi tenant support with own keys for each tenant\nChosen option: ""Asymmetric JWT signing"", because it the only option which allow to use different keys for different tenants.\n### Positive Consequences\n* multiple keys are supported\n### Negative Consequences\n* complex management of keys\n","Use public/private key pairs with RSA or ECDSA for signing and verifying JSON Web Tokens.\n\nThis decision is driven by the requirement for multi-tenant support with each tenant having its own keys. Using a secret would not allow for each tenant to have its own unique key, as the secret would be shared across all tenants. By using public/private key pairs, each tenant can have its own unique private key for signing JWTs and a corresponding public key for verifying them. This provides a more secure and scalable solution for multi-tenant environments."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGiven that we were not going to have the benefit of\noffloading maintenance responsibility to the PaaS we\nwanted to keep the cost and the requirement for\nmaintenance as small as possible.\n### Architecture patterns considered\n#### PaaS\nSee [ADR 3](./0003-not-paas.md).\n#### EC2\nTraditional load balanced EC2 model running something\nlike django or rails.\n#### ECS\nContainerised version of the above\n#### Serverless\nLambda functions using APIGateway and CloudFront\nThere are many benefits to having the tools of the django\ninfrastructure.\nThe downside is that you have a maintenance requirement\nto maintain the server instances. You also somewhat bind\nyourself to running the service 24/7 and paying for it.\nEven with ECS you are still responsible for maintaining\nthe container host.\nWith serverless you only pay when the functions get\nexecuted and there is a significant free tier.\nThe downside of serverless is you can't leverage things\nlike django / rails and the communities who contribute\ncomponents to them.\nThis limits our cost and our exposure since maintaining\nthe infrastructure that hosts the lambda functions is\nAWS' responsibility not ours.\n\n## Decision\n,We will use AWS Lambda for this service.\nIn conjunction with an ENI to allow the lambdas access\nto RDS running inside a VPC if necessary.\n,"Use serverless architecture with Lambda functions, APIGateway, and CloudFront."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMy first instinct for the tool was to use the\nNPM [Serverless](https://serverless.com/) framework.\nThe main advantage of serverless is that it is cloud\nagnostic. However JavaScript is not widely used at GDS.\nAWS [Chalice](https://chalice.readthedocs.io) is a similar\nframework which has a multi-stage deploy process to\nautomate provisioning Lambda and Api Gateway but written\nin Python.\nBoth languages have frameworks for accessing the AWS API:\n* The [SDK](https://docs.aws.amazon.com/sdk-for-javascript)\nfor JavasSript and\n* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\nfor Python.\n\n## Decision\n,Given that we had already made the decision that cloud\nagnostic was not important for this service and that\nPython was more in keeping with GDS common practice we\ndecided to use Chalice.\n,"Use Chalice as the framework for the tool, written in Python."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIf running serverless we could choose to operate outside\na VPC. This has advantages in terms of the latency of\nrequests. We would not need a keep-warm function to\nensure network interfaces were provisioned.\nLambdas running outside the VPC could not easily access\nRDS running inside a VPC.\nGiven the sensitive nature of the data involved it seems\nsensible to protect it further by running RDS inside a VPC\nwhere we have much easier control over ingress and egress.\n\n## Decision\n,Implement a VPC.\n,Run RDS inside a VPC to protect sensitive data
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe needed some persistent storage of audit results.\nWe considered:\n### Schemaless - DynamoDb\nThis would be the most obvious choice for a lambda based\nservice.\nThe dynamo data model is tables of key value pairs.\nThe main problem with Dynamo is the limit of I think 4K\nper value.\nOne of the things we wanted to do was briefly cache API\nresponses which could easily breach that 4K limit.\nWith Dynamo the access control is via IAM which would be\nrelatively easy to manage and encryption at rest can\nbe easily configured.\n### Schemaless - MongoDb\nMongo was a better fit for our service, saving JSON\nrepresentations of API responses and resources.\nThe problem with Mongo is it's not AWS native so we'd\nhave to provision a server, manage the access controls,\nmaintenance and availability.\n### Relational - RDS MySQL / PostgreSQL\nRDS gives us the benefit of running a managed service so\nAWS are responsible for backups and patching minor\nversion.\nRelational databases give us the ability to do on-the-fly\nanalysis more easily.\nWe can store JSON as blob data although not ideal.\nIf we want to re-use the database instance as a shared\nresource across multiple services RDS is more capable.\nIt's not unlikely that a future development may require\ndjango.\nPostgreSQL seems to be the general direction of travel\nin GDS and is much more capable for a wider range of\nuse-cases where we don't know what we're building in the\nfuture.\n\n## Decision\n",Whilst none of the options were perfect we decided that\na PostgreSQL RDS was the best option given the\ninformation available to give us an infrastructure to\nsupport multiple tools and services.\n,"Use MongoDB as the persistent storage for audit results.\n\nMongoDB is a better fit for the service's needs than DynamoDB or RDS MySQL / PostgreSQL. MongoDB is a schemaless database, which means that it can store data in a flexible and dynamic way. This is ideal for storing audit results, which can be of varying sizes and formats. Additionally, MongoDB is a cloud-native database, which means that it is easy to provision and manage."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSince Cloud Security Watch specifically aims to monitor for\nmisconfigurations in AWS accounts it does not make sense to\nmake the tool cloud agnositic.\n\n## Decision\n,Whilst we support the notion of writing cloud agnostic\ncode in general. In this instance it is not appropriate\nor desirable.\n,**Decision:** Keep Cloud Security Watch specific to AWS.\n\n**Rationale:**\n\nCloud Security Watch's primary focus on monitoring AWS accounts for misconfigurations justifies its exclusive focus on that platform. Expanding to other cloud providers would add complexity and potential compatibility issues without providing additional value for the tool's intended purpose.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe first architecture considered was using the PaaS.\nThis tool collects data about misconfigurations in a\ndatabase. Since the data held is quite sensitive we\nwanted to take all reasonable measures to protect that\ndata.\nThere are a lot of advantages to using the PaaS in that\nit limits the amount of work required for common\noperational tasks like deployment and monitoring.\n\n## Decision\n,At the present time we felt the Paas was not a viable\noption for this tool because of the following:\n* Shared tenancy RDS\n* Shared tenancy VPC\n* No ability to natively control ingress\n* No ability to control egress\n__Some of these issues are being addressed by the PaaS so\nwe may revisit this decision in future.__\n,Consider a private cloud solution that provides the same advantages as the PaaS without the security concerns.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWithin GDS product teams operate autonomously and tech-ops\nis relatively new. There is no central administration of\nAWS. There is a shared authentication model but it is not\nused everywhere.\nTeams are busy with their own mission work and don't have\ntime allocated to standardisation or security work.\nTo get things built in a client AWS account we need to:\n* negotiate time from the product team\n* provide them with the code to build the resources\n* code review with them to reassure them that it's OK to\nrun our code in their environments\n* repeat for each team we engage\nWe want to make it easy and low-risk for a team to engage\nwith us.\nIf there were centralised management of all accounts, the\nsensible way to do this would be by configuration the\nconfig service with a set of config rules administered\nand maintained centrally by Cyber Security.\nThese rules would be largely triggered by CloudTrail\nevents notifying us of compliance as soon as resouces were\ncreated or changed.\nHowever this approach would involve quite a large number\nof resources deployed into each client AWS account which\nmeans a longer and costlier engagement process.\nThe simplest way to do this is by provisioning an IAM role\nand associated policy. That way we can work from the\nprinciple of least privilege, granting us only the access\nwe need to run our audit making it low risk.\nBy providing the role and policy definition in a Terraform\nmodule we can also ensure that it requires little work by\nclient account holders.\nThe advantage of the IAM mechanism is that since we're\nquerying the configuration of the user account directly\nvia the API it is very easy to tell whether our service\nhas been tampered with - the only thing that can change\nis the definition of our policy which we can check.\nIn the config service model there are more moving parts\nwithin the client account which could be switched off,\naltered or intercepted.\n\n## Decision\n","Create a terraform module which creates an IAM role and\nassociated policy granting primarily read-only access\n(Get, List, Describe API calls) working from the\nprinciple of least privilege.\n","**Decision:** Implement a simplified configuration model using IAM roles and policies.\n\n**Rationale:**\n\n* **Low impact on client accounts:** Only an IAM role and policy need to be deployed, minimizing the engagement process.\n* **Principle of least privilege:** Grants only the necessary permissions for auditing.\n* **Easy to verify integrity:** The API directly queries the user account configuration, ensuring that no changes have been made without authorization.\n* **Address team constraints:** Avoids the need for centralized AWS management, saving time and cost."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Implement an Architectural Decision Record (ADR) process to document and track architectural decisions made on the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis is planned to be a multi-tenant system. There will be multiple CASA orgs in the system, so every case, case_contact, volunteer, supervisor, casa_admin etc must have a casa_org_id, because no one is allowed to belong to multiple CASAs. Volunteer, supervisor, and casa_admin are all roles for a ""User"" db object. In addition to those existing roles, we want to create a new kind of user: all_casa_admin. We need to handle the case of super users who have access to multiple casa_orgs, so they would be difficult to handle in the existing User table--with null handling around their casa_org_id field. We have used the built-in Devise ability to have multiple user tables, as recommended to us by our Rails expert Betsy. This is to prevent needing null handling around casa_id for User records since all_casa_admin users will not have casa_id populated.\nAdditionally, all_casa_admin users are currently intended to be allowed to create casa_admin users, but NOT to be able to see or edit any CASA data like volunteer assignments, cases, case_updates etc.\n\n## Decision\n","We are using two tables for users: ""user"" table for volunteers,supervisors, and casa_admin (all of which must have a casa_id). ""all_casa_admin"" for all_casa_admins, which will have no casa_id.\n","The system will have three separate User tables: User (for people associated with a single CASA organization), SuperUser (for people with access to multiple CASA organizations), and AllCasaAdmin (for people with access to all CASA organizations).\n\nThe User table will have a casa_org_id column to indicate which CASA organization the user is associated with. The SuperUser table will not have a casa_org_id column, as these users can access data from multiple CASA organizations. The AllCasaAdmin table will also not have a casa_org_id column, as these users can access data from all CASA organizations.\n\nAllCasaAdmin users will be able to create new User and SuperUser records, but they will not be able to see or edit any CASA data like volunteer assignments, cases, or case updates."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want it to be easy for people to join the organization, however we don't want random people signing up and spamming us. We want admin users to have control over who has accounts on the system. We don't have the capacity to handle this properly through the user interface right now.\n\n## Decision\n",We are going to disable Devise 'registerable' for the user model so that there will no longer be a public sign up option on the site. Creation of new accounts will be done on the backend.\n,Create a private invite mechanism to manage users. Each invite should contain a unique token that expires after a set period. Users can only join the organization by redeeming this token through the provided redemption mechanism.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to have an easy-to-use system for consistent styles that doesn't\ntake much tinkering. We propose using the `bootstrap` gem.\n\n## Decision\n,Pending\n,Use the `bootstrap` gem for consistent styling.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",Document architectural decisions in the Architectural Decision Record (ADR) template.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\n\n## Decision\n",We will use Simon Brown C4 model.\n,"**Decision:**\n\nDocument the project's static model using a combination of:\n\n* **Unified Modeling Language (UML) diagrams:** Use UML class diagrams, sequence diagrams, and use case diagrams to represent the structure, behavior, and interaction of the system components.\n* **Architecture Decision Record (ADR):** Create ADRs to capture design decisions and architectural constraints that impact the static model. ADRs can provide additional context and rationale for the design choices made.\n* **Design artifacts:** Supplement the diagrams and ADRs with implementation-specific artifacts such as module hierarchies, package diagrams, and database schemas. These artifacts can provide a granular view of the system's architecture."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\nTo enable different communication ways we need to apply a proper architectural style.\n\n## Decision\n,Adopt Hexagonal Architecture for project.\n,"**Decision:**\n\nImplement a **Publish-Subscribe Architectural Style**.\n\n**Rationale:**\n\n* **Decouples communication:** The publish-subscribe style decouples the sender (publisher) from the receiver (subscriber), eliminating tight coupling and allowing components to communicate asynchronously.\n* **Supports multiple communication patterns:** Publish-subscribe enables different communication patterns, such as one-to-many, many-to-many, and even one-to-one, making it suitable for the flexible communication needs of the SDARS application.\n* **Promotes scalability:** By using a central message broker or bus, the publish-subscribe style allows for the addition or removal of publishers and subscribers without affecting system stability, making it scalable and reliable.\n* **Simplifies communication:** This style simplifies the design and implementation of communication between components, as the messaging infrastructure handles message routing and delivery.\n* **Provides flexibility:** The publish-subscribe pattern allows for different communication strategies and data formats, making it adaptable to the evolving requirements of the SDARS application."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Contextadjecent\nWe need to adopt application architecture style adjecent to our architectural drivers\n\n## Decision\n,We will use ports&adapters architecture style\n,"Adopt a microservices architecture style.\n\n**Rationale:**\n\nA microservices architecture aligns with our architectural drivers as it:\n\n* **Modular:** Microservices are independent, loosely coupled components, allowing for easy maintenance and deployment.\n* **Scalable:** Each microservice can be scaled independently, providing flexibility in handling varying workloads.\n* **Fault-tolerant:** Microservices are designed to be resilient and can fail independently without affecting the entire system.\n* **Agile:** Microservices can be developed and deployed independently, facilitating rapid iteration and adaptability.\n* **Cloud-native:** Microservices are well-suited for cloud environments, leveraging scalability and fault-tolerance features."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","To record the architectural decisions made on this project, we will create an Architectural Decision Record (ADR). The ADR will capture the following information:\n\n* **Decision:** The decision that was made.\n* **Context:** The context in which the decision was made.\n* **Considerations:** The factors that were considered when making the decision.\n* **Consequences:** The expected consequences of the decision.\n* **Recommendation:** The recommended course of action.\n\nThe ADR will be stored in a central repository and will be accessible to all stakeholders."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to adopt system architecture style adjecent to our architectural drivers\n\n## Decision\n,We will use modular monolith architecture style\n,"**Decision:** Adopt a Microservices architecture style.\n\n**Rationale:**\n\n* **Loose Coupling and Independent Deployment:** Microservices allow for independent deployment and scaling of services, meeting the need for flexibility and scalability.\n* **Modularity and Reusability:** Services can be independently developed, tested, and deployed, fostering modularity and code reuse.\n* **Scalability and Fault Isolation:** Each microservice can be scaled independently, ensuring high availability and fault isolation.\n* **Adaptability and Continuous Delivery:** Microservices enable frequent updates and continuous delivery, aligning with the architectural drivers for agility and rapid response to change.\n* **Agility and Experimentation:** The modular nature of microservices supports experimentation with different technologies and approaches, fostering innovation and iteration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to keep extendable architecture in order to prepare for future changes,\nbecause we work in very flexible manner and requirements changes and evolve a lot.\n\n## Decision\n","In backend directory:\n- *libs* - technical parts of software - implementations of database connections etc. In the future will be moved to npm packages,\nbut we leave it now for easier and faster development to avoid unnecessary complexity.\nFolders in backend **src** directory are splitted by names of bounded contexts.\nEach BC, which follow CQRS manner, has following subdirectories:\n- *write-side* - keeps logical separated parts, which keep ubiquitous language + one domain inside each boundary\nEach bounded-context will follow directories in manner of Domain-Driven Design Layered Architecture proposed by Eric Evans.\n- *read-side* - projections from domain events of write-side\n",Adopt a microservices architecture.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCore developers are using JS maps files to keep all the selectors they need to interact with the BO theme. Here is an example of such a file:\n```\nexport default {\nmainDiv: '#order-view-page',\norderPaymentDetailsBtn: '.js-payment-details-btn',\norderPaymentFormAmountInput: '#order_payment_amount',\norderPaymentInvoiceSelect: '#order_payment_id_invoice',\nviewOrderPaymentsBlock: '#view_order_payments_block',\nprivateNoteToggleBtn: '.js-private-note-toggle-btn',\nprivateNoteBlock: '.js-private-note-block'\n}\n```\nQA team is keeping their own set of selectors to interact with during automated tests. Here is an example of such a file:\n```\nthis.documentTab = 'a#orderDocumentsTab';\nthis.documentsTableDiv = '#orderDocumentsTabContent';\nthis.documentsTableRow = `${this.documentsTableDiv} table tbody tr:nth-child(%ROW)`;\nthis.documentNumberLink = `${this.documentsTableRow} td:nth-child(3) a`;\nthis.documentName = `${this.documentsTableRow} td:nth-child(2)`;\n```\nSelectors should be mutualized so that when a dev changes a UI component or update a selector, the changes are reflected in the corresponding JS Map file and automated tests using this selector will keep working as intented without human intervention.\n\n## Decision\n","QA team and Core developers will populate and maintain their own set of selectors. There is no clear advantage to use a fusioned set of selectors.\nHere are the main arguments against this decision:\n* Devs use ES6, QA team use ES5. This means using a transpiler and adding libraries (babel).\n* There is no clear conventions in existing JS Map files (names and content architecture vary).\n* QA team need some selectors with modifiable input (with strings like %ROW or %COLUMN that must be replaced when used), which mean these types of selectors will be unusable for the Core dev team.\n* Not the same needs: QA team need navigation selectors, Core dev team need interactive selectors. There is only a few selectors in common.\n* Low risk: there is no evidence of any selector modification that caused automated tests to fail. When a test break, it's mostly because of a relatively big UI revamp, something that couldn't be fixed with just a selector modification anyway.\n","The decision is to create a single repository that will contain all the selectors. The core developers and the QA team will work together to maintain this repository. This will ensure that the same selectors are used by both teams, and that any changes to the selectors are reflected in both the JS Map files and the automated tests.\n\nHere are the benefits of this decision:\n\n* **Improved collaboration:** The core developers and the QA team will be able to work together more closely to maintain the selector repository. This will help to ensure that the selectors are accurate and up-to-date.\n* **Reduced risk of errors:** By having a single source of truth for the selectors, the risk of errors is reduced. This is because both teams will be using the same selectors, and any changes to the selectors will be tracked in the repository.\n* **Increased efficiency:** The use of a selector repository will increase the efficiency of both the core development and QA teams. The core developers will not need to spend time maintaining multiple sets of selectors, and the QA team will not need to spend time updating their automated tests when a selector changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile the whole JavaScript community welcomes TypeScript easily, our project's JS is not typed at all. To improve the maintainability of the project and use the latest TC39 features, a good idea would be to use TypeScript.\nBenefits of using it for the project:\n- Detect bugs before pushing PRs. TypeScript users say that globally, it allows detecting around 15% of bugs that you would detect by testing.\n- Use latest features such as Optional Chaining, Tuples, and Records... really early.\n- Types are increasing the quality of the project because we would be able to detect dangerous changes, related bugs... If we use it on the PHP side, why don't we use types while using JS?\n- Vue 3 offers a new API: Composition API, this one is pretty easy to use with TypeScript as it's mainly functional programming instead of opinionated APIs of Vue, that would be a good move to preshot the Vue update in the BO.\n\n## Decision\n",Add the possibility to transpile ts files inside every js folder of the project with webpack.\n[Here is a POC](https://github.com/PrestaShop/PrestaShop/pull/23221) - basically using TypeScript on a small part of the PrestaShop Grid system.\n,"Yes, we should use TypeScript.\n\nTypeScript will improve the maintainability of the project and allow us to use the latest TC39 features. It will also help us to detect bugs before pushing PRs, and increase the quality of the project by detecting dangerous changes and related bugs. Additionally, using TypeScript will allow us to use the Composition API in Vue 3, which is easier to use with TypeScript."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe normal behavior of a module is to be self-contained, meaning that its code and all its dependencies are stored in the module's directory, including `vendor` directory and autoloader.\nWhen developing and building PrestaShop, native modules aren't downloaded from marketplace, they are required using composer.\nIn difference with the normal behavior described above, a module installed using composer will have its autoloader and dependencies merged into the core's and placed in the core's `vendor` directory.\nThis can prove problematic for native modules:\n- When the module is uninstalled, both its dependencies and autoloader are left behind in the core.\n- If the module is upgraded using the marketplace sources, the dependencies are now available twice: once in the core and once in the module.\nAlternate systems to avoid having modules leak code into the core have been proved unpractical:\n- Including modules via [Composer script handler](https://github.com/PrestaShop/composer-script-handler) or git clone is too slow.\n- Using submodules would be a step backwards.\n- Retrieving modules from the marketplace would be slow as well.\n\n## Decision\n","1. [The module managment system must be changed to be fully based on composer](https://github.com/PrestaShop/PrestaShop/issues/12586). This will require major changes in the marketplace and will have to be analyzed for feasibility.\n2. In the meantime, we will keep using composer to include native modules.\n3. Native modules must prepend their autoloader.\n4. To avoid leaving dependencies in the core, no composer dependencies are to be added to native modules until step 1 has been resolved.\n- Because of Decision No. 4, we may face technical difficulties in the future.\n",**Decision:**\n\nNative modules installed using Composer will no longer merge their autoloader and dependencies into the core's.\n\n**Benefits:**\n\n* Ensures that native modules are self-contained and can be easily removed or upgraded without affecting the core.\n* Eliminates potential dependency conflicts and cleanup issues when modules are uninstalled.\n\n**Alternatives Considered and Rejected:**\n\n* **Including modules via Composer script handler or git clone:** Too slow.\n* **Using submodules:** Not as practical as the proposed solution.\n* **Retrieving modules from the marketplace:** Also too slow.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSymfony provides a very useful tool called [Autowiring](https://symfony.com/doc/3.4/service_container/autowiring.html). It allows to magically bind classes and their dependencies as long as both are declared as services, and the dependencies are declared using their FQCN as service identifier.\nAdvantages:\n- Less boilerplate configuration code for every service as you don't have to manually bind dependencies manually.\nDisadvantages:\n- Dependencies must be declared using the FQCN instead of a service identifier like ""prestashop.core.foo.bar"".\n- Currently existing services would have to be aliased in order to have service names follow the required naming convention for autowiring. This would lead to confusion as to which service name use in code, and in case a module wanted to replace them, they would have to replace both.\n- Dependencies type-hinted as interfaces can have one and **only one** implementation for autowiring to work.\n\n## Decision\n",Activiting autoriwing is rejected for the 1.7 version.\n,**Decision:** Implement autowiring.\n\n**Rationale:**\n\nThe advantages of autowiring outweigh the disadvantages. Autowiring reduces boilerplate configuration code and simplifies service definition.\n\n**Mitigations:**\n\n* Address the disadvantages by:\n    * Using a consistent naming convention for services.\n    * Creating aliases for existing services that do not follow the autowiring naming convention.\n    * Ensuring that dependencies type-hinted as interfaces have only one implementation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur Production AWS RDS databases are not configured for high availablity which can support blue-green architectures and reduce risk. There exists a few ways to provide HA on RDS. We cover those in this document with the goal of choosing, among other criteria, an offering that aligns with AWS archtectural best practices.\n### Distinguishing between HA and DR:\nHigh Availability (HA) provides a failover solution in the event a database, vpc, or availability zone fails. Disaster Recovery (DR) provides a recovery solution across a geographically separated distance (multi-region) in the event of a disaster that causes an entire data center to fail.\nIn this ADR, we select an architecture that ensures High Availability and defer Disaster Recovery to a separate ADR.\n\n## Decision\n","We will apply the RDS Multi AZ architecture to add high availability to our RDS production instances as it is the recommended best practice to adding HA to existing RDS instances.\n#### Consequences\n- Choosing Aurora DB would be a part of a larger product design decision, outside of this adr scope.\n- High Availability for failover protection improves the stability of the product framework. We did not address Disaster Recovery, which should also be a part of an overall scope of the product framework.\n- Multi AZ architecture should be fairly transparent to RDS applications, but the failover conditions and alerting mechanisms should be understood prior to implementation.\n",Implement Multi-AZ RDS deployments for all production databases.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order for modules to use JavaScript components from the Core, they need to import them using statements like:\n```\n// in order to use translatable type\nimport TranslatableInput from '../../../../../admin-dev/themes/new-theme/js/components/translatable-input';\n```\nThis path is not robust, makes CI/CD harder, and also is not compatible with some development environments using symlinks or containers.\n\n## Decision\n","We have decided about a system which resolves in 4 concepts:\n1. Reusable components in BO will be available globally through `window.prestashop` (name can still be modified in short term).\nAll PrestaShop components will be bundled together and made available in all pages using this mean. Each controller decides which components it chooses to initialize.\n2. Reusable components will be available as a namespace `window.prestashop.component`.\nThe namespace will contain classes like this `prestashop.component.SomeComponent`. If you want to get a new instance of `SomeComponent`, you call `new prestashop.component.SomeComponent(...params)`\n3. Reusable components will be available as initialized instances through `window.prestashop.instance`. These instances are initialized with default parameters by the `initComponents` function.\n4. A function `initComponents` available through `prestashop.component` is responsible for building `window.prestashop.instance`.\n### Why a namespace and a collection of instances\nSince you have access to both constructors and components, developers are free to choose how to initialize and control their components.\nIf you don't want to initialize a given component with default parameters, you can always call `new prestashop.component.SomeComponent(...myOwnParameters)`.\nIf you need to apply some mutation to an already initialized component, you just get the global instance: `prestashop.instance.someComponent.doSomething(...)`.\n",Migrate the JavaScript components to a central package:\n\n- Create a package with a scope @SyliusCore with a name @SyliusCore/component-library\n- Move all components into the new package\n- Replace the import paths with `import TranslatableInput from '@SyliusCore/component-library';`\n- Update CI/CD pipelines to build and publish the new package\n- Update all modules to use the new import paths
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https://build.prestashop.com/news/new-architecture-1-6-1-0/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn't be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\nThe ""no direct dependency between Core and Legacy"" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the ""public API"" is larger.\n\n## Decision\n","The following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to ""Core"" for shortness):\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don't guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method/class (and optionally, making it rely on the new implementation).\n4. **The Adapter namespace MUST be phased out** eventually:\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\n- Adapter services MUST be deprecated and copied into the core namespace as well.\n- Code MUST NOT depend on Adapter classes or services.\n",The decision should be to refactor all the code into the Core namespace and use dependency injection. This will reduce code complexity and duplication and make it easier to maintain the codebase.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To utilize an Architectural Decision Record (ADR) template to capture and document architectural decisions throughout the project lifecycle.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\n\n## Decision\n,"- Not in the project scope for either of the engagements\n- Good to have and can be incorporated at a later date\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\n",The decision was made to analyze the following subsections under Receipts:\n\n\n- Receipt Creation\n\n- Receipt Status\n\n- Receipt Line Items\n\n- Receipt Discounts
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe data extraction w/ transaction timestamp from Koshvani platform is a challenging task.\n- The temporal data directly available on the platform is not easy to extract\n- The validation of temporal data to ensure accuracy depends on factors\n- Districts/Treasuries which have updated their data\n- Data/Time of data upload vs extraction from departments\n## Decision Drivers\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\n\n## Decision\n","The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\n- Do a daily level data extraction from the platform\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\n- Select the accurate time period and mention methodology for the same\n- Ignore previous time period missed in this analysis.\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\n","**Architecture:**\n\n**Data Extraction Process:**\n\n* **Extraction Framework:** Develop a custom extraction framework that automates the data extraction process.\n* **Temporal Data Identification:** Utilize advanced data science techniques to identify temporal data from the platform's raw data feed.\n* **Data Validation:** Implement data validation mechanisms to ensure the accuracy of extracted data. This can include:\n    * Cross-referencing with other data sources\n    * Verifying temporal consistency\n    * Checking for missing or inconsistent values\n\n**Data Analysis and Reporting:**\n\n* **Time-Series Analysis:** Perform time-series analysis on extracted data to identify trends, seasonality, and anomalies.\n* **Data Visualization:** Create interactive data visualizations to communicate findings to stakeholders.\n* **Data Reporting:** Generate actionable reports based on data analysis, highlighting data quality issues and suggesting improvements."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\n## Decision Drivers\n* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\n\n## Decision\n,* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\n,"**Decision:** Utilize a multi-phased approach to gradually establish sustainable jobs for various sub-sections within the Expenditure category. Leverage data from multiple sections to impute accounting head information, perform data checks, and establish a resilient architecture with a three-year pipeline."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDuplicacy of data scraping from Koshvani platform.\nFor the following sections, the data has already been extracted from the `DDO-wise expenditure` section.\n- `Grant-wise (Revenue/Capital) expenditure`\n- `Division-wise expenditure`\n## Decision Drivers\nBoth these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\n\n## Decision\n",Both these section repeat the same data granulariy or lesser than the `DDO-wise expenditure` section.\nOnly the main pages of the aformentioned sections contain new information that will require extraction.\n,"Extract data for ""Grant-wise (Revenue/Capital) expenditure"" and ""Division-wise expenditure"" sections only if the granularity of data is different from the ""DDO-wise expenditure"" section."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\n\n## Decision\n,"- Structure of the Koshvani platform\n- Platfrom links do not reflect selection criteria\n- Automation job requirements for data scraping\n- Periodical jobs and access to new data\nUse [Selenium](https://www.selenium.dev/) to create robust, browser-based automation.\n","**Framework:** Python libraries (BeautifulSoup, Selenium, Pandas)\n\n**Rationale:**\n\n* **Robust scraping:** BeautifulSoup and Selenium offer powerful capabilities for extracting data from complex web pages, including Koshvani.\n* **Data manipulation and storage:** Pandas provides efficient tools for data wrangling, cleaning, and storage in machine-readable formats such as CSV or JSON.\n* **Open-source and flexible:** These libraries are widely used and supported by a large community, ensuring ease of implementation and maintenance.\n* **Cost-effective:** Python is free and open-source, making it a cost-effective solution for data scraping and storage."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\n**_Platform 1_**\n**Name** : Budget - Government of Uttar Pradesh\n**URL** : http://budget.up.nic.in/\n**Usage** : Analyse budget documents and understand accounting information available.\n**_Platform 2_**\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\n**URL** : http://koshvani.up.nic.in/\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\n## Decision Drivers\n- Create better underdstanding Uttar Pradesh fiscal data\n- Scrape data to be consumed in machine readable format\n\n## Decision\n",- Create better underdstanding Uttar Pradesh fiscal data\n- Scrape data to be consumed in machine readable format\nExplore both platforms to understand the open fiscal data available and techinical requirements.\n,"**Decision:** Use both platforms, **Budget - Government of Uttar Pradesh** and **Koshvani web**, to obtain data and information for the **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\n\n**Rationale:**\n\nBoth platforms provide valuable data for the projects' requirements:\n\n- **Budget - Government of Uttar Pradesh** offers in-depth budget documents and accounting information for analysis.\n\n- **Koshvani web** enables scraping of open budget data in a machine-readable format, facilitating further analysis and consumption.\n\nBy utilizing both platforms, the projects can access comprehensive and accessible data to create a better understanding of Uttar Pradesh's fiscal data."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n\n## Decision\n,The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\n,Extract the `Grant-wise expenditure` section from Koshvani.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\n- Expenditure\n- Receipts\n\n## Decision\n,Explore both sections to identify sub-sections from which data needs to be extracted and analysed.\n,Data in the Expenditure and Receipts sections of the Koshvani platform should be considered in scope for scoping and analysing the data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe GEWST project with HAQ: CRC has a platform development component, which includes creating exploratory dasboard and conducting analysis on an interactive dashboard.\n## Decision Drivers\n### Platform Development Requirements\nAccording to the plan, the data platform comprises of the following components:\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they’re released as part of the platform.\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\n- **Data Visualization Layer/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\n### Other Drivers\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\n- There is still no clear direction or plan what the tool will look like.\n- There might be numerous research routes and iterations before finalisation.\n- Data stories and analysis will play a big part in creating the final platform.\n\n## Decision\n","### Platform Development Requirements\nAccording to the plan, the data platform comprises of the following components:\n- **Data Mining Component**: Developing the infrastructure and other components for mining data from the Koshvani platform. These will be developed following ethical data mining practices and will ensure that the data is mined responsibly from the Koshvani platform.\n- **Data verification component**: This will ensure that the results produced by the Data Mining Component are verified and thoroughly tested by manual & automated processes before they’re released as part of the platform.\n- **Data sharing and Exploratory Data Analysis (EDA) component**: Sharing the data, qualitative and quantitative research with the community and will also ensure that users can perform basic EDA on the data hosted on the platform.\n- **Data Visualization Layer/Component**: Develop an interactive dashboard containing dynamic data visualizations to track and explore girl education and well-being spending for data collected in the development process.\n### Other Drivers\n- The bandhus in the Public Finance team are already occupied on OBI and OCP project.\n- There is still no clear direction or plan what the tool will look like.\n- There might be numerous research routes and iterations before finalisation.\n- Data stories and analysis will play a big part in creating the final platform.\nYou can track the decision updates / inputs on the following [issue](https://github.com/CivicDataLab/up-fiscal-data-shiny/issues/1).\n","**Decision**: Build an in-house dashboard tool\n\n**Rationale**:\n\n* The Public Finance team's workload is already high, making it difficult for them to allocate resources to this project.\n* The dashboard tool's requirements are still unclear, and there may be multiple iterations before it is finalized.\n* Data stories and analysis will play a significant role in the final platform, and an in-house tool will allow for greater flexibility and customization.\n\n**Consequences**:\n\n* The in-house dashboard tool will require additional resources and time to develop.\n* The tool may not have the same level of functionality or support as a commercially available solution.\n* The team will need to invest in training and knowledge sharing to ensure that the tool is used effectively."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n\n## Decision\n,"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\n### Positive Consequences <!-- optional -->\n* Recommended approach by [Martin Fowler](https://martinfowler.com/bliki/MonolithFirst.html)\n","The components in the PlanQK platform should **not** be splitted into individual repos.\n\n**Benefits**\n- **Easier code navigation and debugging**: A single repo allows developers to easily see the relationships between the components and how they interact with each other, making it easier to track down bugs and errors.\n- **Reduced duplication**: Keeping all the components in a single repo helps to reduce code duplication and ensure that all components are using the same code base.\n- **Improved collaboration**: A single repo fosters collaboration by allowing developers to easily share code changes and work together on new features and enhancements.\n\n**Drawbacks**\n- **Increased repo size**: A single repo can become quite large, which can make it slower to clone and work with.\n- **Potential conflicts**: With all the components in a single repo, there is a greater potential for code conflicts when multiple developers are working on different parts of the system.\n\n**Alternatives**\n- **Split the components into smaller repos**: This could help to reduce the size of the main repo and make it easier to work with, but it could also lead to increased code duplication and make it more difficult to collaborate on changes that span multiple repos.\n- **Use a monorepo tool**: A monorepo tool can help to manage a single repo by breaking it down into smaller, more manageable units, while still keeping all the code in a single location. This can provide the benefits of a single repo without the drawbacks.\n\n**Recommendation**\nGiven the benefits and drawbacks of the above options, the best decision is to **not** split the components in the PlanQK platform into individual repos. Instead, consider using a monorepo tool to help manage the single repo and make it easier to work with."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\nWe need references for the links.\n\n## Decision\n","Chosen option: ""[URIs]"", because UUIDs are generated and thus depend on the underlying database system.\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\n### Positive Consequences <!-- optional -->\n* We follow solid [W3C specification](https://www.w3.org/Addressing/URL/uri-spec.html)\n","Establish link references via a pattern identifier composed of 10-digit string (e.g., Alg1Pat2345678901)."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want a test framework that has good support for TypeScript and Node. Jest is\na fast testing framework with good resources for mocking.\n\n## Decision\n,We will use Jest as our testing framework.\n,Use Jest as the test framework.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be aware of CVEs (Common Vulnerabilities and Exposures) before they\nend up in production, and make sure to block deployments with known high\nseverity CVEs. Snyk allows us to scan our PRs for CVEs and fail builds if there\nare any vulnerabilities in the code we've written.\n\n## Decision\n","We will use Snyk to:\n- Run scans on PRs provide results and fail builds if any known high severity\nCVEs are found.\n- Run scans on main and fail builds on high severity CVEs, posting the results\nto the Snyk platform for monitoring.\n- Run nightly scans on the docker image and app dependencies.\n",**Decision:**\n\nIntegrate Snyk into our CI/CD pipeline to monitor for high severity CVEs and block deployments with known vulnerabilities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, including React itself, and has become a standard.\n\n## Decision\n",We will enforce that everything supported by Prettier has its style enforced by\nit.\nWe will set up Git hooks to automatically run the formatter before committing.\nWe will set continuous integration up to reject commits that are not correctly\nformatted.\n,Use Prettier to enforce a consistent code style across all projects.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to build a web-based application in Node. Express is a minimal and\nflexible framework that sets up a lot basic configuration for us like routing\nand middleware, and makes it easy to incoporate the GOV UK design system UI\ncomponents.\n\n## Decision\n",We will use Express as the basis for this project.\n,**Decision:** Express.js will be used as the web framework for the application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to automate testing end-to-end user journeys through our\napplication. Cypress is an alternative to Selenium that runs in the browser to\ndo this for us, and is used across multiple projects at MOJ.\n\n## Decision\n",We will use Cypress for integration tests.\n,"**Decision:** Use Cypress for end-to-end user journey testing\n\n**Rationale:**\n\n* Cypress is an established tool used successfully in multiple projects at MOJ for end-to-end user journey testing.\n* Cypress runs entirely in the browser, providing a high degree of fidelity and stability compared to other testing frameworks that rely on simulating browser behavior.\n* Cypress offers robust and expressive testing capabilities, allowing for automated testing of complex user interactions and scenarios.\n* By leveraging Cypress, we can streamline and accelerate our testing process, improving the quality and reliability of our application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n\n## Decision\n",We will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n,We should use ESLint with TypeScript plugins to lint our code and enforce consistency.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n\n## Decision\n",We will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n,**Decision:** We will use Stylelint to lint our CSS and SASS code.\n\n**Rationale:**\n\n* Stylelint is a popular CSS linter with support for SASS.\n* Stylelint is easily configurable for our purposes.\n* Linting the code is good practice to enforce consistency and catch errors automatically.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be confident about the code we write, and for it to be\nself-documenting as much as possible. TypeScript is a compiled language with\noptional typing. It's a superset of JavaScript, so is familiar to developers\nwho know JavaScript. It has wide editor support.\n\n## Decision\n",We will use TypeScript by default.\n,"We will use TypeScript for our codebase. TypeScript is a compiled language with optional typing. It's a superset of JavaScript, so is familiar to developers who know JavaScript. It has wide editor support. This will give us confidence in the code we write, and make it self-documenting as much as possible."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Background\nOur service frequently needs to take a user through a journey which involves asking them a series of questions over multiple pages, and then playing all the answers back to the user before finally submitting all the answers to the interventions service API.\nThis means that we need somewhere to store the user’s answers as they progress through the journey. For some journeys, the interventions service provides this storage. For example, in the journey of submitting a referral, the interventions service provides endpoints for creating and updating a draft referral.\nHowever, we have other journeys where the interventions service does not provide this storage. For example, assigning a referral to a caseworker, or cancelling a referral. For these journeys, the UI application needs to provide its own storage.\n### Why not have the interventions service store everything?\nWe _could_ make the interventions service provide storage for the data of every page of every journey in the UI. However, this would be very tied to the user journeys of this UI and less of a general-purpose interventions API. We should reserve interventions service storage for long journeys which the user might want to complete in multiple sittings.\n### The solution so far\nWe pass data from page A to page B to page C of a journey by making sure that the HTTP request for page C includes all of the data that was submitted on pages A and B. We do this either by:\n- accepting the data from previous pages as data encoded in the request URL’s query, and then passing it in a GET request to subsequent pages by placing the data from previous pages into the query of the URL that’s used for navigating to the next page\n- accepting the data from previous pages as `application/x-www-form-urlencoded` form data, and then passing it in a POST request to subsequent pages by placing `<input type=""hidden"">` fields on each page, replaying the data from the previous pages\n### The problem with this solution\nUsing a GET request limits the amount of data a user can submit on a page, since many clients and servers do not support URLs over 2000 bytes long.\nUsing a POST request means that we cannot redirect to different pages in the journey (for example, check your answers) based on the user’s input, since a redirect response cannot instruct the browser to make a POST request.\nAlso, the approach of embedding the data in the HTML is laborious. We have to make sure that every possible route through the pages in the journey preserves the data. This becomes particularly easy to get wrong when we have non-linear sequences of pages — for example, a link on the check your answers page that allows the user to edit a previous answer.\n### Other requirements for a solution\nThe solution must also:\n- make sure that a user is not able to access data entered by a different user\n- not prevent the user from performing the same journey multiple times concurrently — for example, they should be able to assign two different interventions at the same time in different browser tabs\n- preserve the data that the user entered on previous pages when they use the browser’s Back button\n- give us maximum flexibility in deciding how to meet [WCAG 2.1’s Success Criterion 2.2.1 Timing Adjustable](https://www.w3.org/TR/WCAG21/#timing-adjustable) — for example, by making sure that the data is kept for at least 20 hours before it expires\nIt would also be good if the solution could:\n- not introduce new dependencies to the service — for example a database\n- allow us to continue using the same kinds of coding patterns as we do when interacting with the interventions service API — creating and updating a resource\n- allow us to identify and clean up old data\n\n## Decision\n","We’ll use the UI application’s existing Redis server as our storage. It allows us to store essentially unlimited amounts of data.\nIn Redis, we’ll store “draft” objects. These are containers of arbitrary JSON data, along with:\n- a globally unique identifier\n- the unique identifier of the user who created the draft\n- timestamps of creation and last update\n- an identifier explaining what type of data the draft represents\nWe’ll:\n- provide a “CRUD” API for creating, fetching, updating and deleting these draft objects\n- include the draft’s ID in all URLs in a journey\n- allow these drafts to be created by a GET request, so that we can use `<a>` tags to link to journeys\n- prefer to use POSTs to pass data to a page instead of GET, so we don't have to worry about body size constraints\nThe aforementioned API will:\n- enforce access control — making sure that a user is only allowed to access drafts that they created\n- make sure that drafts are automatically removed after they are no longer accessed for a certain amount of time (for example, 1 day), using Redis’s expiry functionality, to make sure that drafts do not consume storage in Redis indefinitely\n### Alternatives\nWe might consider using the (Redis-backed) Express `session` object. However, this object expires after 2 hours of inactivity, which is insufficient for our needs. We don’t want to increase this timeout since there are security implications to increasing the amount of time that a user remains logged in to the service.\n",We will use a server-side HTTP session to store the user’s journey data.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are a few reasons we might not want to put a new feature in front of users as soon as it's been merged into the `main` branch:\n- The API might not be up-to-date with the latest version of the UI (and vice versa) - because we're building the UI and API side of the service independently, there are times when the two may be out of sync: an endpoint may not yet be providing all the data we need; the backend functionality may not be finished at the time of writing the UI code.\n- We want to satisfy the Pact contracts between the two sides of the service but not use the new data structure until the UI has been updated.\n- The new functionality may need to be further tested (either with users by developers) and iterated upon before release.\n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes - this means we'd want to merge smaller chunks of work at a time, which might not be ready for users.\n- We want to test interactions between systems (e.g. the Community API) on the Development environment but not release these changes to the public.\n\n## Decision\n","Any features or behaviour that isn't ready to be interacted with by users will be placed behind a config-based feature flag, configured in `server/config.ts`, e.g. as below:\n```\nfeatures: {\npreviouslyApprovedActionPlans: get('FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS', 'false') === 'true',\n}\n```\nThis can then be turned on for each environment by adding the environment variable (e.g. `FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS`) to the intended environment.\nWe'll usually want to enable this for the development environment and test environment, possibly the pre-prod environment but not the production environment.\nBefore deploying the changes to the production environment, it's a good idea to double check the configuration is as expected e.g. by checking it's hidden in the pre-production environment.\nOnce the feature is ready to be interacted with by users, we'll remove the feature flag from the UI configuration.\n",**Defer UI code updates until API is ready to release.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to make sure we're setting the correct HTTP headers for security e.g.\nContent Security Policy to protect against XSS attacks.\n[Helmet](https://helmetjs.github.io/) is a package that works well with Express\nto make it easy to set various HTTP headers for secutiy.\n\n## Decision\n,We'll use Helmet to set secure HTTP headers.\n,"**Decision:** Use Helmet with Express to set security-related HTTP headers.\n\n**Rationale:**\n\n* Helmet is a popular and well-maintained package that simplifies the process of setting security-related HTTP headers.\n* It integrates seamlessly with Express, our chosen web framework.\n* The headers suggested in the Context provide essential protection against XSS and other security vulnerabilities.\n\n**Impact:**\n\n* **Benefits:**\n    * Enhanced security against XSS and other attacks.\n    * Improved compliance with security best practices.\n    * Simplified header configuration and maintenance.\n* **Costs:**\n    * Additional HTTP headers may increase latency slightly.\n    * Requires some setup and configuration in the Express application.\n\n**Alternatives Considered:**\n\n* **Writing custom middleware:** This option would provide more flexibility but would be more time-consuming and error-prone.\n* **Using other header setting packages:** There are other similar packages available, but Helmet is a widely used and well-regarded option.\n\n**Decision Owner:** Lead Security Engineer"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement an Architectural Decision Record (ADR) process.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Problems with the current approach\n- The `sub` attribute might be opaque and not usable between systems, e.g. when sending entitlements to other systems\n- Different login methods may have different formats for external user identifiers\n- Different login methods might use conflicting values for claims\n- E.g. `foo@csc.fi` via HAKA or CSC SSO (TODO: do we know of cases like this?)\n- A single user might have multiple external identifiers (TODO: do we know of cases like this?)\n\n## Decision\n","We'll add internal random user ids to REMS. This internal user id will\nbe the key that users are referred to within REMS. The user's external\nid will be stored in the attributes JSON blob.\nThis will allow us more flexibility in the future when identity\nrequirements and use cases change, and might also make all sorts of\nmigrations easier (since internal user ids don't need to be touched).\n","**Decision:** Use a standard claim for external user identifiers.\n\n**Rationale:**\n\nUsing a standard claim for external user identifiers addresses the following problems with the current approach:\n\n* **Opaque `sub` attribute:** The `sub` attribute is not always usable between systems, especially when sending entitlements to other systems. A standard claim would ensure that all systems can use the external user identifier in a consistent way.\n* **Different formats for external user identifiers:** Different login methods may have different formats for external user identifiers. A standard claim would ensure that all external user identifiers are in a consistent format, making it easier to compare and match them.\n* **Conflicting values for claims:** Different login methods might use conflicting values for claims. For example, two different login methods might use the same claim for two different pieces of information. A standard claim would prevent this by defining the specific meaning of each claim.\n* **Multiple external identifiers for a single user:** A single user might have multiple external identifiers, such as an email address and a social media username. A standard claim would allow systems to store and track multiple external identifiers for a single user, making it easier to manage user identities."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n# Problems with the current applicant UX\n\n## Decision\n,Here's a step-by-step plan for fixing these.\n1. (Problem A) Make save succeed even with validation errors. Show validation errors in yellow (instead of red). [#2766](https://github.com/CSCfi/rems/issues/2766)\n2. (Problem B) Run validations in the frontend after the user has stopped typing. [#2614](https://github.com/CSCfi/rems/issues/2614)\n3. (Problem B) Autosave in the background by replacing/patching the latest draft-saved event. If the latest event is not a draft-saved then create a new one. [#2767](https://github.com/CSCfi/rems/issues/2767)\n,**Decision:** Improve the applicant UX by implementing the following changes:\n\n* Redesign the application form to be more user-friendly and intuitive.\n* Add clear instructions and examples to help applicants complete the form correctly.\n* Implement a progress tracker to show applicants where they are in the application process.\n* Provide applicants with regular updates on the status of their application.\n* Create a dedicated support channel for applicants to ask questions and get help with the application process.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFlask turned out to be poorly designed piece of software which relays on too\nmuch magic like manipulations of global objects like `g`.\nSeems like we will also decide to use relational database.\n\n## Decision\n,"We will switch to Django. It's not only well written server but it has also\n""batteries included"" like a good ORM layer. And some other features like\nmiddlewares will simplify things.\n",Migrate from Flask to a more robust web framework with a cleaner design and better encapsulation of state.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpen Lobby Server will be written in API First design. Frontend applications\nwill be based on it's API.\n\n## Decision\n,"We will use GraphQL as API standard. From other options like REST and Graph API\nit's the most client friendly approach. GraphQL query language is easy to use,\nvalidated and self documenting.\nGraphQL allows clients to get everything they need in one request without any\noverhead of not needed data. That is very important for mobile frontends.\nGraphiQL tool also provides easy way for developers to inspect and try API. So\nit's easy to adopt by frontend applications developers or other API users.\n","The Open Lobby Server will follow an API First design pattern, which prioritizes the development and documentation of the API before the user interface. This approach provides several benefits:\n\n- **Early API definition:** By focusing on the API first, the team can establish a clear and stable contract between the frontend and backend components, reducing miscommunications and rework later in the development process.\n\n- **Improved API quality:** The API-first approach encourages rigorous API design and documentation, ensuring that the API is well-defined, consistent, and easy to use.\n\n- **Frontend flexibility:** By decoupling the frontend from the backend, the API-first approach allows the frontend developers to build and iterate on the frontend independently, without being constrained by the backend implementation.\n\n- **Enhanced developer collaboration:** By involving both frontend and backend developers in the API design process, the API-first approach fosters collaboration and knowledge sharing, leading to a more cohesive and maintainable solution.\n\n- **Improved maintainability:** The API-first approach promotes a separation of concerns, making it easier to maintain and evolve the API and frontend independently over time."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to add relational database.\n\n## Decision\n,We will use PostgreSQL. It's a mature database with handy features like JSON\nand hstore data types. It's fully ACID compliant including schema changes. It\nhas very good support in Django's ORM.\nAnother popular option is MySQL/MariaDB. But because it has a major bug `#28727`\n(10 years since it has been reported and it's still not fixed) breaking ACID in\nschema changes it can't be used for any serious project.\n,**Decision:** Use PostgreSQL as the relational database management system (RDBMS) for the application.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose webserver.\n\n## Decision\n,We will use Flask. Server should be simple - pretty much just with a GraphQL\nendpoint and GraphiQL.\n,"**Decision:** Use Nginx\n\n**Rationale:**\n\n* Nginx is known for its high performance and low memory consumption.\n* It is easy to configure and maintain.\n* It supports a wide range of features, including load balancing, caching, and SSL termination.\n* Nginx has a strong community and support ecosystem."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make API friendly for clients and design pagination.\n\n## Decision\n,We will adopt GraphQL Relay specification. It solves pagination so we don't\nhave to reinvent a wheel. It has handy Node interface for re-fetching objects.\nIt has a way to define inputs in mutations.\nGraphene lib has good support for creating API following Relay specifications.\n,"To make the API friendly for clients and design pagination, we can use a RESTful API design with JSON as the data format. The API should provide a consistent and well-documented interface that is easy to use and understand.\n\nFor pagination, we can use a standard approach such as offset-based pagination or cursor-based pagination. Offset-based pagination uses a limit and offset parameter to specify the range of results to return, while cursor-based pagination uses a cursor parameter to specify the point in the data set from which to start returning results.\n\nWe can also consider using a pagination library to simplify the process of implementing pagination in our API. There are a number of open source pagination libraries available, such as django-rest-framework-pagination for Django and will_paginate for Ruby on Rails."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to simplify code reviews and unify code style.\n\n## Decision\n,Use Black code formatter: https://github.com/ambv/black\n,"We will use a static code analyzer to automatically check for coding style and quality issues. This decision was made after considering the following factors: Cost: Using a static code analyzer is relatively inexpensive compared to manual code reviews and external code quality audits. Effectiveness: Static code analyzers can automatically detect a wide range of coding style and quality issues, reducing the time and effort required for manual code reviews. Speed: Static code analyzers can analyze code quickly and efficiently, allowing for faster code reviews and reducing the time it takes to identify and fix code issues."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose main programming language for this project.\n\n## Decision\n,I'm now fluent in writing Python and JavaScript. Both languages are widely used\nthese days and may attract developers to join this project.\nPython is more mature with more stable libs for writing server side code and\ntesting. JS may have better libraries for writing GraphQL APIs because\nGraphQL originated in JS ecosystem.\nI'm doing this in free time and for fun. I want to focus on project's features\nand not to waste a time with poorly designed language lacking mature and\nstable libraries. So for obvious reasons I'm choosing Python.\n,**Decision**: Use Kotlin as the main programming language for the project.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a database with fulltext search capable of searching in various\nlanguages especially in Czech.\n\n## Decision\n,"We will use Elasticsearch. It's well known database with great fulltext search\ncapabilities based on Apache Lucene. It has also aggregations, highlighting of\nresults, and many other useful features.\nWe will use it as database for all data so we have just one database in the\nsystem.\n","Use Elasticsearch, an open-source, distributed, full-text search and analytics engine with support for various languages, including Czech."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose framework for tests.\n\n## Decision\n,We will use Pytest. It's much more Pythonic and simplier to use than Unittest\nfrom standrd library.\n,"**Decision:** Use Pytest\n\n**Justification:**\n\n* Pytest is a popular and widely used framework for Python testing.\n* It provides a simple and intuitive syntax that makes it easy to write and maintain tests.\n* Pytest offers a rich set of features, including fixtures, parametrization, and reporting.\n* It integrates well with other popular Python testing tools, such as Selenium and Django.\n* Pytest has a large and active community, which provides support and resources for users."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNumber of document types which does not use Elasticsearch's fulltext\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\nper index which means management of many indices.\n\n## Decision\n,We will add relational database as primary database. Elasticsearch will be used\nfor denormalized reports and related data intended for fulltext search.\n,Use Elasticsearch's `_all` fields for full-text search.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpen Lobby must have open stable API.\n\n## Decision\n,"Open Lobby will be server just with an API based on API First design. Frontend\nwill be written as separate application (web based, mobile, ...). This will\nensure that there is a working API for anything that frontend application(s)\nwill do.\n","Open Lobby will have a stable and well-documented API that will allow other systems to interact with it. The API will be designed to be flexible and extensible, so that it can be used to support a variety of use cases. It will also be documented in a way that makes it easy for developers to understand and use."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in\nthis article:\nhttp://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",Create an Architectural Decision Record template.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need an authentication mechanism for users. It must be secure and\nfrontend application independent.\n\n## Decision\n,We will use OpenID Connect. Open Lobby Server will provide all the hard stuff\nfor a frontend applications. Ideally over the GraphQL API.\n,"**Use OpenID Connect (OIDC)**\n\n**Benefits**\n\n* OIDC is an industry-standard authentication protocol that is secure and well-supported.\n* It is independent of the frontend application, so it can be used with any type of application.\n* It provides a single sign-on (SSO) mechanism, so users can access multiple applications with a single set of credentials.\n\n**Alternatives Considered**\n\n* **Custom authentication mechanism:** This would require developing a custom authentication protocol and implementation, which would be time-consuming and error-prone.\n* **Third-party authentication service:** This would be less secure than using OIDC, as the third-party service would have access to user credentials.\n\n**Rationale**\n\nOIDC is the most secure and flexible authentication mechanism that meets the requirements. It is a standard protocol that is supported by a wide range of applications and services. It also provides a SSO mechanism, which makes it easy for users to access multiple applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA web framework is needed in order to provide a level of abstraction over low\nlevel language features. Effort spent solving non-domain specific problems\nshould be kept to an absolute minimum.\n\n## Decision\n,"We will use the [Express](http://expressjs.com/) web framework. This will\nprovide a lot of well\n[documented](http://expressjs.com/en/resources/books-blogs.html) features and\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\nframework.\nExpress is one of the most popular web frameworks available for NodeJS and as\nsuch has been well tested and is deployed across many\n[organisations](http://expressjs.com/en/resources/companies-using-express.html).\n",Use Spring Framework
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nInformation about pharmacies is required by the application in order to display\nto users. A Docker image of pharmacies\n([pharmacy-db](https://hub.docker.com/r/nhsuk/pharmacy-db/)) running a MongoDB\ninstance has been created for use by other applications.\nNodeJS uses a single threaded event loop architecture and as such works best\nwhen the work it is doing is non-CPU intensive. Searching through datasets is\npotentially CPU intensive.\n\n## Decision\n,We have decided to use the existing Docker image rather than spend effort\nacquiring the data again.\n,The application should use the pharmacy-db Docker image to access pharmacy information.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n\n## Decision\n,We will use Prometheus to monitor and alert on the state of the application.\n,"Implement application monitoring using metrics (e.g., Prometheus) and tracing (e.g., Jaeger)."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe primary (only) consuming application for this API needs to show both open\nand nearby services on separate pages (and more of them). Previously the\napplication had shown a mix of open and nearby services within a\nsingle page.\nHaving the API so closely aligned to the needs of the consumer is not ideal.\nThere is scope to increase the flexibility of the API along with increasing the\nease with which it can be used both by the current and future consumers.\n\n## Decision\n,The decision is to add a new endpoint i.e. `/open` alongside the current\n`/nearby` endpoint. The former endpoint will return only services that are open\nwhere the latter will be refactored to return only services that are nearby\nregardless of their opening state.\n,"**Decision:**\n\nIntroduce a new API endpoint that returns only nearby services, in addition to the existing endpoint that returns both open and nearby services.\n\n**Rationale:**\n\n* Decouples the API from the specific needs of the primary (only) consuming application.\n* Makes the API more flexible and easier to use by both current and future consumers.\n* Supports the application's requirement for separate pages for open and nearby services."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nElasticsearch is configured as a cluster for reliability and failover, and\nprovides a single point for data updates. MongoDB runs as a single instance and\nis not clustered.\n\n## Decision\n",nearby-services-api will consume data from Elasticsearch rather than MongoDB.\n,"MongoDB should be configured as a cluster for reliability and failover. This will ensure that data is always available, even if one node fails."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",Record architectural decisions in a centralized repository.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\n\n## Decision\n,"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\n","Use a templating engine to generate the JSON configurations at build time, then deploy the generated JSON files as configuration artifacts to Elasticsearch."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nElasticsearch timeouts can cause 'orphaned' indexes from remaining in the cluster after an update.\nRunning two updaters simultaneously, as happens for pull requests that last several days, can leave\ntwo indexes with the same alias.\nElasticsearch watches are run to export data to Prometheus throughout the day.\nTwo new date stamped 'monitor' and 'watch' indexes are created every day to track the status of the watch job.\n\n## Decision\n","Orphaned and indexes with duplicate aliases will be removed as part of the update.\nThe indexes created every day by the watch will also be deleted within the elastic-search updater.\nThis could be done by an Elasticsearch watch, but a license is required to use this functionality.\nTo avoid adding more infrastructure components to the system rather than having a standalone service, the cleanup will be\nperformed in the elastic-search updater. This can be moved into another service in future if required.\n","**Decision**: **Implement a mechanism to clean up 'orphaned' indexes and ensure unique aliases**\n\n**Rationale**:\n\n* Orphaned indexes and duplicate aliases can lead to confusion and data integrity issues.\n* By implementing a cleanup mechanism, we can ensure the cluster is maintained in a clean and consistent state.\n* This will prevent orphaned indexes from consuming resources and avoid potential conflicts when creating new indexes with the same alias."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany of the vulnerabilities identified by snyk have no fixes, and can only be ignored in the synk config file for a set number of days.\nAfter the number of days are up, the vulnerabilities will fail the build and prevent merge on new PRs, with the only option to ignore the\nvulnerabilities again.\nMany of the snyk failures are for build tools that are not exposed when applications are deployed.\nThe overhead of ignoring many snyk vulnerabilities to get builds passing outweighs the potential risks.\n\n## Decision\n","Snyk will be removed and replaced with a combination of Greenkeeper and Github vulnerability subscriptions.\nGreenkeeper will ensure the dependencies are up to date, and the subscription notifications will provide visibility of serious vulnerabilities.\nAn automated replacement will be put in place in the near future.\n","**Decision:** Ignore the identified vulnerabilities that have no fixes and are not exposed when applications are deployed.\n\n**Justification:**\n\n* Ignoring the vulnerabilities reduces the overhead of managing Snyk failures and prevents unnecessary delays in merging PRs.\n* The vulnerabilities are not exposed to users or the application's functional capabilities, minimizing the potential risks.\n* The decision is based on the trade-off between overhead and risk, with the reduced overhead being the primary consideration.\n\n**Consequences:**\n\n* Some vulnerabilities may remain unresolved in the codebase.\n* The decision should be monitored and reviewed regularly to assess if the risk tolerance remains acceptable.\n* The team should consider alternative measures to address the underlying security concerns."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** To implement an Architectural Decision Record (ADR) process using a dedicated tool or template to track and document architectural decisions made in the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need API for the Zoo to expose issues, analytics data, etc.\n\n## Decision\n","We will create API by GraphQL specification. Pagination will be done according\nto Relay server specification. We considered to build REST API, but we decided\nthat for fresh new APIs is the GraphQL right choice.\n","**Decision:** Create a new API for the Zoo.\n\n**Rationale:**\n\n* The existing API is not designed to handle the specific needs of the Zoo.\n* A new API can be tailored to meet the specific requirements of the Zoo, such as exposing issues, analytics data, etc.\n* A new API can be designed to be more scalable, reliable, and secure than the existing API.\n* A new API can be developed more quickly and easily than modifying the existing API.\n\n**Consequences:**\n\n* The Zoo will need to develop and maintain a new API.\n* The Zoo will need to migrate its existing data to the new API.\n* The Zoo will need to update its documentation to reflect the new API."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to use code formatter to standardize code look so we stop bothering about it\nin code reviews and focus on how code actually works.\n\n## Decision\n,We will use Black: https://github.com/ambv/black\n,"**Decision:** Use Prettier.\n\n**Rationale:**\n\nPrettier is a widely used and highly configurable code formatter. It supports a variety of languages, including JavaScript, TypeScript, and CSS, which are used in this project. Prettier's configuration options allow the team to customize the code style to meet their specific requirements. By using Prettier, the team can save time and reduce the potential for human error when formatting code. Prettier also has a large community of users and contributors, which ensures ongoing support and development."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe were using [Semantic UI](https://semantic-ui.com/), as it's an open source and robust framework.\nIt has a wide range of components, it's easily customizable and it uses modern\napproaches. It's really well documented, although usually documentation is not even\nneeded as it's really intuitive to work with it.\nUnfortunately, the project is not as maintained as before, and the development of new components\nis stale.\n[Fomantic UI](https://fomantic-ui.com/) picked up where Semantic UI left, and offers a drop-in replacement,\nwith a higher quantity of components, and the development can be considered active.\n\n## Decision\n","We will use [Fomantic UI](https://fomantic-ui.com/), the community fork of `Semantic UI`. The\nsupport on this project is fast and on point, and the roadmap aligns nicely with our development,\nas the maintainers are planning to offer first party support for Vue components, and ditch the\njQuery dependency.\n",**Decision:** Migrate to Fomantic UI\n\n**Rationale:**\n\n* Fomantic UI is a drop-in replacement for Semantic UI.\n* Fomantic UI has a higher quantity of components.\n* Fomantic UI is actively developed.\n\n**Consequences:**\n\n* The project will benefit from the increased functionality and active development of Fomantic UI.\n* The migration process should be relatively straightforward.\n* There may be some minor compatibility issues that need to be addressed during the migration.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe always wanted to have a consistent commit message styling, we used coala's CommitBear to help with\nthis task but besides just a bit more clarity in the commit history, it doesn't bring that much value.\nNow that we'll be getting rid of coala in favour of pre-commit hooks, we need an alternative solution,\nand we can use this change to also step up our game regarding commit messages.\n\n## Decision\n","We will use the [conventional commits](https://www.conventionalcommits.org) convention for writing our commit messages\nfrom now on. Besides forcing us to keep a consistent commit message styling, it will also bring much\nmore value, like:\n* Automatically generating CHANGELOGs\n* Automatically determining a semantic version bump (based on the types of commits landed)\n* Communicating the nature of changes to teammates, the public, and other stakeholders\n* Having different CI steps based on the nature of the commits added to the MR\n* Having insights about how much time/effort is spent on the different areas of the development\n","**Decision:** Implement a pre-commit hook that enforces a consistent commit message format.\n\n**Rationale:**\n\n* Pre-commit hooks allow for more control and customization compared to CommitBear.\n* A consistent commit message format improves the readability and understandability of the commit history.\n* Enforcing a consistent format through a pre-commit hook ensures that all commit messages adhere to the established guidelines.\n* By leveraging the opportunity to upgrade commit messaging practices, the team can enhance the overall quality and value of the commit history."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need dir to save ADRs into.\n\n## Decision\n,"We will save ADRs into top level `adr` dir. Another option was to put them into `docs/adr`\nalong with Sphinx docs, but they might be bit hidden there.\n", ADRs should be saved into `docs/adr` directory.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Implement an Architectural Decision Record (ADR) system to document and track architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe were using [Material Design Lite](https://getmdl.io/) as our front-end CSS framework.\nThe selection of offered components is quite limited and most of them just include\nthe basic logic. We have to dedicate a big amount of time writing front-end code and it's\nhard to compose the provided components as they are mainly meant to be used in a single\nway because of the strong Material design guidelines.\nWe aim to focus on logic, not on developing front-end components. We need a solution\nthat can offer a high number of already built components. The components should be\neasy to configure and to compose together as our UI sometimes needs components that\nare not contemplated on the common guidelines.\n\n## Decision\n","We will use [Semantic UI](https://semantic-ui.com/), as it's an open source and robust framework.\nIt has a wide range of components, it's easily customizable and it uses modern\napproaches. It's really well documented, although usually documentation is not even\nneeded as it's really intuitive to work with it.\n","**Decision:** Migrate to a more modular and versatile front-end CSS framework, such as [Modular UI](https://modularui.js.org/).\n\n**Rationale:**\n\n* **Wide range of components:** Modular UI provides a comprehensive library of components that covers a wide range of use cases. This reduces the need for custom front-end development.\n* **Modular design:** The components are designed to be modular and composable, allowing for flexible and complex UI layouts.\n* **Easy configuration:** Modular UI provides extensive configuration options for its components, enabling customization to meet specific design requirements.\n* **Focus on logic:** By using Modular UI, the development team can focus on business logic and functionality, rather than spending time on front-end development.\n* **Improved maintainability:** Modular UI's consistent naming conventions and well-structured codebase enhance code readability and maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn PFE, workflows have a complex logic of programmatic workflow selection. [ADR 18](adr-18.md) has documented the original context. This ADR is a partial superseding of the decision on how to select the workflow when navigating to `/classify`\n\n## Decision\n","We will continue to use the routing behavior defined by [ADR 18](adr-18.md)\n### `/classify` behaviour\nWe will not programmatically select a workflow except for one case: when there is only one active workflow. For all other cases, we have a UI prompt for the volunteer to manually select which workflow they wish to contribute to.\n### Error handling\nCases when the workflow is not available are:\n- The workflow does not actually exist, so this will 404.\n- The workflow exists, but is in an inactive state. The activeness state effectively functions as a permissions mechanism since users with the project owner, collaborator, or expert role or Zooniverse admins can still request and load inactive workflows. Users with the correct role should be able to load an inactive workflow with a visual indication in the UI it is inactive. All other users will receive a 404.\n- The workflow exists and is active, however, the project uses workflow assignment and the workflow has not been assigned to the volunteer yet. The classify page should load, the classifier itself doesn't, and the workflow selection prompt is rendered for the volunteer to choose between the workflows they have been assigned.\n",The workflow for `/classify` will use the new workflow ID that encodes the original `<theme>-<distribution>-default` pattern.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\n\n## Decision\n,"### Changes from the TESS LCV\nThe TESS LCV is built only for their use case and is not configurable. It is hard coded to expect brush stroke annotation for the classifications, zoom only in the x-axis direction, and for only one data series. We will build a generally configurable plots so that other projects can have the flexibility they need. The new plot viewers will be modular so that it can be placed into a composite, complex subject viewer as needed.\nPreviously, the TESS LCV was built using d3.js, however, mixing d3 and react can be dangerous. The decision at the time was to use d3 because of the custom requirements needed for the TESS LCV and the react + d3 libraries were too opinionated to be used for our needs. The library d3 is also difficult to write tests for because of its chaining API. For this reason, the original TESS LCV is largely untested.\nSince then, a library called [vx](https://vx-demo.now.sh/) containing reusable low-level visualization react component that uses the d3 math utilities, has become more mature to start using. This fits our needs to have the DOM solely rendered by React, but still has the usefulness of a mature library like d3 to do calculations as needed. The new plots will be built using vx.\nThe long term goal is to swap the TESS LCV over to the new `ScatterPlotViewer`, however, this means adding support for brush annotations which will be investigated at a later time.\n### ScatterPlotViewer\nThe scatter plot will be built with support to configure:\n- Multiple data series\n- Customizable data series colors to represent information as needed\n- Pan and zoom in both axes directions or constrainable\n- Axis inversion\n- Customizable axis label\n- Customizable number of axis ticks and direction\n- Customizable margin and padding for the plot area\n### BarChartViewer\nA bar chart plot will be built with support to configure:\n- Multiple data series\n- Customizable data series colors to represent information as needed\n- Labels for axes and for individual bars\n### VariableStarViewer\nThe scatter plot and the bar chart together along with the `SingleImageViewer` and a few additional controls will be a complex composite viewer built as the `VariableStarViewer`. The `VariableStarViewer` will have its own control bar that has a toggle for axis inversion, period, data series visibility, and phase focus. Each scatter plot will be individually pan and zoomable.\n### DataImageViewer\n_Note: Naming still TBD_\nThis will be a complex composite consisting of a scatter plot and a single image. We may want to support up to N images, but this is still TBD. The initial build will be just the single scatter plot and single image.\n### Future plots\nThere may be requests to build more plot types like a line plot or map plot that renders GeoJSON. We will continue to evaluate our usage of vx at that time and ideally will continue to use it.\n",Implement a reusable module in the `subjects` package that can parse and manage the display of JSON data as a subject for classification. This module should be able to be easily integrated into other research projects that have JSON data to classify.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\nSee the [InVision document for the Pages Viewer](https://projects.invisionapp.com/d/main#/console/12924056/393421254/preview) and Issue #1142.\n\n## Decision\n","1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https://github.com/zooniverse/front-end-monorepo/issues/1262).\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement/future feature.\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement/future feature.\n","Use the name ""multiFrame Viewer"" for the component to display multi-frame subjects in the classification interface."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we first started the rebuild, @rogerhutchings set it up as a monorepo. The idea being that there are benefits to separating our code between multiple apps and packages:\n- We can reuse code more easily\n- Error boundaries are built-in (if we break one thing, we don't necessarily break everything)\n- To make changes to one thing, we don't need to recompile and rebuild _everything_ in development, at least\nWorking across multiple repos can be pretty tedious: adding a feature which requires changes to more than one package requires multiple pull requests to different repos, for example, and publishing updated packages in the correct order. So Roger set this project up as a monorepo using Lerna. This started out fine, but we hit some issues in the development of Planet Hunters TESS:\n- `styled-components` wasn't collect styling from symlinked dependencies, causing a FOUC on initial render: https://github.com/zooniverse/front-end-monorepo/issues/327\n- Adding the `@zooniverse/standard` package broke the build because Lerna wasn't correctly linking it up: https://github.com/zooniverse/front-end-monorepo/pull/958\n- `npm audit` doesn't work with symlinked dependencies\nIn the course of investigating these issues, we learned:\n- Symlinking packages has side effects that need to be worked around: https://github.com/styled-components/styled-components/issues/2322\n- Lerna is aimed at publishing multiple packages from a repository, **nothing more**, including managing local packages for development (https://github.com/lerna/lerna/issues/1243#issuecomment-401396850). We're currently using it solely for local development. 😢 It's also reasonable to suppose that future major versions of Lerna _could_ modify the API to further that aim, and deprecate features that we're currently (incorrectly) relying on.\nSo @rogerhutchings started investigating alternatives to a pure Lerna setup.\n- [`bazel`](https://bazel.build/), [`buck`](https://buck.build/), [`pants`](https://www.pantsbuild.org/index.html), [`please`](https://please.build/) etc support multiple langauges, but not Javascript out of the box, and we'd need some additional tooling to get them to work in the way we want.\n- [`rushjs`](https://rushjs.io/) looked promising, but at time of writing is limited to `npm` v.4.5.0, which is pretty old.\n- [`yarn`](https://yarnpkg.com/) has a monorepo feature called Workspaces, and has some other features like `audit`, hoisting etc. Lerna will use `yarn` workspaces under the hood where available.\n\n## Decision\n","We're going to switch the monorepo tooling from Lerna to Yarn and Lerna. Yarn will be used for managing dependencies across the monorepo and running commands against packages. Lerna will be used to run commands across multiple packages, which is what it's designed to do.\n","We should use the `yarn` workspaces feature in place of Lerna. This will address the issues we have been experiencing with Lerna, such as the FOUC with `styled-components`, the problems with `@zooniverse/standard`, and the inability of `npm audit` to work with symlinked dependencies. Additionally, `yarn` workspaces is a more modern approach to monorepo management and has other helpful features like `audit` and hoisting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFuture projects using the `front-end-monorepo` will require a video player. Some projects may simply need a video to play. Some projects will require users to interact with a video file; such as make annotations on the video to mark locations or item sizes. The video player must be highly customizable, able to be used in a React code base project and support all major browsers.\n\n## Decision\n","We will implement `react-player` [Github](https://github.com/CookPete/react-player).\nDemo Page: [Demo](https://cookpete.com/react-player/)\nThe lengthy list of attributes (props) and Callback Props makes `react-player` a great choice for developers who need customization. Many of these attributes are simply booleans. For example, to play a video, pass in `playing={boolean}`. Full list of [Props](https://github.com/CookPete/react-player#props)\nCompared to the native HTML video player, `react-player` makes it easy to customize styling so the player looks the same across different browsers.\nMaking `react-player` responsive is easy by targeting pre-defined classNames.\n[Responsiveness](https://github.com/cookpete/react-player#responsive-player)\nOne of the biggest wins of using `react-player` is ease-of-use. This will reduce developer time and reduce the amount of custom methods in our code base.\n### Media\n`react-player` is a React component for playing a variety of URLs, including file paths, YouTube, Facebook, Twitch, SoundCloud, Streamable, Vimeo, Wistia, Mixcloud, and DailyMotion. [Supported Media](https://github.com/CookPete/react-player#supported-media)\nAlthough `react-player` supports a number of externally hosted videos, we will only support video files uploaded to the Zooniverse platform.\nFile types will be validated to ensure the files are mp4.\n### Customization\nA wide range of [Props](https://github.com/CookPete/react-player#props) can be passed in to control playback and react to events.\nPlease read through the [Github](https://github.com/CookPete/react-player) for a full list of features.\n",Use the open-source React video player library [React-Player](https://github.com/CookPete/react-player).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPanoptes-Front-End's drawing tools largely developed from previous custom projects and/or were added one at a time to support a specific project. Because of this, several inconsistencies have been discovered in downsteam analysis and aggregation. To explain the inconsistencies, a few definitions are needed:\n- RHC: A right handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the +y axis with angle=0 along the +x axis\n- LHC: A left handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the -y axis with angle=0 along the +x axis\n- Domain: The range of values a number can take [ or ] is inclusive, ( or ) is exclusive.\n- Upper origin: The point 0, 0 is in the upper left of the plot\n- Lower origin: The point 0, 0 is in the lower left of the plot\nThe inconsistencies comprise of:\n- The browser SVG coordinate systems use _RHC_ with an _upper origin_ resulting in positive angles rotating clockwise. Most plotting software (R, Python, Matlab) are _RHC_ with a _lower origin_ resulting in positive angles rotating counter-clockwise.\n- The position of origin has been inconsistent between tools which has an effect on the final annotation too. Most use the center x, y point, but some don't\n- Some of the drawing tools use _LHC_\n- Some tools' annotation use `angle` some use `rotation`\n- It's unclear when the x, y annotation refers to the center point of the shape\n- It's unclear when the x, y annotation is being used as the point of rotation\nSome of the mark annotation models have a few other issues as well:\n- Some shapes have default values which an create bias. For example, the ellipse has a default axis ratio of 0.5 and many volunteers have left the default creating a bias ([comment](https://github.com/zooniverse/front-end-monorepo/issues/500#issuecomment-516788821))\n- The freehand drawing tool has peformance impact on the browser as the drawing is being created and with the job to create classification exports as well. This is because the current annotation consists of every single x, y point created\n\n## Decision\n","The shape's mark annotation models should change for consistency and improved post-classification analysis in the following ways:\n- The annotation should use the mathematical standard of _RHC_ with a domain of `[-180, 180]` for consistent angle calculation\n- The annotation model should use `angle` for naming rotation angles. This replaces usage of `rotation`.\n- The annotation model should replace `x` and `y` with `x_center` and `y_center` for shapes where that is applicable\n- The exceptions are non-shapes like point, line, and transcription line tools, and non-symmetric shapes like fan.\n- Shape clustering in aggregation is always done with the center\n- All rotations should be defined about `x_center` and `y_center` point. If the rotation cannot be defined around the center point, then the point used should be clearly recorded in the annotation as `x_rotation` and `y_rotation`\n- Conditional logic in component code can be avoided by using a Mobx-State-Tree's computed view functions to get either `x_center` or `x_rotation` mapped to `mark.x`. Code example:\n```js\nconst CircleModel = types\n.model('CircleModel', {\nx_center: types.optional(types.number, 0),\ny_center: types.optional(types.number, 0),\nradius: types.maybe(types.number),\nangle: types.maybe(types.number)\n})\n.views(self => ({\nget coords { // this is naming is reusing what's already been done with Point and Line for consistency.\nreturn {\nx: self.x_center,\ny: self.y_center\n}\n}\n}))\n```\n```js\n// non-symmetrical shapes like fan use x_rotation, y_rotation\nconst FanModel = types\n.model('FanModel', {\nx_rotation: types.optional(types.number, 0),\ny_rotation: types.optional(types.number, 0),\nradius: types.maybe(types.number),\nangle: types.maybe(types.number),\nspread: types.maybe(types.number)\n})\n.views(self => ({\nget coords { // this is naming is reusing what's already been done with Point and Line for consistency.\nreturn {\nx: self.x_rotation,\ny: self.y_rotation\n}\n}\n}))\n```\n- Default values should be removed wherever possible. We will replace these with project builder configurable values set in the project builder lab when the tools are setup.\n- The parameters will _not_ have default values suggested by us. If the parameters are not set in the lab, then when attempting to use that drawing tool in the classifier, the classifier should display an error message that the tool is not fully setup yet. The lab should also prompt for inputing a value.\n- The lab should include instructions and a warning about the biasing effect.\n- The tools that have defaults are ellipse, rotate rectangle, fan.\n- The freehand drawing tools mark annotation will be a string of the SVG's path and it will be the responsibility of post-classification analysis to convert this to usable x,y points. We will include a sample script in the `DataDigging` repo for project owners to reference on how to do this. Aggregation in Caesar will have to be updated to do the conversion first.\n- `tool` will change to `toolIndex` to clarify it is referring to the index of the input from the task area. The `toolIndex` is useful to distinguish between multiple instances of the same tool tip in a drawing task.\n- Drawing annotations and drawing tool marks will have a `taskType` and `toolType` attribute added that map to an enumeration of the type of task or tool like `drawing` or `point`, `ellipse`, etc respectively. This enables the aggregation for Caesar code to auto-configure which extractor to use without checking data types ([comment](https://github.com/zooniverse/front-end-monorepo/issues/823#issuecomment-493896524)).\n- Certain annotation models may have internal properties used denoted by a preceding underscore. These properties will be removed with the classification complete action. We will remove them to help prevent confusion by project owners in downstream analysis.\n","**Establish a consistent coordinate system across all drawing tools:**\n\n* Adopt RHC with upper origin, aligning with browser SVG coordinate systems.\n* This ensures positive angles rotate consistently clockwise.\n\n**Define a standard origin for all tools:**\n\n* Use the center x, y point as the origin for all shapes.\n* This eliminates inconsistencies in annotation positioning.\n\n**Standardize terminology for angles:**\n\n* Use the term ""angle"" consistently throughout the system.\n* Deprecate the term ""rotation"".\n\n**Clarify the use of x, y annotations:**\n\n* Specify when x, y annotations refer to the center point of a shape.\n* Specify when x, y annotations are used as the point of rotation.\n\n**Address mark annotation model issues:**\n\n* Remove default axis ratio for ellipse to eliminate bias.\n* Optimize freehand drawing tool for performance by storing a simplified representation of the drawing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe big question: which Subject Viewer should we use to view a given Subject?\nAt the moment, the Classifier tries to ""guess"" which Subject Viewer to use (see\n`lib-classifier/src/store/Subject.js`, `get viewer()`) by analysing the Subject\nitself. e.g. if the Subject has multiple images, show a multi-image viewer. If\nthe Subject has a video, show a video viewer.\nWhile this method works in a majority of projects, certain projects with\nextremely specific Subject structures breaks the generalised ""guessing"" logic,\nor else warps the logic with so many ""what if"" clauses that it collapses into a\nwrithing mass of nonsense.\nFor example, a TESS Planet Hunters Subject looks like...\n```\nSubject 12345 = {\n...\nlocations: [\n{ ""image/png"": ""tess-data-12345.png"" },\n{ ""application/json"": ""tess-data-12345.json"" }\n]\n}\ntess-data-12345.json = {\nx: [1,2,3,4,5],\ny: [6,7,8,9,0]\n}\n```\nIf we were to add to the ""guess the Subject Viewer"" logic by stating that _""any\nSubject that has a JSON, and that JSON has x-y coordinates, should use the Light\nCurve Viewer"",_ then we'd have trouble if, say, a future project needs similar\nJSONs with x-y coordinates for a Map Viewer or a Line Graph Viewer, etc.\nSide note:\n- The Light Curve Viewer can still also serve as a generic Scatterplot Viewer\ngiven proper tweaks - this should be marked for future dev so JSON data with\ngeneric x-y data can be ""guessed"".\n\n## Decision\n","The ""Choose a Subject Viewer"" logic (again, see\n`lib-classifier/src/store/Subject.js`, `get viewer()`) will _first_ try to check\nif there's a **Workflow Configuration** stating the preferred Subject Viewer.\nIf this specific configuration does not exist, the logic will _continue_ to\n""guess"" the correct Subject Viewer to use, as per the current system.\n```\n//Example of a workflow.configuration for TESS project:\n{\nsubject_viewer: ""lightcurveviewer""\n}\n```\nWe've also decided _not_ to specify the ""Custom/Specific Subject Viewer"" logic\nwithin the _Subject_ itself, since it'll make the Subjects more complicated and\nmaintenance troublesome. (Compare changing the config field of one Workflow vs\nupdating the metadata or JSON of a million Subjects.)\ni.e. we should **not** do something like:\n```\ntess-data-12345.json = {\ntype: 'lightcurve',\nx: [1,2,3,4,5],\ny: [6,7,8,9,0]\n}\n```\n","**Decision:** We will create a new Subject Viewer selection system that will use a combination of heuristics and project-specific configuration to determine which Subject Viewer is used for a given Subject.\n\n**Rationale:** This approach will allow us to handle the majority of projects with a simple set of heuristics, while also providing the flexibility to handle more complex projects with project-specific configuration. It will also allow us to add new Subject Viewers in the future without having to rewrite the guess logic.\n\n**Implementation:**\n\n* Create a new `SubjectViewerSelector` class.\n* Move the existing guess logic from `Subject` to `SubjectViewerSelector`.\n* Add a new `projectConfig` property to `SubjectViewerSelector`. This property will be an object that contains project-specific configuration for Subject Viewer selection.\n* Create a new `configureSubjectViewerSelector` function that takes a projectConfig object as input and configures the SubjectViewerSelector accordingly.\n* Call `configureSubjectViewerSelector` with the projectConfig object for each project.\n* When selecting a Subject Viewer for a Subject, the SubjectViewerSelector will first apply the heuristics. If the heuristics do not result in a unique selection, the SubjectViewerSelector will then consult the projectConfig object."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMaking changes to drawing tools in production - specifically, to the structure of the data they output - is an occasionally necessary evil. Unfortunately, there is currently no way to link a marking tool, whether by version, commit or whatever, to its output in a classification.\nThis means that when changes happen to the tools, we need to be careful to communicate those changes to project managers, since there is every chance that they will break their data pipelines. And in the absence of a schema for the tools' output, these changes have to be handled manually in code by project managers.\n\n## Decision\n","We include a schema for each marking tool used as part of the classification object submitted to the API.\nThe schema will be written in JSON, and will live alongside to the marking tool code. It should describe the expected data structure of a marking tool.\n### Example\nFor a point tool with the following example output:\n```json\n{\n""x"": 152.96875,\n""y"": 164\n}\n```\nThe schema could look like:\n```json\n{\n""$schema"": ""http://json-schema.org/draft-06/schema#"",\n""$id"": ""https://zooniverse.org/schemas/tools/point.schema.json"",\n""type"": ""object"",\n""title"": ""Point"",\n""description"": ""A single point on the subject"",\n""properties"": {\n""x"": {\n""description"": ""X value of the point, where `0` is on the left"",\n""type"": ""number""\n},\n""y"": {\n""description"": ""Y value of the point, where `0` is at the top"",\n""type"": ""number""\n}\n},\n""required"": [\n""x"",\n""y""\n]\n}\n```\n### Alternatives\nWe could version the schema, and simply reference it within the classification by its URL. For example, the point tool, instead of including the entire schema in the classification, has a reference to `https://zooniverse.org/schemas/tools/point.schema.v1.json`. A change to the point tool output then becomes a new version number.\nThis would result in smaller classification object sizes, although forgoes the convenience of having a self-contained classification object.\n",Implement a schema for drawing tool output that can be versioned and queried.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA drawing mark's sub-task is designed to support volunteers answering additional questions for each drawing mark annotation. It allows single choice or multiple choice question task, text task, dropdown task, and slider task.\n### Annotation JSON structure\nThe current sub-task annotation JSON structure is:\n```json\n// a point with sub-task consisting of a question task and a dropdown task\n{\n""annotations"": [\n{\n""task"": ""T0"",\n""value"": [\n{\n""frame"": 0,\n""tool"": 0,\n""x"": 452.18341064453125,\n""y"": 202.87478637695312,\n""details"": [\n{""value"": 0},\n{""value"": [\n{""value"": ""option-1""},\n{""value"": ""option-2""},\n{""value"": null}\n]}\n]\n},\n{\n""frame"": 0,\n""tool"": 0,\n""x"": 374.23454574576868,\n""y"": 455.23453656547428,\n""details"": [\n{""value"": 1},\n{""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]}\n]\n},\n{\n""frame"": 0,\n""tool"": 1,\n""x"": 404.61279296875,\n""y"": 583.4398803710938,\n""details"": [\n{""value"": 1},\n{""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]}\n]\n}\n]\n}\n]\n}\n```\nThe annotation structure for the sub-task, under `details`, has a few issues because it solely relies on an array index to relate back to the original sub-task. This makes it difficult to make downstream analysis and aggregation scripts. The aggregation code now has to parse the details array and make a ""mock annotation"" of the correct structure to be passed along to the next reducer.\n### Sub-task UI\nThe sub-task UI positioned itself fixed below relative to the position of the mark. Notably transcription projects have commented that this interferes with being able to transcribe successfully since the dialog may cover up part of the subject and cannot be moved without moving the drawing mark.\n\n## Decision\n","For initial support, we will support the single and multiple choice question tasks and the text task in the sub-task. Slider task may be deprecated and dropdown task may be changing in ways we do not have a plan for yet, so they can be supported later if it makes sense to add them.\n### Annotation JSON structure\nThe annotations in the details array will be updated to be an object that just contains a reference to the sub-task's unique identifier. The task annotation itself will be stored in the classification's annotations array flattened.\nThe main benefit of this reorganization will be with downstream analysis and aggregation. When aggregating drawn shapes the first step is clustering. Once the clusters are found the subtasks need to be aggregated within each cluster. This will be easier to do if the structure of each subtask annotation is the same as if that task was asked on its own. The code can just take all subtask annotations within a cluster and just pass it to the reducer as if it is a list of main task annotations without having to reshape them.\nAn addition of `markIndex` is being added for the sub-task annotations for the purpose of having an identifier relating it back to the parent drawing task annotation value array which represents marks.\nAn example of the new sub-task annotation JSON structure at classification submission:\n```json\n{\n""annotations"": [\n{\n""task"": ""T0"",\n""taskType"": ""drawing"",\n""value"": [\n{\n""frame"": 0,\n""toolIndex"": 0,\n""toolType"": ""point"",\n""x"": 452.18341064453125,\n""y"": 202.87478637695312,\n""details"": [\n{""task"": ""T0.0.0""},\n{""task"": ""T0.0.1""}\n]\n},\n{\n""frame"": 0,\n""toolIndex"": 0,\n""toolType"": ""point"",\n""x"": 374.23454574576868,\n""y"": 455.23453656547428,\n""details"": [\n{""task"": ""T0.0.0""},\n{""task"": ""T0.0.1""}\n]\n},\n{\n""frame"": 0,\n""toolIndex"": 1,\n""toolType"": ""point"",\n""x"": 404.61279296875,\n""y"": 583.4398803710938,\n""details"": [\n{""task"": ""T0.1.0""},\n{""task"": ""T0.1.1""}\n]\n}\n]\n},\n{\n""task"": ""T0.0.0"",\n""taskType"": ""single"",\n""markIndex"": 0,\n""value"": 0\n},\n{\n""task"": ""T0.0.1"",\n""taskType"": ""dropdown"",\n""markIndex"": 0,\n""value"": [\n{""value"": ""option-1""},\n{""value"": ""option-2""},\n{""value"": null}\n]\n},\n{\n""task"": ""T0.0.0"",\n""taskType"": ""single"",\n""markIndex"": 1,\n""value"": 1\n},\n{\n""task"": ""T0.0.1"",\n""taskType"": ""dropdown"",\n""markIndex"": 1,\n""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]\n},\n{\n""task"": ""T0.1.0"",\n""markIndex"": 2,\n""taskType"": ""single"",\n""value"": 1\n},\n{\n""task"": ""T0.1.1"",\n""markIndex"": 2,\n""taskType"": ""dropdown"",\n""value"": [\n{""value"": ""option-3""},\n{""value"": ""option-4""},\n{""value"": ""option-5""}\n]\n}\n],\n""metadata"": {\n""classifier_version"": ""2.0""\n}\n}\n```\nThe sub-task identifiers follow a convention of `TASK_KEY.TOOL_INDEX.DETAILS_INDEX`.\nNote that this is the structure at classification submission. The classifier's internal store models may have differences for the purposes of keeping track of in-progress annotations and marks being made.\n### Drawing sub-task UI\nThe UI will change to adopt the design of Anti-Slavery Manuscripts (ASM) with this [generalized design](https://projects.invisionapp.com/d/main#/console/12923997/396381420/preview). It will be a pseudo-modal, but with a few notable differences from a true modal:\n- The initial position will be near the associated mark made\n- Interactions will be allowed with the image toolbar to allow zoom, rotate, as well as opening of the tutorial, field guide, and task help. Submission of the classification should not be allowed.\n- If the sub-tasks are required, the modal should not be closeable until the required annotations are made or the mark is deleted if cancelled\n- The dialog can be moved and resized\nTo support movability and resizing, we will leverage [react-rnd](https://github.com/bokuweb/react-rnd) which is the same library ASM used. Grommet's `Layer` cannot be used since it is intended for actual modal or side panel use and cannot be arbitrarily positioned or moved.\n","1. **Change the annotation JSON structure:**\n   - Add a `subTask` field to each drawing mark annotation.\n   - The `subTask` field will contain an array of objects, each representing a sub-task.\n   - Each sub-task object will have a `type` field (e.g., ""question"", ""dropdown"", ""slider""), a `value` field, and an `options` field (if applicable).\n\n2. **Update the sub-task UI:**\n   - Make the sub-task dialog resizable and movable.\n   - Allow users to position the sub-task dialog anywhere on the screen."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor Engaging Crowds, we wanted to support workflows with both branching steps (a single answer question leads to different tasks, depending on the selected answer) and recursive steps (a given task can be returned to and annotated multiple times for a single subject.) Volunteers should be able to move backward and forward through these workflows without losing any work they'd already done.\n\n## Decision\n","We solved this problem by adding an undo/redo history stack to the classifier task area. [#2050](https://github.com/zooniverse/front-end-monorepo/pull/2050) added a global history stack (`store.annotatedSteps`). [#2520](https://github.com/zooniverse/front-end-monorepo/pull/2520) refines this by moving from a global history manager to individual history management for each subject (`subject.stepHistory`.)\nHistory is managed by the MST middleware [`UndoManager`](https://github.com/mobxjs/mobx-state-tree/blob/master/packages/mst-middlewares/README.md#undomanager), which records snapshots of a single subtree in the store. Each history item is a single step key and one or more annotation IDs, representing the current state of the task area. 'Back' loads the previous step key and annotation IDs from history. 'Next' loads the next step key and annotation IDs, or creates a new history item if one doesn't exist already. Going back then changing branches, for a branching workflow, clears any existing redo history and starts a fresh history from the current point.\n","We decided to use a tree data structure to represent the workflow, with each node representing a step in the workflow. Each node can have multiple children nodes, which represent the different possible paths that the workflow can take. This allows us to easily support branching steps, as we can simply add a new child node to the current node to represent the new path. We can also easily support recursive steps, as we can allow a node to be its own child.\n\nTo prevent volunteers from losing work, we decided to save the state of the workflow each time a volunteer moves to a new node. This way, if the volunteer needs to move back to a previous node, they will not lose any of the work they have already done."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen we launched the NextJS apps, a single domain could only host one NextJS app. We gave each app its own subdomain: `fe-content-pages.zooniverse.org` and `fe-projects.zooniverse.org` then proxied URLs from `www.zooniverse.org` to those domains. Next data requests, on the `/_next` URL, are proxied from `www.zooniverse.org/_next` to `fe-project.zooniverse.org/_next`. We deliberately broke the content pages app, in favour of supporting projects.\n[Next 9.5](https://nextjs.org/blog/next-9-5), in July 2020, added support for multiple apps running on the same domain via the [`basePath`](https://nextjs.org/blog/next-9-5#customizable-base-path) config setting.\n\n## Decision\n",Set base paths of `/about` and `/projects` for the content pages app and project app respectively.\nhttps://github.com/zooniverse/front-end-monorepo/pull/2519\n,"**Migrate NextJS apps to the same domain, using the `basePath` config setting.**\n\n**Benefit:**\n\n* Eliminates the need for separate subdomains, simplifying the architecture and URL structure.\n\n**Considerations:**\n\n* Verify that NextJS 9.5 is supported in the existing environment.\n* Implement the `basePath` config setting for each app to ensure correct routing and prevent conflicts.\n* Test thoroughly to ensure that all functionality is working as expected after the migration."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn PFE, there are a couple of different ways workflows are routed:\n- Navigating to `/classify`, the user sees either the default workflow for that project, a random active workflow, or the workflow stored in project preferences for logged-in volunteers.\n- Navigating to `/classify?workflow=[workflow_id]` shows the workflow determined by the query parameter when the ""Use workflow query"" feature is enabled for the project, or if the user has the correct role (owner, collaborator, tester, or admin).\nThis is not optimal for several reasons. Users cannot bookmark specific workflows unless they use the query parameter; there can be uncertainty over which actual workflow is being shown (particularly as the workflow name isn't shown in the UI); different workflows can be shown at the same URL at different times; and ultimately, workflows are static resources that should be routed to - that's what URLs are for.\nAdditionally, [we currently have a requirement for CSSI CitSci to be able to route to specific subjects](https://github.com/zooniverse/front-end-monorepo/issues/806#issuecomment-495685027). It's conceivable that a project will have a requirement for routing to a specific subject set linked to a workflow as well.\n\n## Decision\n","We adopt the following URL structure:\n```\n/projects/[owner]/[project]/classify/workflow/:workflow-id/subject-set/:subject-set-id/subject/:subject-id\n```\nThis would be facilitated by the [dynamic routing feature in Next.js 9](https://github.com/zeit/next.js/#dynamic-routing) (see [#1071](https://github.com/zooniverse/front-end-monorepo/pull/1071)).\nWorkflow links from the home page would route directly to their workflow-specific URLs.\n### `/classify` behaviour\nWhen navigating to `/classify`, the page should redirect the user to the correct workflow URL in the following priority:\n1. Workflow ID set in user project preferences by the user\n1. Workflow ID set in user project preferences by the project\n1. Project default workflow\n1. Random active selection\nThe new URL that we redirect the user to should replace the entry for `/classify` in the browser history.\n### Error handling\nWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in `getInitialProps`, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, `404`; incorrect permissions get a `401`/`403` (depending on login status) etc.\n","**Decision:** Workflows should be routable by URL by default for all projects.\n\n**Rationale:**\n\n* **Consistency:** This approach provides a consistent way to route to workflows for all projects, regardless of project settings or user permissions.\n* **Bookmarkability:** URLs can be bookmarked, allowing users to easily return to specific workflows.\n* **Transparency:** Users will always know which workflow they are viewing, as the workflow name will be displayed in the URL.\n* **Extensibility:** This approach allows for future requirements to route to specific subjects or other workflow attributes.\n\n**Details:**\n\n* Each workflow will have its own unique URL, following the format:\n    * `/classify/[workflow_id]`\n* When a user navigates to `/classify`, they will be redirected to the default workflow for the project.\n* The ""Use workflow query"" feature will be discontinued."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use [Sentry](https://sentry.io) to provide centralised JavaScript error logging for the project and content pages apps. Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\n\n## Decision\n","The monorepo will be versioned in Sentry, using the git commit SHA to version a release. New releases are deployed to staging on each push to master. A release is finalised and deployed to production when the production-release tag is updated to point to that release. [#1599](https://github.com/zooniverse/front-end-monorepo/pull/1599) and [#1601](https://github.com/zooniverse/front-end-monorepo/pull/1601) implement this using GitHub actions.\n",**Decision:** \n\nContinue using Sentry for centralised JavaScript error logging.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\n### [multiFrame](https://github.com/zooniverse/front-end-monorepo/tree/master/packages/lib-classifier/src/components/Classifier/components/SubjectViewer/components/MultiFrameViewer)\nA workflow using the multi-frame subject viewer might have a preference regarding:\n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https://www.zooniverse.org/projects/aprajita/space-warps-hsc/classify) or [Power to the People](https://www.zooniverse.org/projects/alycialeonard/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (`multi_image_clone_markers` [in PFE](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/classifier/tasks/drawing/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.\n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https://www.zooniverse.org/projects/zooniverse/wildcam-gorongosa/classify) or [Backyard Worlds](https://www.zooniverse.org/projects/marckuchner/backyard-worlds-planet-9/classify) (in flipbook mode, not separate frames)\n### [subjectGroup](https://github.com/zooniverse/front-end-monorepo/tree/master/packages/lib-classifier/src/components/Classifier/components/SubjectViewer/components/SubjectGroupViewer)\n- A workflow using the subject-group subject viewer might want to define the subject cell width, height, or style, or the subject viewer grid columns or grid rows.\n\n## Decision\n","Subject viewer configuration is an object stored in `workflow.configuration.subject_viewer_config` and is structured as follows for the subject viewers noted:\n`multiFrame`:\n```javascript\n{\nfilter_marks_per_frame: <boolean>  // replaces multi_image_clone_markers in PFE\npositioning: <enumerable> // includes pan, zoom, and rotation, enumerable i.e. ""maintain"", ""reset""\n}\n```\n`subjectGroup`:\n```javascript\n{\ncell_width: <number of pixels>\ncell_height: <number of pixels>\ncell_style: { [CSS property]: <CSS property value> }\ngrid_columns: <number>\ngrid_rows: <number>\n}\n```\nSubject viewers to define the configuration object in the related subject viewer README, if applicable.\nThe ScatterPlotViewer and BarChartViewer accept a configuration object directly in the JSON structure of the subject data to support variability in the display settings including the plot labels since it's possible this might vary per subject. The `subject_viewer_config` object should only be used for  configuration options that apply to all subjects linked to the workflow.\n","Workflow creators will be able to configure the following properties for the multi-frame subject viewer:\n- marks per frame: marks filtered per frame (like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page) or marks persist between frames (like [Space Warps](https://www.zooniverse.org/projects/aprajita/space-warps-hsc/classify) or [Power to the People](https://www.zooniverse.org/projects/alycialeonard/power-to-the-people))\n- positioning: pan, zoom, and rotation reset per frame (like a transcription workflow where each frame represents a unique page to transcribe) or pan, zoom, and rotation maintained between frames (like [Wildcam Gorongosa](https://www.zooniverse.org/projects/zooniverse/wildcam-gorongosa/classify) or [Backyard Worlds](https://www.zooniverse.org/projects/marckuchner/backyard-worlds-planet-9/classify) (in flipbook mode, not separate frames))\nWorkflow creators will be able to configure the following properties for the subject-group subject viewer:\n- subject cell width, height, or style\n- subject viewer grid columns or grid rows"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\n\n## Decision\n","We will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\n","To ensure the quality and consistency of our codebase, we establish the following requirements for all code submissions:\n\n* **Code Style:** All code must adhere to our established coding conventions, as outlined in our style guide.\n* **Modular Design:** Contributions should be designed to be modular and easily integrated into our existing codebase.\n* **Unit Tests:** All new features and changes should be accompanied by comprehensive unit tests demonstrating correct functionality.\n* **Documentation:** All contributed code must be well-documented, including clear explanations of the purpose and implementation of the code.\n\nTo ensure a smooth and efficient code acceptance process, we establish the following workflow:\n\n* **Code Review:** All submitted code will undergo a thorough code review by designated team members to assess its compliance with our requirements.\n* **Quality Assurance:** Accepted code will be subjected to automated quality assurance checks, including static analysis and unit tests, to ensure its stability and reliability.\n* **Testing:** The contributed code will be integrated into our build and testing pipelines to verify its compatibility with our existing codebase.\n* **Acceptance:** Once the code has passed all reviews and testing, it will be正式 accepted into the project"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIMLS funded us to experiment with transcription tools. The project [Anti-Slavery Manuscripts](https://www.antislaverymanuscripts.org/classify) has a workflow where volunteers transcribe lines of text first by marking the line, then adding the transcription in a sub-task. This task is slightly unique in that:\n- Incomplete classifications can be submitted\n- Previous transcriptions from caesar are loaded and presented as an option in the sub-task\n- volunteers can select the previous transcription, edit it, and submit a new transcription\n- lines with previous transcriptions or retired lines are displayed visually by color\n\n## Decision\n","We will be porting the ASM functionality to the main classifier as a new transcription task. The task will be composite of:\n- A drawing line task for transcription that is created by two pointer down and up events to mark the points of the line.\n- the starting point and ending point will have visual indicators communicating the direction of creation.\n- the line color will indicate the current status of completion\n- completed lines cannot be edited\n- Note that ASM leveraged a hacked polygon tool to do this, but we should have a line tool variant specifically for this instead. Downstream aggregation can leverage polygon type aggregations if it fits.\n- A sub-task will display once the transcription line mark is made\n- a text input will display suggestions from previous caesar aggregations.\n- The suggestions can be selected and inserted as an editable value in the text input\n- Grommet's [TextInput](https://storybook.grommet.io/?path=/story/textinput--suggestions) with the suggestions prop will be utilized so that it can be styleable with our theme.\n- sub-task will not be a modal, but a movable div linked to the currently selected line mark\n[More detailed user stores are in this google doc](https://docs.google.com/document/d/16abI-wkRlEXsWgACfFQVqwO76aEopohIjQiRfNQKWiw/edit)\n",Use the classification task type with a subtask workflow to allow incomplete classifications and the ability to submit new classifications based on previous classifications.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\n- add new code in three places:\n- [task views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks).\n- [task models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/tasks).\n- [annotation models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/annotations).\n- import the new modules by name in several places, and register them:\n- [registered views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks/helpers/getTaskComponent.js).\n- [import tasks models for workflow steps](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/WorkflowStepStore.js#L5-L18).\n- [import all annotations to the classification model](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/Classification.js#L3).\n- [register annotations with the classifications store](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/ClassificationStore.js#L111-L120).\nIt was easy to forget one of these steps and a lot of this could be automated in code.\n\n## Decision\n","- Keep all the code together. Store task views and models next to each other in the filesystem. (#1212)\n- Import named modules to a registry object (or similar) then load them in to other code from that register. (#1212)\n- Delegate responsibility from the classification to individual tasks. (#1228)\n### Implementation\n- Task code was moved to `lib-classifier/src/plugins/tasks`. Each task has its own directory, with these subdirectories:\n- _components_: React components to render the task.\n- _models_: MobX State Tree models for the task. One Task model and one Annotation model.\n- a _taskRegistry_ object was added, which is described in the [tasks README](https://github.com/zooniverse/front-end-monorepo/blob/master/packages/lib-classifier/src/plugins/tasks/readme.md).\n- Responsibility for creating new annotations was removed from the classifications store, removing the need for the classifications store to know about different types of tasks and how to create an annotation for each. New methods were added to the task models to delegate responsibility and make tasks more flexible:\n- _task.createAnnotation()_ creates a new annotation of the correct type for a specific task.\n- _task.defaultAnnotation_ (read-only) returns the default annotation for a specific task.\n",Create a plugin system for defining and loading new task types.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Solar Jets project requires a TemporalRotatingRectangle drawing tool for volunteers to capture the width, height, angle and time properties of a solar jet. At the time of this writing, the `front-end-monorepo` has a standard Rectangle drawing tool which gives the width and height properties. Based on the standard Rectangle drawing tool, we want to create a RotatingRectangle drawing tool and a TemporalRotatingRectangle drawing tool.\nThe RotatingRectangle drawing tool is a common tool used in the `Panoptes-Front-End` code base and eventually needs to be written in the `front-end-monorepo`. This is a good opportunity to complete this task.\nThe idea is to create two new rectangle drawing tools based off the standard Rectangle drawing tool. Once complete, the `front-end-monorepo` will have the following rectangle drawing tools:\n**Rectangle**\n```json\n{\n""details"": array,\n""frame"": number,\n""height"": number,\n""toolIndex"": number,\n""toolType"": string,\n""width"": number,\n""x_center"": number,\n""Y_center"": number\n}\n```\n**RotatingRectangle**\n\*everything the `Rectangle` tool has plus this addition:\n```json\n{\n""angle"": number\n}\n```\n**NOTE:** angle is Degrees (-180, 180)\n- positive angles rotate from +x to +y (a right-handed coordinate system)\n- rotation is about the center of the rectangle\n**TemporalRotatingRectangle**\n\*everything the `RotatingRectangle` tool has plus this addition:\n```json\n{\n""displayTime"": string\n}\n```\n**NOTE:** displayTime will be formatted as:\n- If minutes exist: mm:ss:ms\n- If minutes do not exist: ss:ms\nms (milli-seconds will be to 3 decimal places)\n**Temporal Tools**\nA temporal tool will include a time property. This time property refers to the point in time on a video file when the drawing tool is created.\nFor example:\nIf a video is 4 seconds long. A user plays the video for 2.0 seconds and draws a rectangle, the rectangle will have a displayTime equal to 2:000. The rectangle will only display from the 2:000 timestamp to the end of the video.\nThe Solar Jets project will have mp4 video files as subjects in the new classifier in the `front-end monorepo` (FEM). Using a Temporal drawing tool will allow volunteers to identify the location and time of an action within the videos.\n\n## Decision\n","We will develop a `RotatingRectangle` and `TemporalRotatingRectangle` in the FEM.\nThe first step will be to extend the `Rectangle` to create a new `RotatingRectangle` adding only the functionality to allow the rectangle to rotate which will add the `angle` property to the model.\nLastly, we will extend the `RotatingRectangle` to create a new `TemporalRotatingRectangle` adding the `displayTime` property to the model.\nAn example of this tool extension can be seen in this PR: [PR#2099](https://github.com/zooniverse/front-end-monorepo/pull/2099)\n","Create three Rectangle drawing tools in the `front-end-monorepo`:\n**Rectangle**\n```json\n{\n""details"": array,\n""frame"": number,\n""height"": number,\n""toolIndex"": number,\n""toolType"": string,\n""width"": number,\n""x_center"": number,\n""Y_center"": number\n}\n```\n**RotatingRectangle**\n\*everything the `Rectangle` tool has plus this addition:\n```json\n{\n""angle"": number\n}\n```\n- positive angles rotate from +x to +y (a right-handed coordinate system)\n- rotation is about the center of the rectangle\n**TemporalRotatingRectangle**\n\*everything the `RotatingRectangle` tool has plus this addition:\n```json\n{\n""displayTime"": string\n}\n```\n- displayTime will be formatted as:\n- If minutes exist: mm:ss:ms\n- If minutes do not exist: ss:ms\nms (milli-seconds will be to 3 decimal places)"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFairly early on in the rebuild of the Classifier, we started using newer technologies such as [CSS Grid](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Grid_Layout), which are not supported in older browsers like Internet Explorer 11.\nAnd, while Edge is now several major versions in, we still have a percentage of users on IE11. As such, we're now at a point where we have to determine whether to drop support for legacy browsers and risk inconvenience for that segment, or invest significant time in coding and testing fallbacks.\n\n## Decision\n","We will only officially support the following browsers:\n### Desktop\n- Safari\n- Chrome\n- Firefox\n- Edge\n### Mobile\n- Safari\n- Chrome\n- Opera\nOf these, we will support the current and last two major versions.\n",It is decided to drop support for legacy browsers and risk inconvenience for that segment rather than investing significant time in coding and testing fallbacks.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current classifier was based on a few assumptions, that, while accurate at the time, became outdated as additional functionality was added:\n1. That a workflow step consisted of a single annotation action. However, project builders wanted to e.g. annotate via drawing _and_ selecting from a list of options in a single step, which gave rise to the combo task.\n1. That we would always want to show a summary. When the `hide classification summaries` tool became public, more projects actually wanted to use it than we expected.\n1. That we wouldn't need to show any information to a volunteer until the end. That changed firstly with the MS interventions experiment, and later feedback, which is shown once a workflow task is completed.\nThe combo task and drawing sub-tasks specifically create issues since the tasks array becomes a list of both tasks and references to tasks, and it's up to the code to suss out which tasks should be in the combo and take them out of the normal workflow sequence, which requires a disproportionate amount of code for what amounts to an edge case in terms of project building.\nAn example combo task with one of them as a drawing task with defined sub-tasks.\n``` javascript\n{\n""id"": ""3264"",\n""display_name"": ""Combo task test"",\n""tasks"": {\n""T0"": {\n""help"": ""task 1"",\n""type"": ""drawing"",\n""tools"": [\n{\n""type"": ""point"",\n""color"": ""#00ff00"",\n""label"": ""Tool name"",\n""details"": [\n]\n}\n],\n""instruction"": ""Task 1""\n},\n""T1"": {\n""type"": ""combo"",\n""tasks"": [\n""T0"",\n""T2""\n]\n},\n""T2"": {\n""help"": ""task 2"",\n""type"": ""drawing"",\n""tools"": [\n{\n""type"": ""point"",\n""color"": ""#ff0000"",\n""label"": ""Tool name"",\n""details"": [\n{\n""help"": """",\n""type"": ""single"",\n""answers"": [\n{\n""label"": ""Foo""\n},\n{\n""label"": ""Bar""\n}\n],\n""question"": ""What is it?"",\n""required"": ""true""\n}\n]\n}\n],\n""instruction"": ""Task 2""\n}\n},\n""first_task"": ""T1"",\n""prioritized"": false,\n""grouped"": false,\n""pairwise"": false,\n""configuration"": { }\n}\n```\n\n## Decision\n","We implement the classification process like this:\nThe classification of a subject will consist of a series of __steps__. A single step consists of a __task hook__, and a __notification hook__. A task hook consists of an __array of one or more workflow tasks__. A notification could be an __intervention__, __feedback__, a __Sugar notification__, or some other information conveyed to the volunteer.\nIn practice, this will probably mean that the current workflow store is only used to store the resources from the Panoptes API. Once the project and workflow are loaded, we will derive a store for the workflow steps and that will drive the user interface.\nThe workflow resource will need updating to support the new step structure (as discussed in [zooniverse/front-end-monorepo#123](https://github.com/zooniverse/front-end-monorepo/issues/123)):\n- `workflow.steps` will be an [ES6 Map](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map) which can be stored as a [Mobx Observable Map](https://mobx.js.org/refguide/map.html)\n- Almost all features of ES6 Maps are supported by all major browsers that we will support.\n- Each value for the key-value pairs will be an object with `taskKeys` that are set to an array of task keys, and optionally a `next` property for a step key.\n- The optionally defined next step is to support recursive workflows. The order is otherwise assumed to be the order of the steps Map:\n> The keys in Map are ordered while keys added to object are not. Thus, when iterating over it, a Map object returns keys in order of insertion.\n- Since the order can be reliably derived from the steps Map, then we can drop `workflow.first_task` from use\n- `workflow.tasks` will remain as is for backwards compatibility\n- Single question task branching will still use `next` properties in the answer object, but will be set to a step key instead of task key.\n- A step taskKeys property set to `['summary']` will load an optional summary step for the end of the classification, which shifts us to having summaries be opt-in rather than opt-out. If this is not present, then a summary will not display.\nAn example of what this could look like:\n``` javascript\n{\nid: '1',\ntasks: {\nT1: {\nanswers: [\n{ label: 'yes', next: 'S4' }, // Branching single question task\n{ label: 'no', next: 'S2' }\n],\ntype: 'single'\n},\nT2: {...},\nT3: {...},\nT4: {...}\n},\nsteps: [\n['S1', { taskKeys: ['T1'] }]\n['S2', { taskKeys: ['T2', 'T3'] }],\n['S3', { taskKeys: ['T4'], next: 'S1' }] // Recursion back to Step 1\n['S4', { taskKeys: ['summary'] }]\n]\n}\n```\n","**The current combo task implementation is no longer meeting the needs of the product. It is too complex and error-prone, and it does not support the full range of desired functionality.**\n\n**An alternative approach is to create a new type of task that can contain multiple sub-tasks. This would allow for more flexibility in creating workflow steps, and it would eliminate the need for the complex logic that is currently required to handle combo tasks.**\n\n**The new task type would be called a ""composite task"". It would consist of an array of sub-tasks, each of which could be of any type. The sub-tasks would be executed in the order in which they appear in the array.**\n\n**This approach would have several advantages over the current combo task implementation:**\n\n* **It would be more flexible.** Composite tasks could be used to create a wider range of workflow steps.\n* **It would be less complex.** The code to handle composite tasks would be simpler and more maintainable.\n* **It would be more consistent.** The behavior of composite tasks would be consistent with the behavior of other task types."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith new drawing tools being developed for the classifier, we need an API that's common to all drawing tools and marks, which can be easily extended by tool developers. This document lays out an overview of the drawing tool model and the public interfaces common to all tools and all marks.\nTo support drawing, the subject viewer also needs to support:\n- rendering a static list of marks from previous drawing task annotations.\n- interacting with pointer events to create, edit and delete new marks for the current drawing task annotation.\n\n## Decision\n","### The subject viewer\nThe subject viewer will render two components.\n- _DrawingToolMarks_ takes an array of marks from drawing task annotations in the classification and renders it as a static, read-only list.\n- _InteractionLayer_ wraps a _DrawingToolMarks_ component and adds pointer event support, so that the rendered array of marks can be edited and updated. THis component only acts on marks for the active drawing task.\nMarks created by the _InterctionLayer_ are added to a new drawing task annotation, for the current classification, when we click Next or Done to complete the current task.\nPointer event support is polyfilled in older browsers with [PEP](https://github.com/jquery/PEP).\n### The drawing model\nA drawing task has drawing tools. Each tool creates marks. On task completion, a drawing annotation is created, which is an array of all drawn marks. Each mark has a corresponding React component which renders the SVG for that particular shape.\n","**Decision:** Create a common API for all drawing tools and marks that can be easily extended by tool developers.\n\n**Rationale:**\n\nThis decision will address the need for a common API that:\n\n* Supports the development of new drawing tools by providing a consistent interface.\n* Facilitates the extension of the API by tool developers to accommodate new features or functionalities.\n* Enables the subject viewer to render static lists of marks and interact with pointer events to create, edit, and delete marks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAt the Zooniverse Team Meeting April 2018, it was decided that Panoptes Front End would be broken up into modular libraries and client-side apps. To aid in this rewrite, it was also decided that we would try out server-side rendering (SSR) since now there are several frameworks for React that make this easy. SSR has several benefits including improved load times and rendered HTML that search providers could crawl to index.\nIt was initially chosen to use a SSR framework called [Next.js](https://github.com/zeit/next.js/). Next.js is an opinionated framework that supports out of the box SSR, routing, production build compiling. In particular its API provides a method, `getInitialProps` to easily hydrate client-side app state. However, in the brief experience we have had so far:\n- It's been difficult to configure. We had to configure it to use [Grommet 2](https://v2.grommet.io/) and [styled-components](https://www.styled-components.com/) as these are not defaults.\n- It's been difficult with major version upgrades. Version 6 was released right when we started and the upgrade path took a lot of time.\n- It's had issues with [Mocha](https://mochajs.org/). This is concerning since Mocha is the most popular test framework for javascript, and our current default.\nThere is an [open issue](https://github.com/zeit/next.js/issues/1632) confirming that Next.js will be integrating React Router v4 as its routing solution. React Router v4 is not a router that we wish to use due to its significant API changes.\nIn addition, Sarah had been recently advised by fellow developers in the Chicago Javascript community that Next.js is difficult to configure which can outweigh any benefits it might provide.\nRoger and Sarah built several prototypes with other libraries, including [react-server](https://react-server.io/), [razzle](https://github.com/jaredpalmer/razzle), and [react-universally](https://github.com/ctrlplusb/react-universally). However, none of these provided the same ease-of-use as Next.js. Additionally, react-server doesn't appear to be actively maintained.\n\n## Decision\n","For the time being, we will continue to use NextJS. We're currently using v5; v6 is available, and likely to be the last version that is router-agnostic, so we can revisit this decision then.\n","After further consideration and testing, the team decided to continue using Next.js as the SSR framework for Panoptes Front End.\n\n**Reasons for Decision:**\n\n* Despite the initial challenges with configuration and upgrades, Next.js remains the most user-friendly and comprehensive SSR framework available.\n* The team has already invested significant time and effort in learning and using Next.js, and switching to a new framework would require additional resources and a steep learning curve.\n* The reported issues with Mocha are being actively addressed by the Next.js team, and the upcoming integration of React Router v4 is not a major concern for the project's requirements.\n* The ease of use and out-of-the-box features provided by Next.js outweigh the potential drawbacks, especially considering the benefits of SSR for improved load times and search engine crawling."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have identified a gap in our automated and manual testing process from the deployment of [zooniverse/front-end-monorepo#1038](https://github.com/zooniverse/front-end-monorepo/pull/1038). This pull request had been reviewed and tested manually in a development environment on the reviewer's local machine, tests passed, and the app appeared to work as intended with the added bug fix. After automatic deployment to production by merging to master, however, we received a report that Planet Hunters: TESS classification interface was no longer functioning. The classify page was returning a 404.\nWe had acknowledged previously that we had a need for staging environment deploys for the purposes of design reviews in [zooniverse/front-end-monorepo#694](https://github.com/zooniverse/front-end-monorepo/issues/694). We now have a need to have staging deployments so we can manually check that the pull request functions in a deployed, production-like environment. The next.js builds and creates files specific for the production deployment that running the app locally for development does not replicate, nor is it replicated in automated unit testing.\nInitially we were considering branch deploys for both of these cases, but in order to do this we would need to use wildcard sub-domains. At this time, [kubernetes ingress does not support wildcards](https://github.com/containous/traefik/issues/3884). Therefore, we need to devise a different solution.\n\n## Decision\n","In practice, we're going to have two kinds of pull request: one that changes a single app (e.g. new widget on project home page), and one that affects multiple apps (e.g. update to the shared component library). For PRs on a single app, we'd like to manually deploy it as a staging branch deployment so it can be tested in isolation. On merging to master, that gets deployed to staging automatically. We'd then do manual integration testing before manually deploying to production. For PRs across multiple apps, we'd test it locally before merging to master using a local Docker image setup to use Panoptes in production. Once it's deployed to staging, we'd do integration testing before manually deploying to production.\nWe're going to setup a staging deployment that matches production as closely as possible to fill the gap of the need for manual reviews to confirm that the app is functioning. To accomplish this:\n- Merging to master will be switched to deploy to staging to https://frontend.preview.zooniverse.org\n- Production deployment will now be done manually triggered by lita command on slack and using a git tag for production\n- The Jenkins file will be updated to use the git tags to determine the location of the deployment\n- Cloudfront will be configured to load the correct microservice app depending on route:\n- Both the staging (https://frontend.preview.zooniverse.org) and production domains (www.zooniverse.org) will have cloudfront configurations that will match URL traffic against rules setup in Cloudfront. The staging rules will map to the staging apps and the production rules will map to the production apps.\n- The cloudfront rules match paths on the incoming URL, i.e. `/about/team` maps to a registered service via DNS, e.g.\n+ When a `GET` request for URL `www.zooniverse.org/about/team` hits cloudfront, it maps to the `fe-content-pages.zooniverse.org` service domain.\n+ Cloudfront then proxies that request via DNS lookup to the Kubneretes (K8) ingress service\n+ The K8 ingress then looks up the registered service domain and forwards the request, in this case, to a `fe-content-pages` service pod to serve the request and respond to the client.\n- Generally staging and production would have the same behaviour mappings in Cloudfront and staging will be a place to test these mapping out before setting up in production.\nA future enhancement will be added for branch deploys for manual reviews. This can possibly be accomplished by:\n- Lita command on slack\n- Script is written to deploy to a branch\n- Helm could be used to tear down the pods after the branch is merged\n","We will create a staging environment that will be a replica of the production environment but will be accessible only via our staging sub-domain. Once this is in place, we will configure our CI/CD flow to automatically deploy any new feature branches and pull requests to the staging environment, allowing us to manually test them before merging to the master branch and deploying to production."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs of writing, Zooniverse apps are being moved from Docker Swarm to Kubernetes, but both require a working Docker image of the app for deployment. However, building separate images from start to finish duplicates a lot of effort in downloading packages, building libraries etc.\n\n## Decision\n","The [Jenkinsfile](../../Jenkinsfile) builds Docker images in two stages:\n1. Build an image containing the entire monorepo at the current commit, then install top- and package-level dependencies, and finally build production versions of the library packages. This image is [zooniverse/front-end-monorepo](https://cloud.docker.com/u/zooniverse/repository/docker/zooniverse/front-end-monorepo).\n1. Loop through all folders in the `packages` folder starting with `app-`, and build a Docker image for each one. These Docker images use the `zooniverse/front-end-monorepo` image as a base, and then run the production build and start scripts for that app. An example image is [zooniverse/fe-project](https://cloud.docker.com/u/zooniverse/repository/docker/zooniverse/fe-project).\n","**Decision:** Create a base image which we can use to kickstart the build process for both Docker Swarm and Kubernetes. This will allow us to reuse the work done in building an image (downloading dependencies, building libraries etc.) when deploying to either platform."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. We needed a solution that would allow us to search a set, via indexed subject metadata, and present the results to a volunteer.\nThe subjects table in Panoptes is too large to allow for fast querying, so we also needed to build our own indexing system for indexed sets.\n\n## Decision\n","- Project owners can flag subject metadata columns as searchable by prefixing the heading with `%` in the manifest eg. these manifest headings `subject_id,image_name_1,%origin,link,%attribution,license,#secret_description` mark `metadata.origin` and `metadata.attribution` as searchable.\n- Subject sets with indexed subjects have `metadata.indexFields` set to a list of indexed fields eg. `indexFields: 'origin,attribution'`.\n- Subject metadata for indexed sets is copied to a separate database running on [Datasette](https://datasette.io). Each set is given its own table, named by subject set ID. Datasette gives us a RESTful API out of the box, allowing us to browse and search subject data as HTML or JSON. See https://subject-set-search-api.zooniverse.org/subjects.\n- From a volunteers point-of-view, the Datasette service is used to find specific subject IDs to work on. Those IDs are then sent to the Panoptes API `/subjects/selection` endpoint, which returns those subjects, in order, for classification.\n",Implement a search system using Elasticsearch.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMarkdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https://github.com/zooniverse/markdownz), that uses  [markdown-it](https://github.com/markdown-it/markdown-it). The library markdown-it is mature and has several plug-ins available for it that we've added to markdownz as well as some of our own customizations.\nMarkdown, however, isn't totally free from being exploitable, nor is React. Markdownz relies on a React method, `dangerouslySetInnerHTML` that potentially open us to vulnerabilities (see this line: https://github.com/zooniverse/markdownz/blob/master/src/components/markdown.jsx#L99).\nNow that we've adopted Grommet as general React component library, Grommet also provides a React [markdown](https://v2.grommet.io/markdown) renderer ([code](https://github.com/grommet/grommet/blob/master/src/js/components/Markdown/Markdown.js)). Grommet's markdown component uses [markdown-to-jsx](https://github.com/probablyup/markdown-to-jsx) which instead converts markdown to React components to use instead of relying on `dangerouslySetInnerHTML`. However, after extensive evaluation, `markdown-to-jsx` does not have the plugin eco-system that we need and so we would need to rewrite a lot of customizations to get to basic parity with what we already support. This defeats the purpose of reducing the maintenance of our own code for common markdown support.\n\n## Decision\n","We will make a new `Markdownz` React component that will be a part of the Zooniverse React component library. This new component will be built using [`remark`](https://github.com/remarkjs/remark). Remark is a popular markdown rendering library with a good plugin eco-system. It is supported by Zeit, which also supports Next.js, the server-side rendering library we have decided upon.\nHere is how markdown-it's plugins will map to remark's plugins:\n|markdown-it plugin/custom plugin|remark plugin/custom plugin|notes|\n|--------------------------------|---------------------------|-----|\n|markdown-it-emoji|remark-emoji|remark-emoji does not support emoticons like `:-)` but does gemojis like `:smile:`|\n|markdown-it-sub|remark-sub-super||\n|markdown-it-sup|remark-sub-super||\n|markdown-it-footnote|built in|Remark supports this and can be enabled by passing `footnote: true` into its settings object|\n|markdown-it-imsize|N/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we have defined a custom function that will set the `width` and `height` props on the Image component if the sizing syntax is defined in the alt tag of the markup. This is in contrast to the sizing syntax originally being defined in the src markup. We do not want to modify the sanitization remark-react does on source urls, so instead we have moved support of syntax to the alt tag area of the markup|\n|markdown-it-video|deprecating|We are deprecating this because we don't want project owners embedding youtube videos with ads|\n|markdown-it-table-of-contents|remark-toc|This works instead by looking for a heading that has case insensitive `table of contents`, `toc`, or `table-of-contents`|\n|markdown-it-anchor|N/A|Remark has basic anchor support.|\n|twemoji|N/A|Do we really need to use images of Twitter's emojis? Unicode support for emojis is fairly ubitiquous now.|\n|markdown-it-html5-embed|N/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we define a custom function that returns HTML 5 video instead of an image of the src is a video mime-type or returns HTML 5 audio if the src is an audio mime-type|\n|replaceSymbols|our own fork of remark-ping|remark-ping supports our needs for doing at-mentions of users, but it is forked to also support talk hashtags and the subject mentions using `^S`|\n|relNofollow|remark-external-links|This plugin adds nofollow to absolute urls|\n|markdownNewTab|remark-external-links|remark-external-links plugin adds `target='_blank'` and nofollow, noopener, noreferrer to all absolute urls. `+tab+` in front of the url will no longer work because of the sanitization that remark-react does. It may not be a good idea to modify how the sanitization works to allow this and instead just update our users on how this works instead.|\n`remark-react` is added to parse the markdown to jsx which is inherantly safer than using `dangerouslySetInnerHTML` and allows customizing which react components get used for html elements like `markdown-to-jsx`.\n","**Decision:** Continue using markdownz for markdown rendering in Panoptes-Front-End.\n\n**Rationale:**\n\n* markdownz provides the required flexibility and customization through a mature library (markdown-it) and additional plugins.\n* Switching to Grommet's markdown component would require significant development effort to rewrite existing customizations.\n* While markdownz uses `dangerouslySetInnerHTML`, the potential security vulnerabilities are mitigated by our existing sanitization practices."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the FEM Classifier's default ""Subject Viewer sizing/fitting"" behaviour is _""fit to width, no max height""._\nThis is analogous to PFE's special ""no-max-height"" behaviour, which had to be _explicitly set._\nWhile the default ""fit to width, no max height"" behaviour is good in many cases, there can be issues for certain workflows, e.g. when the Subject is a tall image (such as a portrait photo) and the user's window is very wide, causing the bottom half of the image to be ""cut off"".\nWe need to consider how to size/fit viewers for a variety of Subjects and Subject Viewers.\n\n## Decision\n","The answer is to use **Layouts**.\n- The plan for FEM is that every workflow should be able to _set their own Layout_ (e.g. portrait, landscape, fullscreen, etc) each with their own Subject Viewer sizing/fitting behaviour.\n- The choice of Layout (presumably per workflow) should be controllable by the project owner.\n- Whether or not a specific subject viewer size configuration is supported will depend on the Layout.\nSee also:\n- The current [Layout code](../../packages/lib-classifier/src/components/Classifier/components/Layout), as of June 2021, currently only has DefaultLayout.\n","**Decision:** Change FEM Classifier's default ""Subject Viewer sizing/fitting"" behaviour to _""fit to viewport, max height <viewport height>""._\n\n**Consequences:**\n\n* **Positive:**\n    * Addresses the issue of the bottom half of tall images being ""cut off"" in wide viewports.\n    * Provides a more consistent behaviour with PFE's default ""no-max-height"" behaviour.\n    * Allows users to better control the size and fit of Subject Viewers.\n* **Negative:**\n    * May not be the optimal behaviour for all workflows.\n    * May require additional configuration for some use cases.\n* **Risks:**\n    * Potential performance issues if the viewport height is very large."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse/Panoptes-Front-End#4992] (https://github.com/zooniverse/Panoptes-Front-End/issues/4992)).\nThe default behavior in existing libraries like [JSON8](https://github.com/sonnyp/JSON8/tree/master/packages/json8#ooserialize) or MobX's [`toJS`](https://mobx.js.org/refguide/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\n\n## Decision\n","We decided to instead implement our own [type conversion utility function](https://github.com/zooniverse/front-end-monorepo/blob/master/packages/lib-classifier/src/store/utils/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\n``` js\nconst workflow = {\nid: '1',\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] // How they will be stored on Panoptes\n}\n```\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\n**A note about the use of arrays for the key-value pairs**\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\n",We decided to implement a custom serialization and deserialization function that preserves the ordering of map key-value pairs and therefore the ordering of workflow steps.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor the upcoming the upcoming TESS project (aka Planet Hunters 2019), we need  to create a special Light Curve Viewer component for showing interactive brightness curves.\nFurther information available on the [TESS Front End documentation](https://docs.google.com/document/d/1BcX4PyC2khmtC9g035G2e5I1zirZa3z9mWINkWATaPs/edit?usp=sharing).\n@rogerhutchings, @shaun.a.noordin and @srallen researched available charting libraries, particularly ones designed for seamless integration with React. Of those, prototypes were built with [Plot.ly](https://plot.ly/javascript/react/), [Victory](https://formidable.com/open-source/victory/) and [vanillaD3](https://d3js.org/). Sample data was taken from the Planet Hunters project.\nFrom these, we discovered that:\n1. Both Plot.ly and Victory suffered from slow performance, especially when using their out-of-the-box features (such as the selection tools).\n2. Plot.ly and Victory had some API limitations that made fulfilling spec requirements difficult. Plot.ly, for example, had a limited set of events available (`click` events, but not `mousemove`, `drag` etc).\n3. The out-of-the-box features were often almost perfect, but not quite, and hard to extend. Plot.ly, for example, has a range select tool, but it's designed for selecting data subsets for plotting. To create annotations, we'd have needed to use the range selections and turn them into a variable width bar chart plotted on top of the scatter plot.\n\n## Decision\n","We decided that trying to use a library to meet our fairly unusual requirements would end up causing more work than would be saved by having the basic features available. As such, the Light Curve Viewer will be built using native D3, implemented into the classifier's React components.\n",We decided to build a custom light curve viewer component instead of using an existing charting library.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the development of Panoptes.js, we came across the issue of authentication. Currently, the auth library is part of [panoptes-javascript-client](https://github.com/zooniverse/panoptes-javascript-client), and provides support for first-party login and OAuth2 (for custom front ends).\nHowever, the current client has the following drawbacks:\n- It only provides the relevant API methods and no front end components, which need to be written by the library consumer.\n- As a result, the login experience is slightly different across each project.\n- The OAuth client has been buggy in the past.\n\n## Decision\n","To write a separate auth library. This will expose:\n- the __API helpers__ for the first-party auth process, if the consumer wants/needs to use them.\n- __+a first-party auth client__ as a standalone React app. This will open an IFrame to the existing Panoptes views for sign in, sign out, registration, and profile management (password reset and account deletion)\n- __an OAuth client__, also as a standalone React app. This will wrap an existing package to provide OAuth support, such as [js-client-oauth2](https://github.com/mulesoft/js-client-oauth2), and will handle session length, provide session timeout prompts, and provide a session expiry hook or event for a consumer app (e.g. as a prompt to save the current state).\n","We decided to extract the authentication library out of Panoptes.js into a separate package, called `@zooniverse/auth-client`. This will allow us to:\n\n- Provide a consistent login experience across all projects that use the library.\n- Improve the stability of the OAuth client.\n- Make it easier to develop and maintain the library."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https://github.com/zooniverse/Panoptes-Front-End/)) has traditionally been [baked](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/locales/en.js) [into](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/lib/publications.js) the front end code.\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\nAs a result, we're considering using a content management system (CMS) that would allow us to more easily maintain our static content.\nThese are the options we have:\n1. Continue to hard code content into the front end code\n2. Create our own CMS to manage our content\n3. Install a third party CMS into one of our servers\n4. Use an online third party CMS\n\n## Decision\n","We've decided to use [Contentful](https://www.contentful.com/), an online third party CMS, to host our content. (Or at least, part of our content, such as the often-changing Publications page.) There will be two users registered: one admin account with full access to the Content Model editor _and_ content, and an editor account for day-to-day use which only has content editing access.\n**Reasons for choosing Contentful**\n- [@rogerhutchings](https://github.com/rogerhutchings) has used it for projects in the past\n- It has a free tier\n- It has a well-documented API\n- It has an excellent admin interface for editing content and models\n- It's one of the larger hosted CMS offerings, meaning it's less likely to disappear overnight\n**Pros**\n- CMS makes content management easier and updates faster, especially for non-developers.\n- External platform reduces the need for any sort of code maintenance.\n- We can run our Zooniverse content on the Free Tier (see [Note of Caution 3: Database Limits](#note-of-caution-3-database-limits))\n**Cons**\n- Reliance on external platfrom for storing data introduces the usual third-party risks. (See [Note of Caution 1: Exit Strategy](#note-of-caution-1-exit-strategy)) Possible dangers include:\n- external CMS has downtime, shutting down our content by extension.\n- external CMS changes pricing structure, forcing us out of the free tier.\n- external CMS shuts down, forcing us to find an alternative data store.\n- We need to maintain an admin account. (Shouldn't be a worry as long as we have Passbolt.)\n",Use a third-party CMS installed on one of our servers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https://github.com/zooniverse/panoptes-javascript-client/).\nWhile working on [#1306](https://github.com/zooniverse/front-end-monorepo/issues/1306), I ran into a few issues with it:\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https://12factor.net/config), or whatever you want, but that's up to the consuming app to decide.\n\n## Decision\n","Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\nThe [existing config](https://github.com/zooniverse/panoptes-javascript-client/blob/master/lib/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\n",Switch to using `next-auth` for authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\n- Annotation / classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\n\n## Decision\n","What we do not want to do:\n- Re-render on every pointer or touch event.\n- update annotation state while drawing is in progress.\n- support more than one drawing task in a step.\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\nWhat we do want to do:\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse/Panoptes-Front-End#5411\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer/observable event stream.\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan/zoom function. We have two proposed options in implementation:\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\n- Projects have requested each frame to have the same pan/zoom function, but we were unable to implement in PFE: zooniverse/Panoptes-Front-End#3465\n- Are there any cases where projects want separate pan/zoom function for each frame?\n- Have a schema, or set of schemas, describing annotations.\n","**Decision:** Refactor the drawing tools to improve performance, separation of concerns, API simplicity, and payload consistency.\n\n**Rationale:**\n\nThe current implementation of the drawing tools on PFE has several limitations, including:\n\n* Unnecessary re-rendering of the DOM due to frequent updates of the classification annotation\n* Lack of clear separation of concerns between components and stores, making it difficult to debug and add new features\n* Complex API that involves exposing static methods to be called by parent components\n* Inconsistent payload formats for annotation and classification data\n\nBy refactoring the drawing tools to address these limitations, we can improve the performance and maintainability of PFE. The refactored implementation should:\n\n* Update the classification annotation only when necessary, reducing DOM re-rendering\n* Clearly separate concerns between components and stores, making it easier to debug and extend the tools\n* Provide a simpler API that does not require exposing static methods\n* Define consistent payload formats for annotation and classification data"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe plan to implement simple feedback for the survey task. Simple survey task feedback will compare target to actual choice(s), excluding any comparison of choice question answers, multiple selection of the same choice, or any other aspect of the survey task annotation.\nCurrently in PFE, the only feedback related to the survey task is for the Gravity Spy project, and is referred to as ""Gravity Spy Gold Standard"". Gravity Spy Gold Standard is a PFE experimental feature (per workflow configuration object) that checks a subject's metadata for `'#Type' === 'Gold'` and provides feedback after classification completion with the [`gs-gold-standard-summary`](https://github.com/zooniverse/Panoptes-Front-End/blob/master/app/classifier/gs-gold-standard-summary.jsx). Survey task annotation values are compared to subject metadata `'#Label'` values. A message is then shown accordingly (i.e. success, failure, or special message per subject metadata `'#post_classification_feedback'`).\n\n## Decision\n","To implement a simple survey task feedback strategy within [the existing feedback framework](https://github.com/zooniverse/front-end-monorepo/tree/master/packages/lib-classifier/src/store/feedback/strategies). The simple survey task feedback will be titled ""Survey: Simple"". The simple survey task feedback will presume the annotation has one annotation value per survey task choice. The subject metadata will include:\n- #feedback_N_id (required) - ID of the corresponding workflow task rule.\n- #feedback_N_choiceIds (required) - comma separated target choice ID(s) (i.e. ""BLIP"" or ""AARDVARK,ELEPHANT,ZEBRA""). A choice's ID is determined at survey task creation and can be viewed in in the Project Builder from the relevant workflow and survey task editor *Raw task data* section or from the workflow data export. The choice ID is the key for the choice object. For survey tasks created since 2017, the choice ID is the choice name per the provided ""Choices"" CSV with non-word characters removed and all uppercase (`[choice name per CSV].replace(/\W/g, '').toUpperCase()`).\n- #feedback_N_successMessage (optional) - message to show when the target is correctly annotated. Overrides the default success message set on the workflow task rule.\n- #feedback_N_failureMessage (optional) - message to show when the target is incorrectly annotated. Overrides the default failure message set on the workflow task rule.\nThe target choice ID(s) provided must match the choice IDs in the annotation values exactly for the annotation to be considered a success. Any missing target choice or any additional annotation choice will result in failure.\nFeedback for annotation value (choice specific question) answers and multiple annotation values of the same choice will be addressed with additional survey task feedback strategies.\n","Implement simple feedback mechanism for the survey task. This feedback will include comparing target to actual choice(s). Any comparison of choice question answers, multiple selection of the same choice, or any other aspect of the survey task annotation will be excluded."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Engaging Crowds project (UK collaboration with The National Archives, Royal Museum Greenwich, Royal Botanical Gardens Edinburgh) involves building an indexing tool to examine levels of user engagement and agency. The indexing tool will be built on the new classifier in the front-end monorepo (FEM). This tool will allow volunteers to select a subject-set or a subject to classify on. All 3 Project Builder projects being built to test the use of the indexing tool will require Dropdown tasks. The Dropdown task is not currently in the Front-end monorepo. We have two options to support the requirements  of these projects: using the existing dropdown task available in Panoptes-Front-End (PFE), the legacy single-page front-end app and build the new indexing tool in the classifier in the FEM or build both the dropdown task and the indexing tool in the FEM. Building the dropdown task in the new classifier in the FEM is something we need to do eventually anyway and we have the opportunity to evaluate how the existing dropdown task functions and how we may want to change it.\nKnown issues for the dropdown task in PFE include:\n- Long selection lists, particularly if there are multiple for cascading select options, can create massive task objects on the workflow resource resulting in slow loading and browser performance and slow exports.\n- The annotations are machine readable unique identifier strings to support cascading dropdowns. Machine readable annotations make analyzing the post-classification extraction and aggregation more complicated, particularly because of workflow versioning and translations. Caesar does not store the workflow task contents and so project owners have to reference the original workflow task by version in an export to get meaningful aggregations for actual study.\n- The dropdown task also allows for user submitted values which essentially adds in a text task into the dropdown task. The annotation includes a boolean which, if false, lets the aggregation code know that this annotation should no longer aggregate as a dropdown task, but as a text task.\n### Sample task and annotation JSON from PFE dropdown task\n**PFE dropdown task structure**\n``` json\n{\n""T0"":{\n""help"":"""",\n""next"":""T1"",\n""type"":""dropdown"",\n""selects"":[\n{\n""id"":""070b610fbf5d9"",\n""title"":""Favourite colour"",\n""options"":{\n""*"":[\n{\n""label"":""Red"",\n""value"":""hashed-value-R""\n}\n]\n},\n""required"":false,\n""allowCreate"":false\n}\n],\n""instruction"":""Choose your favourite things""\n}\n}\n```\n**PFE dropdown task annotation structure**\n```json\n{\n""annotations"":[\n{\n""task"":""T0"",\n""value"":[\n{\n""value"":""hashed-value-R"",\n""option"":true\n}\n]\n}\n]\n}\n```\n**PFE dropdown task translation strings**\n```json\n{\n""tasks.T0.help"": ""Pick a colour from the menu."",\n""tasks.T0.instruction"": ""Choose your favourite things."",\n""tasks.T0.selects.0.title"": ""Favourite colour"",\n""tasks.T0.selects.0.options.*.0.label"": ""Red"",\n""tasks.T0.selects.0.options.*.1.label"": ""Blue""\n}\n```\n\n## Decision\n","We will develop a simplified dropdown task in the new classifier in the FEM. Creating a simple dropdown task contains the following functionality:\n- Limited dropdown list options of a minimum (4) and of a maximum (20) number of options (justification: if less than 4, this can be a single choice task using radio buttons)\n- No cascading, dependencies, or effect on other select inputs\n- No free-text entry on the dropdown. It will be recommended to project builders use the new [workflow steps](https://github.com/zooniverse/front-end-monorepo/blob/master/docs/arch/adr-05.md) feature to have a dropdown task and text task in a single step.\nMore complex dropdown tasks will be built in the future as separate task types based on analysis of actual usage in PFE. These will include:\n- Cascading dropdowns for\n- Locations\n- Custom (TBD)\n- Date picker\n- Possibly a text input with validation rather than dropdowns\n- Asynchronous loading long lists by text input search\n### Proposed task and annotation JSON structure examples\n**Task**\n``` json\n{\n""T0"":{\n""help"": """",\n""type"": ""dropdown-simple"",\n""options"":[\n""Red"",\n""Blue"",\n""Green"",\n""Yellow""\n],\n""required"": false,\n""allowCreate"": false,\n""instruction"": ""Choose your favourite colour""\n}\n}\n```\n**Annotation**\n```json\n{\n""annotations"": [\n{\n""task"":""T0"",\n""value"": {\n""value"": 1,\n""option"": true\n}\n}\n]\n}\n```\n","Based on the identified issues with the existing dropdown task in PFE, the better decision is to build both the dropdown task and the indexing tool in the FEM. This will allow us to address the known issues and build a more robust and maintainable solution in the long run. Additionally, building the dropdown task in the new classifier in the FEM will provide an opportunity to evaluate how the existing dropdown task functions and how we may want to change it."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need internationalization support for the new app-project, lib-classifier, and any other libraries that will have volunteer facing content. We should decide on an organizational structure that makes it easy for both translation volunteers and Zooniverse developers to maintain. Currently translation locale files exist in a single folder in projects like Panoptes-Front-End that support translations. The development convention we've adopted for the rewrite, however, would have the locale files in the same folder of each component.\n@eatyourgreens notes:\n> Volunteers have always found it confusing that they have to add new files to [zooniverse-readymade](https://github.com/zooniverse/zooniverse-readymade) and [zooniverse](https://github.com/zooniverse/Zooniverse) in order to add a new language to Penguin Watch. I'd rather avoid that for whoever manages the translations for this repo.\n\n## Decision\n","We have a few proposed options:\n- A `lib-locales` library that has all of the locale files for the strings. Could be a single file or a directory of folders and files. The library could be imported into the other libs or apps where needed.\n- Larger locale files quickly get unwieldy to edit, and hard to find errors in. Moving them into one place reduces the portability of components. We could write a couple of scripts to bundle up locale files for our translators, and split them out again afterwards [back into their component folders in each library].\n",Create one folder per component in lib-classifier and app-project for locale files. Make the locale files available to the components through Babel. Keep the single folder in projects like Panoptes-Front-End for locale files.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\n\n## Decision\n","- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\n",Allow volunteers to (easily) go backwards and forwards through the subject queue to review previous subjects within a given subject set.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCode must be organized to be compatible with:\n- Static type-checking via [mypy](https://github.com/python/mypy)\n- Runtime execution during normal usage and running tests via [pytest](https://docs.pytest.org/en/stable/)\n- Static doc generation via [sphinx-autodoc-typehints](https://github.com/agronholm/sphinx-autodoc-typehints)\nAdditionally:\n- Functions should be able to refer to any type\n- Most types depend on other types non-recursively, but some types (e.g. `SubAttribute` and `AttributeType`) do depend on each other recursively / cyclically.\n\n## Decision\n",Put types (`@dataclass(frozen=True)`) into the `_types` module\nand have all function modules depend on the `_types` module to define their inputs and outputs.\n,"Organize the code according to the following pattern:\n- Subpackage for each high-level module (e.g., files, directories, archives)\n- If no natural sub-packages are present, all modules will be placed in a single sub-package.\n- All types (classes, functions) must be explicitly added to the `__init__.py` module of each sub-package where they are used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are multiple, equally-effective ways to implement many features.  In some cases, the most\nstraightforward implementation might involve making more API calls than are strictly necessary\n(e.g. `tc.dataset.create` makes an additional call to retrieve the created dataset from the server\nto construct the returned `Dataset`).\n\n## Decision\n","The simplest and most understandably-written implementation of a feature should be prioritized over\nperformance or reducing the number of API calls.  When real performance issues are identified,\noptimization should be done on an as-needed basis.\n","**Decision:** Make the most straightforward implementation the default.\n\n**Rationale:**\n\n* Simplifies development and maintenance.\n* Minimizes the number of API calls required for common operations.\n* Conforms to the principle of least resistance, which encourages developers to choose the simplest option available.\n\n**Consequences:**\n\n* May result in additional API calls for some operations, potentially impacting performance.\n* May create a dependency on the server-side implementation of the API, which could potentially change in future releases."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\nFor this codebase, we will focus on consistent Python versions and dependency versions.\n\n## Decision\n",Manage multiple Python versions via [pyenv](https://github.com/pyenv/pyenv).\nManage dependencies via [poetry](https://python-poetry.org/).\nDefine tests via [nox](https://nox.thea.codes/en/stable/).\nRun tests in automation/CI via [Github Actions](https://github.com/features/actions).\n,Use a virtual environment to manage Python versions and dependency versions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\n\n## Decision\n",Type-check via [mypy](http://mypy-lang.org/).\n,Use a static type-checker for Python to improve code quality and reduce the number of runtime errors.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython has many ways of importing:\n```python\n# option 1: import module\n# option 1.a\nimport foo.bar.bazaar as baz\nbaz.do_the_thing()\n# option 1.b\nfrom foo.bar import bazaar as baz\nbaz.do_the_thing()\n# option 2: import value\nfrom foo.bar.bazaar import do_the_thing\ndo_the_thing()\n```\nNot to mention that each of these styles may be done with relative imports (replacing `foo.bar` with `.bar` if the `bar` package is a sibling).\nConfusingly, Option 1.a and Option 1.b are _conceptually_ the same, but mechanically there are [subtle differences](https://stackoverflow.com/questions/24807434/imports-in-init-py-and-import-as-statement/24968941#24968941).\n\n## Decision\n","Imports within `tamr_client`:\n- Must import statements for modules, classes, and exceptions\n- Must `from foo import bar` instead of `import foo.bar as bar`\n- Must not import functions directly. Instead import the containing module and use `module.function(...)`\n- Must not use relative imports. Use absolute imports instead.\n","**Import by Value for Import Simplicity**\n\n**Rationale:**\n\n* **Less confusion:** Value import (Option 2) directly imports the desired function/class, reducing ambiguity compared to Option 1 (import module).\n* **Simpler syntax:** Option 2 uses a single `from` statement without additional as clauses, making it easier to read and maintain.\n* **No subtle differences:** Option 2 avoids the subtle differences between Option 1.a and Option 1.b, simplifying code comprehension and debugging."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a reasonable tradeoff between ease-of-use and maintainability.\nSpecifically, we need composable, combinable units that can be improved independently.\n### Approach 1: Classes + Methods\nOne approach is to embrace Object-Oriented Programming (OOP) with fluent interfaces (i.e. method chaining):\n```python\nproject\n.create(...)\n.update(...)\n.delete(...)\n```\nCharacteristics:\n- Ease-of-use is maximized, but this requires each method to `return self`.\n- Also, this approach implies that if a function can be called with X different object types,\neach of those object types should have a corresponding method that applies that functionality and then `return self`.\nHow to enforce these characteristics?\nAny solution will be a tax on maintainability, as code that adheres to these characteristics will include many non-semantic lines simply going through the motions of `return self` and copying function usage into dedicated methods for each class.\n### Approach 2: Types + Functions\nAnother approach is to embrace a functional programming style: simple types and functions (no methods).\nUsage is not as terse as for OOP:\n```python\np = tc.project.create(...)\nu = tc.project.update(p, ...)\nd = tc.project.delete(p, ...)\n```\nCharacteristics:\n- Ease-of-use is not optimized, but still reasonable.\n- With tab-completion, ease-of-use is comparable to OOP.\n- Each type can be made immutable\n- Each function can be made pure\n- Functionality can be shared by calling the same function in user-land, not copying function calls in contributor-land.\n\n## Decision\n",Use `@dataclass(frozen=True)` to model types and plain Python modules and functions to capture business logic.\n,"**Approach 2: Types + Functions**\n\nAdopt a functional programming style with simple types and functions for the following reasons:\n\n* **Maintainability:** Enforces immutability and purity, reducing complexity and potential errors.\n* **Composability:** Functions allow for easier composition and combination of units.\n* **Independent improvement:** Functions can be updated independently, reducing the impact on other parts of the system.\n* **Reasonable ease-of-use:** Tab completion provides comparable convenience to method chaining, while ensuring code remains maintainable."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\nBut, we want to rearchitect this codebase as a [library of composable functions](/contributor-guide/adr/0005-composable-functions).\n\n## Decision\n",Implement rearchitected design as a new package named `tamr_client`.\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\n,Encapsulate the existing user-facing interface in a composable function. This will allow us to maintain backwards compatibility while still being able to refactor the underlying codebase.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDocumentation can take four forms:\n1. Explanation\n2. Tutorial\n3. How-to\n4. Reference\nWe need a way to author and host prosey documentation and generate reference docs based on source code.\n\n## Decision\n,"Doc compilation will be done via [sphinx](https://www.sphinx-doc.org/en/master/).\nProsey documentation (1-3) via [recommonmark](https://github.com/readthedocs/recommonmark).\nReference documentation (4) will be generated based on type annotations and docstrings via:\n- Automatic docs based on docstrings via [sphinx-autodoc](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), [sphinx-autodoc-typehints](https://github.com/agronholm/sphinx-autodoc-typehints)\n- Google-style docstrings via [napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html)\n- Hosting on [ReadTheDocs](https://readthedocs.org/) (RTD)\n- Build docs in CI and fail on errors or warnings.\n","Use Docusaurus with Sphinx plug-in.\n\nDocusaurus is a modern static site generator that can be used to author and host prosey documentation. It is built on top of React and provides a number of features out of the box, such as:\n\n* **Easy-to-use interface:** Docusaurus is designed to be easy to use, even for non-technical users.\n* **Mobile-friendly design:** Docusaurus sites are mobile-friendly, so users can easily read your documentation on any device.\n* **Search engine optimization:** Docusaurus sites are optimized for search engines, so your documentation can be easily found by users.\n\nThe Sphinx plug-in for Docusaurus allows you to generate reference documentation based on your source code. This can be helpful for providing detailed information about your codebase to developers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nInconsistent code formatting slows down development and the review process.\nCode should be linted for things like:\n- unused imports and variables\n- consistent import order\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\n\n## Decision\n","For linting, use [flake8](https://flake8.pycqa.org/en/latest/) and [flake8-import-order](https://github.com/PyCQA/flake8-import-order).\nFor formatting, use [black](https://github.com/psf/black).\n",Use a linter to automatically format and check code.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Decision:** To use an Architectural Decision Record (ADR) template to document the architectural decisions made on this project. The ADR template should include the following sections:\n\n* **Title:** A short, descriptive title for the decision.\n* **Context:** A brief description of the problem or issue that the decision addresses.\n* **Decision:** The decision that was made.\n* **Rationale:** The reasons why the decision was made.\n* **Consequences:** The potential consequences of the decision.\n* **Status:** The current status of the decision (e.g., Proposed, Approved, Implemented).\n* **References:** Any relevant documentation or resources that support the decision.\n\nThe ADR template should be used to document all significant architectural decisions made on the project. The ADRs should be stored in a central repository and reviewed regularly to ensure that they are up to date and reflect the current state of the architecture."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are 2 possible solutions within the table loading part of the Armadillo.\n- Make use of `.RData` files to store the data frames you want to use\n- Make use of `.parquet` files to store the data frame you want to use\nIn the Armadillo we have a general concept of projects. These projects represent all the data of a cohort or study specific data. e.g. It is the only level you can set permissions.\n- gecko-all\n- gecko-diabetes\n### Managing data with `.RData` files\nWithin the projects we have `.RData` files containing all the tables (R data frames) that are bound in a specific context.\n- gecko-all\n- 1_0_core_1_1.RData\n- non_rep\n- yearly_rep\n- monthly_rep\n- trimester_rep\n- 1_0_outcome_1_1_non_rep.RData\n- gecko-diabetes\n- 1_0_core_1_1.RData\n- non_rep\n- yearly_rep\n#### Make the data available\nWhen you login to the Armadillo you specify which `.RData` files you want to have available in your analysis environment.\n`folder=gecko-diabetes/1_0_core_1_1&folder=gecko-all/1_0_outcome_1_1`\nSpecifying these folders (`.RData` files) allow you to assign all the columns that are available in the `data.frame` that are in the `.RData` files.\nTechnically it loads the `RData` files in memory to be able to see which tables are available.\n#### Assigning the data\nWhen you want to use the data in a DataSHIELD analysis you need to assign the data. So when you login the data becomes available in a private environment in R. When you assign data it will be copied to the analysis environment in R.\nYou need to assign all data or parts of the data depending on the analysis you want to do. Assigning parts of the data is more efficient and analysis will become faster.\nIn the `RData` solution all the data in the private and the analysis environment is in memory. When you assign the data, it will be copied to the analysis environment. You can specify a subset of the data as well. The copy will contain only the selected variables.\n#### Advantages\n- You can use native `R` components to get the data available for your analysis\n- You can implement *resources* fairly easy in `.RData`\n#### Disadvantages\n- It uses large amounts of data per user when loading large files. Which means you can not work with a large number of users at the same time on the Armadillo.\n- Logging in takes a lot of time, because folders are `.RData` files and contain all the tables. Tables need to be loaded to be checked by the DataSHIELD package.\n### Managing data in `.parquet` files\nManaging data in `.parquet` files allows us to deal with larger data more efficiently. This changes the concepts a little within our file storage (minio). We already work with buckets as described above. `Parquet` files do not allow more than one `data.frame` in one `parquet` file. The `parquet` files are more or less substitutes for `RData` files. The logical structure is the same, however the technical structure is somewhat different.\n- gecko-all\n- 1_0_core_1_1 (minio -> folder concept)\n- non_rep.parquet\n- yearly_rep.parquet\n- monthly_rep.parquet\n- trimester_rep.parquet\n- 1_0_outcome_1_1_non_rep (minio -> folder concept)\n- gecko-diabetes\n- 1_0_core_1_1 (minio -> folder concept)\n- non_rep.parquet\n- yearly_rep.parquet\n#### Make the data available\nWhen you login you only specify the fully quelified names of the tables you want to load:\n`project/folder/table.parquet`\nFor example:\n`gecko-all/1_0-core-1_1/non-rep.parquet`\nSpecifying these projects and folders allow you to assign all the columns that are available in the `data.frames` that are in the `.parquet` files.\nThis is not loading the folders (`RData` files) in memory, but loads them when you assign the tables. At login time you only check wether you may access the projects.\n#### Assigning the data\nThe difference with the `RData` solution is that the files will not be loaded in memory initially only when the data is assigned. More specifically only the selected data will be assigned. You do not have a copy of the whole data in memory in a private environment.\n#### Storing complex structures\nAnother format to store complex structures is `rds`. The `rds` format is capable to store 1 object in 1 file. You can directly see what is in the file. With `RData` you need to load the file when you want to know what it is containing. In `rds` you see directly what the contents is.\n#### Advantages\n- You do not need to specify the folder(s) (`RData` files) or table(s) during login which makes it faster\n- You are able to delay loading the data in the R environment, which makes it possible to to login faster\n- When you specify specific columns from the dataset it is highly efficient to load the data in the R environment\n- Loading and saving parquet files is faster, so data handling actions will be more efficient.\n#### Disadvantages\n- An extra dependency in the package\n- More risk when converting that is not R-native (think of dates, number of significant bits, etc.)\n\n## Decision\n",The principle we are going implement is to store 1 object per file. Which means that for table-like object we are going to use `.parquet`and for more complex structures we are going to use `.rds` (such as *resources* used in Opal).\nWe are going to use the parquet files in the Armadillo package. As data manager you do not *have* to work with the parquet format.\n,"The decision is to use `.parquet` files to store the data frames you want to use within the Armadillo.\n\n**Reasons for the decision:**\n\n- `.parquet` files are more efficient for dealing with larger data, which is important for the Armadillo's use case.\n- `.parquet` files allow for more efficient loading and saving, which will make data handling actions more efficient.\n- `.parquet` files are faster to load than `.RData` files, which will make logging in to the Armadillo faster.\n- `.parquet` files allow for more efficient assignment of specific columns from the dataset, which will make data handling actions more efficient."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn LifeCycle we use a 2 step approach to share the data within the LifeCycle project.\n**Step 1** - Datasets containing all the harmonised data within LifeCycle\n- lc_core_gecko_1_1 - *projects in Opal, workspaces in the Armadillo*\n- 1_1_core_non_rep_1_1 - *tables in Opal, R data.frame objects in Armadillo*\n- 1_1_core_yearly_rep_1_1\n- 1_1_core_monthly_rep_1_1\n- 1_1_core_trimester_rep_1_1\n- lc_outcome_gecko_1_1 - *projects in Opal, workspaces in the Armadillo*\n- 1_1_core_non_rep_1_1 - *tables in Opal, R data.frame objects in Armadillo*\n- 1_1_core_yearly_rep_1_1\n- 1_1_core_monthly_rep_1_1\n- 1_1_core_trimester_rep_1_1\n**Step 2** - Study specific datasets which contains only the variables you want to expose\n- lc_core_gecko_1_1 - *projects in Opal, RData files in the Armadillo*\n- 1_1_core_non_rep_diabetes_1_1 - *views in Opal, R data.frame objects in Armadillo*\n- lc_core_gecko_1_1 - *projects in Opal, RData files in the Armadillo*\n- 1_1_core_non_rep_diabetes_1_1 - *views in Opal, R data.frame objects in Armadillo*\n### Versioning\nIn the current version scheme of LifeCycle we distinguish 2 versions:\n- dictionary version\n- data version\nThe scheme is as shown below:\n`y_y_#dictionary_kind#_#cohort#_x_x`.\nThe `y_y` version = the dictionary version (data model version)\nThe `x_x` version = the data version (version of the data release)\n### Accessing data\nBased upon the cohort guidelines on exposing data to researchers one way or the other is chosen to give researchers access to the data.\nIn Opal you need both datasets.\n- a dataset containing all the harmonised data\n- (if applicable) views to expose specific datasets\nThese views do not contain the data, only the representation of the variables. In Opal you can manage permissions per view, which makes it possible to put all the tables and views related to one dictionary version in one project.\n\n## Decision\n",Within the Armadillo we have the ability to nest the dictionaries. Which makes it possible to specify the dictionary version and data version on a sublevel. The structure in the Armadillo will be used in the following manner:\n- gecko-all\n- 2_1-core-1_0\n- non-rep\n- yearly-rep\n- monthly-rep\n- trimester-rep\n- 1_1-outcome-1_0\n- non-rep\n- yearly-rep\n- monthly-rep\n- weekly-rep\n- gecko-diabetes\n- 2_1-core-1_0\n- non-rep\n- yearly-rep\nWe currently do not support more than 1 level of nesting folders in a project.\nThree levels can be distinguished:\n### 1. Project level\nThis can be all the data or a study specific data. On this level we can manage permissions.\n### 2. Folder level\nThe second level contains the collection of data frames or tables you want to expose. This usually contains the version of the model and data release as well.\n### 3. Table level\nYou can interpret this level as the data level containing tables that can be queried.\n,"**Decision:** Researchers should access data via Opal datasets and projects, rather than direct access to RData files in Armadillo.\n\n**Rationale:**\n\n* **Cohort guidelines:** The cohort guidelines specify that data should be exposed to researchers through specific datasets and projects in Opal.\n* **Permission management:** Opal allows for granular permission management per view, enabling researchers to access only the data they are authorized to use.\n* **Data integrity:** Opal ensures the integrity of the data by controlling access and maintaining a consistent data model.\n* **Data lineage:** Opal provides a clear audit trail of data access and usage, which is important for reproducibility and compliance.\n* **Version control:** Opal tracks both dictionary and data versions, allowing researchers to access the correct version of the data for their study.\n* **Ease of use:** Opal provides a user-friendly interface for researchers to access data, reducing the technical burden on them."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn this project we manage the data in the Armadillo suite.\n\n## Decision\n,We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\n,"We will use a NoSQL database, such as MongoDB, to manage the data in the Armadillo suite."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n","We will use the Architectural Decision Record (ADR) template to record architectural decisions made on this project. This template provides a structured way to document the decision, its context, and the rationale behind it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\n* There are existing client libraries for the S3 API.\n* It is nontrivial to proxy large file uploads through the armadillo server.\n\n## Decision\n,The Armadillo client will be written as a high-level library on top of an existing S3 API.\n,Do not build an admin UI within the armadillo microservice
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Data management\nData managers (in LifeCycle) use R to upload their data into Opal as this stage. Data managers often create scripts to generate the (harmonised) data they need to expose for a certain collection of variables.\n### Usage Armadillo\nWe are required to upload .RData files into the Armadillo service to be used in DataSHIELD. So .RData files need to be created before it can be pushed to the service.\n### Possible solutions\nPossible solutions regarding buildframeworks are R, Python or Java. Every framework has pros and cons.\n**R**\nTo connect to the current way of working with the data in LifeCycle, R is the most integrated platform to use. You can use the Armadillo client to integrate in the existing scripts of the data managers. You do not need a lot of training to incorparate the Armadillo client in the workflow. RData is the file format Armadillo uses and R is the platform to build .RData in which makes it easier to build the RData file in. When there is a need to do basic checks in the future, you can implement them.\n**Python**\nPython can be used together in one script with the molgenis-commander. Which is usefull when you use the MOLGENIS data provider. It allowes you to automate the whole process, from extracting data from MOLGENIS to converting into the right format for the Armadillo service (if Python supports generating .RData). From there you can upload it into the Armadillo. Another advantage is that Python is a language we are more experienced to program in.\nA disadvantage is that at this point prospect customers are not used to work with python. You will need to teach them how to use Python and then how to use the client.\n\n## Decision\n",The Armadillo client will be written based on R to integrate easily in the current prospect customers infrastructure. We will look at the possibility of writing other clients in the near future when we need to add other customers as well.\n,"**Decision:** Use R as the primary build framework for creating .RData files for Armadillo.\n\n**Reasoning:**\n\n* **Integration with current data management practices:** R is closely integrated with the existing data management workflows in LifeCycle, where data managers use R scripts to generate harmonized data for Opal. This allows for a seamless transition and minimizes disruption to current processes.\n* **Simplicity and ease of use:** R is familiar to data managers and does not require extensive training to incorporate the Armadillo client into existing scripts.\n* **Direct conversion to .RData format:** RData is the native file format used by Armadillo. This eliminates the need for additional conversion steps or dependencies on third-party libraries.\n* **Future potential for data validation:** R provides a convenient platform for implementing basic data checks and validation routines as needed in the future.\n* **Alignment with customer familiarity:** While Python offers automation benefits, prospect customers are currently more familiar with using R. Choosing R reduces the training burden and improves user adoption."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nNeed to create a way for the player to interact with their character and potential with their allies such that there is choice involved in combat on the player side instead of randomly decided actions.\n\n## Decision\n,Create a child class of character to give branching functionality to player-specific characters.\n,**Decision:** Implement a turn-based combat system that allows players to select actions for their characters and allies from a set of available options.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWant to give more variation to the combat system beyond physical attacks. This will allow for reasons to create separate builds and more options for player characters and enemies to use during a fight. This will give more of a *game* element to the combat as opposite to a system based on luck alone.\n\n## Decision\n,Create spells and integrate them into the character and combat systems.\n,Implement a magic system that allows players and enemies to cast spells with varying effects.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nNeed to create a basic structure for combat involving characters that know how to fight and a middle man referee that ensures that everyone gets a turn and works as a go between to ensure that the characters interact properly. This will create a basic means for the game to function.\n\n## Decision\n,There should be a combat module specific to the characters and an overarching combat entity that handles the mediation.\n,Implement a **turn-based combat system** with a **game master (GM)** who mediates player actions and ensures fair play.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWe need a way for player characters to be generated intentionally where certain statistics can be decided on by the player instead of having a random outcome. This will give the player some opportunity to have some control over their character's strengths and weaknesses and add to the playability of the game.\n\n## Decision\n,Create a player module with user interaction to affect the starting statistics of a character.\n,"**Decision:** Implement deterministic character generation with configurable parameters.\n\n**Details:**\n\n* Allow players to specify values for key statistics, such as strength, intelligence, and agility.\n* Provide sliders or input fields for players to adjust these values within predefined ranges.\n* Provide guidance and recommendations to help players understand the impact of their choices on gameplay.\n* Ensure that the total stat points assigned remain within a reasonable limit to maintain game balance.\n* Supplement deterministic character generation with a limited number of randomly generated secondary stats to maintain some element of chance.\n\n**Rationale:**\n\n* Gives players agency in shaping their character's abilities, fostering ownership and investment.\n* Enhances gameplay by allowing players to customize their characters to suit their playstyles and strategic preferences.\n* Balances player control with game balance by allowing for configurable stats within predetermined limits.\n* Preserves a degree of randomness in secondary stats to introduce an element of uncertainty and surprise."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nWhile we have a class for items already, we need a way for them to be used during combat. This way we can add in more items that will change the flow of battle and eventually a money and shop system to add to combat variation.\n\n## Decision\n",Create a way to use items and integrate into the combat flow.\n,"We should create a separate class for combat items that will inherit from the base item class. This will allow us to add additional properties and methods specific to combat items, such as damage, healing, or status effects."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context:\nIt is desirable that we have a way for characters to be randomly generated with different statistics and abilities in order to keep combat fresh and new with an element of surprise. Creating a way to generate random character systematically will keep us from having to create specific new characters.\n\n## Decision\n,Create a random character factory that will pump out characters.\n,"**Decision:** Implement a random character generator that can create characters with varying statistics and abilities.\n\n**Rationale:**\n\n* Provides a systematic way to create random characters, reducing the need for manual creation.\n* Ensures that characters have diverse statistics and abilities, keeping combat fresh and preventing it from becoming repetitive.\n* Introduces an element of surprise by creating characters with unpredictable abilities.\n\n**Implementation Details:**\n\n* Define a set of possible statistics and abilities for characters.\n* Create a system that randomly selects values for these statistics and abilities within a specified range.\n* Implement logic to generate a unique combination of statistics and abilities for each character.\n* Allow for customization options to enable adjustments to the random generation process.\n\n**Benefits:**\n\n* Automates character creation, saving time and effort.\n* Enhances gameplay variety by introducing diverse characters.\n* Reduces predictability in combat, making battles more engaging.\n\n**Drawbacks:**\n\n* May require careful balancing to ensure that characters are not overpowered or underpowered.\n* May require additional development time to implement and maintain."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala / Play / Twirl implementations of the components provided\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\nplay-frontend implementations diverge from their “base” repositories?\n## Decision Drivers\n* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\n\n## Decision\n","* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\nGDS.\n### Positive Consequences\n* Design of case classes to follow GDS / HMRC design system means PlatUI as library maintainers do not have to create\nviewmodel structure from scratch every time\n* Adding new components can follow a clear and straightforward path\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\n### Negative Consequences\n* Feedback suggests that some developers do not find the API to be intuitive\n* Separate decisions need to be made on handling multilingual support\n* Enrichment of library needs to be done via separate Twirl helpers\n(see https://github.com/hmrc/play-frontend-hmrc/blob/main/docs/maintainers/adr/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\n","The play-frontend implementations should diverge only as much as is necessary to integrate them with the Scala / Play / Twirl ecosystem. \nIn particular, the:\n*  functionality should match the Nunjucks versions as closely as possible\n*  APIs should be as similar to the Nunjucks versions as possible (with the obvious variations necessary to reflect the differences between Scala and JavaScript)\n* tests should ensure that the Scala / Play / Twirl versions behave as closely as possible to the Nunjucks versions"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nplay-frontend-govuk relies on a webjar for [alphagov/govuk-frontend](https://www.github.com/alphagov/govuk-frontend)\npublished to www.webjars.org. This has a number of drawbacks:\n* publishing is a manual process\n* it can take many hours to complete\n* webjars.org has been down in the past and HMRC has no support arrangements with webjars.org\nThe main impact of the above is an excessive lead time for making improvements in the\nunderlying govuk-frontend library available in production via play-frontend-govuk.\nPreviously we considered self-publishing this WebJar, as we did for the hmrc-frontend WebJar (see related [ADR]((../adr/0009-self-publish-webjar.md)\n)). However, this is complicated by the fact that we do not own govuk-frontend and\nself-publishing would involve additional engineering and ongoing maintenance.\nSince v0.42.0 of [play-frontend-hmrc](https://github.com/hmrc/play-frontend-hmrc/releases/tag/v0.42.0), we have recommended teams\nuse the minified CSS and JS bundles provided as part of the now self-published `hmrc-frontend` webjar via `hmrcHead` and `hmrcScripts`.\nThese bundles include the CSS and JS from both govuk-frontend and hmrc-frontend. For teams using this approach, the only\nassets still being retrieved from the govuk-frontend webjar are the handful of icons referenced\nin `govukTemplate` and `govukHeader`. Architecturally, this split of assets between the two libraries is awkward and a potential source\nof confusion for future users and maintainers, addressing it would reduce complexity and ease maintenance.\nBearing the above in mind, should we remove the hard dependency on the govuk-frontend webjar by:\n* inlining the images into this repository, for teams directly using `govukTemplate` `govukLayout` or\n`govukHeader`\n* provide a mechanism to override the `assetsPath` parameter in\n`govukTemplate` and `govukHeader` so that `play-frontend-hmrc` can supply its own images and\n* remove the govuk-frontend webjar dependency now or at some point in the future?\n## Decision Drivers\n* The need to make improvements and upgrades to govuk-frontend\navailable in play-frontend-govuk quickly\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\nimprovements.\n* The desire to reduce boilerplate in consuming services.\n* The high number of services still referencing `lib/govuk-frontend/govuk/all.js`\n* The fact that the images have not changed for a long time (since September 2018)\n* The need to minimise the impact of breaking changes on service teams.\n* The hardship, frustration and toil the current manual publishing process is causing the team.\n\n## Decision\n","* The need to make improvements and upgrades to govuk-frontend\navailable in play-frontend-govuk quickly\n* The increasing user base of play-frontend-govuk, and accelerating demand for new features and\nimprovements.\n* The desire to reduce boilerplate in consuming services.\n* The high number of services still referencing `lib/govuk-frontend/govuk/all.js`\n* The fact that the images have not changed for a long time (since September 2018)\n* The need to minimise the impact of breaking changes on service teams.\n* The hardship, frustration and toil the current manual publishing process is causing the team.\nChosen option: Option 2, because doing so will (a) put is in a better position to eliminate the use of the webjar eventually,\n(b) allow us to make changes to play-frontend-hmrc to eliminate the `/govuk-frontend` route and (c) not introduce any\nbreaking changes.\n","Remove the dependency on govuk-frontend by inlining the images and providing a mechanism to override the assetsPath parameter in govukTemplate and govukHeader.\n\nThis will allow us to make improvements and upgrades to govuk-frontend available in play-frontend-govuk quickly, and will also reduce boilerplate in consuming services. The high number of services still referencing lib/govuk-frontend/govuk/all.js indicates that this is a widespread issue.\n\nInlining the images will also reduce the number of requests that are made to webjars.org, which will improve performance. The fact that the images have not changed for a long time means that this is a low-risk change.\n\nProviding a mechanism to override the assetsPath parameter in govukTemplate and govukHeader will allow service teams to continue to use the govuk-frontend webjar if they need to, but will also give them the flexibility to use their own images if they prefer.\n\nThis change will be made in a future release of play-frontend-govuk."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nplay-frontend-govuk is intended as a direct Scala/Twirl port of govuk-frontend that can in theory be\nused by any government department wanting a Scala/Twirl implementation of the GOV.UK design system.\nplay-frontend-hmrc includes play-frontend-govuk while adding to it HMRC/MDTP specific components and helpers. Examples include\nstandardised headers and footers, standardised HMRC content for GOV.UK\ncomponents in both English and Welsh, support for cookie consent, language switching, session timeout,\naccessibility statements and more.\nInlining play-frontend-govuk into play-frontend-hmrc has the potential to simplify our\ndevelopment and testing workflow and reduce lead times for making improvements\navailable to service teams.\nGiven we are not aware of any scenarios where it would be desirable for HMRC services to\nuse play-frontend-govuk on its own nor any other government departments using play-frontend-govuk, should we fold the\ngovukfrontend package into play-frontend-hmrc and archive the play-frontend-govuk library?\n## Decision Drivers\n* The confusion and discoverability issues that having two similarly named libraries creates for service developers.\n* The extended lead times caused by the need to publish play-frontend-hmrc every time a change is made to play-frontend-govuk.\n* The additional cognitive friction created for maintainers in deciding which library to add changes to while maintaining\nthe conceptual integrity of both.\n* The split of user and maintenance documentation between the two libraries.\n* The duplication of code and build scaffolding between the two libraries, for example, implicits and fixture generation\n* The overhead of supporting two libraries rather than one, for example, dependency upgrades or security fixes.\n* The question around whether having two separate libraries helps enforce the conceptual integrity of the uk.gov.hmrc.govukfrontend\npackage as a pure Twirl port of govuk-frontend free from HMRC/MDTP specific features.\n* The need to maintain the existing test strategy and not have this compromised by the merging of the two libraries.\n* The desirability of maintaining git commit history for the merged govukfrontend package.\n* An historical view, now superceded, that teams should be able to use play-frontend-govuk on its own.\n* The desirability of providing a Twirl port of govuk-frontend to other government departments.\n* The impact on teams currently using only play-frontend-govuk.\n* The possibility of other government departments using play-frontend-hmrc and ignoring any HMRC-specific features.\n\n## Decision\n","* The confusion and discoverability issues that having two similarly named libraries creates for service developers.\n* The extended lead times caused by the need to publish play-frontend-hmrc every time a change is made to play-frontend-govuk.\n* The additional cognitive friction created for maintainers in deciding which library to add changes to while maintaining\nthe conceptual integrity of both.\n* The split of user and maintenance documentation between the two libraries.\n* The duplication of code and build scaffolding between the two libraries, for example, implicits and fixture generation\n* The overhead of supporting two libraries rather than one, for example, dependency upgrades or security fixes.\n* The question around whether having two separate libraries helps enforce the conceptual integrity of the uk.gov.hmrc.govukfrontend\npackage as a pure Twirl port of govuk-frontend free from HMRC/MDTP specific features.\n* The need to maintain the existing test strategy and not have this compromised by the merging of the two libraries.\n* The desirability of maintaining git commit history for the merged govukfrontend package.\n* An historical view, now superceded, that teams should be able to use play-frontend-govuk on its own.\n* The desirability of providing a Twirl port of govuk-frontend to other government departments.\n* The impact on teams currently using only play-frontend-govuk.\n* The possibility of other government departments using play-frontend-hmrc and ignoring any HMRC-specific features.\nChosen option: Option 2, because overall we believe the benefits listed below vastly outweigh the negatives.\n### Positive Consequences\n* Service teams need only consult a single library repository for user documentation.\n* Teams only using play-frontend-govuk will be forced to upgrade to use play-frontend-hmrc. This should reduce friction\naround migrating to use the latest HMRC components that in turn will eventually increase the consistency and\naccessibility of HMRC services.\n* Lead times for changes are reduced because only one library needs to be published.\n* Routine library dependency upgrade overheads are reduced.\n* Fixture generation logic is no longer duplicated between two libraries\n* We are in a better position to eventually consolidate the various implicit classes and helpers currently split\nbetween the two libraries.\n* Developer onboarding is simplified – we no longer need to explain why there are two separate libraries.\n### Negative Consequences\n* Potential loss of git commit history depending on merge approach taken. We would intend to investigate this\n[approach](https://medium.com/altcampus/how-to-merge-two-or-multiple-git-repositories-into-one-9f8a5209913f), falling back\nto a link to the archived play-frontend-govuk repository if this is not possible.\n* Slight risk of unintended side effects for teams upgrading to play-frontend-hmrc from play-frontend-govuk. 16 such\nrepositories have been identified to date.\n* User documentation via the play-frontend-hmrc README will be less friendly to other government departments using\nScala/Play/Twirl. For example, they are likely to want to use `govukLayout` rather than the recommended `hmrcLayout` component.\nNo departments other than HMRC have yet been identified using Scala/Play.\n","The play-frontend-govuk library will be archived, and its package will be folded into play-frontend-hmrc."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince launching `play-frontend-hmrc` we have included deprecated static helpers which were required for Play 2.5 projects.\n## Decision Drivers\n* There is complexity involved in maintaining multiple interfaces for the same behaviour\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\n* the platform opinion is to use DI\n* soft advice from Play has been to use DI since Play 2.6 (https://www.playframework.com/documentation/2.8.x/ScalaDependencyInjection)\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\n* some teams may regard the deprecation notices as small print\n\n## Decision\n","* There is complexity involved in maintaining multiple interfaces for the same behaviour\n* these helpers have been deprecated for 2 years, they were only added originally for Play 2.5 users\n* the platform opinion is to use DI\n* soft advice from Play has been to use DI since Play 2.6 (https://www.playframework.com/documentation/2.8.x/ScalaDependencyInjection)\n* Guice DI is completely standard in the Play Framework and not new - all services will be using Guice\n* some teams may regard the deprecation notices as small print\n* we should not re-instate static helpers for Play 2.8\n* we should remove them in version 1.0.0\n* we should publish a blog article sooner rather than later that we will be removing them in 1.0.0\n### Positive Consequences\n* Our codebase becomes easier to maintain\n* We have to use DI in our tests which makes our tests more accurate to how these components will be used\n### Negative Consequences\n* Anyone who is using the static helpers will have to update - that's why we're including this in version 1.0.0\n",**Decision:** **Remove deprecated static helpers from `play-frontend-hmrc`**\n\n**Rationale:**\n\n* Maintaining multiple interfaces is unnecessarily complex.\n* The helpers have been deprecated for two years and are no longer necessary.\n* Play Framework recommends using DI instead.\n* Guice DI is the standard dependency injection framework in Play Framework and is widely used throughout the platform.\n* Removing the deprecated helpers will align with Play Framework best practices and reduce the maintenance burden.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nNearly all services running on MDTP need to include a link to their landing page in the\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\nor be able to configure it based on the path. For example, in the case of shared frontend\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\nWhen integrating with play-frontend-hmrc we want to\nminimise boilerplate for service teams while providing flexibility to cope with edge\ncases.\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\nparameter, specify the service name in the\n`service.name` message in `conf/messages`, providing any Welsh translation in `conf/messages.cy`.\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\nmechanism is awkward for services not requiring a service name – they have to override with a blank message –\nand services needing more than one are unable to use this functionality. This pattern\nassumes a 1:1 correspondence between frontend microservices and public-facing\ntax services.\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\nservices, insist on explicitly passing the service name or support some combination of the above?\n## Decision Drivers\n* Minimising of boilerplate and lines of code needed in frontend microservices\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\npublic-facing services.\n* The need to keep things simple and unsurprising\n\n## Decision\n","* Minimising of boilerplate and lines of code needed in frontend microservices\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\npublic-facing services.\n* The need to keep things simple and unsurprising\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\nbetween frontend microservices and public-facing services.\n","**Decision:** Services should override the `headerBlock` parameter to provide custom header logic and determine the service name dynamically based on the current path.\n\n**Rationale:**\n\n* This approach provides maximum flexibility for frontend microservices to handle various scenarios, including:\n    * Services with no service name\n    * Services with multiple public-facing services\n    * Services that share frontend components with other services\n\n* It allows services to tailor the header content to their specific context, ensuring that the service name and landing page link are relevant and meaningful to users.\n\n* While it may introduce some additional complexity, it promotes maintainability and reduces the need for special-casing or boilerplate code in the future. By explicitly defining the header logic, developers can have a clear understanding of how the header is rendered and make necessary adjustments as needed.\n\n**Consequences:**\n\n* Frontend microservices will need to implement custom header logic, which may require additional development effort compared to using the default `govukLayout` component.\n\n* However, this decision enables a more flexible and extensible architecture that can accommodate a wider range of use cases and future requirements."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n\n## Decision\n","* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n",Implement an endpoint on [hmrc-ssoagent](https://github.com/hmrc/sso-agent) that allows services to interrogate the active user's session. This endpoint should return the session's current ttl and a flag indicating whether the session has expired according to the session management configuration in place. This endpoint should be exposed using the same authentication mechanism as with other ssoagent endpoints and should be excluded from session management (i.e. should use a non-secure cookie of the appropriate duration to retain the session). The bootstrap-play SessionTimeoutFilter should be enhanced to use this endpoint to determine the time remaining on the user's session.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the context of the first major version release of play-frontend-hmrc, are there any fundamental\nchanges we would like to make to the package structure and naming conventions of the library?\n## Decision Drivers\n* Major changes are painful but will only get more painful the longer they are left.\n* No teams have, to our knowledge, complained or provided any negative feedback around\nthe naming conventions used in the play-frontend libraries.\n* The benefits of any name changes need to be weighed against the cost and disruption to\nservice teams.\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\nbreaking change. Deprecation notices left in too long create compilation noise and alert\nfatigue for teams leading to a risk that they will be ignored.\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\non their next library upgrade.\n* Repetition of suffixes/prefixes in the same source file makes it more verbose and harder to read.\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\nentropy that over time increases the cost of maintenance.\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\ntax services.\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\nmaking changes available to teams.\n\n## Decision\n","* Major changes are painful but will only get more painful the longer they are left.\n* No teams have, to our knowledge, complained or provided any negative feedback around\nthe naming conventions used in the play-frontend libraries.\n* The benefits of any name changes need to be weighed against the cost and disruption to\nservice teams.\n* Deprecation notices alert teams that a feature is going away without forcing an immediate\nbreaking change. Deprecation notices left in too long create compilation noise and alert\nfatigue for teams leading to a risk that they will be ignored.\n* Teams with a practice of removing all compilation warnings will be forced to remove all deprecated references\non their next library upgrade.\n* Repetition of suffixes/prefixes in the same source file makes it more verbose and harder to read.\n* Poor naming and a lack of consistency negatively affect conceptual integrity and increase architectural\nentropy that over time increases the cost of maintenance.\n* Poor naming confuses developers, increasing lead times to production and increases the risk of bugs in\ntax services.\n* A reduction in duplication will make maintaining the play-frontend libraries easier and reduce the lead times for\nmaking changes available to teams.\nChosen option: option 1, do nothing, because the cost of change is very high while the\ntheoretical benefits are extremely difficult to quantify.\n","Do not make any fundamental changes to the package structure or naming conventions of play-frontend-hmrc for the first major version release.\n\nWe will use compilation warnings to gently encourage migration to the new recommended approach by labelling the old approach as deprecated. Teams will then be notified when compiling their projects that the old approach is deprecated and should be migrated to the new approach. In time, the old approach will be removed.\n\nThis will give teams time to migrate to the new preferred approach without a hard deadline and to focus on delivering value to users, rather than spending time on non-urgent architectural and code changes.\n\nWe will continue to monitor feedback from teams and will revisit this decision if new information becomes available."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the context of classic services' requirement for contact HMRC and Welsh information links in their footer, facing the\nfact that these links are missing from hmrcStandardFooter, should we add them?\nThe additional links needed are:\n* ""Contact"", linking to https://www.gov.uk/government/organisations/hm-revenue-customs/contact\n* ""Rhestr o Wasanaethau Cymraeg"", linking to https://www.gov.uk/cymraeg\n## Decision Drivers\n* The need for consistency across HMRC services.\n* Our belief that including them is likely to improve the user experience for tax users.\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\n* We have a time sensitive opportunity of acting now while teams are in the process of\nuplifting their frontend libraries.\n* The HMRC design community have been consulted on multiple public Slack channels and two\nsuccessive design system working group meetings, with no objections noted.\n* Classic services support multiple live services. Not including these links as standard would mean their having to\nduplicate these links, and associated English and Welsh content, across tens of repositories.\n\n## Decision\n","* The need for consistency across HMRC services.\n* Our belief that including them is likely to improve the user experience for tax users.\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\n* We have a time sensitive opportunity of acting now while teams are in the process of\nuplifting their frontend libraries.\n* The HMRC design community have been consulted on multiple public Slack channels and two\nsuccessive design system working group meetings, with no objections noted.\n* Classic services support multiple live services. Not including these links as standard would mean their having to\nduplicate these links, and associated English and Welsh content, across tens of repositories.\nChosen option: ""Add the links to hmrcStandardFooter"", because this\nwill benefit tax users, and we have a unique window of opportunity to act now.\n### Positive Consequences\n* Tax users have better information provided to them\n* Teams do not need to duplicate content and URLs across hundreds of repositories\n* We can more easily maintain the content and links in a central repository\n### Negative Consequences\n* Teams currently using a Welsh link as their language toggle will likely need to switch to using one of the standard components\nfor language switching e.g. hmrcLanguageSelect.\n* Teams already including a contact link manually will need to remove it when upgrading.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nGiven a service is using the twirl template imports recommended by the play-frontend libraries. When a developer tries\nto use an import with a name which is present in both libraries without explicitly importing the one they want. Then\nthey will get a compilation error:\n<img alt=""Example of ambiguous import compilation exception"" src=""0010-ambiguous-import-exception.png"" width=""450"">\nCurrently, we recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.\nThe compilation error can be resolved by explicitly importing the class they want.\n## Decision Drivers\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\n* As much as possible avoid breaking changes for existing services\n\n## Decision\n",* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\n* As much as possible avoid breaking changes for existing services\nWe've decided to immediately implement Option 1.2 and defer further changes to be discussed as part of PLATUI-1294.\n### Positive Consequences\n* Less confusing for new developers where imports are coming from when we avoid recommending using TwirlKeys config.\n* Developers can see as part of our recommended usage instructions the possibility for an ambiguous import exception and\nwhat they can do to resolve them without.\n### Negative Consequences\n* Without an API change the naming of components and view models is not as intuitive as we think it could be which may\nconfuse new developers.\n,"**. Developers expecting to be able to use the short import name without getting a compile-time error will be unsurprised.**\n**. Avoids breaking changes for existing services, as that would prevent play-frontend from adopting the new behaviour.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala / Play / Twirl implementations of the components provided\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\nplay-frontend implementations diverge from their “base” repositories?\n## Decision Drivers\n* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\n\n## Decision\n","* Need to create frontend components that are easy to use by Scala developers on the platform\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\n`hmrc-frontend` are released\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\nGDS.\n### Positive Consequences\n* Design of case classes to follow GDS / HMRC design system means PlatUI as library maintainers do not have to create\nviewmodel structure from scratch every time\n* Adding new components can follow a clear and straightforward path\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\n### Negative Consequences\n* Feedback suggests that some developers do not find the API to be intuitive\n* Separate decisions need to be made on handling multilingual support\n* Enrichment of library needs to be done via separate Twirl helpers,\nsee [related ADR](../adr/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\n","The play-frontend implementations should be as close as possible to their base repositories, only diverging when necessary to:\n\n* Provide a Scala interface to the underlying Nunjucks components\n* Provide a clear and quick upgrade path when new versions of  govuk-frontend and hmrc-frontend are released\n* Have a robust testing strategy for library developers to have faith in when upgrading"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n- `play-fronted-govuk` library is not Play Framework aware\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\n## Decision Drivers\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\nappropriate helper class alongside the view models\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\n\n## Decision\n","- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\nappropriate helper class alongside the view models\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\nChosen option: Option 5, because it adds useful functionality (class enrichment of `Radios` via implicit\n`RichRadios`, without adding new Twirl templates to maintain, and without adding new methods directly\nto `Radios`, which would cause a divergence between `play-frontend-govuk` and `govuk-template` (undesirable).\n### Positive Consequences\n* Adds useful optional class enrichment to `Radios`\n* Reduces code repetition between `Field` and `Radios` (DRY)\n* Pattern is extensible, i.e. similar implicit helpers can be added to other form inputs\n### Negative Consequences\n* Doesn't add as much value as we would like\n* Need to document carefully what behaviour occurs when information provided via both `Radios` and\n`Field`, i.e. which takes precedence (currently intended to be `Radios` values take precendence)\n* Need to document to teams that this helper is available (service developers won't use it if they don't know it's there)\n","Add helper methods to the `play-frontend-govuk` library to utilise Play Framework features, reducing repetition in service code."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFacing the need to create wrappers to make using the Nunjucks components ported from hmrc-frontend more\nstraightforward and idiomatic on the Scala/Play MDTP platform, how should we name these components?\nThe components ported from hmrc-frontend include\n* implementations of govuk-frontend components that support the Welsh language e.g. hmrcHeader, hmrcFooter\n* components that meet specific HMRC needs e.g. hmrcBanner, hmrcInternalHeader\n* components that have not been standardised by GDS but are needed by HMRC e.g. hmrcNotificationBadge, hmrcTimeoutDialog\nBeing entirely presentational and ported from Nunjucks means they cannot leverage features built into Scala/Play nor\nmake use of any MDTP platform knowledge that would simplify their use on MDTP. For example,\n* they do not and should not know the url structure for common MDTP components e.g. tracking-consent-frontend,\ncontact-frontend or the accessibility-statement-frontend, services that need to wired in on every public-facing service via\nstandard headers and footers.\n* they cannot make use of Play's i18n features and automatic language wiring\n* they cannot make use of any knowledge encoded in the requests users make e.g. the request URL, referrer URL, cookies\nor headers.\nFor the above reasons, we are creating wrappers that implement standardised Play/platform wiring, to avoid teams having to\nduplicate this wiring across 100s of repositories. Once implemented we will encourage teams to use these helpers rather than\nthe underlying Nunjucks ports.\n## Decision Drivers\n* The fact that the ideal component names have already been taken by the Nunjucks components.\n* The preference for not relying on the package name to differentiate the components\n* The preference for not repeating the word Helper in the package and component name\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\ncomponent.\n* The preference for consistency in naming across the components\n\n## Decision\n","* The fact that the ideal component names have already been taken by the Nunjucks components.\n* The preference for not relying on the package name to differentiate the components\n* The preference for not repeating the word Helper in the package and component name\n* The preference for having names that are unsurprising and will encourage use of the helper in preference to the underlying Nunjucks\ncomponent.\n* The preference for consistency in naming across the components\nChosen option: ""Suffix with 'Helper'"", because this is the only option the team are happy with bearing in mind the decision\ndrivers listed above.\n### Positive Consequences\n* We have a consistent naming scheme\n* Friction around needing to come up with new names for helpers reduced, increasing speed of development.\n### Negative Consequences\n* There is a risk that service teams will not know the helpers exist\n* Repeating the word Helper in the component and having it in the package name is ugly\n","The components will be named with the prefix ""hmrc"" followed by an uppercase word matching the component's name. For example:\n* hmrcHeader\n* hmrcFooter\n* hmrcBanner\n* hmrcInternalHeader\n* hmrcNotificationBadge\n* hmrcTimeoutDialog"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFrequently occurring usage patterns of play-frontend-hmrc components require repetitious boilerplate\ninvolving excessively nested case class instantiations. This has driven many teams to create wrappers to\nsimplify the construction of components, which are then copied and pasted from service to service.\nFor example, in order to add a legend to a GovukRadio the following boilerplate is required:\n```scala\nSome(Fieldset(\nlegend = Some(Legend(\ncontent = Text(""message.key.for.legend""),\nclasses = ""govuk-fieldset__legend--l"",\nisPageHeading = true\n))\n))\n```\nIn the above example, the only thing that changes between instantiations is the `message.key.for.legend`, everything\nelse remains the same.\nShould we solve this problem through the introduction of implicit conversions that will automatically wrap simple\nobjects such as Strings with the additional boilerplate necessary for commonly occurring cases?\n## Decision Drivers\n* The need for API consistency in play-frontend-hmrc.\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\nentirely in a future Scala version:\n* See https://www.rallyhealth.com/coding/implicit-implications-part-2-implicit-conversions,\nhttps://contributors.scala-lang.org/t/can-we-wean-scala-off-implicit-conversions/4388\n* The risk of unintended side effects if these conversions are added unilaterally\nwithout service developers needing to specifically opt in to them.\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\nHint, Key, Label, Legend.\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\nshow implicit hints.\n\n## Decision\n","* The need for API consistency in play-frontend-hmrc.\n* The mixed sentiments towards implicit conversions in the Scala community and slight risk the feature may be removed\nentirely in a future Scala version:\n* See https://www.rallyhealth.com/coding/implicit-implications-part-2-implicit-conversions,\nhttps://contributors.scala-lang.org/t/can-we-wean-scala-off-implicit-conversions/4388\n* The risk of unintended side effects if these conversions are added unilaterally\nwithout service developers needing to specifically opt in to them.\n* The repeated invocation of the `Messages` apply method to populate component content parameters, e.g.\nHint, Key, Label, Legend.\n* The difficulty of debugging code involving implicit conversions without turning on advanced IDE features that\nshow implicit hints.\nChosen option: Option 1, because it solves the problem, is consistent with how we have been doing things, maintains\nthe internal consistency of play-frontend-hmrc and has less risk of breaking services.\n","**Decision:** Do not currently introduce this feature.\n\n**Rationale:**\n\n* The problem of boilerplate is only a concern for a small subset of components.\n* It is unclear if the benefits of wrapping outweigh the possible costs, i.e. the potential downsides and risks outlined in the Decision Drivers.\n* The Scala programming language's use of implicit conversions is controversial, and there are concerns that this feature may be removed in a future version.\n* Implicit conversions can be difficult to debug and understand, especially for new developers.\n* There is a risk of unintended side effects if these conversions are added unilaterally without service developers needing to specifically opt in to them."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nService developers seeking to use play-frontend to build their frontend microservices need to follow around\n12 separate integration steps involving two separate Scala libraries: play-frontend-govuk and play-frontend-hmrc. Failure to\nperform any one of these steps correctly, or making changes that break any of this configuration,\nresults in failed builds, incorrectly styled components or broken Javascript-enabled functionality. These steps involve:\n* Adding play-frontend-govuk and play-frontend-hmrc as SBT dependencies.\n* Setting up an SBT asset pipeline to compile and optimise the CSS and Javascript into bundles suitable for production.\n* Adding multiple asset routing rules.\n* Adding the boilerplate SCRIPT and LINK tags to the HTML page template to link to the assets at runtime.\n* Adding Javascript to initialising the GOV.UK and HMRC components.\nFollowing investigation, we discovered that the integration process could be drastically simplified\nby providing head and script helpers that would link to pre-compiled and optimised asset bundles from\nthe hmrc-frontend webjar, subsuming the govuk-frontend assets and taking responsibility\nfor component initialisation. This move would:\n* simplify the migration path from assets-frontend/play-ui/govuk-template to play-frontend\n* particularly benefit teams lacking an embedded full time frontend developer\n* not radically change the way microservices integrate with play-frontend\n* still allow teams to create custom components and add their own custom CSS where needed\n* continue to allow for local development without reliance on networked dependencies\nOverall, the total number of integration steps would be reduced from 12 to 6.\nFacing these considerations, should we therefore\n* change the pre-compiled hmrc-frontend bundles to also include govuk-frontend and to initialise all components\n* add the pre-compiled and optimised CSS and Javascript bundles into the published hmrc-frontend npm package and\n* create Twirl helper components to render the HTML snippets necessary to link to them?\n## Decision Drivers\n* The integration difficulties teams have reported over the past 12 months.\n* The problematic nature of diagnosing build issues in frontend microservices due to the requirement for a deep\nknowledge of both Scala and frontend build tooling, which is rare.\n* The pressing need to migrate services away from assets-frontend for reasons of accessibility, security and sustainability.\n* That services do not generally need extensive service-specific custom CSS. UI components are heavily standardised by GDS.\n* That services do not generally need extensive service-specific Javascript. The mantra of progressive\nenhancement demands services build without Javascript if possible.\n* That teams should be free to create custom components with custom CSS where there is a genuine need, without relying\non other teams. Historically denying teams this ability arguably led to many of the problems with assets-frontend.\n* That teams should continue to be able to develop locally without relying on networked dependencies.\n\n## Decision\n","* The integration difficulties teams have reported over the past 12 months.\n* The problematic nature of diagnosing build issues in frontend microservices due to the requirement for a deep\nknowledge of both Scala and frontend build tooling, which is rare.\n* The pressing need to migrate services away from assets-frontend for reasons of accessibility, security and sustainability.\n* That services do not generally need extensive service-specific custom CSS. UI components are heavily standardised by GDS.\n* That services do not generally need extensive service-specific Javascript. The mantra of progressive\nenhancement demands services build without Javascript if possible.\n* That teams should be free to create custom components with custom CSS where there is a genuine need, without relying\non other teams. Historically denying teams this ability arguably led to many of the problems with assets-frontend.\n* That teams should continue to be able to develop locally without relying on networked dependencies.\nChosen option: ""Add pre-compiled assets with auto-initialisation"", because this is a major step forward in improving\nthe ease of use of play-frontend and goes a long way to address many of the issues\nusers have experienced.\nThe change to the S3 assets is not of concern due to the fact that the existence of these\nassets was never publicised or fully documented and we have seen only a tiny number of requests for these assets\nin server logs.\n### Positive Consequences\n* A radically simplified integration process for Scala developers.\n* Fewer moving parts in frontend microservices and reduced boilerplate.\n* No changes needed by teams wanting to continue to consume the original un-compiled assets.\n* Users of the npm package should not be affected by this change as it adds files only.\n* Simplified usage outside Scala microservices e.g. in outage/waiting pages.\n* Bundling govuk-frontend and hmrc-frontend together means there is no longer the possibility for the use\nof incompatible versions of govuk-frontend with hmrc-frontend.\n* Best practice is to initialise all GOV.UK and HMRC components rather than\ncherry picking to avoid the risk of breaking accessibility features. Doing this automatically\nmeans there is one less thing for teams to worry about getting right.\n### Negative Consequences\n* Teams using the S3-distributed hmrc-frontend assets will no longer have the option to separately initialise the\nGOVUK and HMRC components. However, they will retain access to window.GOVUKFrontend and window.HMRCFrontend if\nneeded for initialisation of dynamically injected content.\n* On their next upgrade, teams using the S3-distributed hmrc-frontend assets will need to remove the SCRIPT tag referencing the\ngovuk-frontend bundle and any calls to GOVUKFrontend.initAll() or HMRCFrontend.initAll().\n* The distribution build task diverges from the pattern set by govuk-frontend. In govuk-frontend, the distributable does not\nauto-initialise the govuk components. This may affect developers consuming hmrc-frontend using git rather than via the npm package.\n","Yes, we should change the pre-compiled hmrc-frontend bundles to also include govuk-frontend and to initialise all components, add the pre-compiled and optimised CSS and Javascript bundles into the published hmrc-frontend npm package and create Twirl helper components to render the HTML snippets necessary to link to them."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe need to provide an HMRC footer translated into Welsh (PLATUI-752)\n## Decision Drivers\n* the lack of support for Welsh in the GDS govukFooter\ncomponent\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\nwith some non-presentational helpers to aid use in Scala/Play - conceptual integrity\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\nmarkup - separation of concerns\n\n## Decision\n,"* the lack of support for Welsh in the GDS govukFooter\ncomponent\n* the desire to maintain play-frontend-hmrc as a pure port of hmrc-frontend\nwith some non-presentational helpers to aid use in Scala/Play - conceptual integrity\n* the desire to maintain hmrc-frontend as the source of truth for all presentational\nmarkup - separation of concerns\nChosen option ""maintain parity"" because we want hmrc-frontend to be the source of truth for presentation and\nmaintain the separation of concerns between hmrc-frontend and\nplay-frontend-hmrc\n### Positive Consequences\n* Maintain separation of concerns between hmrc-frontend and play-frontend-hmrc\n* Maintain conceptual integrity for play-frontend-hmrc\n* Be able to test markup using existing test strategy\n### Negative Consequences\n* We will need to add and maintain a new hmrcFooter component in\nhmrc-frontend and play-frontend-hmrc providing a mirror of govukFooter with localised content,\n* The new hmrcFooter component in hmrc-frontend will not be able to make use of any I18n features,\n* We will need to create a new `helpers` package within play-frontend-hmrc to clearly demarcate them\nfrom presentational components,\n* We will need to redesign the existing hmrcFooter helper to wrap the new hmrcFooter component\nand move to the helpers package,\n* The new hmrcFooter component will need to be deprecated when GDS provide a localised version\nof govukFooter,\n* We will need to liaise and get approval from the HMRC design system team for adding\nhmrcFooter to the hmrc/design-system and hmrc/hmrc-frontend\n* We will be adding features that are unlikely to be useful or used by designers because\nat the prototyping phase content is not stable enough for translation into Welsh\n","Implement hmrc-play-frontend-footer, a component that renders the HMRC footer in Welsh, as a dependency of hmrc-play-frontend."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nplay-frontend-hmrc relies on a webjar for [hmrc/hmrc-frontend](https://www.github.com/hmrc/hmrc-frontend)\npublished to www.webjars.org. This has a number of drawbacks:\n* publishing is a manual process\n* it can take many hours to complete\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\nThe main impact of the above is an excessive lead time for making improvements in the\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\nBintray, should we:\n* automate the creation of the webjars within our own deployment pipelines with no dependency\non webjars.org\n* publish the resulting webjars to this repository automatically?\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\nwebjar for [alphagov/govuk-frontend](https://www.github.com/alphagov/govuk-frontend), which is\ncurrently a dependency for [hmrc/play-frontend-govuk](https://www.github.com/hmrc/play-frontend-govuk).\n## Decision Drivers\n* The need to make improvements and upgrades to hmrc-frontend\navailable in play-frontend-hmrc quickly.\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\nimprovements.\n* The hardship, frustration and toil the current manual process is causing the team.\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\nthe overhead of maintaining those repositories\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\n* Parity between the hmrc-frontend NPM package and the webjar.\n\n## Decision\n","* The need to make improvements and upgrades to hmrc-frontend\navailable in play-frontend-hmrc quickly.\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\nimprovements.\n* The hardship, frustration and toil the current manual process is causing the team.\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\nthe overhead of maintaining those repositories\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\n* Parity between the hmrc-frontend NPM package and the webjar.\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\nadditional dependencies.\n### Existing architecture\n<img alt=""Existing architecture"" src=""0009-webjars-existing.png"" width=""450"">\n### To be architecture\n<img alt=""To be architecture"" src=""0009-webjars-tobe.png"" width=""450"">\n### Positive Consequences\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\nproduction.\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\n* Maintaining play-frontend-hmrc is a less frustrating process.\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\nproduced indicate webjars.org as the developer in the POM files.\n* There are fewer external dependencies and moving parts.\n### Negative Consequences\n* We have an additional moving part to maintain ourselves.\n",Automate the creation of the webjars within our own deployment pipelines with no dependency on webjars.org and publish the resulting webjars to this repository automatically.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe previously decided to keep inputs and outputs of play-frontend-govuk components a 1-to-1 match with govuk-frontend\nnunjucks example implementations.\nGenerally components from govuk-frontend accept their copy as input, however the skip link in govukTemplate is\nhard-coded which is causing accessibility failures for services.\nThis is an accessibility failure because a page translated into welsh may cause assistive technology to mispronounce the\ncontent of the english skip link because it has no explicit language attribute.\n## Decision Drivers\n* Because it's an accessibility failure we want to fix this as a priority quickly\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\n* At the same time we don't want to reduce test coverage / diverge from our current testing strategy\n\n## Decision\n","* Because it's an accessibility failure we want to fix this as a priority quickly\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\n* At the same time we don't want to reduce test coverage / diverge from our current testing strategy\nWe've chosen option 1 because unlike other components, we don't have any automatically generated fixtures or integration\ntests for govukTemplate. We maintain test fixtures for the templates manually in the additional fixtures folder. This\nmakes it possible for us to patch this change into govukTemplate with minimal maintenance cost without reducing our test\ncoverage / diverging far from our current testing strategy.\n","**Decision:** Remove the hard-coded skip link from govukTemplate.\n\n**Rationale:**\n\nThis decision addresses all of the decision drivers:\n\n* **Accessibility:** Removing the hard-coded skip link will fix the accessibility failure by allowing services to provide a language-appropriate skip link.\n* **Wide availability:** The fix will be available as widely as possible without requiring template changes from services, as it will be implemented in govukTemplate.\n* **Test coverage:** The fix will not reduce test coverage, as the skip link will still be tested in the govukTemplate tests."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKuona instances need to be secured - particularly for deployments that have public access.\n\n## Decision\n,https://funcool.github.io/buddy-auth/latest/#example-session Buddy seems to fit the bill and is compatible with Compojure and Ring. Provides a number of options and possible persistance mechanisme.\n,**Decision:** Implement [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) to manage access to Kuona instances.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use ADRs (Architectural Decision Records) as the primary mechanism for recording architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\ndecrypt Response Assertions.\nThey will need to provide these to the verify-service-provider in some, reasonably\nsecure way. Different users may have different opinions on how best to do this.\n\n## Decision\n",Initially we'll use files for this.\nWe chose not to use environment variables because they're visible to other processes.\nWe chose not to use a more complicated solution because it would be more complicated.\n,The verify-service-provider will provide a mechanism that will allow RPs to securely upload their private keys.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe client and the service provider will have to communicate using some API.\nWe need to decide how the requests and responses will look like.\n\n## Decision\n,We will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\n,"**Decision**\n\nThe API will follow the RESTful architectural style. Requests will be made using HTTP verbs (GET, POST, PUT, DELETE) and will include a JSON payload. Responses will also be in JSON format."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to make sure that whatever we build meets the users' needs. To make sure of this we need to\nconduct some user research, which should involve putting software in front of users and observing them using it.\n\n## Decision\n","We will build a simple ""prototype"" which we will use to test our assumptions about whether our proposed\nsolution is the best way of meeting our users needs.\nThe prototype will be architecturally similar to the product we envisage building, but won't be able to\ndo the SAML interactions with Verify at this stage.\nADRs for the first prototype will live in the [prototype-0](prototype-0) directory.\n",**Conduct user research by observing users interacting with software prototypes.**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAt least one user is currently using node js and passport. We want to provide as\nfrictionless as possible an integration for them.\nOther users will be using other languages and frameworks.\n\n## Decision\n,We will initially build only a node / passport client. We will want to build\nanother client in another language as soon as possible to make sure the API\nis well designed.\nUsers should also be able to interact with the API directly if we haven't built\nan appropriate client for their use case.\n,"Integrate passport with node.js to provide users with a seamless integration experience, while supporting multiple languages and frameworks for other users."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to secure the interaction between the ""client"" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n\n## Decision\n",If possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\n,"Implement a mutual TLS (TLS mTLS) security mechanism.  mTLS provides both authentication and data encryption, and can be configured to work with a variety of client and server technologies. It is a good choice for securing the interaction between the ""client"" code and the server side code.\n\n**Benefits**:\n\n* **Authentication**: mTLS provides strong authentication by requiring both the client and the server to present valid certificates. This makes it difficult for unauthorized users to access the service.\n* **Data encryption**: mTLS encrypts the data transmitted between the client and the server, protecting it from eavesdropping.\n* **Flexibility**: mTLS can be configured to work with a variety of client and server technologies, making it a good choice for a wide range of applications.\n\n**Drawbacks**:\n\n* **Complexity**: mTLS can be complex to configure and manage, especially in large-scale deployments.\n* **Performance**: mTLS can add some overhead to the communication between the client and the server, which can impact performance.\n\n**Alternatives**:\n\n* **API key**: An API key is a string that is used to identify the client to the server. API keys are simple to implement and manage, but they do not provide strong authentication or data encryption.\n* **OAuth 2.0**: OAuth 2.0 is an authorization framework that can be used to secure the interaction between the client and the server. OAuth 2.0 provides strong authentication and data encryption, but it can be complex to implement and manage.\n\n**Recommendation**:\n\nWe recommend implementing mTLS to secure the interaction between the ""client"" code and the server side code. mTLS provides both strong authentication and data encryption, and can be configured to work with a variety of client and server technologies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn SAML RPs can provide some extra data along with the request. This is\ncalled RelayState. Some existing RPs use this, but we're not sure what\nthey use it for.\nWe're not aware of any need for the service-provider to use relay state itself.\n\n## Decision\n",Users will be able to specify whatever relay state they want to and it will be\nprovided in the response.\n,Do not implement RelayState support.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAuthnRequests contain an ID attribute the value of which will be sent back in\nthe Response as an ""InResponseTo"" attribute.\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\n\n## Decision\n",The service provider will generate a random GUID to use as the AuthnRequest ID.\n,"**Decision:** Use a UUID for the ID attribute of AuthnRequests.\n\n**Rationale:**\n\n* UUIDs are universally unique and can be randomly generated, ensuring that each AuthnRequest has a unique ID.\n* UUIDs are relatively short and easy to work with, making them suitable for use as an attribute in an XML message.\n* Validating the InResponseTo attribute against the ID attribute is straightforward, as both attributes will contain the same UUID value.\n\n**Consequences:**\n\n* Using UUIDs for the ID attribute will ensure that AuthnRequests can be uniquely identified and that the InResponseTo attribute can be validated correctly.\n* This will help prevent replay attacks and other security vulnerabilities.\n* The use of UUIDs may incur a slight performance overhead compared to using simpler identifiers, but this overhead is likely to be negligible in most cases."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe wish to run regular user research and testing against the prototype Service Provider.\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\nThe area we are most interested in is the interface between the Service and the Hub.\n\n## Decision\n,"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\nIt will not provide a SAML implementation.\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\ncan be selected.\nAny information that needs to be stored from the original request or between pages will be stored in a session\ncookie. The session cookie does not need to be signed or encrypted initially.\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\nsend its responses to.\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\nprototype.\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\nService/client.\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\nThe Stub Verify Hub will provide the following responses:\n| Response Scenario     | Message                                                                                                                   |\n| --                    | --                                                                                                                         |\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\n| No Match              | scenario: NO_MATCH                                                                                                         |\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\nA diagram of a potential stub architecture is available at: `prototypes/prototype-0/docs/diagrams/stub_service_architecture.png`\n",Use an integration test harness as a test double for the Service Provider.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Establish an Architectural Decision Record (ADR) process to capture architectural decisions made during the project.\n\n**Rationale:**\n\n* Ensures consistent documentation of architectural decisions throughout the project lifecycle.\n* Provides a centralized repository for decision-making history and rationale.\n* Facilitates knowledge sharing and understanding among project stakeholders.\n* Reduces the risk of ad-hoc or poorly considered architectural decisions.\n\n**Consequences:**\n\n* Requires effort to create and maintain ADRs.\n* May introduce some overhead in the decision-making process.\n* ADRs may become outdated if not regularly reviewed and updated.\n\n**Additional Considerations:**\n\n* Define a template or format for ADRs to ensure consistency.\n* Establish a review and approval process for ADRs to ensure quality.\n* Make ADRs accessible to all relevant stakeholders.\n* Consider using a dedicated tool or platform to manage ADRs.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\n## Decision Drivers\n* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\n\n## Decision\n,"* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\nChosen option: ""AWS"", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\n","**Decision:** Host the web application on AWS Elastic Beanstalk.\n\n**Rationale:**\n\nAWS Elastic Beanstalk meets all the decision drivers:\n\n* **Easy to update:** Elastic Beanstalk automates the deployment and update process, making it easy to deploy new code or updates with minimal downtime.\n* **High availability:** Elastic Beanstalk deploys applications across multiple Availability Zones, ensuring high availability and resilience to failures.\n* **Easy to configure HTTPS:** Elastic Beanstalk supports SSL termination and provides options for managing HTTPS certificates.\n* **Observability:** Elastic Beanstalk integrates with AWS CloudWatch, providing metrics, logs, and insights into the performance and health of the application."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe have to choose the framework and language to build this web application.\nWhich framework and language should we use?\n\n## Decision\n,"Chosen option: ""React with Typescript"", because\n* The single developer has experience with React and Ember, but no experience with Vue\n* This developer favors React over Ember, because he thinks the first contributes to a better separation of concerns\n* This developer has experiece with TypeScript and JavaScript, but no experience with Elm\n* This developer favors TypeScript mostly because it's typed.\n",The decision is not provided in the context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've implemented 2 different methods in the Editor interface:\n1. `write()`\n1. `readThenWrite()`\nMost refactorings use `write()`. They create new code from the AST, then write it in the editor. Still, some refactorings use `readThenWrite()`: they first read some code `string` from the editor, and use it to write new code.\nThe main problem of `readThenWrite()` is that we end up manipulating strings instead of the AST. This usually means the implementation is more complex. Instead of manipulating the AST, we need to retrieve the correct selection we need to read, then we have to mix this read code with the transformation to produce the final output. Final code is less straightforward and more cases need to be tackled by us, instead of relying on the AST parser to do the job.\nHowever, there is one key advantage of doing so: it absolutely preserve the code as it was written in the editor. Even though [we use recast to preserve code style as much as possible][recast-usage], it's not perfect. If the refactoring consist in re-using exactly code that was written, `readThenWrite()` is the only way we know to preserve exactly the original style.\n\n## Decision\n","We'll use `readThenWrite()` if we need to preserve a code exactly as it was written. E.g. if the refactoring consists in moving existing code, without transforming it (Extract Variable, Inline Variable).\nFor other refactorings, we'll use `write()`. E.g. if the refactoring transforms the code, it's OK to change it while preserving the original style as much as possible with recast.\n",Deprecate `readThenWrite()` and encourage the adoption of `write()` by refactorings.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the ""Convert to Pure Component"" Quick Fix took most of the time:\n![][flame-chart]\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https://github.com/reactjs/react-codemod). It was originally meant to be run through a CLI command.\n\n## Decision\n",We will stop proposing a Quick Fix for this refactoring.\n,"**We will not make any changes to the current behavior of the ""Convert to Pure Component"" Quick Fix.** \n\nThis refactoring is not suitable for all cases and is only applied under specific circumstances. The performance issue is caused by the limitations of the underlying codemod, which cannot be improved within the extension."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe used Babel to parse code into AST, transform this AST and re-generate code.\nWe went for Babel because it's popular, very active and is capable of parsing a lot of different syntaxes. Notably, it parses JSX, TS and TSX out of the box. That's great!\nWhat is not great is the code generation part. Babel formats the generated code. That means the code contained inside a transformed node gets reformated. This is not cool.\n\n## Decision\n","As we want to preserve the original style of the transformed code as much as possible, we went for [Recast][recast].\nAs the library says:\n> The magic of Recast is that it reprints only those parts of the syntax tree that you modify.\nThus, we now use Recast to parse and generate the code. AST transformation is still performed by Babel. Recast uses Babel to parse the code into AST, so we keep Babel benefits such as parsing JSX, TS and TSX out of the box.\n",We should use another tool (like swc) that doesn't format the code.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVS Code has documented [how to test an extension][testing-extension] in integration with VS Code API.\n> These tests will run inside a special instance of VS Code named the `Extension Development Host`, and have full access to the VS Code API.\nBut VS Code test runner is adapted to be used with [mocha][mocha] or [Jasmine][jasmine].\nFor unit tests, we want to use [Jest][jest]. But Jest and mocha have conflicting types, preventing TS to build. A solution would have been to create a custom test runner for VS Code API, that would run integration tests with Jest. But we didn't managed to create one that works.\nThat's partly because Jest doesn't have an official way to run tests programatically. We weren't able to make VS Code test runner work with Jest `runCLI()` (async) method.\nAlso, integration tests are not the most important part of the project for the moment.\n\n## Decision\n","We won't do integration tests. We'll rely on unit tests (e.g. state-based tests, collaboration tests and contract tests).\n","Do not use Jest for now, continue using mocha."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nRelative paths for images won't work when the README is displayed somewhere where the images are not hosted. From experience developing npm libraries, images won't appear on the package manager website.\nTo solve this, we use absolute paths to hosted images. Since images are hosted in GitHub along the rest of the code, we use these absolute paths.\nVS Code has a way to deal with that issue. It won't allow you to create a package if README contains relative paths. It will emit this error:\n> Couldn't detect the repository where this extension is published. The image './docs/logo/abracadabra-logo.svg' will be broken in README.md. Please provide the repository URL in package.json or use the --baseContentUrl and --baseImagesUrl options.\nThus, we could technically use relative paths and provide the correct arguments to make images work on VS Code Marketplace.\nHowever, using absolute paths is something we're used to, and doesn't come up with a lot of downsides. So we decide to go with absolute paths, until downsides justify otherwise.\n\n## Decision\n",We use absolute paths to GitHub hosted images in the README (and other documentation files).\nWe don't use relative paths to images.\n,Use absolute paths for images in the README.md file to ensure that they can be displayed regardless of where the file is hosted.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nJest [`it.each` seemed great][jest-it-each], but it turns out to be limited.\nSpecifically, it doesn't allow us to have a `.only` on a single test of the list.\nOf course, this can be done at runtime through Jest runner. But if we use tools like [Wallaby.js][wallaby], we can't do that. We need a way to add things like `.only` on individual tests.\n\n## Decision\n",We've decided to implement a custom `testEach()` function that will provide a convenient API to run the same test over different data.\nThis function will provide a way to run individual test of the list with `only: true`.\n,"Use a custom implementation of `.only` that allows for specifying which test to run.\n\nThis implementation could be based on the `it.each` function, but would add additional functionality to allow for specifying which test to run. This would allow for more flexibility and control over which tests are run, while still providing the benefits of `it.each`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCreating a new refactoring kinda always follow the same steps. There is a bunch of boilerplate code that needs to be created.\nWe could try to refactor this boilerplate into some good abstraction. But we still need to scaffold the same files over and over: the refactoring file, the test file, the command declaration file and eventually the action provider one.\n\n## Decision\n","To speed up this process, we've decided to use a code generator.\nWe went for [hygen](https://www.hygen.io/) because it's quite simple to use, very fast and flexible.\nWe're abstracting the usage of hygen behind a npm script alias: `yarn new` will ask you few questions to scaffold a new refactoring.\n",**Decision:** Create a tool to generate the boilerplate code for a new refactoring.\n\n**Rationale:**\n\n* This will save time and effort when creating new refactorings.\n* It will ensure that all refactorings follow the same conventions and best practices.\n* It will make it easier to maintain and update the refactoring framework.\n\n**Consequences:**\n\n* **Positive:**\n    * Faster and easier to create new refactorings.\n    * Improved consistency and quality of refactorings.\n* **Negative:**\n    * May require some additional development time to create the tool.\n    * May make it more difficult to customize refactorings.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe had to enhance editor capabilities and implement new editor adapters with the latest features. Not having integration tests to cover these changes became more and more risky.\nAfter giving it another try, we were able to get Jest & Mocha installed together, without compilation errors because of type conflicts. Therefore, it was possible to create integration tests that would have access to VS Code API.\n\n## Decision\n","We will now cover adapters with integration tests.\nBecause _integration tests_ has different meanings for different people, we have decided to call them **contract tests** instead. Our intention is to test that all adapters of an interface do follow the same contract.\n",Integrate Jest & Mocha testing frameworks for improved test coverage and enhanced editor capabilities. This combination enables access to VS Code API and mitigates risks associated with insufficient testing.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to automate the process of deploying a bit more. At this point, crafting a new release is done manually by @nicoespeon.\nAlso, we want to start deploying to [the Open VSX Registry](https://open-vsx.org/) so Abracadabra would be available for VS Code alternatives as [VS Codium](https://vscodium.com/). You can read [the original issue](https://github.com/nicoespeon/abracadabra/issues/163) for more context.\nSource code is hosted and managed in GitHub, so GitHub Actions make sense.\nThere is [a custom GitHub Action](https://github.com/HaaLeo/publish-vscode-extension#readme) that would take care of that precise use-case. But what we need to do isn't very complex. Having less intermediate would make it easier to maintain.\n\n## Decision\n",We've created 2 GitHub Actions:\n1. One for deploying to the VS Code Marketplace\n2. One for deploying to the Open VSX Registry\nThese actions trigger on published releases.\n,Use a custom GitHub Action to automate the deployment process. The action should:\n\n* Trigger on a push to the `main` branch.\n* Build the extension using the `vsce` command.\n* Publish the extension to the Open VSX Registry using the `vsce publish` command.\n* Notify @nicoespeon on Slack if the deployment fails.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Utilize an Architectural Decision Record (ADR) template to document architectural decisions consistently and thoroughly.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\n\n## Decision\n","Chosen option: ""Swagger Based"", because this solution supports the fastes development Start without writting any boilerplate code.\n### Positive Consequences\n* No Painfull HTTP Client Implementation\n### Negative Consequences\n* the API Client Implementation dependents to the Swagger Spec Quality...\n",**Decision:**\n\nUse a generated HarborRestAPI client.\n\n**Rationale:**\n\n* **Faster development:** Generating a client will save time and effort compared to manually writing one.\n* **Improved accuracy:** A generated client is less likely to contain errors than a manually written one.\n* **Consistency with Terraform Provider Functions:** Using a generated client will ensure consistency with the Terraform Provider Functions development process.\n\n**Consequences:**\n\n* May require additional maintenance if the Harbor API changes.\n* May not be as customizable as a manually written client.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEvery service is required to pass a Caller-Id header with its requests and previously eHOKS frontend had separate\nids for oppija and virkailija. While the codebase is largely shared between oppija and virkailija the services themselves\nare separate and hence the separate ids were created. While the headers were simple to add to requests made by\ncomponents only used by either oppija or virkailija, dynamically figuring out which id should be used in the shared\ncomponents at any given time proved harder.\n\n## Decision\n","The separate ids will be replaced by a single frontend Caller-Id. Since all the requests made by both oppija and\nvirkailija frontends go through the eHOKS backend service and don't call any external services directly this\nshould be sufficient. The requests from oppija and virkailija can be distinquished from each other via other means,\neg. they use different backend APIs altogether.\n",Create a service that all shared components use to fetch the correct Caller-Id header value for the current request.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural and desing decisions made during this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by\nMichael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\nAlso https://medium.com/better-programming/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\n",**Decision:** Use an Architectural Decision Record (ADR) template to consistently document architectural and design decisions.\n\n**Rationale:**\n\n* ADRs provide a structured way to capture and share the reasons behind architectural decisions.\n* Using a template ensures consistency and completeness in the documentation.\n* ADRs facilitate knowledge transfer within the team and future reference.\n* They help avoid repetitive discussions and ensure that decisions are made based on informed analysis.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\nin Finnish but react component props in English even though data might be exactly the same.\n\n## Decision\n,"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\nconventions quite some time.\nHere is one expample:\n```typescript\ninterface LearningEventProps {\nclassName?: string\ntitle?: React.ReactNode\nisDemonstration?: boolean\nsize?: ""small"" | ""large""\ndescription?: string\nstartDate?: string\nendDate?: string\nperiodSpecifier?: string\ndemonstrationEnviromentDescription?: string\n}\n```\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\nOsaamisenOsoittaminen components.\nExample fixed:\n```typescript\ninterface LearningEventProps {\nclassName?: string\ntitle?: React.ReactNode\nisNaytto?: boolean\nsize?: ""small"" | ""large""\ndescription?: string\nstartDate?: string\nendDate?: string\najanjaksonTarkenne?: string\nnayttoymparistoDescription?: string\n}\n```\n","**Decision:** Establish a consistent translation convention for domain words and concepts across all frameworks and languages used in the project.\n\n**Rationale:**\n\n* **Improved consistency:** Ensures that users and developers can easily understand and navigate the codebase, regardless of the language or framework being used.\n* **Reduced confusion:** Prevents misunderstandings and errors caused by inconsistent translations.\n* **Enhanced code readability:** Makes the code more readable and maintainable for all team members.\n* **Increased flexibility:** Allows for easier future additions or modifications to the codebase, as all translations will follow the same conventions.\n\n**Implementation:**\n\n* Define a central repository or style guide that outlines the agreed-upon translation conventions.\n* Ensure that all new code and translations adhere to these conventions.\n* Conduct regular code reviews to check for consistency and identify any deviations from the established conventions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Two Unique Problems to Solve\n* A radio or checkbox field needs to optionally have an ""Other"" input field which consists of a label and freeform text input. All existing radio/checkbox functionality must be preserved.\n* Address can exist as a form group which can contain any combination of street address, city, state, zip, etc. It would be useful to have pre-made templates of form groups.\n\n## Decision\n","* Create additional versions for each individual form field and treat them accordingly in the HTML helper. ""version"" will be a new attribute, ""formtype"" and the rest of the JSON will stay the same.\n```\nExample of field variation\n""data"":\n[\n{\n// otherless radio, notice no version attribute (default)\n""label"":""Icecream?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_1"",\n""formtype"":""s08"",\n""name"":""icecream"",\n""radios"":""yes\nno"",\n""type"":""radio"",\n""required"":""true"",\n""class"":""custom-class""\n},\n{\n// radio with other\n""label"":""Icecream flavor?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_2"",\n""formtype"":""s08"",\n""name"":""icecream_flavor"",\n""radios"":""vanilla\nchocolate"",\n""type"":""radio"",\n""version"": ""other"", // Variation version\n""required"":""true"",\n""class"":""custom-class""\n},\n]\n```\n* Add a new ""groupid"" attribute to the saved JSON form data object.\n* The new ""groupid"" will act as a dynamic id which will group all fields with the same groupid together.\n* The ""groupid"" value will be generated once it is dragged/added to the editing form and be a concatenation of the form group template name (see below) and an incremental number, ie: g_address_streetonly_1\n* Versions of form groups do not depend on each other.\n* Added form groups to the form will be non-editable but will otherwise appear as regular fields within the JSON form data object.\n```\nExample of form data with a group\n""data"":\n[\n{\n// pizza is not in the group\n""label"":""Pizza"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""pizza_1"",\n""formtype"":""s08"",\n""name"":""pizza"",\n""radios"":""unpopular\nreally unpopular"",\n""type"":""radio"",\n""required"":""true"",\n""class"":""custom-class""\n},\n{\n""label"":""Icecream?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_1"",\n""formtype"":""s08"",\n""name"":""icecream"",\n""radios"":""yes\nno"",\n""type"":""radio"",\n""required"":""true"",\n""class"":""custom-class"",\n""groupid"":""g_icecream_all_1""\n},\n{\n""label"":""Icecream flavor?"",\n""placeholder"":""placeholder"",\n""help"":""Supporting help text"",\n""id"":""radio_2"",\n""formtype"":""s08"",\n""name"":""icecream_flavor"",\n""radios"":""vanilla\nchocolate"",\n""type"":""radio"",\n""version"": ""other"", // Variation version\n""required"":""true"",\n""class"":""custom-class"",\n""groupid"":""g_icecream_all_1""\n},\n]\n```\n* HTML Generation: Create a helper listing of form group templates which will describe which form fields belong to which predefined group.\n* Format would be a JSON with an array of formtypes: each formtype would adopt the same attributes as the existing form field JSON.\n* Undefined attributes would be filled in by the default value and defined attributes would override the default.\n* Naming groups will start with g_groupname_templatename\n```\nExample of form group templates\n""address"" : {\n""full"" : [\n{\n""formtype"":""c08"",\n""label"":""Address 1"",\n},\n{\n""formtype"":""c08"",\n""label"":""Address 2"",\n},\n{\n""formtype"":""c10"", //city\n},\n{\n""formtype"":""s14"", //state\n},\n{\n""formtype"":""c14"", //zip\n}\n],\n""streetzip"" : [\n{\n""formtype"":""c08"",\n},\n{\n""formtype"":""c14"", //zip\n}\n]\n}\n```\n","**Decision:** Use a custom field type to solve both problems.\n\n**Justification:**\n\nThis decision addresses both problems by introducing a single, flexible solution.\n\n* **Optional ""Other"" Input Field:** The custom field type can include a boolean property to enable the ""Other"" input field. This preserves all existing radio/checkbox functionality while adding the desired optionality.\n* **Pre-made Form Group Templates:** The custom field type can support templates that define the composition of form groups. This allows designers to create and reuse pre-defined combinations of address elements, simplifying form creation.\n\n**Benefits:**\n\n* **Code Reusability:** A single custom field type handles both requirements, reducing code duplication and maintenance overhead.\n* **Flexibility:** The templates feature allows for easy customization and adaptation to different scenarios.\n* **Improved UX:** The custom field type provides a consistent and user-friendly experience for entering both radio/checkbox values and address information.\n\n**Considerations:**\n\n* **Development Effort:** Creating and testing the custom field type will require some development effort.\n* **Potential Complexity:** The custom field type may become complex if it needs to support a wide range of functionality.\n* **Documentation:** Thorough documentation is essential to ensure developers understand how to use the custom field type effectively."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNATS Server has a number of JSON based messages - monitoring, JetStream API and more. These are consumed,\nand in the case of the API produced, by 3rd party systems in many languages. To assist with standardization\nof data validation, variable names and more we want to create JSON Schema documents for all our outward facing\nJSON based communication. Specifically this is not for server to server communication protocols.\nThis effort is ultimately not for our own use - though libraries like `jsm.go` will use these to do validation\nof inputs - this is about easing interoperability with other systems and to eventually create a Schema Registry.\nThere are a number of emerging formats for describing message content:\n* JSON Schema - transport agnostic way of describing the shape of JSON documents\n* AsyncAPI - middleware specific API description that uses JSON Schema for payload descriptions\n* CloudEvents - standard for wrapping system specific events in a generic, routable, package. Supported by all\nmajor Public Clouds and many event gateways. Can reference JSON Schema.\n* Swagger / OpenAPI - standard for describing web services that uses JSON Schema for payload descriptions\nIn all of these many of the actual detail like how to label types of event or how to version them are left up\nto individual projects to solve. This ADR describes how we are approaching this.\n\n## Decision\n","### Overview\nWe will start by documenting our data types using JSON Schema Draft 7. AsyncAPI and Swagger can both reference\nthese documents using remote references so this, as a starting point, gives us most flexibility and interoperability\nto later create API and Transport specific schemas that reference these.\nWe define 2 major type of typed message:\n* `Message` - any message with a compatible `type` hint embedded in it\n* `Event` - a specialized `message` that has timestamps and event IDs, suitable for transformation to\nCloud Events. Typically, published unsolicited.\nToday NATS Server do not support publishing Cloud Events natively however a bridge can be created to publish\nthose to other cloud systems using the `jsm.go` package that supports converting `events` into Cloud Event format.\n### Message Types\nThere is no standard way to indicate the schema of a specific message. We looked at a lot of prior art from CNCF\nprojects, public clouds and more but found very little commonality. The nearest standard is the Uniform Resource Name\nwhich still leaves most of the details up to the project and does not conventionally support versioning.\nWe chose a message type like `io.nats.jetstream.api.v1.consumer_delete_response`, `io.nats.server.advisory.v1.client_connect`\nor `io.nats.unknown_message`.\n`io.nats.unknown_message` is a special type returned for anything without valid type hints. In go that implies\n`map[string]interface{}`.\nThe structure is as follows: io.nats.`<source>`.`<catagory>`.v`<version>`.`<name>`\n#### Source\nThe project is the overall originator of a message and should be short but descriptive, today we have 2 - `server` and `\njetstream` - as we continue to build systems around Stream Processing and more we'd add more of these types. I anticipate\nfor example adding a few to Surveyor for publishing significant lifecycle events.\nGenerated Cloud Events messages has the `source` set to `urn:nats:<source>`.\n|Project|Description|\n|-------|-----------|\n|`server`|The core NATS Server excluding JetStream related messages|\n|`jetstream`|Any JetStream related message|\n#### Category\nThe `category` groups messages by related sub-groups of the `source`, often this also appears in the subjects\nthese messages get published to.\nThis is a bit undefined, examples in use now are `api`, `advisory`, `metric`. Where possible try to fit in with\nexisting chosen ones, if none suits update this table with your choice and try to pick generic category names.\n|Category|Description|\n|----|-----------|\n|`api`|Typically these are `messages` used in synchronous request response APIs|\n|`advisory`|These are `events` that describe a significant event that happened like a client connecting or disconnecting|\n|`metric`|These are `events` that relate to monitoring - how long did it take a message to be acknowledged|\n#### Versioning\nThe ideal outcome is that we never need to version any message and maintain future compatibility.\nWe think we can do that with the JetStream API. Monitoring, Observability and black box management is emerging, and we\nknow less about how that will look in the long run, so we think we will need to version those.\nThe philosophy has to be that we only add fields and do not significantly change the meaning of existing ones, this\nmeans the messages stay `v1`, but major changes will require bumps. So all message types includes a single digit version.\n#### Message Name\nJust a string identifying what this message is about - `client_connect`, `client_disconnect`, `api_audit` etc.\n","**Decision:** Use JSON Schema for describing all outward facing JSON based communication.\n\n**Rationale:** JSON Schema is a transport agnostic way of describing the shape of JSON documents, which makes it a good choice for describing the messages that NATS Server exposes. JSON Schema is also relatively easy to use and understand, and it is well-supported by a variety of tools and libraries.\n\n**Consequences:**\n\n* **Positive:**\n    * Standardize data validation, variable names, and more for all outward facing JSON based communication.\n    * Ease interoperability with other systems.\n    * Create a foundation for a Schema Registry.\n* **Negative:**\n    * May require additional effort to create and maintain JSON Schema documents.\n    * May not be as expressive as other formats, such as AsyncAPI or CloudEvents."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis document describes NATS Headers from the perspective of clients. NATS\nheaders allow clients to specify additional meta-data in the form of headers.\nNATS headers are similar to\n[HTTP Headers](https://tools.ietf.org/html/rfc7230#section-3.2) with some important differences.\nAs with HTTP headers:\n- Each header field consists of a field name followed by a\ncolon (`:`), optional leading whitespace, the field value, and optional\ntrailing whitespace.\n- No spaces are allowed between the header field name and colon.\n- Field value may be preceded or followed by optional whitespace.\n- The specification may allow any number of strange things like comments/tokens\netc.\n- The keys can repeat.\nMore specifically from [rfc822](https://www.ietf.org/rfc/rfc822.txt) Section\n3.1.2:\n> Once a field has been unfolded, it may be viewed as being composed of a\n> field-name followed by a colon ("":""), followed by a field-body, and terminated\n> by a carriage-return/line-feed. The field-name must be composed of printable\n> ASCII characters (i.e., characters that have values between 33. and 126.,\n> decimal, except colon). The field-body may be composed of any ASCII\n> characters, except CR or LF. (While CR and/or LF may be present in the actual\n> text, they are removed by the action of unfolding the field.)\n### Unique to NATS Headers\n###### Version header\nInstead of an HTTP method followed by a resource, and the HTTP version (`GET / HTTP/1.1`),\nNATS provides a string identifying the header version (`NATS/X.x`),\ncurrently 1.0, so it is rendered as `NATS/1.0␍␊`.\n###### Case preserving\nNATS treats application headers as a part of the message _payload_ and is agnostic to the\napplication use-case between publishers and subscribers; therefore, NATS headers are _case preserving_.\nThe server will not change the case in message conveyance, the publisher's case will be preserved.\nAny case sensitivity in header interpretation is the responsibility of the application and client participants.\n> Note: This is _different_ from HTTP headers which declare/define that web server and user-agent participants should ignore case.\nWith above caveats, please refer to the\n[specification](https://tools.ietf.org/html/rfc7230#section-3.2) for information\non how to encode/decode HTTP headers.\n### Enabling Message Headers\nThe server that is able to send and receive headers will specify so in it's\n[`INFO`](https://docs.nats.io/nats-protocol/nats-protocol#info) protocol\nmessage. The `headers` field if present, will have a boolean value. If the\nclient wishes to send headers, it has to enable it must add a `headers` field\nwith the `true` value in its\n[`CONNECT` message](https://docs.nats.io/nats-protocol/nats-protocol#connect):\n```\n""lang"": ""node"",\n""version"": ""1.2.3"",\n""protocol"": 1,\n""headers"": true,\n...\n```\n### Publishing Messages With A Header\nMessages that include a header have a `HPUB` protocol:\n```\nHPUB SUBJECT REPLY 23 30␍␊NATS/1.0␍␊Header: X␍␊␍␊PAYLOAD␍␊\nHPUB SUBJECT REPLY 23 23␍␊NATS/1.0␍␊Header: X␍␊␍␊␍␊\nHPUB SUBJECT REPLY 48 55␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊PAYLOAD␍␊\nHPUB SUBJECT REPLY 48 48␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊␍␊\nHPUB <SUBJ> [REPLY] <HDR_LEN> <TOT_LEN>\n<HEADER><PAYLOAD>\n```\n#### NOTES:\n- `HDR_LEN` includes the entire serialized header, from the start of the version\nstring (`NATS/1.0`) up to and including the ␍␊ before the payload\n- `TOT_LEN` the payload length plus the HDR_LEN\n### MSG with Headers\nClients will see `HMSG` protocol lines for `MSG`s that contain headers\n```\nHMSG SUBJECT 1 REPLY 23 30␍␊NATS/1.0␍␊Header: X␍␊␍␊PAYLOAD␍␊\nHMSG SUBJECT 1 REPLY 23 23␍␊NATS/1.0␍␊Header: X␍␊␍␊␍␊\nHMSG SUBJECT 1 REPLY 48 55␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊PAYLOAD␍␊\nHMSG SUBJECT 1 REPLY 48 48␍␊NATS/1.0␍␊Header1: X␍␊Header1: Y␍␊Header2: Z␍␊␍␊␍␊\nHMSG <SUBJECT> <SID> [REPLY] <HDR_LEN> <TOT_LEN>\n<PAYLOAD>\n```\n- `HDR_LEN` includes the entire serialized header, from the start of the version\nstring (`NATS/1.0`) up to and including the ␍␊ before the payload\n- `TOT_LEN` the payload length plus the HDR_LEN\n\n## Decision\n",Implemented and merged to master.\n,"Utilize NATS Header feature to add meta-data to messages, preserving case and allowing for multiple headers with the same name."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis document describes the _Lame Duck Mode_ server notification. When a server enters lame duck mode, it removes itself from being advertised in the cluster, and slowly starts evicting connected clients as per  [`lame_duck_duration`](https://docs.nats.io/nats-server/configuration#runtime-configuration). This document describes how this information is notified\nto the client, in order to allow clients to cooperate and initiate an orderly migration to a different server in the cluster.\n\n## Decision\n","The server notififies that it has entered _lame duck mode_ by sending an [`INFO`](https://docs.nats.io/nats-protocol/nats-protocol#info) update. If the `ldm` property is set to true, the server has entered _lame_duck_mode_ and the client should initiate an orderly self-disconnect or close. Note the `ldm` property is only available on servers that implement the notification feature.\n",The client can be notified of the lame duck mode by listening for the `CLUSTER_EVENT_PING` command.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe client library should take a random IP address when performing a host name resolution prior to creating the TCP connection.\n\n## Decision\n,"This was driven by the fact that the Go client behaves as described above and some users have shown interest in all clients behaving this way.\nSome users have DNS where the order almost never change, which with client libraries not performing randomization, would cause all clients\nto connect to the same server.\n","**Decision:** Implement a random IP address selection algorithm for host name resolution in the client library.\n\n**Rationale:**\n\n* **Improved resilience:** Randomly selecting an IP address reduces the risk of a single point of failure in case one or more IP addresses become unavailable.\n* **Load balancing:** By distributing requests across multiple IP addresses, the load is balanced and ensures optimal performance.\n* **Security:** Random IP address selection makes it more difficult for attackers to predict and target specific IP addresses.\n* **Compliance:** Some regulations or policies may require the use of random IP address selection for host name resolution.\n\n**Implementation Plan:**\n\n* Implement a function or method in the client library to select a random IP address from the list of resolved IP addresses.\n* Use a random number generator to select the IP address.\n* Incorporate error handling to handle cases where no IP addresses are resolved.\n\n**Consequences:**\n\n* **Increased complexity:** The client library will require additional code to implement the random IP address selection algorithm.\n* **Potential latency increase:** Selecting a random IP address may introduce a slight latency increase as the client library needs to resolve multiple IP addresses and select one.\n* **Security implications:** If the random number generator is not secure, it could lead to a compromised IP address being selected."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to use OLS Api results as objects (Ontology, Term, ...) instead of standard coreapi Document objects.\nCoreapi API results are Document (a kind of OrderedDict in fact). making access to property annoying for programmer,\nhaving to know expected keys.\n\n## Decision\n",To implement.\n,"We will use the python-semanticweb library to transform the Document objects into Ontology, Term, ... objects."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to be able to loop simply over Ontologies / Terms results, without bothering if a\nnew call is made to change page.\nOLS API results are paginated, the page size is a parameter in Query.\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\nThe actual calls to API are hidden from final users.\n```python\nfrom ebi.ols.api.client import OlsClient\nclient = OlsClient()\nontology = client.ontology('fpo')\nterms = ontology.terms()\nindividuals = ontology.individuals()\nproperties = ontology.properties()\n# work with all 'list' item types\nfor term in terms:\n# do whatever\nprint(term)\n# Direct List'like access on all list types\nterm = terms[1254]\nindividual = individuals[123]\n# ...\n```\n\n## Decision\n",To Implement\n,Create a `LazyObject` layer over pagination.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",Architectural decisions will be recorded using an Architectural Decision Record (ADR). ADRs will be maintained in the project repository in the `docs/architecture/decisions` directory. ADRs will follow a standardized template and will be reviewed and approved by the architecture team before being merged.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What type of arichitecture is to be used ?**\nConsidered options:\n1. Monolith\n2. Modular Monolith\n3. Microservices\nDrivers:\n* Project is realized as GreenField\n* A small development team (~10)\n* Little experience in implementing distributed software\n* Load - (50-150 req/sec) - not so much\n\n## Decision\n,"Option no 2 - Modular Monolith.\nArchitecture suitable for teams with little experience in implementing distributed systems.\nIt gives the possibility of gradual migration to distributed. Suitable for  greenfield systems, where high variation in requirements is expected.\narchitecture.\n","After careful consideration of the drivers and options available, we have decided to adopt a **modular monolith** architecture for this project.\n\nA modular monolith architecture provides the following benefits:\n\n* It allows us to develop and deploy the application as a single unit, which simplifies deployment and maintenance.\n* It provides better performance than a microservices architecture, as there is no overhead from inter-process communication.\n* It is easier to implement and manage than a microservices architecture, as there is no need to worry about service discovery, load balancing, or other distributed systems concerns.\n\nWe believe that a modular monolith architecture is the best fit for this project, given the small development team, the lack of experience in implementing distributed software, and the relatively low load."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What kind of database should be used to storage aggregates data ?**\nConsidered options:\n1. Document database (MongoDB)\n2. Relational database\nDrivers:\n* Simple objects will be saved.\n* There will be no relationship between objects .\n* A Read Model combining data from different aggregates is required .\n\n## Decision\n,Option no 1 - document database - MongoDB.\n,"**Decision:** Use a document database (MongoDB).\n\n**Rationale:**\n\n* A document database is more suitable for storing simple objects without relationships, which is the case for the aggregates data.\n* MongoDB provides flexibility and scalability for storing and querying large amounts of data.\n* The lack of relationships between objects simplifies the data model and makes it easier to maintain.\n* MongoDB supports aggregation and querying features, enabling the creation of a Read Model that combines data from different aggregates."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What technology is to be used for persistence?**\nConsidered options:\n1. Spring Template\n2. JPA + Spring Repositores\nDrivers:\n1. Technology must be simple and well known\n2. Cannot require a lot of code writing\n\n## Decision\n,Option no 2 - JPA+Spring Repositories.\n,**Spring Template**
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\n**What type of application architecture is to be used ?**\nConsidered options:\n1. Layered architecture\n2. Ports and adapters architecture\nDrivers:\n* The Reservation module is an example of a deep module.\n* A large number of business rules.\n* Some of the rules are quite complicated.\n* Application should be well - tested .\n\n## Decision\n,Option no 2 - Ports and Adapters architecture. It is dedicated for modules with high business complexity.\nIt separates domain logic from application logic. It allows to create independent tests for each type of logic.\n,"The decision is to use a layered architecture for the application.\n\n**Reasoning:**\n\n* A layered architecture provides a clear separation of concerns, which is important for a complex application with a large number of business rules.\n* The layered architecture makes it easier to test the application, as the different layers can be tested independently.\n* The layered architecture is also more scalable than a ports and adapters architecture, as it is easier to add new layers to the application as needed."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** Implement an Architectural Decision Record (ADR) system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n* This is Open Source app which may be used by big variety of projects\n* TypeScript is widely known nowadays, so there is high chance to get contribution from other peple if this technology is used\n* Project's author and main contibutor (@kopach) is fluent in this technology, so no need in spending extra efort in learning something differrent\n* TypeScript is type safe, so more secure and potentially should prevent from common mistakes\n* TypeScript integrates well with JavaScript so all libraries from both ecosystems can be used easilly\n\n## Decision\n",Use TypeScript as main and only programming language in this project\n,Use TypeScript as the main programming language for the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe design proposed in [018](018-Dedicated-Ingress-Controllers.md) proved\nimpossible to implement. Every ingress controller requires an AWS Network Load\nBalancer (NLB), and AWS have hard limits on the number of NLBs we can create\nper VPC and availability zone (AZ).\n\n## Decision\n","We will:\n* Disable mod-security on the default ingress-controller\nThis should enable this ingress-controller to comfortably handle thousands of ingresses.\n* Create and manage a set of ingress controllers with mod-security enabled\nMost, if not all, production services will want the protection of a web\napplication firewall, and mod-security is the easiest to enable. We need to\nensure that each ingress controller only handles as many mod-security-enabled\ningresses as it can reliably cope with.\n* Continue to have some dedicated ingress controllers\nDuring our aborted migration to dedicated ingress controllers for every\nnamespace, several ingress controllers were created for specific services. We\nwill leave these in place to avoid additional disruption to these service\nteams.\n",Implement a single NLB-based ingress controller per VPC per AZ.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:\n- Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team\n- Significant variation in deployment, monitoring and lifecycle management across products\n- Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers\nThe last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:\n- Kubernetes\n- Mesos / Mesosphere / DC/OS\n- Docker Swarm\n- AWS ECS\n- CloudFoundry\nGiven the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from ""modern"" cloud-native 12-factor applications through to ""legacy"" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.\nFrom the remaining list of major container platforms, Kubernetes is the clear market leader:\n- Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard\n- Managed Kubernetes services from all major cloud vendors\n- Broad ecosystem of supporting tools and technologies\n- Increasing support for Kubernetes as a deployment target for commercial and open-source software projects\nThere is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.\n\n## Decision\n",Use Kubernetes as the container management component and core technology for our new hosting platform.\n,**Decision:** We will standardise on Kubernetes as the MOJ Digital container management platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team [is transitioning to Concourse CI](003-Use-Concourse-CI.md) internally. Product teams should also be able to use it, however, given that:\n- Switching to Concourse CI would require familiarity with how it works and incur additional overhead\n- We have not yet developed a streamlined approach to deployments through Concourse CI in order to confidently and properly offer support\n- Product teams already use third party CI systems\nWe think it would be good as a starting point to make it easy for teams to deploy directly from the third party CI systems that teams are already using, rather than requiring the deployments to be implemented in Concourse CI.\n\n## Decision\n","We will support deploying applications to the Cloud Platform from third party CI systems and will offer documentation on how to do so, at least for the most commonly used CI systems.\n","Teams can deploy directly from the third party CI systems that they are already using, rather than requiring the deployments to be implemented in Concourse CI."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a lot of scripts, pipeline definitions, terraform files, yaml files and\ntemplates which need to define and use variables. We want a consistent\nconvention for naming these so that, as we write code in multiple,\ninter-dependent repositories, we can be confident that the names we are using\nare correct.\n\n## Decision\n",We will always use snake case (e.g. `foo_bar`) for variable names which appear\nin terraform/yaml files and templates.\n,"We will use a consistent naming convention for variables across all repositories. The convention will be as follows:\n\n* Variable names will be all lowercase.\n* Variable names will be descriptive and specific.\n* Variable names will be prefixed with the repository name, followed by a period.\n* For example:\n\n```\n# This variable is used to specify the name of the S3 bucket to use.\npipeline.s3_bucket_name\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMigrating from live-0 to live-1 cluster. The reason behind this is based on the need to move to a dedicated AWS account (moj-cp), which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region as Cloud Platform requirement to host data in the UK, rather than in Europe.\n\n## Decision\n","After some long consideration of possible options, the decision has been made to migrate from the live-0 cluster to the new live-1 cluster.\nSince we only want to be running a single cluster, we will need to shut down live-0 as soon as it's no longer needed. Also services migrate from live-0 to live-1 sooner will avoid the complexities of running two parallel clusters.\n",Migrate the live-0 cluster to a dedicated AWS account (moj-cp) in the London (UK) region.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we are building our new platform on Kubernetes we have already found the need to build quite a few clusters. These have been for a range of purposes including users, testing new ideas (""sandbox""), testing new functionality (""test""), deploying apps to them (""non-production"").\nAs we are still learning we are finding that:\n1. we need to continue building new clusters for different purposes and\n2. we often need to test the cluster creation process\n3. we want to differentiate between clusters that have users on them and those that are for internal testing purposes\n4. we do not want to differentiate cluster by function (e.g. ""perf-test"", ""sandbox"") or status (""non-production"").\nTo make this easier we propose having a naming scheme that makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.\n\n## Decision\n","We will name all clusters with the following naming scheme:\n- `live-{n}` for any cluster that have users on them, for instance `live-1`.\n- `test-{n}` for any cluster that do not have users on them and are used by the cloud platform team only, for instance `test-2`.\nWe will number the clusters sequentially.\n",We will use the following naming scheme for our Kubernetes clusters:\n- `user-` for clusters that have users on them\n- `internal-` for clusters that are for internal testing purposes\nThis naming scheme makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team self-host an Elasticsearch cluster with Kibana and Logstash (ELK). This cluster has suffered numerous outages (see [CPT-282](https://dsdmoj.atlassian.net/browse/CPT-282) and [CPT-152](https://dsdmoj.atlassian.net/browse/CPT-152) in Jira) that have been difficult to recover from.\nReasons behind this move were:\n* Average of almost one week per month spent on debugging, fixing and reviving ELK\n* Lengthy downtimes which made data recovery pointless\n* Self hosted ELK stacks cost was significantly higher than AWS ElasticSearch solution\n* Not working ELK cluster was also a blocker for product teams as they couldn't see any application logs\n\n## Decision\n",Replace our self hosted ELK stack with the managed AWS Elasticsearch\n,**Migrate the self-hosted ELK cluster to AWS ElasticSearch.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Cloud Platform was originally set up to have a single ingress controller to\nmanage all ingresses in the cluster. So, every new ingress added a config block\nto one, large nginx config file, and all traffic to all services hosted on the\nCloud Platform is routed through a single AWS load balancer.\nAlthough this was both easy to manage, and saved us some money on load\nbalancers (approx. $25/month per ingress), it has become unsustainable. We\nusually have 6 replicas of the ingress controller pod, and we have started to\nsee instances of several of these pods crash-looping (usually because they have\nrun out of shared memory, which cannot be increased in kubernetes. See [this\nissue] for more information).\nWe believe this is because the nginx config has become so large (over 100K\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\nmoved.\n\n## Decision\n","We will create a separate AWS load balancer and ingress-controller for every\nnamespace in the cluster. An ""ingress class"" annotation will cause traffic for\na particular ingress to be routed through the appropriate AWS load balancer and\ningress-controller. See our [module repository] for more details.\n""System"" ingresses (e.g. those used for concourse, grafana, etc.) will continue\nto use the default ingress-controller. There should only ever be a handful of\nthese, compared with hundreds of team ingresses, so the load on the default\ningress-controller should stay within acceptable limits.\n","Migrate from a single ingress controller with a large config file to multiple ingress controllers, each with a smaller config file. This will reduce the load on each ingress controller and improve the stability of the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe maintain a lot of [terraform modules] which teams use to manage AWS\nresources for their namespaces.\nIn the past, lettings different namespaces use different versions of terraform\nmodules has caused problems because there has not always been a clear upgrade\npath from a module that is several versions old, to the latest version.\nIn these cases, it would have been easier to have upgraded every namespace when\nthe significant change was made to the module, because at that point in time\nthe relevant knowledge and understanding are fresh. Leaving it until later\nmakes the whole process much more difficult.\n\n## Decision\n",We decided to ensure that all namespaces are always using the latest version of\nevery module.\n,"Maintain a single version of each terraform module used in namespaces, and enforce that all namespaces using that module are upgraded to any new versions as soon as possible (ideally within one or two weeks of the new version being released)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Cloud Platform needs to host both citizen-facing, production services, and development environments for service teams to iterate on their code, or just set up sandboxes for experimentation and learning.\nTo support this, should we have separate clusters for production, development, and staging? Or, should we run a single cluster hosting all these different types of workload?\n\n## Decision\n","After consideration of the pros and cons of each approach we went with one cluster, using namespaces to partition different workloads.\nSome important reasons behind this move were:\n- A single k8s cluster can be made powerful enough to run all of our workloads\n- Managing a single cluster keeps our operational overhead and costs to a minimum.\n- Namespaces and RBAC keep different workloads isolated from each other.\n- It would be very hard to keep multiple clusters (dev/staging/prod) from becoming too different to be representative environments\nTo clarify the last point; to be useful, a development cluster must be as similar as possible to the production cluster. However, given multiple clusters, with different security and other constraints, some 'drift' is inevitable - e.g. the development cluster might be upgraded to a newer kubernetes version before staging and production, or it could have different connectivity into private networks, or different performance constraints from the production cluster.\nBased on our past experience, these differences tend to increase over time, to the point where the development cluster is too far away from production to be representative. The extra work required to maintain multiple environments becomes wasted effort.\nIf namespace segregation is sufficient to isolate one production service from another, then it is enough to isolate a team's development environment from a production service.\nIf namespace segregation is not sufficient for this, then the whole cloud platform idea doesn't work.\n","**Decision:** Have separate clusters for production, development, and staging.\n\n**Rationale:**\n\n* **Isolation:** Separate clusters isolate production workloads from development and staging, reducing the risk of production outages due to changes in other environments.\n* **Resource Allocation:** Different clusters can be tailored to the specific needs of each environment, optimizing resource allocation and performance.\n* **Security:** Production clusters can implement more stringent security measures than development and staging clusters, ensuring the protection of sensitive data.\n* **Cost Optimization:** Development and staging clusters can be provisioned with lower-cost resources, reducing overall infrastructure costs.\n* **Scalability:** Separate clusters allow for independent scaling of each environment, ensuring performance and availability as needed.\n* **Deployment Flexibility:** Independent clusters provide flexibility in deploying new features or updates, as changes in one environment do not impact the others.\n* **Compliance:** Separate clusters may be required for compliance with regulations or industry standards that mandate segregation of different environments."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want users of the cloud platform to be able to access Kibana so that they can see the logs for their applications in a central place. AWS Kibana does not provide easy ways for users to authenticate. We need to put a proxy in front of Kibana so that users can authenticate with Github and then be redirected to the [Kibana dashboard][kibana-webconsole] to access their logs.\n\n## Decision\n,It has been decided to use a combination of Auth0 and an OIDC proxy app. The application is managed in the [cloud-platform-terraform-monitoring repo][kibana-proxy] and configured ministryofjustice GitHub organization users to access Kibana.\n,Implement an API Gateway that proxies requests to Kibana with a custom authorizer to handle Github authentication. The API Gateway will be configured with a custom domain so that users can access Kibana directly without having to go through the AWS console.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOn the Cloud Platform, there is a need to implement various policies to safeguard our tenant applications and to enforce best practices.\nKubernetes offers various mechanisms that cover some of our needs (eg.: `ResourceQuotas` to prevent resource exhaustion and `PodSecurityPolicies` to enforce non-root containers) but there are other areas for which there is no builtin solution. However, kubernetes implements a Dynamic Admission Control API which introduces [admission webhooks][admission-control]. This API provides an easy way with which to expand on the existing admission controllers (built in the apiserver).\nOur immediate need was to prevent users from reusing hostnames in `Ingresses`. Although our ingress controller prevents hijacking of hostnames, it does so silently and furthermore, this is not a documented behaviour. Therefore, we decided that the user should not be allowed to reuse hostnames already defined in other `Ingresses` and receive a useful error message if they try to do that.\n\n## Decision\n","We explored a number of existing solutions in the open source community, as well as the possibility of implementing our own and we also discussed the issue with other organisations that use kubernetes before reaching a conclusion.\nEventually we decided to introduce the [Open Policy Agent][open-policy-agent]:\n- It is a generic framework for building and enforcing policies (whereas most other existing implementations were designed around specific problems)\n- The policies are defined in a declarative, high-level language\n- It is designed for cloud-native environments\n- It provides a kubernetes integration\n- It provides a way by which to unit test the policies\n- The project is adopted by CNCF\nAlthough the project is still in alpha and very likely to change in the near future, we decided that it is stable enough for our needs and worth adopting even at these early stages, since the benefits outweigh the cost.\n",Implement a Kubernetes Admission Webhook to enforce the uniqueness of hostnames in `Ingresses`.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team currently use a self-host Jenkins server for CI/CD pipeline. This solution is not cloud native. There is a large amount of custom configuration which has equated to a vast amount of user request tickets and an opaque service which would be very difficult to recover [CPT-364](https://dsdmoj.atlassian.net/browse/CPT-364)  There is also a reoccurring issue which has caused a number of outages [CPT-384](https://dsdmoj.atlassian.net/browse/CPT-384).\nReasons behind this move were:\n* Average of almost one week per month spent on debugging, fixing and reviving jenkins\n* Unnecessary downtimes\n* With the move to Kubernetes a cloud native CI/CD solution is needed, all jobs are written as code\n* Non working jenkins was a blocker for product teams\n\n## Decision\n",Replace self hosted Jenkins with self hosted Concourse CI pipeline\n3. Automate deployment of concourse using Terraform and Helm\n4. WIP\n~~Product teams have confirmed their success builds and deploys. zero downtime and minimal maintenance from Cloud Platforms side.~~\n,Migrate CI/CD to a cloud native solution like ArgoCD.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https://github.com/ministryofjustice/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\nPutting technical documentation in GitHub has additional benefits:\n* Using pull requests gives us a clear review and approval process\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\n* The information can be held in the open and viewed by anyone\n\n## Decision\n,1. Our architecture decision log for the new cloud platform will be held in GitHub\n,"**Decision:** Store technical documentation alongside the code it describes in GitHub.\n\n**Rationale:**\n\n* GitHub provides a clear review and approval process through pull requests.\n* It integrates well with daily development workflows (e.g., writing code).\n* The information can be made publicly accessible for transparency.\n* This approach aligns with the team's desire to keep technical documentation close to the code it implements, ensuring consistency and relevance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to be able to test that the production cluster(s) and any test clusters we create behave the way we expect them to.\nSo, we need automated tests that exercise a cluster and confirm that the effect we get is the one we expected.\n\n## Decision\n","We chose the ruby testing framework [rspec] for this.\nAlthough there are some go-based testing frameworks for kubernetes, there are some problems with them, and some benefits to rspec:\n* The kubernetes testing frameworks seem quite immature, with limited documentation, tooling and other resources such as examples\n* There is limited go expertise in the team\n* RSpec is a very mature framework, with a lot of tooling, documentation and support\n* There is a lot of ruby/rspec experience in the wider organisation\n* Ruby is our scripting language of choice, so rspec fits with that\n",Use [ClusterFuzz](https://github.com/GoogleCloudPlatform/cluster-fuzz).
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nService teams' apps currently run on [one Kubernetes cluster](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/012-One-cluster-for-dev-staging-prod.md). That includes their dev/staging/prod environments - they are not split off. The key reasoning was:\n- Strong isolation is already required between apps from different teams (via namespaces, network policies), so there is no difference for isolating environments\n- Maintaining clusters for each environment is a cost in effort\n- You risk the clusters diverging. So you might miss problems when testing on the dev/staging clusters, because they aren't the same as prod.\n(We also have clusters for other purposes: a 'management' cluster for Cloud Platform team's CI/CD and ephemeral 'test' clusters for the Cloud Platform team to test changes to the cluster.)\nHowever we have seen some problems with using one cluster, and advantages to moving to multi-cluster:\n- Scaling limits\n- Single point of failure\n- Derisk upgrading of k8s\n- Reduce blast radius for security\n- Reduce blast radius of accidental deletion\n- Pre-prod cluster\n- Cattle not pets\n### Scaling limits\nMulti-cluster helps us we encounter a scale limitation. For example, we've found ourselves unexpectantly hitting an AWS limit during k8s upgrade. In this situation we could off-load some apps to another cluster. It would be advantageous to put each cluster in its own AWS account, to avoid limits, which are imposed per-account.\n### Single point of failure\nRunning everything on a single cluster is a 'single point of failure', which is a growing concern as more services use CP. Multi-cluster would allow us to quickly move apps off a broken cluster to another cluster.\nSeveral elements in the cluster are a single point of failure:\n- ingress (incidents: [1](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-10-06-09-07-intermittent-quot-micro-downtimes-quot-on-various-services-using-dedicated-ingress-controllers) [2](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-04-15-10-58-nginx-tls))\n- external-dns\n- cert manager\n- kiam\n- OPA ([incident](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-02-25-10-58))\n### Derisk upgrading of k8s\nOnce you start a Kubernetes version upgrade, rolling back becomes infeasible (incidents: [1](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#q1-2020-january-march)).\nWith multi-cluster we could do a ""blue-green"" upgrade - spin up an cluster at the newer k8s version and then carefully move the apps across to it.\n### Reduce blast radius for security\nTaking a layered approach to security, extra isolation is beneficial. It resists lateral movement and minimizes the impact of a breach.\nIsolation is added when you split the workloads across multiple clusters, even if they are in the same VPC. And further isolation is gained with separate VPCs or separate AWS accounts.\nMore sensitive apps may require this isolation.\nPre-prod environments are likely to often be running new code that has not have been through all the reviews, quality and security checks yet, so there may be a case for keeping these more isolated from environments with access to production data.\n### Reduce blast radius of accidental deletion\nIn the case of accidental deletion being run by a Cloud Platform team member, the ability to run administrative commands on only one cluster at a time would reduce the impact of this event, such as [this incident](https://runbooks.cloud-platform.service.justice.gov.uk/incident-log.html#incident-on-2020-09-21-18-27-some-cloud-platform-components-destroyed). Disaster Recovery procedures are in [good shape now](https://runbooks.cloud-platform.service.justice.gov.uk/disaster-recovery-scenarios.html#cloud-platform-disaster-recovery-scenarios), but it's worth minimizing blast radius all the same.\n### Pre-prod cluster\nTest clusters are used by the Cloud Platform team to test changes. These are made as realistic as possible, inheriting IaC and config from the main cluster, and the availability of test apps. However test clusters have a different lifecycle, don't run realistic loads and don't have traffic / loads. Ideally we test CP changes in the most realistic way possible, before changes are deployed to the part of the platform where production workloads are running.\nMulti-cluster will allow us to put pre-prod environments on a separate cluster to prod environments. Then changes to the platform, once tested on a test cluster, we can roll them out to the pre-prod cluster, as a more realistic test, ahead of rolling out to the prod clusters. This is particularly beneficial for more fundamental changes like upgrading k8s, and single points of failure such as ingress or OPA.\n### Challenge of moving apps\nIf we were to create a fresh cluster, and an app is moved onto it, then there are a lot of impacts:\n- **Kubecfg** - a fresh cluster will have a fresh kubernetes key, which invalidates everyone's kubecfg. This means that service teams will need to obtain a fresh token and add it to their app's CI/CD config and every dev will need to refresh their command-line kubecfg for running kubectl.\n- **IP Addresses** - unless the load balancer instance and elastic IPs are reused, it'll have fresh IP addresses. This will particularly affect devices on mobile networks that accessing our CP-hosted apps, because they often cache the DNS longer than the TTL. And if CP-hosted apps access third party systems and have arranged for our egress IP to be allow-listed in their firewall, then they will not work until that's updated.\n\n## Decision\n",TBD\n,"**Decision**: We will move to a multi-cluster architecture. Each cluster will be dedicated to each environment using the same lifecycle as the environment e.g. one cluster for dev/test, one for prod. Environment name will be added to cluster name e.g. 'platform-test-ua'\n\n**Rationale**: As we can see from the Context section, there are several advantages to moving to a multi-cluster architecture, including:\n- Scaling limits\n- Single point of failure\n- Derisk upgrading of k8s\n- Reduce blast radius for security\n- Reduce blast radius of accidental deletion\n- Pre-prod cluster\n\nThe main challenge of moving to a multi-cluster architecture is the need to reconfigure kubecfg and IP addresses for each cluster. However, this is a one-time cost, and the benefits of moving to a multi-cluster architecture outweigh the costs.\n\n**Consequences**: \n- Service teams will need to obtain fresh tokens and add them to their app's CI/CD config.\n- Every dev will need to refresh their command-line kubecfg for running kubectl.\n- CP-hosted apps may experience some downtime while their IP addresses are updated.\n- There is a risk that some apps may not be able to be moved to a new cluster due to technical dependencies."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\n\n## Decision\n","After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\n","Enforce a lifecycle policy with a retention period of 180 days for ECR repositories created for use in the Cloud Platform to ensure that the AWS limit of 1000 images is not hit. This ensures that older images are automatically deleted, keeping the registry within the AWS limit and avoiding potential issues with deploying applications. Additionally, teams are encouraged to implement their own cleanup mechanisms to ensure that only necessary images are retained."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMoJ Cloud Platform team has decided to use [kubernetes for container management platform](https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/004-use-kubernetes-for-container-management.md) following the outcome of MOJ Digital's approach to infrastructure management. The team needed the below features for the infrastructure management:\n- An universal authentication mechanism to manage users without depending on the cloud provider\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\n- Able to integrate external tools easily\n- Able to manage and configure the control plane for any custom changes\n\n## Decision\n,"There are several leading cloud providers who provide managed production-ready kubernetes cluster:\n- Amazon Elastic Kubernetes Service (Amazon EKS)\n- Azure Kubernetes Service (AKS)\n- Google Kubernetes Engine (GKE)\nWe decided to host our cluster on AWS because our service team has good development experience working with AWS services. This made it easier for teams to migrate to the kubernetes platform\nWe decided to manage the kubernetes cluster ourselves rather than using EKS mainly for the below reasons:\n- When the time MOJ needed to build the kubernetes, Amazon EKS was still in the Alpha stage and was not production ready. Also Amazon EKS require to use IAM for user authentication which will be an overhead for managing users of service teams\n- Kubernetes(k8s) allows to authenticate using OIDC and therefore it was easy to manage the authentication externally using Auth0\n",Use [OpenShift Container Platform](https://www.redhat.com/en/technologies/cloud-computing/openshift) as the container management platform.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe key proposition of Cloud Platform is to do the ""hosting"" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\nIn agreeing a good interface for service teams, there several concerns:\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\n\n## Decision\n","1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads/resources.\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\n* RBAC - teams can only administer k8s resources in their own namespaces\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\n4. Isolation between AWS resources is achieved using access control.\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\n6. The namespace definition is held in GitHub.\nGitHub provides a mechanism for peer-review, automated checks and versioning.\nOther options considered for configuring a namespace do not come with these advantages, for example:\n* a console / web form, implemented as a custom web app (click ops)\n* commands via a CLI or API\nNamespace definitions are stored in the [environments repo](https://github.com/ministryofjustice/cloud-platform-environments)\n7. Namespace changes are checked by both a bot and a human from the CP team\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\nTo mitigate these concerns:\n* [automated checks](https://github.com/ministryofjustice/cloud-platform-environments/tree/main/.github/workflows) are used to validate against common problems\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\n8. Pipeline to deploy namespace automatically.\nThe ""deploy pipeline"" is a CI/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\n9. Teams have full control within their Kubernetes namespace\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI/CD, for deploying their app.\n","To address multi-tenancy concerns, adopt the concept of _namespaces_ in Kubernetes. These partitions have limited resources and provide isolation between different workspaces. Service teams can have one namespace per environment (dev, test, prod), and one per application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of our [planning principles](https://docs.google.com/document/d/1kHaghp-68ooK-NwxozYkScGZThYJVrdOGWf4_K8Wo6s/edit) we highlighted ""Building in access control"" as a key principle for planning our building our new cloud platform.\nMaking this work for the new cloud platform means implementing ways that our users &mdash; mainly developers &mdash; can access the various bits of the new infrastructure. This is likely to include access to Kubernetes (CLI and API), AWS (things like S3, RDS), GitHub, and any tooling we put on top of Kubernetes that users will access as part of running their apps (e.g. ELK, [Prometheus](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/026-Managed-Prometheus.md#choice-of-prometheus), [Concourse](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/003-Use-Concourse-CI.md)).\nAt the current time there is no consistent access policy for tooling. We use a mixture of the Google domain, GitHub and AWS accounts to access and manage the various parts of our infrastructure. This makes it hard for users to make sure that they have the correct permissions to do what they need to do, resulting in lots of requests for permissions. It also makes it harder to manage the user lifecycle (adding, removing, updating user permissions) and to track exactly who has access to what.\nWe are proposing that we aim for a ""single sign on"" approach where users can use a single logon to access different resources. For this we will need a directory where we can store users and their permissions, including what teams they belong to and what roles they have.\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\n\n## Decision\n",We will use GitHub as the identify provider for the cloud platform.\nWe will design and build the new cloud platform with the assumption that users will login to all components using a single GitHub id.\n,We will use GitHub as our initial user directory for the new cloud platform. Authentication for the new services that we are building will be through GitHub.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe cloud platforms team currently use Docker Registry for storage of docker images. This solution is self-hosted, needs regular patching, and occasionally has downtime.\nExample of an issue [CPT-274](https://dsdmoj.atlassian.net/browse/CPT-274).\nWe want to update the container registry to avoid some of the problems we have been seeing with it. The container registry will also be a key part of our new [Kubernetes based infrastructure](https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/004-use-kubernetes-for-container-management.md).\nThe criteria for selecting a new solution included:\n* Finding a solution that would work with well GitHub based identity which is one of our [major architectural choices](https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/006-Use-github-as-user-directory.md)\n* Decreasing the amount of day to day ops work, perhaps by using a managed service\n* Good value for money\n* Preference for open source tooling\n* Can or is able to run addons i.e. security scanners\n* We can make images that we host public by default\nFrom this process we tentatively chose ECR. Unfortunately none of the SaaS registries in common use support Github authentication, but as ECR uses IAM for authentication, and IAM supports federated identity with Github via Auth0, it will in theory be possible to define ECR access policies that reference Github teams and roles.\n\n## Decision\n",We will replace our self hosted Docker Registry to managed Elastic Container Registry (ECR).\n,"Migrate to Amazon Elastic Container Registry (ECR) for the following reasons:\n\n- ECR is a managed service, which will reduce the amount of day-to-day ops work required to maintain the container registry.\n- ECR supports federated identity with GitHub via Auth0, which meets our requirement for a solution that works with GitHub-based identity.\n- ECR is a good value for money, and it is open source tooling.\n- ECR can run addons, such as security scanners.\n- ECR allows us to make images that we host public by default."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have to send requests to a server that requires unique Request IDs for each message.\nSo, we need to generate uuids.\n\n## Decision\n",We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\nhttps://www.npmjs.com/package/uuid\n,Use the built-in `uuid` module in Python to generate UUIDs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSupporting multiple Kafka versions is something quite trivial due to the way the\nprotocol was designed. With the [`ApiVersions`](https://kafka.apache.org/protocol#The_Messages_ApiVersions)\nmessage, clients are able to retrieve the minimum and maximum supported version\nfor all API messages in a broker.\nThat is critical part in the connection flow and is suggested to be done right\nafter the connection has been established, however such message is only\navailable as of v0.10.\nIn Kafka v0.11, a new record batch format was introduced. This format offers\nbetter metadata control, headers, and support for transactions.\nMore info:\n- https://cwiki.apache.org/confluence/display/KAFKA/KIP-35+-+Retrieving+protocol+version\n- https://kafka.apache.org/documentation/#messages\n\n## Decision\n","This library will only be usable and guaranteed to be fully working with Kafka\nv0.11+, although it's highly recommended always use the latest version\navailable (v2.2.0 at the moment).\n","**Decision:** Migrate Kafka to v0.11 or higher.\n\n**Rationale:**\n\n* **Support for multiple Kafka versions:** Kafka v0.11 introduces the `ApiVersions` message, which allows clients to retrieve the supported API versions from a broker. This makes it possible to support multiple Kafka versions with the same client library.\n* **Improved record batch format:** Kafka v0.11 introduces a new record batch format that provides better metadata control, headers, and support for transactions. This new format improves performance and efficiency.\n* **Backward compatibility:** Kafka v0.11 is backward compatible with v0.10, so existing clients can continue to access the brokers without any changes.\n\n**Consequences:**\n\n* **Migration effort:** Migrating Kafka to v0.11 or higher will require some effort. The effort will depend on the size and complexity of the Kafka deployment.\n* **Client compatibility:** Clients that do not support the `ApiVersions` message will not be able to connect to brokers running v0.11 or higher. These clients will need to be upgraded to a newer version that supports the `ApiVersions` message."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Java implementation of the protocol uses `final static` properties to define\nthe schema for requests and responses. These are defined using objects, making\nit easy to support multiple versions for each API call.\nPHP doesn't have this feature so we need decide how to solve this.\n\n## Decision\n","We're still going to use objects to manipulate data and write/read content\nto/from Kafka, however it would be simpler to use arrays in constants of each\nrequest/response class.\nEach field would be an array item, the key would be the field name and the value\nwould be the field type (or another array for more complex configuration).\nThe mapping would like this:\n```php\nuse Lcobucci\Kafka\Protocol\Type;\nfinal class DoSomethingRequest\n{\nprivate const SCHEMAS = [\n[\n'error_code'   => Type\Int16::class,\n'api_versions' => [\n'type'     => Type\ArrayOf::class,\n'nullable' => false, // optional, default = false\n'items'    => [ // just type name if items don't have complex structure\n'api_key'     => Type\Int16::class,\n'min_version' => Type\Int16::class,\n'max_version' => Type\Int16::class,\n],\n],\n],\n];\n}\n```\n","**Decision:**\n\nUse PHP constants and arrays to define the protocol schema.\n\n**Rationale:**\n\n* Constants provide a statically defined schema that can be easily accessed and modified.\n* Arrays allow for easy versioning and support for multiple API calls.\n* This solution is compatible with the PHP language and follows its coding conventions.\n\n**Consequences:**\n\n* **Ease of use:** The constants and arrays can be easily used throughout the PHP codebase.\n* **Maintainability:** The schema can be easily changed or updated without having to modify the underlying objects.\n* **Performance:** Constants are evaluated at compile time, improving performance compared to dynamic object properties.\n* **Flexibility:** Arrays allow for dynamic schema definition, enabling support for additional API calls and versions in the future."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKafka's protocol defines some primitive types which are used to send requests\nand parse responses: https://kafka.apache.org/protocol#protocol_types\nProviding an easy way to define the schema of the requests and responses is\nquite critical to make this library extensible enough.\n\n## Decision\n,"We've decided to basically port the Java implementation to the PHP world,\nbecause it was very well written and it simplifies things by a lot.\nSome minor things obviously have to be adapted and for now we'll leave some\ntypes to the upcoming releases - just because they aren't need to implement the\nmessages we're planning to provide at the moment.\n",To encode and decode protocol types we will use reflection.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe've previously decided to require PHP 7.2, however many months have passed and\nPHP 7.3 is quite stable nowadays.\n\n## Decision\n",Bump up requirement to latest stable PHP version (7.3 at the moment).\n,"**Decision:** Upgrade the minimum required PHP version to 7.3.\n\n**Rationale:**\n\n* PHP 7.3 is now stable and widely adopted.\n* The upgrade will bring performance improvements, security enhancements, and new features.\n* The cost of supporting both PHP 7.2 and 7.3 is low compared to the benefits of upgrading.\n\n**Consequences:**\n\n* Users with PHP 7.2 will need to upgrade to 7.3 to continue using the application.\n* The application may need to be tested to ensure compatibility with PHP 7.3."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n",The change that we're proposing or have agreed to implement.\n,"The decision itself, in the form of a title and description.\n## Consequences\nThe implications or effects of making this decision, both positive and negative.\n## Rationale\nThe reasoning or judgment that led to this decision. This section should explain the evaluation of alternatives and why the chosen alternative was selected."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","**Document Architectural Decisions**\n\n**Description:**\nEstablish a process for documenting and tracking architectural decisions throughout the project lifecycle.\n\n**Rationale:**\n* Provides a historical record of the design process and reasoning behind technical choices.\n* Facilitates knowledge sharing and collaboration among team members and stakeholders.\n* Helps identify and address potential architectural risks and challenges early on.\n* Ensures consistency and accountability in decision-making.\n\n**Consequences:**\n\n* **Benefits:**\n    * Improved communication and understanding of architectural intent.\n    * Reduced duplication of effort and wasted time.\n    * Increased traceability and accountability for decisions.\n* **Costs:**\n    * Time and effort required to document and track decisions.\n    * Potential for excessive documentation.\n\n**Alternatives Considered:**\n\n* Informal communication and documentation\n* Using a dedicated tool for architectural decision-making\n* No decision documentation\n\n**Decision:**\n\nImplement a process for documenting architectural decisions using a combination of methods:\n\n* **Online Collaboration Tools:** Use a collaborative platform to capture and share decisions as they are made.\n* **Technical Documentation:** Incorporate decision summaries into project documentation, such as design documents and requirements specifications.\n* **Architectural Decision Logs:** Maintain a central log of all significant architectural decisions, including rationale, alternatives considered, and consequences.\n\n**Owners:**\n\n* Project Architect\n* Lead Developer\n* Technical Team\n\n**Next Steps:**\n\n* Establish guidelines for documenting and reviewing architectural decisions.\n* Train team members on the decision documentation process.\n* Regularly review and update decision logs as the project evolves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeciding which minimum PHP version to require is quite important because it\nmight restrict people from using the library, at the same time supporting many\nversions increases the cost to maintain the library.\n\n## Decision\n","For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\nonly).\nThe main motivation for only allowing it to be installed in 64-bit systems is\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\ncan't be achieve in 32-bit systems.\nI believe that libraries' maintainers also have the role to help the evolution\nof the PHP ecosystem, which makes me want to require a more up-to-date version\nof the language.\n","Based on the provided context, the best decision would be to require the latest PHP version that is supported by at least 95% of users. This can be determined by using a tool like [PHP Package Dependency Checker](https://github.com/sensiolabs/dependency-checker), which can analyze a project's composer.json file to determine the PHP versions that are compatible with the project's dependencies.\n\nRequiring the latest PHP version ensures that you are using a stable and secure version of the language. It also allows you to take advantage of new features and performance improvements that are available in the latest version.\n\nBy using a tool like PHP Package Dependency Checker, you can determine which PHP version is supported by the majority of your users. This will help you to strike a balance between supporting a wide range of users and minimizing the cost of maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application's scope is not well defined and may cover a lot of concepts around the Menu generation\nfeatures, such as:\n- managing a recipes catalog\n- searching in this catalog for recipes based on multiple criteria\n- collaborating on this catalog\n- handling allergies and tastes\n- generating shopping lists\nAll above concepts may be complex to model and should not be considered as simple data in a CRUD system. Other new\nfeatures may emerge in the future. In addition, all those features are related but should not be treated as a whole in\na big ball of mud architecture.\n\n## Decision\n","[Domain-driven design](https://domainlanguage.com/wp-content/uploads/2016/05/DDD_Reference_2015-03.pdf) as defined by\nEric Evans will help modeling the different concepts managed by the `menu-generation` application, keep the ability to\nadapt the model based on new insights and split the whole application into different bounded contexts to avoid mixing\nthose concepts in one giant messy code base.\n","**Decision:**\n\nThe `menu-generation` application should adopt a **Domain-Driven Design (DDD)** approach to address its complex domain and ensure maintainability and scalability.\n\n**DDD Principles:**\n\n* **Identify bounded contexts:** Define clear boundaries for different functional areas within the application, such as recipe management, collaboration, and shopping list generation.\n* **Develop domain models:** Create rich and expressive domain models for each bounded context, capturing the business rules and constraints of the problem space.\n* **Extract use cases:** Identify and document specific use cases that the application must fulfill.\n* **Define aggregate roots:** Determine the primary entities that represent the core business concepts in each bounded context and serve as the root of a collection of related objects.\n\n**Benefits of DDD:**\n\n* **Improved maintainability:** Decouples different functionalities, making it easier to evolve and update the application over time.\n* **Enhanced comprehension:** Clearly defines the domain concepts and their interactions, making the codebase more understandable.\n* **Scalability:** Supports the growth and complexity of the application by isolating different functional areas.\n* **Increased extensibility:** Facilitates the addition of new features and capabilities without compromising existing functionality."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\naggregates and services) from the infrastructure (i.e. application clients and persistence).\n\n## Decision\n,"`menu-generation` application will adopt [hexagonal architecture](https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)),\nas it aims to provide this separation.\n",**Decision:** Isolate domain elements from the infrastructure by:\n- Using a [dependency inversion principle](https://en.wikipedia.org/wiki/Dependency_inversion_principle) via [adapters](https://martinfowler.com/eaaCatalog/adapter.html) to connect domain services to the infrastructure.\n- Using a [repository pattern](https://martinfowler.com/eaaCatalog/repository.html) to abstract the persistence layer from the domain entities.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application will use the Spring framework along with other third party libraries, thus requires a\ndependency management tool.\nEffective development lifecycle requires Continuous Integration, thus a build management tool is necessary.\n[Gradle](https://gradle.org/) is one of the two main build management tools for the Java ecosystem with [Maven](http://maven.apache.org/).\nGradle is considered as more extensive and quicker than Maven. It is also well integrated with many tools, including\nthe Spring framework.\n\n## Decision\n",Gradle will be used to manage project dependencies and build tasks.\n,Use Gradle as the dependency management and build management tool for the `menu-generation` application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application needs to expose data and services in order to be used by multiple different applications,\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\napplication or enhance the provided services.\nWe want the application data and services to be easily consumed through well established communication protocols, such\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\nmanaging cumbersome protocols or data formats.\n\n## Decision\n",`menu-generation` application data and services will be exposed through a REST API.\n,"We decided to leverage RESTful APIs as the primary mechanism for exposing data and services from the `menu-generation` application. RESTful APIs are well-established, widely used, and easy to consume by various applications. They provide a simple and straightforward way to access and manipulate data and services through HTTP requests."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEven a self-discoverable API needs to be documented to let consumers understand the underlying application features and\nthe resources attributes significance.\nA good API documentation will provide requests and responses structure, defining each attribute, giving examples and\nmixing functional and technical information so that consumers do not need to traverse different documents to get a\ncomplete overview of the features and technical concerns.\nThe documentation must also provide links between different parts of the document, so that it is easy to go back and\nforth between those parts.\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\n\n## Decision\n",`menu-generation` will generate its API documentation with [Spring Rest-Docs](https://spring.io/projects/spring-restdocs).\n,"**Decision:** Use OpenAPI Specification (OAS) to document the self-discoverable API.\n\n**Rationale:**\n\n* OAS is a widely-adopted industry standard for describing RESTful APIs.\n* It provides a standardized way to define request and response structures, attribute definitions, examples, and functional/technical information in a single document.\n* OAS supports hypermedia links, making it easy to navigate between different parts of the documentation.\n* OAS is language-agnostic and can generate documentation from code, keeping it in sync with the underlying features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\neither be a command that performs an action, or a query that returns data to the caller, but not both.\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\nuse queries with much more confidence, and only be careful with commands orchestration.\nCommands and queries terminology is already used in the `menu-generation` application.\n\n## Decision\n","Command-query separation will be enforced in the [core hexagon](./0003-adopt-hexagonal-architecture.md), especially in\napplication services.\n",**Decision:** Adopt the command-query separation principle for the `menu-generation` application.\n\n**Rationale:**\n* Command-query separation promotes a clear separation between methods that change state (commands) and those that do not (queries).\n* This separation improves the reliability of queries and simplifies the orchestration of commands.\n* The existing use of commands and queries terminology in the `menu-generation` application supports the adoption of this principle.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe don't want the development tools or Continuous Integration pipeline to be strongly bound to [Gradle](0006-manage-build-with-gradle.md).\n[Make](https://linux.die.net/man/1/make) is an utility agnostic of any language or build management tools.\n\n## Decision\n,"Make will be used to execute build tasks, abstracting Gradle and potential other tools used during build execution.\n",Use Make as the underlying task runner for development tools and the Continuous Integration pipeline.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\nservices implementations dependencies into the services defined in the core hexagon.\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\nSpring for many years and masters many of the provided features.\n\n## Decision\n",Spring framework will be the backbone for `menu-generation` application.\n,Use Spring as the framework for implementing the incoming adapters in the REST API having Spring as a runtime dependency.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\nDefining new acceptance tests must be easy, through reusable step definitions.\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development)\napproach.\n\n## Decision\n",[Cucumber](https://cucumber.io/) will be used to describe and execute acceptance tests in `menu-generation` application.\n,Use BDD (Behavior-Driven Development) and Gherkin syntax with a sufficient level of expressiveness.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a programming language to implement the `menu-generation` application.\n`menu-generation` application will be developed initially during the author's free time, thus this time is limited.\nDevelopment may involve other developers in the future, and the chosen language should not restrict participation.\n\n## Decision\n","Java is a broadly used programming language, and the most well mastered one by the author. Thus Java will be the\nprogramming language used to implement the `menu-generation` application.\n",**Python**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Defining acceptance tests with Cucumber](0009-test-features-with-cucumber.md) will help writing user-oriented acceptance\nscenarii. However, to help maintaining an acceptance tests client library, we need to organize this library to be\nextensible, without mixing concerns between Gherkin interpreter and API unitary client steps.\nThe acceptance tests results report must be readable and help investigating in case of error, providing hints about\nwhat wrong happened during API calls.\n\n## Decision\n",The [Serenity](http://www.thucydides.info/#/) framework will be used to define the acceptance tests library.\n,"**Organize acceptance tests in folders by **feature**.**\n\n**Implement the **Gherkin steps interpreter** in a separate library, with a dedicated DSL.**\n\n**Use a **reporting library** to generate detailed reports in case of error.**"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\ndependent of the underlying system.\n[Docker](https://www.docker.com/) is a widespread container based solution that can be used during development lifecycle\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https://kubernetes.io/).\n\n## Decision\n",`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\nto help running the application and its dependencies in a consistent and isolated environment.\n,Use Docker to containerize the `menu-generator` application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n`menu-generation` application will expose its data and services through a [REST API](0004-expose-services-through-rest-api.md).\nOne of the constraints of the REST architectural style implies exposing **Hyperlinks as the Engine of Application State**,\nmeaning any resource should provide links to other accessible resources based on its current state in the application,\nso that clients can discover which actions are available without interpreting the actual resource attributes.\n[Spring HATEOAS](https://spring.io/projects/spring-hateoas) comes by default with [HAL](http://stateless.co/hal_specification.html)\nmedia type support.\n\n## Decision\n",`menu-generation` application will expose its resources using the HAL media type.\n,Use [Spring HATEOAS](https://spring.io/projects/spring-hateoas) as API framework to enable [Hypermedia as the Engine of Application State](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm) in the `menu-generation` application.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContract-first approach enforces a definition of the API contract before implementation. This ensures that the API is\nwell designed for specific use-cases, based on consumers point of view.\nAdopting a contract-first approach forces to ensure that contract is not broken during implementation and evolution of\nthe application.\nWe need to ensure that the contract is well defined through a specification and implementation respects this specification.\n\n## Decision\n",[OpenAPI Specification](https://swagger.io/specification/) will be used to enforce contract-first approach.\n,**Decision:** Adopt a contract-first approach through a specification and ensure compliance with it during implementation and evolution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany modern systems require up-front data before a device can be discovered and added to a network. To facilitate this, a Provisioning system is to be added as part of the Discovery services, with the aim to provide the information required for devices to join the network.\nTwo use cases are currently forseen.\n1. Users must provide device information prior to discovery. This information is managed by the binding, however a consistent way is needed to provide this data from the user, to the binding. An example of this is systems may require a cryptographic key to be provided before a device is able to be discovered.\n2. Centrally administered systems may want to provision information about a device from a central repository. Thus a user could be shipped the device, plug it in, and it would automatically be discovered and added with little user interaction.\nIt is proposed to add a `ProvisioningService` and `ProvisioningRegistry` to act as an interface between the provisioning information (from the user, or other system) and the binding. The binding will be able to query the `ProvisioningRegistry` to retrieve information about any provisioned devices when discovery is enabled, and will be able to update the `ProvisioningRegistry` if a device is successfully discovered. Additionally, the `ProvisioningRegistry` can be queryable to allow the status of provisioned devices to be retrieved.\nThe provisioning information provided through the `ProvisioningRegistry` is a set of properties that are interpretted by the binding. These are linked by a `ProvisioningUID` which is used to identify the device. This is akin to a `ThingTypeUID`, however used to identify the device before it is discovered.\n\n## Decision\n",TBD\n,"**Decision:** Implement a `ProvisioningService` and `ProvisioningRegistry` to manage and provide provisioning information for devices before discovery.\n\n**Rationale:**\n\n* **Meets use cases:** The proposed solution addresses both use cases identified in the context:\n    * For use case 1, users can provide device information through the `ProvisioningService` before discovery.\n    * For use case 2, centrally administered systems can provision information from a central repository, enabling automatic discovery and addition of devices.\n* **Consistent interface:** The `ProvisioningRegistry` provides a consistent interface for both users and binding to access and manage provisioning information.\n* **Identification and interpretation:** The `ProvisioningUID` allows for unique identification of devices being provisioned, and the binding can interpret the provided properties accordingly.\n* **Scalability and flexibility:** The proposed solution is scalable and can handle multiple provisioned devices and bindings. It also allows for future extensions to support additional provisioning mechanisms or properties."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany advanced features require additional information to be added to `Type`s - this could include secondary information as part of a `Command` or `State` update, or security context, `Command` source identifier, priority, etc. The type system structure inheritted from Eclipse SmartHome is difficult to extend due to the class hierarchy. A wrapper could be used around the `Command` or `State`, but this would require considerable rework of core functions.\nThere is no base class for a `Type` - the ESH structure defines `Type`, `Command` and `State` as interfaces and many concrete `Type`s are derived from an `enum` (eg `OnOffType`, `UpDownType` etc) while others may be derived from Java classes (eg `DecimalType`). The absence of a clear class hierarchy within the framework makes it difficult or impossible for the framework to control and extend the `Type` system - one of the fundamental building blocks of the framework.\nIt is proposed to rewrite the type system to introduce a clear class hierarchy. `Type`, `Command` and `State` remain as interfaces however an `AbstractBaseType` class is introduced from which all concrete `Type`s are derived. `ComplexType` and `PrimitiveType` are not generally used and are to be removed as they add unnecessary complexity. The type system should be implemented to be compatible at the basic level with the Eclipse SmartHome definitions such that backward compatability with existing bindings is not compromised.\nThe ESH type system derives `DecimalType` and `QuantityType` from `Number` which makes it impossible to extend the `AbstractBaseType`. To resolve this we provide the core `Number` functionality with a new `NumberType` class. This provides the same interface, but can now extend `AbstractBaseType`. This potentially means that any code within the framework that checks `instanceof Number` will now fail and such checks must instead use `instanceof NumberType`.\n\n## Decision\n","This change provides a better separation between Java and OpenSmartHouse types, and most importantly allows further enhancements in functionality so is agreed to be implemented.\n","Implement a new type system based on a clear class hierarchy, with `Type`, `Command` and `State` as interfaces and all concrete `Type`s derived from an `AbstractBaseType` class. Migrate existing `Type`s to the new system and introduce a new `NumberType` class to provide the core `Number` functionality, allowing for future extension of the `AbstractBaseType`. Deprecate `ComplexType` and `PrimitiveType` and ensure backward compatibility with existing bindings."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOften a binding will need to provide notifications to users for binding specific administrative purposes. This is not linked to `Item` data, but is designed to provide users feedback on binding specific functions or alerts. Examples of this could be -:\n* Alerting the user about failed joining, or a trust issue with devices joining\nIn general, these notifications are designed for display on an administration UI rather than a user UI, but there is of course nothing to prevent any UI from subsribing for such events.\nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way. This should provide enough information to allow the UI to present the notification in a standard way - showing the severity, and a notification text.\nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted. An enhancement may be to run these through a registry which would allow notifications to be registered and actively dismissed once read by a user with the appropriate access level. At this point, the notification may be removed, or archived to provide an audit trail.\nFurther extensions to the notification system may be considered to provide images, action buttons etc are considered outside of the scope of this initial proposal.\n\n## Decision\n",TBD\n,"**Decision:**\n\nImplement a generic binding notification mechanism that allows bindings to send simple notifications to users.\n\n**Rationale:**\n\n* Provides a consistent mechanism for bindings to send notifications to users.\n* Notifications can be displayed in a standard way by UIs, ensuring users can easily understand and address them.\n* Notifications can be persisted in a registry to ensure they are not lost if the administrative user is not watching the UI.\n* Notifications can be archived to provide an audit trail.\n\n**Consequences:**\n\n* Improves the user experience by providing a consistent and informative way to notify users of binding-specific events.\n* Improves operational efficiency by allowing administrators to quickly identify and resolve binding issues.\n* Provides a foundation for future enhancements to the notification system, such as support for images, action buttons, and more."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEclipse SmartHome defined a static type system. This may not be extended, and uses a static class `TypeParser` to create a `Type`. This limits framework modularity and extensability and is generally against OSGi principals.\nThe architecture is to be changed to introduce a `TypeProvider` service, and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users.\nThe standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatability with ESH/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`.\n\n## Decision\n","The introduction of the `TypeRegistry` and `TypeProvider` services, and refactoring of the current types to be provided by a `CoreTypeProvider` is implemented. Existing users of `TypeParser` within the _OpenSmartHouse_ core bundles is to be refactored to use these services, and new bundles should avoid using the static `TypeParser`.\n","The static type system in Eclipse SmartHome should be replaced with a dynamic system that uses a `TypeProvider` service and `TypeFactory` interfaces. The `TypeProvider` is implemented to provide a `Type`, or set of `Type`s to the system, and `TypeFactory` is used by users to create `Type`s. Between these is a `TypeRegistry` which consumes the `Type`s from multiple `TypeProvider`s, and supplies the `TypeFactory` interface for system users. The standard core types are to be provided by a `CoreTypeProvider`, and the current static `TypeParser` is retained for backward compatibility with ESH/OH, but is not to be used by native _OpenSmartHouse_ bundles. Instead, users requiring `TypeParser` functionality should utilise the `TypeRegistry`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOpenHAB uses OSGi as a runtime.\nThis runtime promotes clear separation between implementation and contract (API/implementation/SPI) packages.\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that ""implementation"" packages stay hidden.\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\nSuch approach promotes tight coupling between contract and implementation.\nMore over, it also promotes exposure of specific implementation classes via public API.\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes/excludes mechanism for dependencies.\nIt would work properly, but openHAB core is a single jar which makes things even harder.\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\nopenHAB utilizes Apache Karaf for provisioning of the application.\nKaraf provisioning itself is capable of verifying its ""features"" based on declared modules, bundles, JAR files, etc.\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\nPresent structure of modules / bundles is as follows:\n```\n[openhab thing core] <--- [openhab rest core]\n```\n\n## Decision\n","Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\nWe decided that a clear separation between API and implementation packages should be made.\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\n```\n[openhab thing api] <--- [openhab rest thing]\n^\n|\n[openhab thing core]\n```\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\n",- Split the current `openhab-core` module into `openhab-core-api` and `openhab-core-impl`.\n- Move all API classes from the `org.openhab.core` package to the `org.openhab.core.api` package.\n- Move all implementation classes from the `org.openhab.core` package to the `org.openhab.core.impl` package.\n- Update all dependencies in the `openhab-core` module to only depend on the `openhab-core-api` module.\n- Update all dependencies in the `openhab-rest-core` module to only depend on the `openhab-core-api` module.\n- Update the `openhab-core-base` feature to only depend on the `openhab-core-api` module.\n- Add a new `openhab-core-impl` feature that depends on the `openhab-core-api` module.\n- Update the `openhab-runtime-base` feature to depend on both the `openhab-core-api` and `openhab-core-impl` features.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo allow the system to know if the ThingType definition has changed, a version identifier should be added to the ThingType. This will allow the system to update the thing based on the new Thing Type definition.\nThe `version` should be added to the `ThingType`, and provided by thing type providers such as the XML provider. This should be reflected into the instantiated `Thing` and provided in the `ThingDTO` for use in the user interface.\nThing Type XML definition -:\n```\n<thing-type id=""sample"" version=""2"">\n```\nIt shall then be possible for the user to decide to upgrade to the new definition. This functionality could be provided automatically, however this may result in unexpected changes to a users system as channels could be removed. Such updates are therefore considered to be better if manually initiated by the user.\nThing REST response (part) -:\n```\n{\nUID: ""test:sample:505f8d7b12"",\nthingTypeUID: ""test:sample"",\nthingTypeVersion: 2\n}\n```\nA new REST API is to be added to allow the user to change the thing type via the UI. This uses the `/thing/{{thinguid}}/migrate` call. This migrates the specified thing to the latest version of the ThingType.\n\n## Decision\n","This has minimal impact on the framework, and allows the user and UI to manage the migration of things without the user having to delete and re-add things, which is quite complex as item links also need to be re-added.  Further enhancements could be added to allow this to (optionally) be undertaken automatically.\n","**Decision:** Add a `version` field to the `ThingType` and provide it in the `ThingDTO` and REST API responses.\n\n**Rationale:**\n\n* Adding a `version` field to the `ThingType` allows the system to track changes to the ThingType definition.\n* Providing the `version` in the `ThingDTO` and REST API responses allows users to identify the version of the ThingType associated with a particular Thing.\n* This information enables users to decide whether to upgrade to a new version of the ThingType, providing control over potential changes to their system.\n* The addition of a REST API `/thing/{{thinguid}}/migrate` call allows users to manually initiate ThingType upgrades, promoting user agency and avoiding unexpected system changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith large bindings with many things, a ThingType and ThingHandler may not be implemented for each Thing. Instead the ThingHandler may implement dynamic definition by interrogating the device, or through a static definition via the Thing XML file. This may be implemented as a single ThingHandler for numerous different devices whose functionality is defined or discovered.  In cases where the Thing is statically defined, there will be some discovery information that is retieved from the device and used to link to the ThingType, and it is desirable for the ThingType definition and the discovery information to be defined in a single place.\nFunctionally, the ThingType definition, and Discovery information should be separate. The proposal here is to introduce a `DiscoveryIndex` service which provides the binding specific thing properties linked to a `ThingThypeUID`. `DiscoveryProperties` are provided by a `DiscoveryIndexProvider` which the binding can query.\nThe `XmlThingTypeProvider` should be modified to also implement `DiscoveryIndexProvider`, and the ThingType XML definition should be updated to introduce a `discovery-properties` section which can include the discovery information to be passed to the binding. The `discovery-properties` section is similar to the existing Thing `properties` definition, but dedicated to the `DiscoveryIndex` function.\n\n## Decision\n",TBD\n,"**Decision:**\n\nIntroduce a `DiscoveryIndex` service and a `DiscoveryIndexProvider` interface to facilitate dynamic ThingType and ThingHandler implementations by providing binding-specific Thing properties linked to a `ThingTypeUID`.\n\n**Rationale:**\n\n* Separates ThingType definition from Discovery information for functional clarity.\n* Allows for dynamic ThingTypdefinition and ThingHandler implementation by leveraging discovery information.\n* Provides a centralized location to define both ThingType definition and Discovery information, reducing redundancy.\n* Leverages existing `XmlThingTypeProvider` infrastructure by extending it to implement `DiscoveryIndexProvider`.\n* Updates the ThingType XML definition to include a `discovery-properties` section for specifying discovery information to be used by the `DiscoveryIndex` service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt has long been stated in ESH that the existing Configuration Parameter API is only meant for Thing Handler configuration and not device configuration. Many bindings use this for device configuration as well as handler configuration. From discussions on ESH, the following distinction was noted -:\n* Handler configuration is merely about configuration of communication parameters, like ip address/host & port, timeouts, access tokens, thing ids, etc.\n* Thing configuration should describe and model the configuration of the physical device. It should not only provide a view but provide an interface to alter the thing configuration. It is the bindings responsibility to implement the read/write access to the corresponding thing.\nIt is proposed to keep things simple - levering off the existing configuration parameter definition that are already being used for device configuration. There should be an indication in the parameter that this is a thing configuration, and there should be a method to define device specific information that the binding can use when sending data to the device.\nThe proposal is to add a new `device-properties` element to the configuration description. The defines a set of properties similar to other sets of properties defines in the XML descriptions. The properties may be used by the binding to specify binding and device specific information that is interpretted by the binding when sending the configuration to the device. This is implemented in `ConfigurationParameter` with the addition of the `ParameterDeviceProperty` class which contains the binding defined properties required to configure the device. If the `ConfigurationParameter` contains more than one `ParameterDeviceProperty` it is considered to be a device parameter rather than a handler parameter.\nDespite the ESH configuration definitions apparently only being for handler configuration, there is a `ConfigStatusProvider` service that allows the binding to provide configuration status via the `ConfigStatusMessage.Type` enum. This allows the binding to set status as `PENDING`, which ""should be used if the transmission of the configuration parameter to the entity is pending"". This should be persisted along with the `ConfigurationParameter` so that the binding is able to establish if configuration still needs to be sent to the device when it starts. Binding users wanting to provide the configuration status should extend the `ConfigStatusThingHandler`.\nTo allow the device property information to be utilised in the binding, a new method `getConfigParameterDescription` is added to the `BaseThingHandler` to allow the binding to retrieve the configuration description for a parameter (and hence the `ParameterDeviceProperty`)\nCurrently within the core, configuration changes are only persisted by the binding in the `ThingHandler`. It is therefore the bindings responsibility to manage the `PENDING` flag if it implements the `ConfigStatusProvider`. In general the binding shall set the status to `PENDING` if it updates any configuration, and update the status appropriately once the configuration in the device is confirmed. The `ConfigStatusService` will persist the state of all parameters since it does not have access to the configuration descriptions, and the binding must manage the status.\nA binding may get the current persisted state of the configuration status by calling `getPersistedConfigStatusInfo`. This will return the persisted state from the previous request to `ConfigStatusProvider` and allows bindings to retrieve the state on startup.\nExample XML configuration -:\n```\n<parameter name=""param-name"">\n...\n<device-properties>\n<property name=""parameter"">12</property>\n<property name=""size"">2</property>\n</device-properties>\n</parameter>\n```\nExample ThingHandler usage during initialisation -:\n```\nConfigStatusInfo configStatusInfo = getPersistedConfigStatusInfo();\nfor (String parameterName : getConfig().getProperties().keySet()) {\nConfigDescriptionParameter configDescription = getConfigParameterDescription(parameterName);\nif (configDescription.getDeviceProperties().isEmpty()) {\n// Parameter is not a remote parameter\ncontinue;\n}\nCollection<ConfigStatusMessage> configStatus = configStatusInfo.getConfigStatusMessages(parameterName);\nif (configStatus.isEmpty() || configStatus.iterator().next().type != Type.PENDING) {\n// Parameter update is not pending\ncontinue;\n}\n// Update the parameter as the update was PENDING when the handler was shut down\n}\n```\nExample ThingHandler usage during `handleConfigurationUpdate` -:\n```\nConfiguration configuration = editConfiguration();\nfor (Entry<String, Object> configurationParameter : configurationParameters.entrySet()) {\nconfiguration.put(configurationParameter.getKey(), configurationParameter.getValue());\nConfigDescriptionParameter configDescription = getConfigParameterDescription(\nconfigurationParameter.getKey());\nif (configDescription.getDeviceProperties().isEmpty()) {\n// Configuration to be sent to remote device\n}\n}\n```\n\n## Decision\n",The addition of the `ParameterDeviceProperty` allows the system to properly differentiate between handler configuration and device configuration. The persisting of `ConfigStatusMessage`s allows the binding to know on startup if `PENDING` configuration still needs to be transferred to the device. These changes have minimal impact on the wider system and allow _OpenSmartHouse_ to better support the device configuration system.\n,"To keep things simple, leverage off the existing configuration parameter definition that are already being used for device configuration. There should be an indication in the parameter that this is a thing configuration, and there should be a method to define device specific information that the binding can use when sending data to the device."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nItems currently have a single value linked to them - eg `ON` or `OFF`, or `OPEN` etc. However often there is the need to provide multiple elements of supporting information along with the `Command` or `State`. Such information may not be required for the system to run, but may be useful for rules, or advanced users where a finer level of system control is desired.\nExamples of this for both `State`s and `Command`s are -:\n* A `Dimmer` command to change a light level may have a ""rate"" associated with it to tell the device how quickly to change to the commanded state.\n* An `OnOff` state for a door lock may have information such as how the door was opened, or who opened it.\nSuch information needs to be considered in an atomic way along side the primary item `State` or `Command` and cannot in itself be provided as a separate `Item` since this will be be correlated with a specific event. For example a dimmer may be commanded to turn on with different rates in different rooms or at different times - providing multiple items - one per dimmer could work, however the asynchonous nature of the event bus does not guarantee that a rate setting will be received by the binding prior to the command itself. Current solutions within openHAB include utilising multiple items with the resulting correlation problems and potentially to massively increase the number of items, or the approach recommended by openHAB maintainers is to use JSON encoded strings which may then not be used directly in the UI.\nThis ADR proposes to add the concept of `Attributes` to all `Command`s and `State`s. An `Attribute` will provide secondary information that is directly linked to the primary event and should not be used to provide data where multiple `Item`s could be used. For example, a sensor providing both temperature and humidity should provide these as separate items, even if they are received from the sensor at exactly the same time.\n`Attribute`s are defined within the `CommandDescription` and `StateDescription` in a `Channel` definition. All attributes that a channel supports must be defined in the channel definition to allow the system to provide a level of consistency checking, and for user interfaces to provide the user with a list of attributes a channel may provide. The UI may elect to present these to the user in a similar way to other `Item`s, or the information may be ommitted.\nExamples of the definition of attributes is provided below. It is proposed to use a similar construct, and similar data types as configuration parameters - although there is no direct re-use of code, reusing the concept will make it easier for users.\n```\n<channel-type id=""door_state"">\n<item-type>Switch</item-type>\n<label>Door Lock State</label>\n<description>Locks and unlocks the door and maintains the lock state</description>\n<category>Door</category>\n<state>\n<options>\n<option value=""0"">Open</option>\n<option value=""1"">Closed</option>\n<options>\n<attributes>\n<attribute id=""unlockmethod"" type=""Number"">\n<name>Unlock Method</name>\n<description>The way in which the lock was opened</description>\n<option value=""0"">Manual</option>\n<option value=""1"">RFID</option>\n<option value=""2"">UserCode</option>\n</attribute>\n<attribute id=""userid"" type=""String"">\n<name>User Id</name>\n<description>The user who opened the lock. Not provided for manual unlocking.</description>\n</attribute>\n<attributes>\n</state>\n</channel-type>\n```\n```\n<channel-type id=""dimmer"">\n<item-type>Dimmer</item-type>\n<label>Dimmer</label>\n<description>Sets the light level</description>\n<category>Light</category>\n<command>\n<attributes>\n<attribute id=""transition"" type=""Number"">\n<name>Transition Duration</name>\n<description>Sets the transition duration in milliseconds</description>\n<maximum>5000</number>\n</attribute>\n<attributes>\n</command>\n</channel-type>\n```\nIdeally, commonly used attributes should be standardised across bindings in the same way as system channels are with channel definitions. This will improve the user experience by ensuring a unified solution no matter what binding is in use.\n\n## Decision\n",TBD\n,"Add the concept of `Attributes` to all `Command`s and `State`s.\n\nAn `Attribute` will provide secondary information that is directly linked to the primary event and should not be used to provide data where multiple `Item`s could be used.\n\nAll attributes that a channel supports must be defined in the channel definition to allow the system to provide a level of consistency checking, and for user interfaces to provide the user with a list of attributes a channel may provide.\n\nIdeally, commonly used attributes should be standardised across bindings in the same way as system channels are with channel definitions."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe UQ purple header is included in primo through include files like `assets.library.uq.edu.au/primo-sand-box/reusable-components/`, via the primo BO.\nUp until now we have been hard coding the primo-sand-box bit according to which environment we are in, and having to remember to manually change it depending on which environment we were uploading to.\nManual processes are to be avoided.\n\n## Decision\n","Generate the branch to be used, by looking at the host name and the vid parameter on the url\nhttps://github.com/uqlibrary/uqlibrary-reusable-components/commit/4f1c182\n","**Decision:** Automate the process of updating the primo-sand-box bit in the include files for the UQ purple header.\n\n**Rationale:**\n\n* Automating the process will reduce the risk of errors and inconsistencies, as well as save time and effort.\n* It will make the process more efficient and reliable, ensuring that the correct version of the header is always included in Primo.\n* By removing the need for manual updates, it will free up staff time to focus on other tasks."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis needs to work cross platform.\nThe UX will be rendered in browser, which may not appear native.\n\n## Decision\n","To implement Editions as a progressive web app, delivered in a native wrapper. The native wrapper will contain a webview, that renders the PWA.\nThe wrapper will handle required features that are not supported by PWAs such as background fetch, scheduled downloads and push notification handling.\n",Use a cross-platform UI framework such as React Native or Flutter.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe archiver lambda needs to be able to respond to issue published events from tools.\n\n## Decision\n,For the archiver to react to S3 file created events from the fronts tool.\n,"Implement an async publish/subscribe architecture for the pub/sub communication based on Kafka, an event streaming platform."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a consistent and battle tested process for releasing the app on both Android and iOS.\n\n## Decision\n,"### iOS\n#### Internal Beta\nOur internal Beta is managed through testflight on the Guardian developer account. The group which this beta is sent to is labelled `GNM`. This includes the team and internal stakeholders within the organisation. We build this automatically through Fastlane and Github Actions once a day. Occasionally we will set off builds to test things on a number of devices.\nIn github actions we have a [scheduled build](https://github.com/guardian/editions/actions?query=workflow%3Ascheduled-ios-beta) and an [ad-hoc one](https://github.com/guardian/editions/actions?query=workflow%3A%22Upload+ios-beta%22) triggered by a [script](https://github.com/guardian/editions/blob/main/script/upload-ios-build.sh)\nAll builds generate a ['release' in github](https://github.com/guardian/editions/releases) to help us keep track of build numbers against certain commits. This is handled by the [make-release script](https://github.com/guardian/editions/blob/main/script/make-release.js).\n#### External Beta\nBefore every release, we aim to do at least one external beta to gather feedback. We have a number groups within testflight that are prefixed with the name `External Testers...`. These different groups represent the different authentication methods we support. When we decide a build is good enough from an internal test, we add the build to the groups.\n#### Release\nAfter a successful beta period, we release the same build (identified by its build number) through the app store submission process.\n#### Post Release\nWe update the version number in XCode and raise that as a PR. The version number will depend on the goals for the next release. We follow a major and minor number approach with no patch i.e. 5.6\n### Android\n#### Internal Beta\nIn a similar vein as above, the Android internal beta is managed through Google Play. The APK for this is created using Fastlane through TeamCity. The name of this process is `android-beta-deploy`. The list for this is managed within the Google Play console. This process runs once per day. Users will need to update their app through their Google Play store.\n#### External Beta\nWe take a slightly different approach to iOS. Due to not being able to determine within the app whether or not the app is in Beta or release, we have a different build configuration for releasing to production - which hides the 'report bug' button.\nWe build the APK using `android-beta-deploy`. This will release a build to google play to the 'internal beta' group for internal testing by the team. It is then manually promoted within the Google Play console to our external beta testers.\nAs with iOS, releases to the play store can be tracked in [github releases](https://github.com/guardian/editions/releases) - each time the teamcity build is run a new github release is created including the play store version code of that release.\n#### Release\nAfter a successful external beta test, we **DO NOT** promote the external beta. This is because we have code in the app that attempts to determine whether or not the user is in beta. This does not work on Android as there isn't a distinction.\nAs a result, we then use the TeamCity process `android-release-deploy` to then build the APK. As you will only want to release a version that has been beta tested, you can use the [releases](https://github.com/guardian/editions/releases) list to find a release for the build you want to release, and copy the tag for that build. You can then search for this tag in the branch list within teamcity (the `android-release-deploy` config treats github tags as if they were branchs) and run a build on that tag.\nBe warned, this process will automatically release the new version of the app. You will then need to go into the Google Play console to update the release notes.\n","Use the fastlane tool to automate the app release process on both Android and iOS. Fastlane is a popular, open-source tool that provides a streamlined and efficient way to automate tasks related to mobile app development, including release management. By using Fastlane, we can ensure a consistent and battle-tested process for releasing our app on both platforms, reducing the risk of errors and saving time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\n\n## Decision\n","```sequence\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\nTypescript Layer->App Code: Consume! 🍔\nApp Code-->Ophan Analytics Library: Sends events\n```\n### Key\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it ""React Nativey"" to expose methods to the JS.\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\n**App Code** - Consumes the typescript library where its needed.\n","**Decision:** Implement Ophan using the [Segment](https://segment.com/) analytics library.\n\n## Rationale\n\n**Segment** provides a unified API for sending analytics data to multiple destinations, including Ophan. This will allow us to easily add or remove destinations as needed, and it will provide a consistent interface for sending data from both iOS and Android.\n\n**Segment** is also a well-maintained and widely used library, which means that it is likely to be reliable and performant. It also has a large community of users, which means that we can get help with any issues that we encounter.\n\nOther options that were considered include:\n\n* **Google Analytics:** Google Analytics is a popular analytics platform, but it does not have a native integration with Ophan. This would require us to write our own integration, which would be time-consuming and error-prone.\n* **Facebook Analytics:** Facebook Analytics is another popular analytics platform, but it is not as widely used as Google Analytics. This means that we would have a smaller community of users to help us with any issues that we encounter.\n* **Custom implementation:** We could write our own custom implementation for sending analytics data to Ophan. However, this would be a significant undertaking, and it would be difficult to maintain over time.\n\n## Consequences\n\n**Benefits:**\n\n* **Easy to implement:** Segment is a well-documented and easy-to-use library. This will make it easy to get Ophan up and running quickly.\n* **Reliable and performant:** Segment is a well-maintained and widely used library. This means that it is likely to be reliable and performant.\n* **Flexible:** Segment allows us to send data to multiple destinations, including Ophan. This gives us the flexibility to add or remove destinations as needed.\n\n**Drawbacks:**\n\n* **Cost:** Segment is a paid service. However, the cost is relatively low, and it is outweighed by the benefits of using a reliable and well-maintained library.\n* **Data privacy:** Segment collects and stores user data. This could be a concern for some users. However, Segment has a strong privacy policy, and they are committed to protecting user data."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe editions app needs to run and deliver background downloads functionality and a pleasant user experience across a variety of devices. The developers available to work on the project are not native apps developers and are all experienced React developers.\n\n## Decision\n,To develop the editions app as a React Native application.\n,"**Decision:** Develop the editions app using React Native.\n\n**Rationale:**\n\n* React Native allows developers to build cross-platform mobile applications using React, which the available developers are familiar with.\n* It enables efficient code reuse between iOS and Android platforms, reducing development time and costs.\n* React Native provides a consistent user experience across different devices and operating systems.\n* The use of React Native's component-based architecture promotes flexibility and maintainability, allowing for seamless updates and feature iterations.\n* React Native supports background functionality, enabling the implementation of background downloads and other asynchronous operations."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe editions lambda needs to be able to identify specific versions of an issue.\n\n## Decision\n,"To have two deployments of the backend, one for previewing, and a second for published issues.\nThe published issues deployment will replace the issueid path parameter with source/issueid.\n`source` will identify which file in the published bucket will be retreived to form the issue on.\n",Store the issue version inside the issue data.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe archiving process is time consuming and consists of many stages.\n\n## Decision\n,For the archiving process to be run as an amazon step function so we can break the process down into component stages.\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\nThis will also allow the publication process to fire the notification at the desired time.\n,**Decision:** Implement incremental archiving to reduce the overall archiving time.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Problem to solve\nProvide unified way to manage translations for translatable entities in API.\n### Possible solutions\nThe translations from collection could be embedded as objects within a `ProductOption`/`ProductOptionValue` resource\n**or** provided by [IRIs](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)\n### Decision and reasoning\nTranslations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\ndo not provide any value alone.\n\n## Decision\n,Translations should always be embedded as the collection of objects. They're irrelevant outside of the main object and\ndo not provide any value alone.\n,**Translations should be embedded as objects within the `ProductOption`/`ProductOptionValue` resource.**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`/`PUT` HTTP methods),\nRead(`GET` HTTP method), Update(`PUT`/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\nworkflow in a state machine) or command execution.\n## Decision Drivers\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\n\n## Decision\n","* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\nThe ""Taking advantage of the `Controller` REST archetype"" should be considered as a recommended solution. All Sylius\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\nor new schema should be introduced for the JSON-LD structure.\nOption 2: ""Defining custom operations in the style of command pattern"" may be useful once async data processing is\ndelivered with vanilla Sylius installation.\n","Use REST verbs (`POST`, `PUT`, `PATCH`, `DELETE`) to indicate the semantics of the operation and add a resource action in the URI.\n\nE.g., for:\n* state machine transition - `POST /users/{user}/states/{state}/transition`\n* command execution - `POST /orders/{order}/execute`"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nInitial implementation of Unified API used `/new-api` prefix, to aggregate all following endpoints. This prefix does not\nclearly state the version of it and is not future-proof. At some moment of time our ""new api"" can become ""old api"". We should\nhave clear guidance how to version our APIs.\n\n## Decision\n","As the underlaying technology, structure and content have changed significantly and taking into account easiness of first solution\nthe decision is to go with the `/api/v2` endpoint path. In the future it does not block us from the usage of the `Accept`\nheader in addition to this path, however it may be misleading for consumers.\n#### References:\n- https://github.com/api-platform/api-platform/issues/290\n- https://github.com/api-platform/core/issues/972\n- https://stackoverflow.com/questions/389169/best-practices-for-api-versioning\n- https://www.troyhunt.com/your-api-versioning-is-wrong-which-is/\n- https://github.com/api-platform/core/issues/45\n- https://github.com/api-platform/docs/issues/451\n- https://github.com/api-platform/docs/pull/452\n- https://api-platform.com/docs/core/content-negotiation/#supporting-custom-formats\n","Migrate the Unified API to use versioned paths instead of the `/new-api` prefix. For example, the new versions of the API should be accessible at `/api/v2` and `/api/v3`. This will allow us to clearly identify the version of the API being used, and will make it easier to maintain and deprecate older versions of the API."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nCustomer should have access only to locales available in their channel\n\n## Decision\n,"Chosen option: Using Data Provider\nShops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated\nquery in Doctrine Collection extension\n",**Decision:** Create a localized content distribution service that will only deliver content that is available in the customer's current channel.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhile defining new handlers, we may define a bus to which particular handler should be assigned to. If not, handler will\nbe added to all existing buses. Right now, we have only one bus in Sylius, so decision will not change current behaviour.\nHowever, we may add additional buses in the future and more than one bus is not uncommon thing.\n## Decision Drivers <!-- optional -->\n* Driver 1 - easiness of changing default behaviour and usage of the bus\n* Driver 2 - backward compatible possibility of changing behaviour\n\n## Decision\n","* Driver 1 - easiness of changing default behaviour and usage of the bus\n* Driver 2 - backward compatible possibility of changing behaviour\nChosen option: ""[Explicitly declare, where bus should be registered]"", because this options leaves the most options to end user\nwithout forcing him to use them. What is more, we may easily adjust this option in the future\n","**Decision:** Introduce a concept of default bus and allow handlers to specify a bus they should be assigned to.\n\n**Rationale:** This decision meets the following criteria:\n\n* **Easiness of changing default behaviour and usage of the bus:** By introducing a default bus, it is easy for developers to specify which bus a handler should be assigned to. This can be done by adding a `bus` option to the handler definition. If no bus is specified, the handler will be assigned to the default bus.\n* **Backward compatible possibility of changing behaviour:** This decision is backward compatible with the current behaviour of Sylius. All existing handlers will be assigned to the default bus. However, it is possible to change the default behavior in the future by changing the value of the `default_bus` configuration option.\n\n**Consequences:** The following consequences are expected from this decision:\n\n* Developers will have more control over which bus a handler is assigned to.\n* It will be easier to add new buses to Sylius in the future."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want our API to cover all the functionalities that are currently implemented in the UI.\n## Decision Drivers\n* All the functionalities implemented for API should be tested\n* Tracking whether a feature has been covered in the API or not should be easy\n\n## Decision\n,"* All the functionalities implemented for API should be tested\n* Tracking whether a feature has been covered in the API or not should be easy\nChosen *Using Behat for the feature coverage*, because it's the only option, that meets all the decision drivers criteria.\nWe will gradually add `@api` tag to the scenarios currently tagged with `@ui` and then implement the API contexts.\nAs a consequence, we will have to create a testing tool to use it in Behat contexts.\n","We will create a mapping from UI functionalities to API functionalities. This mapping will be maintained in a spreadsheet or other easily accessible document. As new API functionalities are developed, they will be added to the mapping. This will allow us to easily track which UI functionalities have been covered by the API and which still need to be implemented."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Problem to solve\nCover with API all `ProductOption`s related functionality provided by the Admin panel.\n### Context\nDuring the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle\nProduct Option's values collection, to make it efficient and easy to use.\n### Possible solutions\nValues from collection could be embedded as objects within a `ProductOption` resource **or** provided by\n[IRIs](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)\n### Decision and reasoning\nProduct option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\n(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\nresult in a massive and unmanageable response.\n\n## Decision\n","Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource\n(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).\nMoreover, values embedded together with their translations in the `ProductOption` that already has its translations would\nresult in a massive and unmanageable response.\n",Product option values should always be provided with their IRIs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSylius is by design application that will be adjusted to customer needs.\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\n\n## Decision\n","Chosen option: ""Config merging"", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\n","**Decision:** Split the API Platform configurations into smaller, more manageable chunks.\n\nThis will make it easier to customize and replace individual parts of the API Platform configuration, which will make it more flexible and maintainable.\n\n**Benefits:**\n\n* **Increased flexibility:** Smaller configuration chunks will make it easier to tailor the API Platform configuration to specific needs.\n* **Reduced maintenance:** Smaller configuration chunks will be easier to manage and update, reducing the risk of errors.\n* **Improved extensibility:** Smaller configuration chunks will make it easier to extend the API Platform configuration with custom functionality.\n\n**Drawbacks:**\n\n* **Increased complexity:** Splitting the configuration into smaller chunks may increase the overall complexity of the API Platform configuration.\n* **Potential for conflicts:** If the smaller configuration chunks are not carefully managed, there is a potential for conflicts between them.\n\n**Recommendation:**\n\nWe recommend splitting the API Platform configuration into smaller, more manageable chunks. This will provide increased flexibility, reduced maintenance, and improved extensibility. However, it is important to carefully manage the smaller configuration chunks to avoid increased complexity and potential conflicts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo achieve 100% API coverage, we need to handle emails by API.\n\n## Decision\n","Chosen option: ""Using events"", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport.\n","**Decision:** Introduce an API Gateway with email forwarding functionality.\n\n**Rationale:**\n\n* An API Gateway provides a central point of entry for API requests, allowing for easy integration with email services.\n* Email forwarding allows incoming emails to be automatically relayed to the API Gateway for processing.\n* This solution enables the API to handle emails without direct SMTP connectivity.\n\n**Consequences:**\n\n* **Increased security:** The API Gateway acts as a security layer, controlling access to the API and preventing direct email attacks.\n* **Scalability:** The API Gateway can handle high volumes of email traffic, ensuring consistent API availability.\n* **Reduced complexity:** The API does not need to implement complex email handling logic, simplifying API development.\n* **Potential latency:** Email forwarding can introduce some latency compared to direct SMTP processing.\n* **Additional costs:** The API Gateway may incur additional costs for hosting and maintenance."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhile developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\nwe have merged a unification of our endpoints in https://github.com/Sylius/Sylius/pull/11397. This move was dictated by\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\ndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\nand simplify the relations between resources. However, this change rose a lot of new issues to solve:\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\nadmin can see many more links than the regular visitor or shop user.\n* Endpoints like ""Product Show"" are resolved differently depending on the context as well. The shop products are determined\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\ndecision that we want to be explicit, which user is trying to log in.\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\nresources are channel and locale agnostic, and we should use `codes` instead.\n## Decision Drivers\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\nOne resource in the database may aggregate several API Resources\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\n\n## Decision\n","* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\nOne resource in the database may aggregate several API Resources\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\nChosen option: ""Moving back to prefixed paths for back-office and shop"". This option was selected because it allows us to\neasily leverage the Symfony Security component's benefits like firewalls and ACL. The only second and third option provides\npredictable behavior and clear separation of concerns for admin and shop. Still, suffixes on most of the routes seem like\na worse solution compared to the common prefix. The common prefix will indicate that these endpoints are grouped in the\nsame context.\n","**Decision:** Use a dual-route approach with context-dependent serialization and IRI resolution.\n\n**Rationale:**\n\n* **Different serialization for different users:** Use a centralized serialization library with context-dependent configuration to handle different serialization requirements.\n* **Endpoint resolution:** Use different route prefixes for admin and shop endpoints, and design endpoints to explicitly resolve resources based on context-specific identifiers (e.g., slug for shop products, code for admin products).\n* **IRI determination:** Establish a clear mapping between context and IRI patterns, ensuring that each exposed API Resource has a unique and predictable IRI.\n* **Split log in endpoints:** Maintain separate log in endpoints to explicitly indicate the type of user attempting to log in.\n* **Identifier in shop and admin:** Use 'slug' as the identifier in shop endpoints to facilitate URL-based product exposure, and use 'code' as the identifier in admin endpoints to support channel- and locale-agnostic resource management."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nAPI Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.\nWhile we were designing Sylius new API, we have decided to use commands in many endpoints.\nIn some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`/`id`.\nIn the past, we have been using `code`/`id` instead of IRI, then we have been using both approaches.\nNow we are trying to unify the new API and replace codes and ids with IRI everywhere.\nThe main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`/`code`in its commands and command handlers.\n\n## Decision\n","Chosen option: ""Handling and transforming IRI to `id`/`code`"". Request that is based on command and needed information like `code`/`id` should get it as IRI\n","**Decision:** Use IRI in the request and convert it to `id`/`code` in the command handler.\n\n**Rationale:**\n\n* This approach allows us to use IRI in the request, which provides more clarity and allows for easier resource identification.\n* Converting the IRI to `id`/`code` in the command handler ensures that the commands and command handlers operate on the correct resource.\n* This approach is consistent with the recommendation from API Platform to use IRI as an identifier."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\n\n## Decision\n,Halt implementation on CMS Export and focus on scaling GraphQL.\n,Adopt GraphQL as the new CMS Export solution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAttempts were made to upgrade all usage of Python from an older EOL'd version to Python 3. This presented a number of issues as we also use Ansible and Amazon Linux. The combination of these showed difficulty around how we use Ansible to install packages (`yum` module currently). The Ansible `yum` module is reserved specifically for Python 2. Python 3 requires usage of the `dnf` module. However, according to statements and current evidence, AWS has no intention to support a `dnf` extra for the life of Amazon Linux 2. It was also stated [in this forum post](https://forums.aws.amazon.com/thread.jspa?threadID=317863) ""We are committed to keeping Python 2.7 secure for the life of Amazon Linux 2"".\nAnother attempt was then made to replace our underlying OS with CentOS 8 perhaps to see how heavy of a lift this could be. With a bit of tweaks, we were able to at least run the automation that creates encrypted base AMIs. However, this is only the surface of everything that would have to be tested/changed for this move to happen. The referenced tweaks were as follows (for tracking purposes):\n- Change the default value for the `bake_ami` variable found [here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/applications/config/default.yml#L3) to a CentOS AMI ID (ami-0f5f21f0f86d11d6e was used for testing which is a ""hardened"" CentOS 8 image maintained / distributed by Frontline).\n- Change all usage of the `yum` module to the `dnf` module in this [role / configuration file](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml).\n- Create a task to set symbolic links after installing Python 3 for `/usr/bin/python -> /usr/bin/python3` and `/usr/bin/pip -> /usr/bin/pip3` since Python 3 installations follow the `<package>3` naming convention by default.\n- Comment out / remove the task for [installing, starting, and enabling the NTP daemon](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml#L27-L37) as that package was not found with default repositories on the CentOS image and it also already comes with `chrony` installed and enabled.\n- Comment out / remove Amazon Linux distro based conditional for the install of the `epel-release` package as well as an additional task that currently exists specifically for usage of Amazon Linux 2 image ([both outlined here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml#L44-L48)).\n- Comment out / remove task that [removes yum lock](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/build/roles/common/base-os/tasks/amazon.yml#L53-L56) since `dnf` was the package manager / module used in this scenario.\n- Addition of a task to install the ssm-agent as it would no longer come by default if we stopped leveraging Amazon Linux distributions.\n- Drop the usage of `unicode` type in some of our custom Ansible related Python files within the `filter_plugins` directory [here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/filter_plugins/aws_finders.py#L25) and [here](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/filter_plugins/resource_tags.py#L404). More on the ""why"" of that change/dynamic can be found in [this article](https://medium.com/better-programming/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686).\n\n## Decision\n","After multiple days of testing, analysis, troubleshooting, and presenting these numerous issues to the Operations team as well as a chat with Joe to gather thoughts from a security perspective, it has been decided at this time to ""accept risk"" on the continued usage of Python 2 at this time. At this time, our Python code is not end-user-facing (frontend is Ruby) and the bulk of it exists in internal scripts, lambdas, and our CI/CD pipeline.\nIntentions to revisit this issue would largely hinge on one of the following scenarios:\n- Amazon Linux 3 is released with full Python 3 + dnf support\n- Amazon Linux 2 changes direction to fully support Python 3 + dnf\n- Through the work of our infrastructure recommit, if we can get to a fully containerized environment, we will ensure the images used do not leverage the end of life'd version of Python if at all possible.\n","Due to the difficulties presented by attempting to upgrade Python and the lack of support for `dnf` in Amazon Linux 2, we have decided to explore alternative solutions that will not require a change in our underlying operating system.\n\nWe will investigate using a Docker container to run Python 3 and Ansible within our current Amazon Linux 2 environment. This approach will allow us to isolate the Python 3 environment from the rest of the system and avoid the potential issues caused by the lack of `dnf` support."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, users who log onto va.gov during repeat visits have potentially inconsistent behavior from the site, especially if they switch the credential they login with.\nThis is most stark with aspects of the site that leverage persistent data, such as forms that the user has partially filled out, and which must be matched up to the user.\nThe Accounts table is used today to aid in this, it creates a table entry that links an idme_uuid, icn, sec_id, to different data in our schema, and which we can retrieve\neven after a user session has been ended and a new one started.\nThere are some issues with the current implementation, however, which make the intended experience inconsistent. The Accounts table relies on a matching idme_uuid, first and foremost.\nThen will attempt to match with an ICN or a sec_id, depending on the retrieval method. If a user logs in with different credentials, such as ID.me from va.gov, and then later\nDS Logon from MyHealtheVet (then navigates to VA.gov), they may see different forms and other persistent data depending on the credential.\nMy Proposal involes refactoring the work done for the Accounts table into something more along the lines of how we actually deal with our users. This involves the concept\nof a 'credential_user', and a 'real_user', and tables to represent each of these: `user_verifications` and `user_accounts`.\nA 'credential_user' is our perspective of a user simply as information we get from a specific credential provider. For example, if a user logs in with MHV, we would\nexpect to have an MHV uuid representing their account uuid. When a user authenticates, we would store the 'credential_user' by finding or creating an entry in the `user_verifications`\ntable. This table would store the uuid from whatever credential provider they logged in with, whether or not the user was verified (LOA3/IAL2) and a link to an entry in the `user_accounts`\ntable.\nA 'real_user' is our perspective of an actual human behind the entity logging in, regardless of how they logged in. These are reflected in a `user_accounts` table, and would\nonly have an optional icn value (if icn is set, we know they are a real, verified human. Because we can make an MPI call with only an icn, this value represents all of the 'real'\ndata we have on the 'real_user'. If an icn is not set, we know there is a human behind the entity logging in, but we cannot verify it to any human in our records).\nEvery 'credential_user' will have a link to a single 'real_user', and every `user_verifications` entry will have a link to a `user_accounts` record. If a user was not verified\nand then becomes verified, the `user_accounts` entry may either simply be updated with the new icn, or, if an existing `user_accounts` record exists with that icn, it will be\nreplaced by the existing `user_accounts` entry, and the unverified `user_accounts` entry will be moved into a `deprecated_user_accounts` table.\nEvery persistent table on the backend will point to a `user_accounts` uuid, as we want to correlate these to a 'real_user', not to the specific credential. By organizing\nthings this way, we will have consistent behavior for the end user, regardless of whatever credential combination they often use. This\n\n## Decision\n",1 or more sentences summarizing the decision made.\n,"The decision is to refactor the work done for the Accounts table into something more along the lines of how we actually deal with our users. This involves the concept of a 'credential_user', and a 'real_user', and tables to represent each of these: `user_verifications` and `user_accounts`.\n\nA 'credential_user' is our perspective of a user simply as information we get from a specific credential provider. For example, if a user logs in with MHV, we would expect to have an MHV uuid representing their account uuid. When a user authenticates, we would store the 'credential_user' by finding or creating an entry in the `user_verifications` table. This table would store the uuid from whatever credential provider they logged in with, whether or not the user was verified (LOA3/IAL2) and a link to an entry in the `user_accounts` table.\n\nA 'real_user' is our perspective of an actual human behind the entity logging in, regardless of how they logged in. These are reflected in a `user_accounts` table, and would only have an optional icn value (if icn is set, we know they are a real, verified human. Because we can make an MPI call with only an icn, this value represents all of the 'real' data we have on the 'real_user'. If an icn is not set, we know there is a human behind the entity logging in, but we cannot verify it to any human in our records).\n\nEvery 'credential_user' will have a link to a single 'real_user', and every `user_verifications` entry will have a link to a `user_accounts` record. If a user was not verified and then becomes verified, the `user_accounts` entry may either simply be updated with the new icn, or, if an existing `user_accounts` record exists with that icn, it will be replaced by the existing `user_accounts` entry, and the unverified `user_accounts` entry will be moved into a `deprecated_user_accounts` table.\n\nEvery persistent table on the backend will point to a `user_accounts` uuid, as we want to correlate these to a 'real_user', not to the specific credential. By organizing things this way, we will have consistent behavior for the end user, regardless of whatever credential combination they often use. This will also allow us to more easily track and manage user data, as we will have a single, central location for all user information."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Pact Broker has been on Heroku during development, but its database is running out of rows for the free Heroku plan. We either need to use a paid Heroku plan, or move the Pact Broker onto our AWS infrastructure.\n\n## Decision\n",We will move the Pact Broker onto our AWS infrastructure.\n,Move the Pact Broker onto our AWS infrastructure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs part of the larger effort to [refactor the User & Identity on vets-api](https://app.zenhub.com/workspaces/vsp-identity-5f5bab705a94c9001ba33734/issues/department-of-veterans-affairs/va.gov-team/21012), we have decided to more tightly control access to user attributes through specific & improved getter methods. The `va_profile` methods on the User class revealed too much information about a user's MPI profile and has been rendered uneccessary by the updated getter methods.\n\n## Decision\n","The decision made is to transfer the User's `va_profile_status` and `va_profile_error` methods to `mpi_status` and `mpi_error` since that is what they are abstractions of, and to remove the `va_profile` method entirely in favor of relying on getter methods for specific attributes.\n","Deprecate the `va_profile` methods on the User class, replacing them with specific and improved getter methods that provide more granular control over access to user attributes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUsing the Rails 6 zeitwerk file loader requires strict naming conventions of files. So without a Rails inflection, `va_something` class files are expected to have the class name of `VaSomething`. With an inflection, that class name would be `VASomething`. `vets-api` uses a mix of `VA` and `Va` in class names. To make things consistent, as well as support instances of `humanize` outputing `VA` in caps as an acronym, the inflection will be introduced.\nHowever, using this inflection has a downstream affect on json responses transformed into camelCase by the [olive_branch](https://github.com/vigetlabs/olive_branch) gem. Previously keys structured like `the_va_something` were transformed into `theVaSomething`, but with inflection they will now be `theVASomething`.\n\n## Decision\n","Create a patch implemented for the olive_branch middleware that returns keys containing `VA` as if ""VA"" was not an acronym.\nIn the future vets-api should introduce a version that doesn't support olive_branch.\n",**Introduce the inflection.**\n\n**Rationale:**\n\n* Enforces consistent naming conventions in class files.\n* Supports instances of `humanize` outputting `VA` in caps as an acronym.\n\n**Consequences:**\n\n* JSON responses transformed into camelCase by the `olive_branch` gem will now have keys structured like `theVASomething` instead of `theVaSomething`.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described in this Confluence document](https://vfs.atlassian.net/l/c/58vSL6ZP).\n","**Decision:** To use an Architectural Decision Record (ADR) template for documenting and recording architectural decisions.\n\n* **Benefits:**\n    * Provides a consistent and structured approach to documenting decisions.\n    * Ensures that all relevant information is captured, such as the decision, its rationale, and the date it was made.\n    * Facilitates communication and understanding of architectural decisions among stakeholders.\n\n* **Trade-offs:**\n    * Requires discipline to maintain and update ADRs.\n    * May add some overhead to the decision-making process.\n\n**Rationale:**\n\nAn ADR template provides a framework for capturing all essential information about an architectural decision, including its context, options considered, decision made, and implications. By standardizing the format and process for documenting decisions, ADRs improve communication, collaboration, and knowledge sharing within the team.\n\n**Consequences:**\n\n* All architectural decisions will be documented using the approved ADR template.\n* Stakeholders will be responsible for maintaining and updating ADRs as decisions evolve.\n* ADRs will be used as a reference point for design, implementation, and future decision-making."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe class `KnowledgeArtifact` is a helper base class that was not intended to have its own database table (see Issue [#182](https://github.com/UST-QuAntiL/qc-atlas/issues/182)).\nThe current implementation as a joined table generates a database table.\nShould we keep the current joined table implementation for `KnowledgeArtifact`?\n## Decision Drivers <!-- optional -->\n* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\n\n## Decision\n,"* The `KnowledgeArtifact` table is referenced by a foreign key from `DiscussionTopic`\nChosen option: ""[Joined table]"", because it is already implemented, allows for references in both ways and has no significant downside.\n### Positive Consequences <!-- optional -->\n* The current implementation can stay\n",Remove the joined table implementation for `KnowledgeArtifact` and instead store it as a JSON blob in the `DiscussionTopic` table.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince the project started out with both JUnit4 and JUnit5, we only want one unit-testing framework in order to make testing writing consistent.\n\n## Decision\n","Chosen option: ""[JUnit5]"", because it is the newer version and therefore has a higher maturity and a wider feature-set.\n### Positive Consequences <!-- optional -->\n* Uniform tests\n* More and newer features\n","**Decision:**\n\nMigrate all unit tests to JUnit5.\n\n**Justification:**\n\n* JUnit5 is the latest and most actively supported version of JUnit.\n* It offers several advantages over JUnit4, including:\n    * More concise and expressive syntax\n    * Support for parameterized tests\n    * Improved error reporting\n* Migrating to JUnit5 will make our testing code more consistent and easier to maintain.\n\n**Consequences:**\n\n* All unit tests will need to be migrated to JUnit5.\n* This may require some refactoring of existing test code.\n* However, the benefits of migrating to JUnit5 outweigh the costs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n\n## Decision\n","* readable API documentation\n* effort of manually creating client services\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\n### Positive Consequences\n* Standardized documentation of the API\n* Automatic service generation for clients is possible\n### Negative Consequences <!-- optional -->\n* OpenAPI annotations have to be maintained\n","**Decision:** Use OpenAPI (fka Swagger) to document the API in both human and machine readable form.\n\n**Rationale:**\n\n* OpenAPI is a widely accepted standard for describing REST APIs.\n* OpenAPI specifications can be used to automatically generate client services, reducing manual effort and potential errors.\n* OpenAPI documentation is both human and machine readable, meeting the requirement for both forms of documentation."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nDue to the use of two different object types for transfering and storing data, the conversion between the types becomes a necessity. Therefore we need means to perform the respective conversion.\n\n## Decision\n","Chosen option: ""[Model Mapper](http://www.modelmapper.org)"", because it fulfils our requirements, is highly configurable and less error prone then manually written converters.\n### Positive Consequences <!-- optional -->\n* Less boilerplate code\n",**Decision:** Implement a converter class that provides methods for converting between the two object types.\n\n**Justification:**\n\n* A converter class provides a centralized and maintainable way to convert between the object types.\n* It removes the responsibility of conversion from the code that uses the objects.\n* It reduces the risk of inconsistencies and errors in the conversion process.\n* It can be used to enhance future flexibility by allowing for different conversion strategies or additional object types.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nTo ensure high test coverage within the project, functionalities that interact with the database must be tested as well.\nThere are different approaches to testing these functionalities that require a database running.\n\n## Decision\n","Chosen option: ""Use Production-like [Postgres](http://www.postgresql.org) database"", because it ensures a production-like behavior.\n### Positive Consequences <!-- optional -->\n* Almost same configuration for test and runtime.\n* Less database technologies used.\n* Guaranteed that production and tests behave the same.\n","**Decision:** Use an in-memory database for integration tests.\n\n**Rationale:**\n\n* An in-memory database can be started and stopped quickly, which reduces test execution time.\n* It does not require a separate database server to be set up and maintained.\n* It provides isolation between tests, as each test has its own instance of the database.\n* However, an in-memory database may not be suitable for large datasets or complex database operations. In such cases, a dedicated test database should be used."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\nConstructing such objects, as well as adding the desired links to them is a common operation that\nrequires entity-specific boilerplate code.\nHow can duplicate code in nearly all controller methods be avoided?\n## Decision Drivers <!-- optional -->\n* Avoid duplicate code to create HATEOAS models\n* Decouple link creation from normal entity logic\n\n## Decision\n","* Avoid duplicate code to create HATEOAS models\n* Decouple link creation from normal entity logic\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\nand our DTO classes.\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\n","**Create a factory method to generate HATEOAS models.**\n\n### Benefits\n* Reduces the amount of duplicate code in controller methods.\n* Decouples link creation from normal entity logic, making the code more maintainable.\n* Allows for easier testing of link creation logic.\n\n### Drawbacks\n* Adds a small amount of complexity to the codebase.\n* May not be appropriate for all projects.\n\n### Alternatives\n* Use a third-party library to generate HATEOAS models.\n* Create a custom annotation to generate HATEOAS models.\n\n### Recommendation\nIf you are writing a project that uses Spring HATEOAS and you are concerned about duplicate code in controller methods, then you should consider using a factory method to generate HATEOAS models."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n\n## Decision\n,"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\n### Positive Consequences <!-- optional -->\n* Recommended approach by [Martin Fowler](https://martinfowler.com/bliki/MonolithFirst.html)\n","**Decision:** Yes, split the components into individual repos.\n\n**Rationale:**\n\n* **Modularity and maintainability:** Individual repos allow for improved modularity and maintainability of the codebase. Each repo can be managed and updated independently, without affecting the other components. This is particularly advantageous in a large and complex platform like PlanQK, where different teams may be responsible for different components.\n* **Reduced dependencies:** Splitting the components into individual repos helps reduce interdependencies between them. This makes it easier to debug and fix issues, as changes to one component will not automatically affect the others.\n* **Improved collaboration:** Individual repos facilitate better collaboration, as different teams can work on specific components without interfering with each other. This also allows for easier code reviews and merging of changes.\n* **Versioning and deployment:** Individual repos provide better control over versioning and deployment. Each component can be versioned and released independently, allowing for more flexibility and easier management of updates.\n* **Scalability:** Splitting the components into individual repos improves the scalability of the platform. It allows for the isolation and horizontal scaling of individual components as needed, without affecting the rest of the system."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSince we use the OpenAPI for frontend client code generations, the HATEOAS links are no longer needed or used.\n## Decision Drivers\n* readable API documentation\n* development effort to test/gather the HATEOAS links\n\n## Decision\n","* readable API documentation\n* development effort to test/gather the HATEOAS links\nChosen option: remove the HATEOAS links from all entities, because this further simplifies the DTO entities\n","**Decision:** Remove HATEOAS links from the API.\n\n**Reasoning:**\n\n* **Reduced development effort:** Removing HATEOAS links eliminates the need for testing and gathering them, reducing development time and effort.\n* **Improved API documentation:** Without HATEOAS links, the API documentation becomes more concise and easier to understand.\n* **Alignment with frontend client code generation:** Since the OpenAPI is used for frontend client generations, the HATEOAS links are no longer necessary, making the API more consistent and efficient."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n\n## Decision\n,"Chosen option: [Testcontainers](https://www.testcontainers.org/), because it simplifies the test execution process\n### Positive Consequences <!-- optional -->\n* It is not mandatory to ensure postgres is running before starting the tests\n",Use a PostgreSQL Docker container to create an instance that's always up and running for testing purposes.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\nWe need references for the links.\n\n## Decision\n","Chosen option: ""[URIs]"", because UUIDs are generated and thus depend on the underlying database system.\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\n### Positive Consequences <!-- optional -->\n* We follow solid [W3C specification](https://www.w3.org/Addressing/URL/uri-spec.html)\n","The decision is to use ""Hyperlinks"".\n\nSpecifically, we will use web-based hyperlinks from the QC Algorithm to the QC pattern and from the QC pattern to the QC Algorithm.\nThis will allow users to easily navigate between the two entities.\nThe hyperlinks will be implemented using the HTML/CSS syntax."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\n### Who Was Involved in This Decision\n- Alex Ward\n- Chris Weight\n### Relates To\n- N/A\n\n## Decision\n","- Alex Ward\n- Chris Weight\n### Relates To\n- N/A\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\n","**Decision:**\n\nAllow developers to choose the state management library of their choice.\n\n**Rationale:**\n\n* State management libraries are rapidly evolving, with new trends and preferences emerging regularly.\n* Enforcing a particular library could limit the flexibility and adaptability of the codebase.\n* Allowing developers to choose the best tool for the job based on requirements and complexity promotes code ownership and accountability.\n* It encourages knowledge sharing and collaboration among team members.\n* It fosters a culture of learning and exploration, which can lead to the adoption of innovative solutions.\n\n**Consequences:**\n\n* Potential for inconsistent code styles and practices across the codebase.\n* Increased onboarding time for developers who are unfamiliar with the chosen library.\n* Potential conflicts when integrating with existing systems or third-party libraries that support different state management approaches.\n\n**Mitigation Strategies:**\n\n* Provide guidelines and documentation on best practices for state management.\n* Implement a code review process to ensure consistency and quality.\n* Encourage cross-team knowledge sharing and code refactoring to reduce inconsistencies.\n* Allow for experimentation with different libraries and technologies in contained environments, such as sandboxes or dedicated branches."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\nproviding an API key to be used when making a request to the API. These options should include:\n* Storing API keys on the users system\n* Reading an API key from the environment\n* Passing an API key directly to the API request methods\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\nus from storing this additional information.\n\n## Decision\n","The Python client will resolve the API key to be used in a request in the following order:\n1) Passing an `api_key` argument directly to the method\n2) Setting an `MLHUB_API_KEY` environment variable\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\n5) Using the API from the `default` profile\nProfiles will be stored in a `.mlhub/profiles` file in the user's home directory. This file will be an INI file containing at least a\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\npassed to a method must correspond to one of these section names, or it will raise an exception.\n","Implement a keyfile store that reads and writes keyfiles to the user's system in the following format:\n\n```yaml\nkey_1: value_1\nkey_2: value_2\n```\n\nThis file should be stored in a well-known location on the user's system, such as the user's home directory. Users can then provide an API key to the API request methods in any of the following ways:\n\n* Passing the API key directly to the API request method as a parameter.\n* Setting an environment variable with the API key.\n* Specifying the path to a keyfile containing the API key.\n\nThe system will first check for an API key passed to the API request method as a parameter. If no API key is provided as a parameter, the system will then check for an environment variable containing the API key. If no API key is found in the environment, the system will then check for a keyfile at the specified path. If a keyfile is found, the system will read the API key from the keyfile.\n\nThe system will also allow users to store other data related to Radiant MLHub in the keyfile. This data can be accessed by using the `get()` method of the `KeyfileStore` class."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Radiant MLHub API implements the `/items` endpoint as described in the [STAC API - Features](https://github.com/radiantearth/stac-api-spec/tree/master/ogcapi-features)\ndocumentation for retrieving the STAC Items associated with a given Collection. Since this is a paginated endpoint with an opaque next token,\npages of items must be retrieved sequentially. For very large datasets and collections, this means that retrieving all items in a collection\nmay require hundreds or thousands of API requests and can be very slow. Additionally, the spec does not provide a mechanism for determining the\ntotal number of items in a collection, which precludes us from showing overall progress when looping over or retrieving items.\n\n## Decision\n","To avoid a confusing user experience when working with Items, and to avoid inadvertently swamping the API with requests, we will not provide\na method in either the low-level client or on the `Collection` classes to loop over the items in a collection. Preliminary work had adapted\nthe [`Collection.get_items`](https://pystac.readthedocs.io/en/latest/api.html#pystac.Catalog.get_items) method to make paginated requests to\nthe `/items` endpoint. Instead, this method will raise a `NotImplementedError` to indicate that this feature is not available.\n*Work is planned to add an endpoint to the Radiant MLHub API to enable downloading a single archive containing all items associated with a\nCollection. Support for this endpoint in the Python client may be the subject of a separate ADR.*\n",Implement a new `/bulk_items` endpoint that returns all items for a given collection in a single API response.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs/adr` directory within this repository. Instead of using the ""deprecated"" and ""superseded"" status value, we will move ADRs that are no longer applicable into the `docs/adr/archive` directory in this repository.\n","To create an Architectural Decision Record, we should document the following information:\n\n- **Decision:** The decision that was made.\n- **Date:** The date the decision was made.\n- **Reason:** The rationale behind the decision.\n- **Consequences:** The potential consequences of the decision.\n- **Alternatives Considered:** The other options that were considered.\n- **Status:** The current status of the decision.\n\nHere is an example of an Architectural Decision Record:\n\n**Decision:** Use a microservices architecture for the new application.\n**Date:** 2023-03-08\n**Reason:** We need to develop and deploy the application quickly and independently. We expect the application to be used by a large number of users, so we need to be able to scale it easily.\n**Consequences:** The microservices architecture will make the application more complex to develop and maintain. We will need to invest in a good CI/CD pipeline and monitoring system.\n**Alternatives Considered:** Monolithic architecture, serverless architecture\n**Status:** This decision has been approved by the project team and is being implemented."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is the problem where to store different utilities and write every time the same code on all projects.\n\n## Decision\n,"Use [lodash](https://lodash.com/docs/) possibilities as main package for utilities on the JS microservices, in order to spend less time and write less code. And use utils directory as a wrapper for it in order to be able to replace it any time.\n",**Decision:** Create a utility library that can be included in all projects.\n\n**Pros:**\n\n* DRY (Don't Repeat Yourself) principle is followed.\n* Consistent functionality across all projects.\n* Simplified maintenance and updates.\n* Improved code quality and readability.\n* Reduced duplication of effort.\n\n**Cons:**\n\n* Initial effort required to create the library.\n* Potential for versioning conflicts if the library is updated frequently.\n* Additional overhead of maintaining the library separately from projects.\n\n**Implementation:**\n\n* Create a separate repository for the utility library.\n* Include the library as a dependency in all projects.\n* Establish clear versioning and dependency management practices.\n* Regularly review and update the library to ensure compatibility with projects.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA way to document and track my design choices is needed, for myself in order to see how project architecture is changing.\n\n## Decision\n","I will use ADR to document any important architectural decisions I make.\nADRs will be checked into the repository as numbered md files in the folder docs/architecture/adr\nI will follow the template described [here](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\nAll Records will contain, Title, Context, Decision, Status and Consequences\nIf a decision is changed or overruled we don't delete the record, but change the status accordingly (to superseded or deprecated).\nIf a decision is superseded or deprecated we should add a link to the new decision. In the format Superseded by [link]\n","**Decision:** Use an Architectural Decision Record (ADR) template to document design choices.\n\n**Rationale:**\n\n* ADRs provide a structured way to capture design decisions, including the context, considered alternatives, decision, and rationale.\n* They help track the evolution of the project architecture and provide a record of the decision-making process.\n* ADRs facilitate team communication and collaboration by ensuring that design choices are understood and aligned.\n* Using a template streamlines the ADR creation process and ensures consistency across decisions.\n* It allows for easy retrieval and review of design choices throughout the project lifecycle."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn ADR0003, we described the 'Bicycle Network Analysis' task to be run via an asynchronous task queue. Since then, Amazon Web Services (AWS) released a new service simply named 'Batch'. This service provides a managed task queue, with Boto and HTTP API interfaces for creating queues and jobs, and triggering new jobs. Each job in AWS Batch is configured to run a Docker container provided to the job configuration. AWS Batch manages ordering and execution of tasks in the queue. In almost every way, AWS Batch is a superior choice to the strategy outlined in ADR 0003, for a few key reasons:\n- AWS Batch manages the queue and task autoscaling without any management from the parent application. The service can be trivially configured to scale up or down on a few different resource considerations. If there are no jobs in the queue, the pool of workers will automatically scale to zero, saving on hosting costs.\n- AWS Batch, in comparison with a manually managed stack of celery workers + broker + result backend, is easy to configure, as it only requires defining a ""worker"" stack via a JSON cofiguration.\n- Switching from a Celery and ECS task based solution will be easy, as AWS Batch workers are configured with Docker containers in the same way as ECS tasks would be\n- It will be easier to trigger jobs from Django using AWS Batch, since direct calls can be made via Boto, rather than having to write some management layer to trigger ECS tasks or work with the Celery API.\n\n## Decision\n","The team will build the Bicycle Network Analysis task queue on AWS Batch. The reduction in manual task queue management and ease of configuration should vastly outweigh having to learn how to develop applications using an unfamiliar service. While relatively new, AWS Batch has support in both Boto and via HTTP API and manual setup of a Batch stack was relatively straightforward.\n",Use AWS Batch for running asynchronous tasks.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST / Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\n\n## Decision\n","We have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST / Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\nWe will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.\n","Based on the given context, the team's preference for Markdown, and the potential use of documentation generators that may limit format choices, we have decided to use the Markdown format for ADRs on this project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\n- Import neighborhood boundary into a PostgreSQL database\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\n- Download related census block and job data for the boundary and import to PostgreSQL\n- Generate a network graph from the imported data\n- Run a series of client-provided analyses on the graph and imported data\n- Export relevant data to an external file store for archival\n- Generate a tile set of the network graph for display on a web map\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\nCelery has multiple options for brokers:\n| Broker | Advantages | Disadvantages |\n| ------ | ---------- | ------------- |\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http://docs.celeryproject.org/en/latest/getting-started/brokers/sqs.html#caveats) |\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\n\n## Decision\n","The team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\n","**Decision:** Use Celery with an SQS broker.\n\n**Rationale:**\nCelery provides a familiar interface for triggering analysis jobs, and SQS is a cheap, easy-to-set-up broker that now provides stable configuration options to isolate environments. While SQS does not provide a result backend, the analysis workflow is already a self-contained process, so a result backend is not necessary. Additionally, the potential issues with result timeouts in SQS can be mitigated by configuring the Celery task to handle retries and timeouts appropriately."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA backend web framework significantly eases the amount of boilerplate necessary for an application to handle web requests. Additionally, it provides scalable, out of the box, well-tested solutions for common features such as user management, authentication, database interaction, and a public API.\nThis project will mostly require only the common components described above, so a backend web framework that we are familiar with will speed development for the project. The only unknown is the integration of the asynchronous 'Bicycle Network Analysis' task mentioned in [ADR 0001](adr-0001-development-environment.md)\nThe team is most familiar with Python, Django and the Django Rest Framework. Due to project constraints and the desired functionality, no other backend frameworks were considered for this project.\n\n## Decision\n","The team will use Django with the Django Rest Framework plugin, written in Python. The team's familiarity with this stack is too much of a positive to pass up. In addition, Django provides many third-party solutions for integrating the asynchronous 'Bicycle Network Analysis' task. This allows the team to be flexible when choosing a solution, without sacrificing development efficiency.\n","Implement the backend with the Django framework, which is familiar to the development team and provides the necessary functionality for this project. Integrate the asynchronous 'Bicycle Network Analysis' task by leveraging Django's built-in support for background tasks to ensure that the main web application loop is not blocked. Explore the use of Django channels for potential performance benefits in handling the asynchronous task."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac, which attempts to streamline native use of containers on macOS. This tool may eliminate the need for the wrapper VM, but has some potential pitfalls.\nPrevious projects defaulted to an Ubuntu VM, with the containers using the simplest debian-based OS. Azavea maintains a series of Docker containers that provide the building blocks for the application stack we will be building.\nThis project contains a 'Bicycle Network Analysis' task which is runs on a self-contained PostgreSQL instance. Running this via Vagrant+Ansible or a Docker container should be relatively straightforward either way, since the task has known software dependencies and does not have any external dependencies. When this project begain, this task was configured via Vagrant+Ansible. Some additional work would be necessary to convert this task to a Docker container.\n\n## Decision\n","In order to take advantage of the better deployment and provisioning workflows provided by AWS ECS when using containers, we decided to construct the development environment using a Docker Compose environment wrapped within an Ubuntu VM. While Docker for Mac looks compelling, it has a few downsides:\n- We cannot control the version of Docker installed, which could be problematic as the project ages\n- There are potential incompatibilies for users with the older Docker Toolbox installed\n- It may be difficult to cull outdated container images across projects\n- It may be difficult to isolate various project instances and their dependencies\nUsing the wrapper VM avoids these issues and provides us with a relatively 'known good' experience for a project with somewhat limited budget constraints.\n","Use Docker for Mac with Docker Compose for development and deployment of the application. This will leverage the benefits of Docker containers for improved CI/CD workflows while providing a streamlined development environment for users on macOS. The use of Docker Compose will simplify the management of the application stack. Additionally, the existing Docker containers provided by Azavea will be utilized to build the application stack. The ""Bicycle Network Analysis"" task will be converted to a Docker container to take advantage of the benefits of containerization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCorvus.Tenancy supports a hierarchy of tenants. There are two things that this can be used for:\nFirstly, we can control on a per-tenant basis where the child tenant data for that tenant are stored. For example, with two sibling tenants A and B (i.e. tenants that are children of the same parent tenant), the data for the child tenants of A can be stored in a completely different location to that of B. By default, this will be a separate container in the same storage account, but it could be a completely separate storage account.\nSecondly, it can be used to enable better organisation of tenants by using parent tenants to group related tenants together.\nOne of of the functions of tenants is to hold client-specific configuration for the applications that a client is using. An example would be for a client using the Workflow service. Their tenant will contain two pieces of storage information, one for Workflows and one for Workflow instances. This configuration is stored in a collection of key-value pairs attached to the tenant.\nIt is possible for tenants to have child tenants in the hierarchy. If a tenant that uses the Workflow service has children, they may also need to use the Workflow service. In this case we have a choice: we can decide that we will allow the workflow storage configuration from a tenant to be inherited by its children, or we can require each tenant to contain all of its own configuration.\n\n## Decision\n","We have determined that we will not make properties of tenants available to their children by default. Applications which consume this library can implement that functionality for themselves if required - for example, by manually copying properties from parent to children when new tenants are created.\nWhilst property inheritance seems desirable from a development perspective - for example, creating temporary tenants for testing purposes, or setting up tenants for developers - it is likely to be less useful in envisaged production scenarios.\nIn the case when hierarchy is used for organisational purposes, inheritance is not relevant; parent tenants are there solely to group their children and configuration for the parent tenant is irrelevant, as it does not exist to be used as a tenant in its own right.\nIn the case where hierarchy represents a genuine parent-child relationship there are many potential reasons for this, and the goal of the project is not to dictate specific use cases. However, in making the decision not to implement property inheritance it is only necessary to find a use case where it is not desirable.\nOur use case here is a PaaS product providing multiple services - endjin's Marain platform. This platform contains several base services - Tenancy, Workflow, Operations and Claims - which can be licensed by clients.\nA client may choose to use these services to build their own platform, and use Marain's tenancy service to provide their own platform services to their own customers. In this case, the client's customers will be represented by child tenants of its own tenant.\nIn this situation there are two negative outcomes from allowing configuration to inherit from parent to child tenants.\n1. The client may make use of Marain services (e.g. Workflow) to provide services to its customers. Configuration for these services is stored as configuration on the client tenant. Automatic property inheritance would mean that by default, child tenants of the client would also have the ability to access these services, which should not be the case.\n1. The configuration attached to a client's tenant contains various pieces of sensitive information. For example, it may contain storage account details for storage that is not directly owned by the client. For this reason, Marain does not allow clients to view their own configuration data, or that of their parents. However, clients do need to be able to view and modify the configuration of child tenants. If we automatically allowed properties to be inherited by child tenants, it would be possible for a client to create a child tenant and examine those inherited properties to access what is effectively the client's own configuration data.\n",Allow tenants to inherit storage configuration from their parent tenants if they do not override it.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp until now, tenant Ids have always been automatically generated. As we have used Corvus.Tenancy as part of Marain, we have come across scenarios where it is useful to control what a tenant's Id will be. This primarily applies to the tenants we use as top-level containers for Client and Service tenants, as well as the service tenants themselves.\nIn the absence of the ability to know tenant Ids in advance we have fallen back on well known names for these tenants, effectively introducing an alternative well-known Id for them. As well as introducing a potential point of failure due to the need to keep these names unique, it is far less efficient to locate a tenant by name than by Id.\n\n## Decision\n","To avoid the need to effectively introduce an alternative way of identifying tenants, we will allow the Id of tenants to be controlled when they are created.\nThis will be done by adding a new method to the `ITenantProvider` interface: `CreateWellKnownChildTenantAsync`. This allows callers to specify a GUID which is used to generate the new tenant's ID, rather than generating a random GUID internally (which is what the existing `CreateChildTenantAsync` method does).\nThe resulting Id of the new tenant will still be generated by concatenating the parent tenant's Id with the hash of the provided GUID. This means that in order for a tenant to have a well known Id, all of its ancestors must also have well-known Ids.\nIn order to prevent two tenants being created with the same Id, it is necessary to check that the Id is not already in use prior to creating the new tenant.\n","Create a new INameGenerator interface that will replace the current implementation on ITenantStore. This new INameGenerator will be responsible for generating tenant Ids, and will allow us to control the format of the tenant Ids. We will also need to update the ITenantStore interface to include a new method that will allow us to specify the INameGenerator to use."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](./0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\n* a tenant may exist in which only a subset of its storage containers exist\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\n\n## Decision\n","Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\n1. using nothing but v2\n1. using v3 libraries mostly (see below) in v2 mode\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\n1. using v3 libraries in non-transitional mode\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https://github.com/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\n### Phase 2: using v3 libraries, operating in v2 mode\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries—the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\nThe one difference in behaviour (the reason we describe this as ""mostly"" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\n### Configuration migration\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\n",Applications that have already been deployed on `Corvus.Tenancy` v2 will be responsible for creating all necessary storage containers when onboarding a client. This can be done by using the `Corvus.Tenancy` API to create the containers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn `Corvus.Tenancy` v2, the various storage-technology-specific libraries (e.g. `Corvus.Azure.Storage.Tenancy`) could dynamically create new containers for you the first time you asked for them. One of the problems this caused was that the definition types (e.g. `BlobStorageContainerDefinition`) needed to include all of the information required to be able to create a new container on demand. For example, with blob containers, that meant specifying the container's public access type. This was not a great idea, because it muddied the role of the definition types. These were primarily logical names, but they also ended up containing the default configuration settings to use in these auto-container-generation scenarios.\nThe tenant onboarding process (the process of enabling a new tenant to use an application, or some particular piece of application functionality) necessarily includes these steps:\n* determining the storage account (and relevant credentials) to use\n* picking a suitable container name, ensuring proper tenant isolation\n* creating the container\nIn V2, applications were in control of the first of these. But in most cases, the second and third were handled by `Corvus.Tenancy`. These last two were unhelpfully tied together because of an unfortunate comingling of concerns. This happened due to good but misguided intentions. We were aiming to enable applications to have a single configuration serving multiple logical containers. For certain kinds of storage (e.g., Azure blob storage) it's common for an application to split data across multiple containers (e.g., putting all the user profile details in one container, and, e.g., to-do list entries in another container). In a non-tenanted application you'd expect to configure settings such as account name and credentials just once—it wouldn't normally make sense to have per-container configuration settings because you'd expect to use the same account across all the logical containers. When it came to tenanted storage, the v2 libraries tried to support the same approach by offering convention-based mechanisms to enable multiple container 'definitions' (logical names) to refer to the same underlying configuration. However, this was inextricably linked to letting the storage libraries pick the container name.\nThe problem arose because one of the things V2 tried to do for us was to map from the logical container names in the definition types (e.g. `BlobStorageContainerDefinition.ContainerName`) to an actual container name. To enable isolation of data across tenants even when they shared a storage account, this container name mapping would typically incorporate the tenant ID in the real container name. However, this naming scheme was initially an undocumented implementation detail, preventing applications from anticipating what the container would actually be called. If the application doesn't know the container name, it can't create the container itself prior to first use, and so these tenanted storage providers also automatically created the container too.\nThis is how we ended up with the definition types (e.g. `BlobStorageContainerDefinition`), which were meant to be logical identifiers, needing to include all of the information required to be able to create a new container on demand.\nIt was technically possible for application code to take control of all three of the steps listed above itself in V2, but it was problematic. You could disable the tenanted container name generation, giving you control over the container name, making it possible for the application to know the right container name, but an unfortunate side effect of this was that for each new tenant, you ended up needing to create one configuration for every logical container. (I.e., the decision to take control of container creation unavoidably meant using more complex configuration.) We did make some changes enabling applications to predict the names that would be using, so they could get in ahead, but with hindsight, we ended up regretting ever making the tenanted storage libraries create containers in the first place.\nAnother problem with the automatic create-on-first-use behaviour was that any problems that would prevent creation became visible rather late in the day: you might think you'd successfully onboarded a new tenant, only to discover later that not everything is going to work.\n\n## Decision\n","In `Corvus.Tenancy` v3, applications are responsible for creating any containers they need. The new tenanted storage client libraries will never create a container for you.\nAlso, applications determine the strategy for picking tenant-qualified names to ensure isolation in cases where multiple tenants are sharing a storage account. The tenancy libraries will provide mechanisms that do most of the work, so the main change is that the application has to opt in explicitly.\n","The decision is to **deprecate the functionality that dynamically creates new containers in the `Corvus.Tenancy` v3 libraries.**\n\nThe reasoning behind this decision is that this functionality is not a good fit for the role of the definition types, which are primarily logical names. Additionally, it led to problems where applications had to create one configuration for every logical container for each new tenant, and problems with creating containers could become visible late in the day."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context or Problem Statement\nOur handling of user data is far from bullet-proof. We have stumbled upon cases where SQL injection is possible and the current password encryption is only md5. Additionally, we are building a new mobile app in React and Flask and need to implement a way to handle authentication there.\n## Decision Drivers\n1. Security / Risk for us\n2. Ease of Use\n3. Cost\n\n## Decision\n",1. Security / Risk for us\n2. Ease of Use\n3. Cost\nWe are going for Auth0 since\n- We have prior experience with Auth0 in the team\n- a first test in an afternoon coding session were satisfying\n- Auth0 offers a free plan for Open-source projects\n- Auth0 is one of the product leaders in CIAM (Visit Gartner for detail at [https://www.g2.com/categories/customer-identity-and-access-management-ciam#grid] - Read recent Leadership Compass Identity API Platforms By KuppingerCole [https://static.carahsoft.com/concrete/files/6315/6840/5267/kuppingercole-leadership-compass-identity-api-platforms.pdf])\n- Using Auth0 management API and automatic migration and bulk import it can be integrated to PHP app\n- Auth0 offers full featured authentication and authorization solution and it's capable of customized rules and flows.\n- We are not building our own authentication solution to reduce the security risks coming with handling of passwords.\n,"**Decision:** Implement a modern authentication and authorization framework that addresses the identified security concerns and supports the new mobile app development in React and Flask.\n\n**Reasons:**\n\n* **Security:** The framework will address the SQL injection vulnerabilities and strengthen password encryption.\n* **Ease of Use:** The framework will provide a consistent and user-friendly authentication experience across the existing and new mobile app.\n* **Cost:** While implementing a new framework may require initial investment, the long-term benefits of improved security and reduced risk outweigh the costs."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nVisual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose.\n\n## Decision\n,We will use Docker for packaging and running the application in a Linux container.\n,Implement Docker Compose support for Visual Studio 2019.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe application needs to be deployed somewhere.\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https://devcenter.heroku.com/articles/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\nHeroku provides a free hosted PostgreSQL option. It will handle setting a ""DATABASE_URL"" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\nWe want the setup process to be as simple as possible.\n\n## Decision\n","We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\n",Use Heroku for application deployment.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the sources for links that are eventually saved to Pocket are email newsletters. However, a user often does not want to put all\narticles from a newsletter in to Pocket. So, newsletters waiting to be reviewed can clutter an email inbox.\nA user could be provided with an email address that they could send emails to. Links would then be extracted from a received email\nand be added to a backlog. A webpage would need to be provided for users to view this backlog, which could be part of the existing\nchrome extension.\nHandling inbound email would require the use of a transactional email service, such as MailGun, that can POST to an API endpoint\nwhen an email is received.\nThe recipient email address, containing a unique identifier associated with a particular user, would be used to determine what list to add\narticles to.\nA protected account ID in the email address is likely to be too long for users to want to use.\nUsing an unprotected account ID in the email address for a user would expose the plaintext and cipher, which could make it easier for an\nattacker to determine the private key material.\nTo prevent both of the above issues, a new UUID would need to be generated for use in the email address for a user.\nEmails would consist of a variety of formats, which may cause issues in identifying links.\nSome links would not be relevant and would need to be removed (i.e. unsubscribe links).\nSome emails contain a link to a webpage containing the same content, in case the user's email client is not able to correctly render the\nemail. This link could be added to Pocket, rather than the links within the email. However, there is not a common format to identifying\nthis email address.\nOther services, such as If This Then That, allow triggers to be setup where links can be added to Pocket when an email is received.\nThe suggested backlog approach may be viewed as unnecessary by users. It may be easier for them to add to Pocket and filter while\nbrowsing through their list of articles. Linked with the difficulties in parsing emails, this feature could be difficult to implement\nand be of little actual use.\n\n## Decision\n",We will not provide users with an email address they can send emails containing articles to.\n,**Decision:** Do not implement a feature to automatically add links from emails to a Pocket backlog.\n\n**Reasoning:**\n\n* The benefits of such a feature are uncertain. Users may not find it useful and may prefer to manually add links to Pocket.\n* Implementing the feature would require significant effort due to the complexity of parsing emails and identifying relevant links.\n* The feature could introduce security risks if user account IDs are exposed in email addresses.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA Controller is responsible for receiving a request, executing it and returning an appropriate response.\nA service layer can be added to remove knowledge of how an operation is performed from a Controller, allowing it to focus on the responsibilities mentioned above.\n\n## Decision\n",We will use a service layer to ensure that Controllers do not contain business logic.\n,Use a service layer to decouple the Controller from the implementation details of the operation.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo access and modify a user's articles stored in Pocket, they have to give us permission. Once permission has been granted, we have to be careful not to leak this privillaged access to malicious parties.\nASP.NET Core has [Data Protection APIs](https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/introduction) for managing the encryption and decryption of data. These APIs requires the private key data to be persisted in a location where it will not be destroyed when the application is restarted.\n\n## Decision\n","We will use the encrypted ID of a PocketAccount entity as an access token, representing the right to access a particular user's articles.\nWe will encrypt the IDs of Pocket articles returned in API responses, using the ASP.NET Core Data Protection APIs.\nWe will store the private key material in the database.\n","**Decision:** Use the ASP.NET Core Data Protection APIs to protect the user's Pocket access token.\n\n**Rationale:**\n\n* **Security:** The Data Protection APIs provide strong encryption and decryption capabilities, making them suitable for protecting sensitive data such as access tokens.\n* **Persistence:** The APIs allow for the private key data to be persisted in a secure location that will not be destroyed upon application restart.\n* **Convenience:** The APIs are provided by the ASP.NET Core framework, making them easy to integrate into the application.\n\n**Consequences:**\n\n* **Increased security:** The user's Pocket access token will be protected from unauthorized access, reducing the risk of data breaches and account takeovers.\n* **Simple implementation:** Integrating the Data Protection APIs into the application will be straightforward, saving time and development effort."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.\n\n## Decision\n",We will use ASP.NET Core.\n,Use ASP.NET Core.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA particular database technology needs to be chosen for the storage of data.\nHistorically Microsoft's SQL Server has been the default choice for ASP.NET applications. SQL Server could only be run on a machine running Windows until the release of SQL Server 2017.\nPostgreSQL is a popular choice for use with other web frameworks (i.e. Rails) and is widely used on a range of platforms, including Linux, MacOS and Windows.\nPostgreSQL is open source and free to use for commercial use. SQL Server has a free version for development purposes but require the purchase of a license for commercial use.\n\n## Decision\n",We will use PostgreSQL for the storage of data.\n,"Use PostgreSQL as the database technology for our application. It is open source and free to use for commercial use, and it runs on a variety of platforms."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen releasing some features, we might want to only make them available to a sub set of users initially, to gain feedback and reduce the potential impact of bugs.\nWe also want to be able to continue development of a feature in master over a longer period of time, without it being available in an unfinished state.\n\n## Decision\n",We will associate a set of feature toggles with a Pocket Account.\n,"Use feature flags. Feature flags are a way to enable or disable a feature based on a predefined criteria. They can be used to control the availability of a feature to a specific subset of users, or to enable or disable a feature based on other criteria."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want a mechanism that allows for the tests to be run whenever changes are made to the project.\nWe also want the project to be deployed whenever these tests pass.\nCircleCI allows for builds to run in a Docker container based on a specified ""root"" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests.\nCircleCI provides open source projects with four free linux containers for running builds.\nAll of the configuration for a project built with CircleCI is stored in the project, except for environment variables.\nMicrosoft provides the ""aspnetcore-build"" Docker image, which contains all the dependencies required to build an ASP.NET Core application.\n\n## Decision\n",We will use CircleCI to handle continuous integration builds and deployments.\n,"Use CircleCI along with its Docker integration feature to create a workflow for the project that executes whenever changes are made to the project.\n\nThe workflow should consist of the following steps:\n1. Create a Docker image that contains all the dependencies required to run the tests.\n2. Run the tests in the Docker image.\n3. If the tests pass, deploy the project."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:**\n\nUtilize an Architectural Decision Record (ADR) template to document architectural decisions in a structured and consistent manner.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany ASP.NET applications use [Entity Framework (EF)](https://docs.microsoft.com/en-us/ef/), an Object Relational Mapper (ORM) that helps access data stored in database.\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\nDapper is a library that is commonly referred to as a ""micro-ORM"". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https://samsaffron.com/archive/2011/03/30/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\n\n## Decision\n",We will use Dapper with the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database.\n,"**Decision**\n\nDapper is the preferred ORM for new ASP.NET projects.\n\n**Rationale**\n\nDapper offers several advantages over EF for ASP.NET applications:\n\n* **Performance:** Dapper is significantly faster than EF, especially for complex queries.\n* **Simplicity:** Dapper is a lightweight library with a straightforward API, making it easier to learn and use.\n* **Flexibility:** Dapper allows you to write raw SQL queries, giving you more control over the database operations.\n* **No tracking:** Dapper does not track changes to entities, reducing memory overhead and improving performance.\n\nWhile EF provides more features and automatic functionality, these benefits come at a cost of complexity and performance. For most ASP.NET applications, the benefits of Dapper outweigh the advantages of EF.\n\n**Considerations**\n\n* EF may still be a better choice for projects with complex data models or frequent changes to the database schema.\n* It is possible to use both Dapper and EF in the same project, leveraging the strengths of each ORM for specific tasks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide which front-end web framework to use for the Datalabs application.\nThe choices we evaluated were [React.js](https://reactjs.org/) and\n[Angular](https://angular.io/).\n\n## Decision\n,"We have decided to use the React framework for the following reasons:\n* Preferred the ""batteries not included"" approach of React vs the ""batteries\nincluded"" approach of Angular.\n* Better development tooling available for React.\n* Larger community for React.\n* The team had previous experience with React.\n","Based on the evaluation criteria and the team's preference, we decided to use React.js for the Datalabs application.\n\nReact.js is a popular front-end web framework that offers advantages such as:\n\n* **Declarative programming:** React.js uses a declarative programming approach, making it easier to maintain and update the codebase.\n* **Component-based architecture:** React.js promotes a component-based architecture, which allows for code reuse and easier maintenance.\n* **Virtual DOM:** React.js uses a virtual DOM, which improves performance by reducing the number of actual DOM updates.\n* **Strong community support:** React.js has a large and active community, which provides extensive documentation, tutorials, and support."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe would like to minimise our externally exposed footprint. To achieve this we have\nalready decided that access to cluster servers will be via a Bastion server. We do need\nto have some externally accessible services and need to decide how to achieve this.\n\n## Decision\n,We have decided to have external proxy servers for each environment that expose ports 80\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\n,"**Decision:** Expose services through a reverse proxy.\n\n**Rationale:**\n\n* **Minimized external footprint:** Reverse proxies act as a single point of entry, reducing the number of exposed ports and potential attack surfaces.\n* **Centralized control:** Reverse proxies provide a centralized control point for managing access to services, simplifying security management.\n* **Load balancing and failover:** Reverse proxies can distribute traffic across multiple servers, ensuring high availability and performance.\n* **Header manipulation:** Reverse proxies can modify request headers, enabling additional security features such as request filtering or authentication.\n* **Improved security:** Reverse proxies can add an additional layer of security by implementing rate limiting, intrusion detection, or SSL termination."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to store persistent data in a form that is easy to query and need to select the\nappropriate data store. We consider the choice to be between a relational database or a\ndocument database.\n\n## Decision\n,"We have decided to use [MongoDB](https://www.mongodb.com/) as our database for datalabs\ndata. We felt that the data model for the parts of the system known at this time, a\ndocument structure provided more flexibility and easier integration with our Node.js\napplication.\nMongoDB also provides the ability to run in cluster providing the option for greater\nresilience. For the time being we are opting to run a single node storing data to\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\ndoes not give service resilience or time based backups.\n",**Decision:** Implement a **Document Database** as the persistent data store.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to configure access to the servers in the cluster and need to decide between\npassword authentication and SSH key pair authentication.\n\n## Decision\n,We have decided that all server log on authentication will use SSH key pair\nauthentication. The public keys will be provisioned onto the server at server creation\nfor the default deploy user. A password will be required for sudo.\n,Use SSH key pair authentication for secure access to cluster servers.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\nfrom notebook environments. However since this decision a number of other options\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\nschedulers.\nWe are now moving to a pattern of users being able to spin up their own clusters\nwhen required.\n\n## Decision\n","We have decided to collapse the centralised Dask & Spark clusters in favour of writing\ndocumentation/working with users to provision their own clusters using projects such\nas [Dask Labextension](https://github.com/dask/dask-labextension), which is now supported\nwithin the Labs environment.\n","**Decision:** Adopt native Kubernetes scheduling for Dask and Spark clusters.\n\n**Rationale:**\n\n* **Improved resource utilization:** Kubernetes scheduling enables more efficient resource utilization, as it can dynamically allocate and scale resources based on cluster demand.\n* **Simplified cluster management:** With Kubernetes scheduling, users can directly manage their clusters without relying on centralized provisioning. This streamlines cluster creation, deletion, and scaling operations.\n* **Cost optimization:** Kubernetes scheduling can help optimize costs by allowing users to only pay for the resources they use, instead of paying for a fixed amount of capacity with centralized clusters.\n* **Enhanced portability:** Kubernetes is a widely adopted platform that can be deployed on various cloud providers and on-premises environments. By adopting native Kubernetes scheduling, we can ensure portability and consistency of our cluster deployment across different platforms."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have reached a point where we need to secure third party web applications that provide\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\nsource version and we haven't had and response from our efforts to contact them.\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\nwas released provides a Kubernetes native route for the same functionality.\n\n## Decision\n,We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\nThis should provide all of the same functionality as Kong and in addition should provide\na mechanism for token and cookie authentication using the `auth_url` annotation that\nwraps the underlying Nginx `auth_request` module.\n,Implement the Nginx Ingress controller as a Kubernetes native solution for securing third party web applications without built-in security.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a testing framework to use for testing node.js applications. There are\nmany choices in this space including [Jasmine](https://jasmine.github.io/) and\n[Jest](https://facebook.github.io/jest/).\n\n## Decision\n,We have decided to use Jest as our testing framework as it provides mocking and\nexpectation functions which would have to be provided separately to Jasmine. Additionally\nthe snapshot testing mechanism can be used to simplify certain types of testing and is\nparticularly beneficial to front end unit testing.\n,"**Decision:** Use Jest as the testing framework for node.js applications.\n\n**Rationale:**\n\nJest is a popular testing framework for JavaScript applications, including node.js. It offers several advantages over Jasmine:\n\n* **Out of the box support for async/await:** Jest natively supports testing asynchronous code using async/await, making it easy to write concise and readable tests.\n* **Snapshot testing:** Jest allows for snapshot testing, which compares the output of a test to a predefined snapshot. This is useful for ensuring UI consistency and preventing regressions.\n* **Built-in test coverage:** Jest has a built-in feature for generating test coverage reports, which helps identify areas that need more testing.\n* **Fast and efficient:** Jest is known for its speed and performance, which can significantly reduce the time spent on testing.\n* **Wide ecosystem and community support:** Jest has a large ecosystem of plugins and tools, making it easy to integrate with other testing frameworks and tools.\n\nBased on these advantages, we believe that Jest is the better choice as the testing framework for our node.js applications. It aligns well with our need for a comprehensive, async-friendly, and efficient testing framework."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\nenvironments as currently we would have to manually update them in order to apply them to\ndifferent environments. The options available are to either use\n[Helm](https://github.com/kubernetes/helm) or to build a custom tool.\n\n## Decision\n,We have decided to build a custom tool called Bara to deploy our templates. This will use\nthe simple mustache rendering engine to allow template YAML files to be rendered and then\napplied using the command line `kubectl` tool.\nThis approach seemed easier than learning and deploying Helm and building Helm charts for\neach independent component given our current requirements are very simple and the tool\nwill only take a few hours to write.\n,"Use Helm, and implement custom tooling that runs on top of Helm to provide a more user-friendly experience and additional functionality."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the aims of the Datalabs project is to provide the climate science community easy\naccess to the compute capabilities of JASMIN. In order to do this we need to find a way\nto distribute computationally expensive calculations across a cluster and need to explore\nthe available options for doing this.\n\n## Decision\n,"We have decided to use [Apache Spark](https://spark.apache.org/) as our first option for\ndistributed compute. It has established itself as the successor to Hadoop and provides\nstrong integration with interactive notebook technologies such as Jupyter and Zeppelin.\nSpark also provides support for multiple languages including Scala, Java, Python and R.\nThis makes is a flexible platform that should appeal to many users.\nAdditionally, Spark provides Streaming and Machine Learning capabilities which may be of\ninterest later in the project.\n",Use the JASMIN Dynamic Queue Manager (DQM) with SLURM workload manager to distribute computationally expensive calculations across the JASMIN cluster.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe originally provisioned a discourse instance alongside DataLab as a user forum, however\nin practise we have found that it is not used as discussion takes place either in\nperson or on Slack, and we can use the documentation page where required.\n\n## Decision\n",Discourse will be removed from the stack.\n,**Decision:** Decommission the discourse instance and use the documentation page as the primary source of information.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a language for our web services. We restricted our options to Java,\nNode.js and Go.\n\n## Decision\n","We have decided to use [Node.js](https://nodejs.org/en/) for our web services. We felt that the existing team\nexperience, combined with the flexibility provided by a dynamic language made this the\nright choice.\nJava would have provided static typing and object orientation but we opted against this\nas it felt a little heavy weight.\nGo would have provided a lighter weight modern statically typed option but given neither\nthe team nor Tessella had existing experience we viewed it as too great a risk for this\nproject.\n",We decided to use Node.js for our web services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDataLabs makes extensive use of reverse proxying, to give users access to resources (such as Minio or JupyterLabs).  These resources need individually from an external URL to an internal service URL.  There are four design options for reverse proxying (<http://sawers.com/blog/reverse-proxying-with-nginx/>):\n1. Subdomain - this allows the external path and internal path to be the same, probably with a default root base path (/).  Different services are identified by the external URL's hostname.  This has some disadvantages - multiple hostnames require a wildcard certificate, or multiple certificates if a wildcard certificate can not be acquired; and it makes the development environment more difficult, because you can not just use localhost.\n2. Port - this also allows the external path and internal path to be the same, probably with a default root base path (/).  Different services are identified by the external URL's port.  This has the disadvantage that some organisational firewalls restrict http traffic to unusual hosts.\n3. Symmetric Path - this allows the external path and internal path to be the same, but with that path configured.  Different services are identified by the path.  This is the best option, but the internal service must allow the path to be configurable.\n4. Asymmetric Path - here the external and internal paths are different.  Different services are identifiable by the external path.  This requires a search-and-replace of the path on the rendered HTML and JavaScript, so unless these are simple, then this is too fragile.\nHistorically DataLabs has used Subdomain proxying.\n\n## Decision\n","Where possible, Symmetric Path or Asymmetric Path proxying should be used.  If this is not possible, a ConfigMap option should determine whether the remaining proxying strategy should be Subdomain or Port proxying.\n","Option 3: Symmetric Path is the best option, as it allows the external path and internal path to be the same, but with that path configured. Different services are identified by the path. This is the most flexible and configurable option, and it does not require any changes to the internal service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nContainers running on Kubernetes only provide ephemeral storage. We need to provide\npersistent storage that allows volumes to be mounted into multiple containers. This\nrestricts us to [Kubernetes Volume Types](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) that support ""Read Write Many"". Specifically, we are selecting\nbetween [NFS](https://help.ubuntu.com/lts/serverguide/network-file-system.html),\n[GlusterFS](https://www.gluster.org/) and [Rook](https://rook.io/).\n\n## Decision\n","We have decided to use GlusterFS to provide distributed persistent storage.\nWe have opted not to use Rook as it feels that it isn't yet ready for production usage. Also, while it would be easy to deploy hyper-converged we would need a second Kubernetes\ncluster to run isolated storage as we require.\nWe feel that simple NFS storage isn't sufficient as it won't give any data resilience.\nGiven we have no backups, the data replication will give us limited disaster recover\ncapability.\n",Rook
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a growing number of Docker containers and it is useful to have at a glance\ninformation available about them. [MicroBadger](https://microbadger.com/) provides\na way to inspect and visualise Docker containers.\n\n## Decision\n,We have decided to use MicroBadger for new containers and will update existing containers\nas we make updates to them.\n,#### Decision\nUse MicroBadger to inspect and visualise Docker containers.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe JASMIN cloud portal and vCloud director portals give manual options for provisioning\nservers into the JASMIN tennancy. This brings significant effort to rebuild a cluster\nas all servers would need to be manually deleted and recreated. The new OpenStack\ninfrastructure is supposed to be available in a matter of months and we need to decide\nwhether we want to invest effort in automating server creation.\nIf we do decide to automate we need to decide which technology to use between Ansible\nand Terraform. Terraform is known to work with vCloud director but it would be preferable\nto use the same tool for server creation as software provisioning.\n\n## Decision\n,We have decided to use Ansible as our server provisioning tool as there are significant\nbenefits to keeping a single tool for all provisioning steps.\n,"Automate server creation using Ansible, as it is the preferred tool for both server creation and software provisioning."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to run multiple instances of the Datalabs system to allow us to continue to\ndevelop while giving early adopters access to the system. We intend to run both a test\nand production environment and need to decide whether to do this as a completely separate\nKubernetes cluster or to isolate the environments using Kubernetes namespaces.\n\n## Decision\n,We have decided to run both environments on the same Kubernetes cluster but with a\nseparate reverse proxy to allow testing of the proxy configuration. This decision was\ntaken to avoid the maintenance overhead of having two clusters.\n,Use Kubernetes namespaces to isolate the test and production environments.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe core [Material UI React component library](https://material-ui.com/) is used for much of\nthe DataLabs UI. Before new components make it into the core library, they pass through\n[""The Lab""](https://material-ui.com/components/about-the-lab/). This provides a platform\nfor the new components to be tested in real world applications.\nAt the time of writing, the [Autocomplete](https://material-ui.com/components/autocomplete/)\ncomponent would be beneficial to use but it is currently in the lab. Therefore, a decision\nneeds to be made about whether we are happy to use components from the lab. The\nkey considerations are as follows.\n### Pros\n* Gives us access to components that will reduce development effort required to\nimplement functionality.\n* Gives components that have a simpler interface to the components they would be replacing\n(e.g. [Downshift](https://github.com/downshift-js/downshift) in the case of Autocomplete)\nmaking them easier to maintain and adapt.\n### Cons\n* Components in the lab are free to have breaking changes on a more regular basis\ncompared to the ones in the core library.\n* The components being used might not migrate from the lab into the core library.\n\n## Decision\n",We will use the Material UI Lab component library. The ease to develop and maintain the\nuser interface using the components in the lab outweighs the downsides. We don't have to\nupdate the lab package if a new version removes a component that is currently in use\nor introduces breaking changes into the component.\n,"**Decision:** Use components from the Material UI React Lab with caution.\n\n**Rationale:**\n\nWhile components in the Material UI React Lab offer potential benefits such as reduced development effort and simplified interfaces, there are risks associated with using them. Lab components are subject to more frequent breaking changes and may not be guaranteed to migrate to the core library.\n\nTo mitigate these risks, the following guidelines should be followed:\n\n* **Carefully evaluate the stability of components before using them.** Review the component's documentation and track its progress in the Lab. Consider using components that have been stable for a reasonable period of time.\n* **Use version pinning to minimize the impact of breaking changes.**\n* **Be prepared to handle breaking changes.** Monitor the Lab for updates and make necessary adjustments to the application as they arise.\n* **Consider using an abstraction layer.** This will allow for easier swapping of components if they migrate to the core library or need to be replaced due to breaking changes.\n\nBy following these guidelines, it is possible to leverage the benefits of Lab components while minimizing the potential risks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIt will be necessary to access the servers that form the Datalabs network but we do not\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\n\n## Decision\n,"We have decided that all access to the cluster will be via a Bastion server over an SSH\nconnection on port 22. We will restrict access through the firewall to known IP address\nranges including the development workstations, the STFC VPN and the Tessella public IP\naddress.\nThis excludes public facing services that should be available over HTTPS on port 443 via\na different route.\n",Establish a private network link between the JASMIN tenancy and the Datalabs tenancy.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe NOC use case requires access to a large (~1TB) NetCDF dataset currently stored on the\nARCHER system. The current usage requires data to be extracted using shell scripts and\nthis process takes a long time. We need to identify a better way to access this\ndataset to allow the Datalabs environment to make best use of it.\n\n## Decision\n,"We have decided to use a [Thredds](http://www.unidata.ucar.edu/software/thredds/current/tds/)\nserver to present a unified view of the dataset as it should provide significant\nperformance improvements over manual scripting.\nIn order to achieve this, we need the data to be moved to a JASMIN Group Workspace (GWS)\nto allow us to provision a Thredds server in the JASMIN managed cloud.\n","Implement a Datalabs library that uses the NetCDF4 library to access the NetCDF dataset on ARCHER. This library should provide a Pythonic interface to the dataset, making it easy to use in Datalabs notebooks. The library should also be able to handle large datasets efficiently."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, creating Dask clusters is not ideal:\n* The service has create-pod permission which is unwise\n* Different projects can currently use each other's clusters, which is poor resource management and could allow algorithm inspection from the Desk scheduler plan\n* There is sometimes a requirement to give Dask clusters storage access, to share data or Conda environments.  This is not self-service, and potentially exposes project data since the Dask clusters are not secured by project.\n* Dask Gateway was investigated, but does not fit in our authentication model (since it does not use OIDC), and it still provides no straight-forward route to storage access.\n\n## Decision\n","After prototyping a working solution, we have decided to handle Dask cluster creation within the infrastructure-api service.\nThis will allow us to securely mount volumes and utilise user-defined Conda environments.\n","Use a Kubernetes Service Account specifically annotated for Dask cluster creation, and set up a Kubernetes Role and RoleBinding to limit permissions within the cluster to the minimum required."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are expecting to have to run a large number of containers across several servers\nand in different environments. Given this, we think that we need a Container\nOrchestration tool and are selecting between Kubernetes, Docker Swarm and Mesos.\n\n## Decision\n",We have selected to use Kubernetes as our container orchestration platform. This is due\nto it being the choice that the JASMIN team have made and also that it has established\nitself as a clear industry favourite.\n,"**Decision:** Adopt Kubernetes as the container orchestration tool.\n\n**Justification:**\n\nBased on a comprehensive evaluation of the available options, Kubernetes emerged as the most suitable solution for our requirements:\n\n* **Scalability:** Kubernetes can effectively handle a large number of containers across multiple servers, meeting our need for scalability.\n* **Flexibility:** Its open-source nature, extensive ecosystem of third-party tools, and support for various cloud platforms provide us with the flexibility to meet our diverse deployment needs.\n* **Maturity:** Kubernetes is a well-established and widely adopted technology, ensuring stability and a large support community.\n* **Features:** Kubernetes offers a comprehensive range of features, including container scheduling, self-healing, load balancing, and storage integration, addressing our functional requirements.\n* **Community Support:** Kubernetes has a robust and active community, providing valuable resources and support for both implementation and maintenance.\n\nConsidering these factors, we believe that Kubernetes is the optimal choice to meet our container orchestration needs."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile Docker containers now provide good root isolation from the host compared to earlier\nversions of Docker there are still security risks. We intend to provide sudo access\nfor users of the Notebook containers and this significantly magnifies the risks.\n\n## Decision\n,We have decided to use [AppArmor](https://wiki.ubuntu.com/AppArmor) to improve our\ncontainer security with the intention to make all policies as restrictive as possible.\nTo make it easier to build AppArmor profiles we have also decided to use\n[Bane](https://github.com/jessfraz/bane).\nTo further secure containers we have also decided to run all of our custom build\ncontainers as a non root user.\n,Enable user namespaces to provide isolation between users and the host.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to dynamically orchestrate the containers running in the Datalab environment we\nneed to interact with the Kubernetes API. There are several choices for this:\n* Use one of the officially [supported clients](https://kubernetes.io/docs/reference/client-libraries/#officially-supported-kubernetes-client-libraries).\n* Use one of the Node.js community clients\n* Directly interact with the Kubernetes REST API.\n\n## Decision\n,We have decided to directly interact with the Kubernetes REST API as this presented the\neasiest option for development.\nWe ruled out using a supported client as we didn't want to have to write a service in a\nlanguage we were not familiar with.\nWe trialled all of the Node.js community clients but didn't feel that they were complete\nenough to meet our needs and were poorly documented.\n,Use the [official client library for Kubernetes](https://github.com/kubernetes-client/javascript) for Node.js.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe are using a mono-repository structure for the Datalabs project, this permits\nsharing of components across independent services. In NodeJS sharing of code\nwithout duplication requires linking of modules to the dependant service.\nThere are a few libraries to manages code-sharing:\n* NPM Link\n* Yarn Workspaces\n* Learna\n\n## Decision\n","We have decided to use Yarn Workspaces as is very lightweight and offers\nmanagement of the links required for module share, including auto discovery of\nother services.\nWe have ruled out using straight NPM Links as they are difficult to set-up and\nshare between development team. We have used Learna on other project and found\nit to be very heavyweight and requires that is adds git commits to releasing new\nversions.\n","Use Yarn Workspaces to manage code-sharing within a mono-repository structure.\n\n**Reasoning:**\n\n* **Yarn Workspaces is a simple and lightweight solution.** It does not require the installation of any additional packages or plugins.\n* **Yarn Workspaces is tightly integrated with the Yarn package manager.** This makes it easy to manage dependencies and versions across all workspaces in the mono-repository.\n* **Yarn Workspaces supports multiple workspaces.** This allows us to have multiple independent services in the same mono-repository.\n* **Yarn Workspaces is a widely adopted solution.** It is used by many large organizations, including Google and Facebook.\n\n**Additional Considerations:**\n\n* **NPM Link** is a simple and straightforward solution, but it can be difficult to manage dependencies and versions across multiple workspaces.\n* **Learna** is a more complex solution that offers more features than Yarn Workspaces. However, it can be more difficult to configure and use.\n\nBased on these factors, we believe that Yarn Workspaces is the best solution for managing code-sharing within a mono-repository structure for the Datalabs project."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUser Authentication is a complex problem, can be time consuming to implement and errors\nin implementation can lead to security vulnerabilities. We feel that authentication,\nwhile critical, is not a differentiating factor and want to offload the work to a\nmanaged service.\n\n## Decision\n",We have opted to use [Auth0](https://auth0.com/) as our Identify provider. This gives us\na quick way to integrate authentication into our application with minimal effort and as\nan open source project we are able to use the service free of charge.\n,We will use Amazon Cognito for user authentication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn [ADR 34](0034-custom-k8s-deployment-tool.md), it was decided that a custom tool would be created instead of using [Helm](https://helm.sh/).\nSince this decision was made, Helm has matured and established itself as _the_ way to share and install applications that run in Kubernetes.\nIt is now desirable to ensure that DataLabs is a portable solution that could easily be installed in other Kubernetes clusters.\nWith this in mind, the decision to write and use the custom tool, Bara, needs to be reconsidered.\n\n## Decision\n","We have decided that we should use Helm rather than continuing to use Bara.\nThis will aid with the portability of the system as Helm is the established way of installing applications into Kubernetes meaning it should be familiar to others installing DataLabs.\nAs Helm is well established, cloud providers tend to have support for installing via Helm, simplifying potential deployments to the cloud.\nHelm also provides mechanisms that allow for the installation of complicated applications into Kubernetes, therefore should  provide DataLabs with plenty of room to grow.\n","**Re-adopt Helm as the preferred method of deploying Data Labs:** \n\n- Helm should be used for both local development and production installations\n- The custom tool, Bara, should be retired and replaced with Helm tooling\n- Migration from Bara to Helm should be a high priority"
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to dynamically provide secure Notebook containers it will be necessary to\ndynamically generate and securely store secrets. We want to isolate this from the\ndatabase and use a dedicated solution for this problem.\n\n## Decision\n,We have decided to use [Hashicorp Vault](https://www.vaultproject.io/) to store secrets.\nIt provides a dedicated system to securely store and manage access to secrets.\n,Use a dedicated secrets management system.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently we have an Nginx Ingress controller deployed in the same namespace\nas the Datalabs application. This means that when multiple instances of\nDatalabs are deployed to a single cluster (such as prod and test), ingress\ncontrollers are deployed to each of these namespaces.\nAs we are in the process of implementing multiple project functionality within\nDatlabs, there is now a need for an ingress controller than can fulfill ingress\nrules across all namespaces. This also will move control of ingress to be a\nplatform service and not part of the deployment of the application itself,\nwhich will aid to decouple Datalabs into being more of a standalone\napplication.\n\n## Decision\n",We have decided to deploy a single Nginx Ingress Controller into the\nkube-system namespace that will handle the ingress rules for the entire\ncluster.\n,"Deploy a shared Nginx Ingress controller that is not deployed in the same namespace as the Datalabs application. This will allow a single ingress controller to fulfill ingress rules across all namespaces, thereby decoupling Datalabs from the ingress controller deployment and moving control of ingress to a platform service."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a front end component framework to style the web application. The\nchoices available are [MaterialUI](https://material-ui-next.com/),\n[SemanticUI](https://react.semantic-ui.com/introduction) and\n[ReactBootstrap](https://react-bootstrap.github.io/).\n\n## Decision\n","We have decided to use MaterialUI for the front end component library. Additionally, we\nhave chosen to use the vNext version that is still in alpha as it provides a\nsignificantly different API and we expect it to be released prior to the end of the\nproject.\n","**Decision:** Implement MaterialUI as the front end component framework.\n\n**Rationale:**\n\nMaterialUI is a popular and well-maintained framework that provides a set of high-quality, customizable components. It is based on Google's Material Design principles, which offer a consistent and user-friendly design system.\n\n**Advantages of MaterialUI:**\n\n* **Widely used and supported:** MaterialUI has a large community and extensive documentation, making it easy to find support and resources.\n* **Material Design compatibility:** As it is based on Material Design, it provides a consistent design experience that aligns well with Google's products and services.\n* **Customization options:** MaterialUI offers a high degree of customization, allowing developers to tailor the components to specific branding and design requirements.\n* **Regular updates and new features:** MaterialUI is actively developed and receives frequent updates with new features and improvements.\n\n**Considerations:**\n\n* **Learning curve:** MaterialUI may have a steeper learning curve than other frameworks for developers who are new to Material Design.\n* **File size:** MaterialUI's bundle size can be larger than some other frameworks, which may need to be optimized for performance on mobile devices.\n\n**Alternatives considered and rejected:**\n\n* **SemanticUI:** SemanticUI is also a well-known framework, but it does not have as strong of a focus on Material Design as MaterialUI.\n* **ReactBootstrap:** ReactBootstrap is a good choice for applications that require a traditional Bootstrap-like design, but it may not be as suitable for modern, Material Design-based applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to use Ansible to provision the servers there needs to be a user with sudo\naccess. The default administrator user configured onto the base VM is not configured\ncorrectly and we are unable to use SSH keypairs with this user. After investigation\nwe are not clear what the issue is with the administrator user and need alternative\noption.\n\n## Decision\n,We have decided to remove the password authentication from the administrator user having\nfirst provisioned a new deploy user with ssh keys for the team in the authorized keys.\nThis user will require a password for sudo which will be stored in the ansible vault to\nallow automated provisioning.\n,Create a new user with sudo access and use that user for Ansible provisioning.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether to run our storage cluster as a standalone cluster or\nhyper-converged by running pods on the Kubernetes cluster.\n\n## Decision\n,We have decided to run a standalone storage cluster. The reason for using a separate\ncluster is that by keeping the persistent data separate we keep flexibility over the\nKubernetes cluster and can drop and recreate it without having to worry about the data.\n,Run the storage cluster as a standalone cluster.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a base operating system to install on all virtual machines that form\nthe Datalabs environment. There are three choices available through the JASIMN portal\nUbuntu 14.04, Ubuntu 16.04 and CentOS 6.9.\n\n## Decision\n",We have selected Ubuntu 16.04 as the base operating system for our servers for several\nreasons:\n* The team are more familiar with Ubuntu over CentOS.\n* Packages are likely to be more easily available on Ubuntu.\n* CentOS 6.9 is no longer being updated (last update 10/5/2017).\n* Ubuntu 16.04 will be supported for far longer. 14.04 end of life is early 2019.\n,"The decision is to use Ubuntu 16.04 as the base operating system for all virtual machines that form the Datalabs environment.\n\n**Reasons for the Decision:**\n\n* Ubuntu 16.04 is the latest LTS (Long Term Support) release of Ubuntu, which provides a stable and secure base for the Datalabs environment.\n* Ubuntu 16.04 is a widely used and well-supported operating system, with a large community of users and developers. This means that there is a wealth of documentation and resources available to help with troubleshooting and maintenance.\n* Ubuntu 16.04 includes a number of features that are well-suited for the Datalabs environment, such as:\n    * A modern kernel and package manager.\n    * A user-friendly desktop environment.\n    * A good selection of pre-installed software packages.\n    * Excellent support for virtualization."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide whether to configure our web application code base ourselves or make\nuse of the [Create React App](https://github.com/facebookincubator/create-react-app)\nto provide project configuration.\n\n## Decision\n,We have decided to use the Create React App project to provide the base for our React\nproject as this brings best practice configuration and build and is actively being\ndeveloped.\n,"**Decision:** Use Create React App for project configuration.\n\n**Rationale:**\n\n* **Reduced effort:** Create React App provides a pre-configured environment, eliminating the need for manual configuration, saving significant time and effort.\n* **Improved developer experience:** The tool streamlines the setup process, simplifying onboarding for new developers and reducing setup-related issues.\n* **Consistency:** Using Create React App ensures that all developers work with a standard configuration, reducing variations and potential conflicts.\n* **Community support:** Create React App is widely adopted and maintained by a large community, providing access to extensive documentation and support resources.\n* **Compatibility with popular tools:** The tool integrates seamlessly with other popular development tools such as Babel and Webpack, simplifying the integration of additional features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is demand from the users to be able to ""share"" their Notebooks such that other users\nare be able to open the Notebook, change some values and then re-run cells within the\nNotebook.\nThere is also the requirement to be able to ""share"" the results of a notebook in a way\nthat involves less interaction from the viewer. This requirement is fulfilled by Sites,\nbut there needs to be a way to configure who is able to see the Site.\nThere is the need to define whether these are two truly separate concepts and how\n""sharing"" should be implemented in Datalabs.\n\n## Decision\n","We have decided to split these into two different concepts: `sharing` and `visibility`.\n`Sharing` is the means through which a user will allow others access to make changes to\nthe item that is being shared.\n`Visibility` is the means through which a user will allow others to view the output of\ntheir work.\nIt is possible for an item to have both a `sharing` and `visibility` status. For example,\na Site might have a `visibility` status that means anyone is allowed to look at the\ncontent and it might also have a `sharing` status that anyone in the Project the Site\nbelongs to can edit the configuration of the Site.\n","Datalabs will implement two types of sharing:\n\n* **Notebook sharing:** This will allow a user to share a Notebook with other users, allowing them to open the Notebook, change some values and then re-run cells within the Notebook.\n* **Site sharing:** This will allow a user to share the results of a notebook in a way that involves less interaction from the viewer. Sites will be configured to control who is able to see the Site.\n\nThese two types of sharing are separate concepts and will be implemented as follows:\n\n* **Notebook sharing:** Notebook sharing will be implemented using a permission-based system. Users will be able to grant other users permission to view, edit, or run their Notebooks.\n* **Site sharing:** Site sharing will be implemented using a URL-based system. Users will be able to create a URL for a Site and share it with other users. Anyone with the URL will be able to view the Site, but only the user who created the Site will be able to edit it."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBuilding a complex web application brings challenges around how to manage state. The\nRedux website provides an excellent [motivation page](http://redux.js.org/docs/introduction/Motivation.html) that discusses the challenges in detail.\n\n## Decision\n,"We have decided to adopt the [Redux](http://redux.js.org/) architecture to provide a clean separation between\nour views, actions and state store.\n","**Adopt Redux for state management.**\n\n**Rationale:**\n\n* Redux provides a predictable, unidirectional data flow model that makes state management easier to reason about and maintain.\n* It promotes a clear separation of concerns between state and UI logic, improving the testability and maintainability of the application.\n* Its middleware capabilities allow for the handling of asynchronous actions and side effects, making it a versatile solution for complex web applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Datalabs project has challenging vision for dynamic creation of scientific analysis\nenvironments. We need to decide what technology will enable us to meet this vision.\n\n## Decision\n,We do not think that regular VM orchestration will meet the vision and have instead\nopted to deploy all services (where possible) as Docker Containers. Alternative container\ntechnologies were not evaluated as the team has previous Docker experience and it is the\nclear leader in this space.\n,Use Docker and Kubernetes to create and manage scientific analysis environments.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to decide between [Kube Prometheus](https://github.com/coreos/kube-prometheus)\nand [Prometheus Operator](https://github.com/helm/charts/tree/master/stable/prometheus-operator)\nfor use in deploying the prometheus monitoring solution for DataLabs.\n\n## Decision\n,We have decided to use the [Prometheus Operator Helm Chart](https://github.com/helm/charts/tree/master/stable/prometheus-operator)\nas it gives us the option to use an Helm chart over kubectl used in the kube-prometheus\noption. The Prometheus Operator Helm chart provides a similar feature set to the\nkube-prometheus option.\n,"**Decision:** Use the Prometheus Operator.\n\n**Rationale:**\n\n* **Ease of Management:** The Prometheus Operator provides a more user-friendly interface and more extensive support for managing Prometheus deployments. It allows for easy upgrades, configuration management, and monitoring of Prometheus components.\n* **Expanded Functionality:** The Prometheus Operator offers additional features not available in Kube Prometheus, such as automatic service discovery, secure communication, and support for multiple Prometheus deployments.\n* **Maturity and Adoption:** The Prometheus Operator is a well-established and widely adopted solution, with a large community and a proven track record in production environments.\n* **Integration with Helm:** The Prometheus Operator integrates seamlessly with Helm, a popular package manager for Kubernetes. This simplifies deployment and management, enabling the use of Helm charts for consistent and reproducible configurations.\n* **Support for Custom Resources:** The Prometheus Operator allows for the creation of custom resource definitions (CRDs), providing flexibility in customizing and extending Prometheus's functionality."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to select a proxy server to proxy requests to services running in the private\nKubernetes cluster.\n\n## Decision\n,"We have decided to use [RedBird](https://github.com/OptimalBits/redbird) as a reverse\nproxy. This was selected as it is written in Node.js which we expect our applications\nto be written in and will allow us to extend to support authentication, logging and other\nedge concerns.\n","**Decision:**\n\nImplement and use Envoy Proxy as the proxy server.\n\n**Rationale:**\n\n* Envoy Proxy is a high-performance, open-source proxy built by Lyft.\n* Envoy Proxy supports a wide range of features, including:\n    * Load balancing\n    * TLS termination\n    * Rate limiting\n    * Circuit breaking\n* Envoy Proxy is highly extensible, allowing for the addition of custom filters and protocols.\n* Envoy Proxy is well-supported, with a large community of users and contributors."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to choose a rendering library to allow testing of React components.\n\n## Decision\n,We have chosen to use [Enzyme](https://github.com/airbnb/enzyme) to provide a library\nfor shallow rendering.\n,Use React Testing Library (RTL).
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving selected to run all services and applications in Docker containers we need a\nregistry to store them in.\n\n## Decision\n,We have decided to store the Docker containers in [DockerHub](https://hub.docker.com/u/nerc/dashboard/). Given the project is Open Source this seemed to be the easiest option\nas most tools default to this registry.\n,Use [Docker Hub](https://hub.docker.com/) as a registry to store Docker images.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nKubernetes does not provide an overlay network out of the box and we need to choose\nwhich one to use from [here](https://kubernetes.io/docs/concepts/cluster-administration/networking/).\n\n## Decision\n,We have decided to use the [WeaveNet](https://www.weave.works/oss/net/) network as this\nhas already been used by the JASMIN team. It also appears easy to use and there is good\ndocumentation.\n,"We will use [Flannel](https://github.com/coreos/flannel) as our overlay network because:\n\n* It is a lightweight and easy-to-use overlay network that is well-suited for Kubernetes.\n* It supports a variety of network backends, including VXLAN, UDP, and host-gw.\n* It has a large community of users and contributors.\n* It has a simple configuration that can be deployed with a single command."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Traefik proxy does not fully support WebSockets causing problems with the Dask\nDashboard. We have tested using an Nginx proxy and have found that this provides the\nsupport required but does not provide an API for configuration.\n\n## Decision\n,We have decided to use [Kong](https://getkong.org/) for our reverse proxy as it is a\ncustom build of Nginx that provides an API. We have tested with all of our services and\nit appears to meet our needs.\n,**Decision:** Implement an Nginx proxy with a custom API for configuration.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Redbird proxy does not support WebSockets which are required to support the\ninteractive notebooks. We have also had problems with the reliability\nof the proxy and have found it difficult to configure.\n\n## Decision\n,We have decided to replace the custom Redbird proxy with a [Traefik](https://traefik.io/)\nproxy as this looks easier to configure and claims Web Socket support.\n,"**Decision:** Replace the Redbird proxy with a more reliable and WebSocket-supporting proxy, such as HAProxy or nginx."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need a tool to provision servers and software for the datalabs project.\n\n## Decision\n,We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\nteam have experience using it.\n,Use Terraform as the provisioning tool for the datalabs project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, secrets that are dynamically created for notebooks etc. are stored in Hashicorp Vault as decided in [24. Vault for Secret Store](0024-vault-for-secret-store.md).\nVault has been the source of operational challenges such as sealing when it is rescheduled.\nThis has added extra complexity such as requiring a cronjob to periodically check that Vault has not been sealed.\nVault is also non-trivial to initially configure for use; a concern when aiming for portability across different services, especially those that would be self-serve e.g. JASMIN's Cluster as a Service (CaaS).\n\n## Decision\n",It has been decided to move to using [Kubernetes' native secret solution](https://kubernetes.io/docs/concepts/configuration/secret) to make DataLabs simpler to both deploy and maintain.\n,Use the Kubernetes Secrets Store for storing secrets dynamically created for notebooks etc.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\ngreat success using [Dask](https://dask.pydata.org/en/latest/) as their distributed\ncompute environment. Dask appears that it could be easier to use than Spark for users\nwho already know Python and NumPy.\n\n## Decision\n,We have decided to offer Dask in addition to Spark within the Datalabs platform. This\nenables us to appeal to more users at limited cost.\n,"Based on the feedback from the Met Office, Dask will be investigated further as a possible replacement for Spark for Python users. The investigation will focus on the following areas:\n\n* **Ease of use:** Dask should be easier to use than Spark for users who already know Python and NumPy. This is critical for adoption by the Python user community.\n* **Performance:** Dask should be able to provide performance that is comparable to Spark for a variety of workloads. This is essential for meeting the needs of our users.\n* **Scalability:** Dask should be able to scale to large datasets and complex workflows. This is important for supporting the growing data needs of our users.\n\nThe investigation will be conducted by a team of engineers with experience in both Dask and Spark. The team will provide a report on their findings within six weeks."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Use an Architectural Decision Record (ADR) template to document and track architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSupporting a new feature, the so called [signal enrichment](https://github.com/eclipse-ditto/ditto/issues/561), raises a few\nquestions towards throughput and scalability impact of that new feature.\nIn the current architecture, Ditto internally publishes events (as part of the applied ""event sourcing"" pattern) for\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\nchanged fields.\nThe ""signal enrichment"" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\nThe following alternatives were considered on how to implement that feature:\n1. Sending along the complete `Thing` state in each event in the cluster\n* upside: ""tell, don't ask"" principle -> would lead to a minimum of required cluster remoting / roundtrips\n* downside: bigger payload sent around\n* downside: a lot of deserialization effort for all event consuming services\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\n* downside: overall a lot of overhead for probably only few consumers\n2. Enriching the data for sessions/connections which selected `extraFields` for each incoming event\n* upside: no additional payload for existing events\n* upside: data is only enriched for sessions/connections really using that feature\n* upside: policy enforcement/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\n* delayed event publishing\n* additional deserialization efforts\n* potentially asking for the same static values each time\n3. Cache based enriching of the data for sessions/connections which selected `extraFields` for each incoming event\n* upsides: all upsides of approach 2 except that policy is always up-to-date\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\n\n## Decision\n","We provide 2 different facade implementations providing approach 2 and 3:\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\n* the overall size of this cache is configured, by default to `20,000` entries\n* there is an additional ""smart-update"" mechanism for cache entries related to enrichment of twin events:\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\n","The following decision was made:\nWe decided to go with **Cache based enriching** of the data for sessions/connections which selected `extraFields` for each incoming event\n* upsides: all upsides of approach 2 except that policy is always up-to-date\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\nThe cache will be implemented in the Event Hub which is anyway responsible for enriching and filtering events. The cache will only be used for ""extraFields` which are not part of the original event payload but have been selected during the initial subscription."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEclipse Ditto project exited the incubation phase with release 1.0.0.\nHenceforth, any change to the Ditto API enters a Ditto release according to [semantic versioning](https://semver.org):\n- Incompatible API changes increment major version number (e.g., 1.7.5 -> 2.0.0);\n- Compatible API changes increment minor version number (e.g., 1.2.3 -> 1.3.0);\n- Changes in the implementation without any API change increment patch version number (e.g., 1.0.0 -> 1.0.1).\nThis document defines what _API compatibility_ means,\nthe modules which are considered API and for which semantic versioning holds,\nand the enforcement of semantic versioning.\n\n## Decision\n","### API compatibility\nFor Eclipse Ditto, API compatibility means _binary compatibility_ defined by\nthe [Java Language Specification, Java SE 8 Edition, chapter 13](https://docs.oracle.com/javase/specs/jls/se8/html/jls-13.html).\nExamples of binary-compatible changes:\n- Adding a top-level interface or class.\n- Making a non-public interface or class public.\n- Adding classes to a class's set of superclasses without introducing circular inheritance.\n- Adding type parameters without changing the signature of existing methods.\n- Renaming type parameters.\n- Deleting private members.\n- Adding enums.\n- Adding abstract methods to interfaces.\n- Adding members to a class that do not collide with any other member in all its subclasses in Ditto.\n- Adding default methods to an interface that do not collide with any other method in all subclasses of the interface\nin Ditto.\nBinary compatibility guarantees that any user code of Ditto does not break on minor version upgrades, provided that\n- the user code does not implement Ditto interfaces, and\n- the user code does not extend Ditto classes.\nInheritance from Ditto classes and interfaces is excluded from API compatibility because Ditto interfaces are often\ndefined to hide implementation details from user code. Compatibility for user-defined subclasses, or source\ncompatibility, is not a part of Ditto's semantic versioning. Inheriting user classes may break after a minor Ditto\nversion upgrade.\n### Modules considered API\nPublic classes, interfaces and their public members of the following modules, and their submodules are considered\nDitto API. Changes to them must enter Ditto release in accord with semantic versioning.\nModules not on this list are not considered API; they may contain incompatible changes for any Ditto version change.\n```\nditto-json\nditto-base-model\nditto-messages-model\nditto-jwt-model\nditto-rql-model\nditto-rql-query\nditto-rql-parser\nditto-rql-search-option-parser\nditto-policies-model\nditto-things-model\nditto-thingsearch-model\nditto-connectivity-model\nditto-placeholders\nditto-protocol\nditto-utils-jsr305\n```\n### Javadoc documentation using `@since`\nWhen adding new public visible API (e.g. new interfaces, classes or methods in existing code) in the defined API modules,\na `@since <version>` javadoc annotation shall be added.\nExample:\n```java\n/**\n* Returns the extra information which enriches the actual value of this change.\n*\n* @return the extra data or an empty Optional.\n* @since 1.1.0\n*/\nOptional<JsonObject> getExtra();\n```\nExisting public API without `@since` can be interpreted as `@since 1.0.0` and can be added when adjusting a class.\n### Enforcement of semantic versioning\nSemantic versioning is enforced through binary compatibility check by `japicmp-maven-plugin`.\n```xml\n<plugin>\n<groupId>com.github.siom79.japicmp</groupId>\n<artifactId>japicmp-maven-plugin</artifactId>\n</plugin>\n```\nDeviations of the behavior of `japicmp-maven-plugin` from binary compatibility defined by the Java language\nspecification are to be corrected through overrides. If `japicmp-maven-plugin` breaks the build for a branch,\nthen a major version increment for the next release is required to merge the branch into Ditto master.\nCheck with the whole Ditto team before adding anything to the exclusion list of `japicmp-maven-plugin`.\n","**Decision:**\nThe Ditto API encompasses all public interfaces and message types accessible to external consumers.\nSpecifically, the following modules are considered API:\n- java.ditto.model\n- java.ditto.api\n- java.ditto.protocol\n- java.ditto.web\n- java.ditto.storage\n\nChanges to these modules will follow semantic versioning as defined in the Context."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nSonarQube [RSPEC-2142](https://rules.sonarsource.com/java/tag/multi-threading/RSPEC-2142) complains when we catch\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\ninterruption.\n\n## Decision\n",We will ignore [RSPEC-2142](https://rules.sonarsource.com/java/tag/multi-threading/RSPEC-2142).\n,**Decision:** Suppress SonarQube warning RSPEC-2142 for exceptions caught in JUnit tests.\n\n**Rationale:** Calling `Thread.currentThread().interrupt()` in JUnit tests can have unpredictable consequences due to the way JUnit reuses threads. Suppressing the SonarQube warning ensures that the build will not break due to false positives.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to record architectural decisions made on the Ditto project to keep track of the motivation behind certain decisions.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this\n[article](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) and also use the proposed\n[template](https://github.com/joelparkerhenderson/architecture_decision_record/blob/master/adr_template_by_michael_nygard.md).\n",Use ArchUnit for Architectural Driven Development in Java to ensure that the system's architecture is implemented correctly and consistently.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to allow partial or merge updates of things with a single request.\n\n## Decision\n,A merge request\n* uses HTTP `PATCH` method.\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https://tools.ietf.org/html/rfc7396).\n* has the request header `content-type` set to `application/merge-patch+json`.\n,Use Patch Semantics
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nUp to now Ditto used Pekko's distributed publish/subscribe in order to emit e.g. `ThingEvent`s to interested other services:\n* gateway\n* websocket/SSE sessions publishing events\n* connectivity\n* AMQP 1.0 / AMQP 0.9.1 / MQTT / Kafka sessions publishing events\n* things-search\n* ThingUpdater updating the search index\nThat naive approach works, but does not provide *horizontal scalability*:\n* each single service instance generally interested in `ThingEvent`s gets all of them, regardless of whether someone is actually interested in them\n* as a result a lot of avoidable JSON deserialization is done\n* when Ditto needs to scale the event publishing horizontally, adding new gateway or connectivity instances will not help scaling the event publishing\n* still all instances will have to process each `ThingEvent` and discard if not relevant\n\n## Decision\n","We will implement a custom Ditto pub/sub which\n* uses ""authorization subjects"" as topics when subscribing\n* uses ""read subjects"" as topics when publishing\n* manages and distributes the active subscriptions via Pekko Distributed Data (ddata)\n* emits `ThingEvent`s only to service instances where at least one consumer consumes the event\n","Use a message broker that allows horizontal scaling of topics (e.g. RabbitMQ, Kafka, Pulsar) as an event distributor, where each service has its own (durable) subscription. Then the registry component can publish `ThingEvent`s to the message broker with topic `thingEvent`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeprecated design:\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\nThe problem:\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\n- However, this would have made the Practitioner resource more complex.\*\n\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\n\n## Decision\n","- We decided to use the existing OpenMRS model for creating attributes for each resource.\n- There is a one-to-many relationship between a resource and its attributes.\n- Each attribute has an attribute type, which is stored in the attribute_type table.\n- For each new piece of data to be added to a resource:\n- Create a new attribute_type that describes the data type - this should be included in the seed data.\n- Create a new attribute with the attribute_type that you just created and link this new attribute to your resource.\n- Update the translator for the resource (from FHIR to OpenMRS and vice versa) to ensure that the new piece of data is populated.\n- Please see [the addition of the SDS Role Profile Id field to the Practitioner resource](https://github.com/Open-GP/openmrs-module-gpconnect/commit/f0ff30c18f0d386e338d87f22a847b0dc3dff5ff) to see an implementation of the following pattern:\n![Provider Attribute Design](../images/ProviderAttributeDesign2.png)\n","**Use a separate table to represent one-to-many relationships.**\n\nThis decision is made to address the problem of managing one-to-many relationships in the OpenMRS database. The current solution, which involves extending the existing resource table with a new column for each data field, becomes complex and difficult to maintain when there are one-to-many relationships.\n\nBy using a separate table to represent one-to-many relationships, we can simplify the OpenMRS model and make it more flexible. This will allow us to easily add new data fields to the resource without having to make changes to the existing table.\n\nThe separate table will have a foreign key that references the primary key of the resource table. This will allow us to easily retrieve all of the data fields for a given resource.\n\nThis decision is aligned with the principles of data normalization. Normalization is a process of organizing data in a way that reduces redundancy and improves data integrity. By using a separate table to represent one-to-many relationships, we are reducing the redundancy in the OpenMRS database and making it more efficient to query and update data."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPackage versions specified in `package.json` are often never any more precise than the major version.\nBy default, [Yarn](https://yarnpkg.com/lang/en/) records the exact version of the packages. In contrast, [npm](https://www.npmjs.com/) version 4 uses an opt-in feature called [npm-shrinkwrap.json](https://docs.npmjs.com/files/shrinkwrap.json) and in version [5](http://blog.npmjs.org/post/161081169345/v500) this has been superseded by a new file - [package-lock.json](https://docs.npmjs.com/files/package-lock.json). The feature no longer opt-in for version 5 of `npm`.\n\n## Decision\n",We will use `Yarn` as we have more experience of this than version 5 of `npm` and `Yarn` has been proven to work for our use cases.\n,We will use Yarn to manage package versions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe styling of sites developed for NHSUK need to be as consistent as possible.\nCurrently each new site starts by copying what the last developed site had done\nand building on that. This results in the newer sites looking different than\nthe older sites, which would not be a problem if each site was easily updated\nto use the same, most recent, set of styles. Currently there is no central\nstore for the styles which means there is no easy way to find and use them\neither.\nThe ability to review the styles in a development environment before they are\nmade available to a wider audience is important. As is the ability to use well\nknown technologies such as [SASS](http://sass-lang.com/).\n\n## Decision\n",We will use [Brunch](http://brunch.io/) to accomplish a number of tasks\nincluding:\n* Running the site used in the development environment\n* Generating the compiled assets\n,"Use a single styles repository, versioned and maintained on GitHub, to allow for easy reuse and review of styles across sites. The repository will use SASS to allow for easy maintenance and extension of styles."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n",**Decision:** Employ an Architectural Decision Repository (ADR) to document and track all architectural decisions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nCurrently, we extract technical metadata per-object and run one extraction job serially per-file. This takes a problematically long time for objects with many files; blocks other objects from accessioning; and complicates restarts which must begin again and process the entire object.\nNOTE: Needs discussion: Fedora 3 does not support concurrent writes on the same datastream so we can either split out filesets as a first-class objects in the F3 data model or use temporary caching to generate a consolidated techMD datastream.\n## Decision Drivers <!-- optional -->\n* Blocker for Google Books project\n* Slows down accessioning process\n\n## Decision\n","* Blocker for Google Books project\n* Slows down accessioning process\nTBD!\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n",Use F3 multiple datastream capability to create a separate datastream for techMD.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\n* Each service must have knowledge about many other services.\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\nIn addition, there are already places within SDR were we have re-invented a message broker and/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\n\n## Decision\n","TBD.\n### Positive Consequences\n* Increased ability to scale number of services.\n* Increased ability to scale under load.\n* Increased resilience to service failures.\n* Provide mechanism for notification of the completion of long-running jobs.\n### Negative Consequences\n* Message broker must be supported by Ops.\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\n* The evolution work to begin using the Message Broker.\n",**Introduce a message broker to support asynchronous communication**
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n","[Describe the decision made, including a summary of the alternatives. Elaborate on the decision process if appropriate.]"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nSDR is implemented by way of many different discrete services, some of which make network requests of each other to do their work.  For example, at present:\n* Argo calls Preservation Catalog to retrieve computed checksums for file content, and to retrieve actual preserved file content.\n* Many different services use dor-services-app for read and write operations on our digital repository objects.\nOf course, we want to make sure that not just any client on the network can use these services, since access to them should be limited to authorized callers.\nIn the past, this was accomplished primarily through network access restrictions (e.g., firewall whitelisting IPs of services that should have access, limiting access to clients inside the VPN, etc).  However, sole use of this approach has been deprecated by the industry at large and by Stanford UIT in particular (though it is still an important component of security).\n## Decision Drivers <!-- optional -->\n* We want to secure access to our API endpoints.\n* UIT wants us to secure access to our API endpoints.\n* We would like an approach that's relatively easy to understand and maintain.\n\n## Decision\n","* We want to secure access to our API endpoints.\n* UIT wants us to secure access to our API endpoints.\n* We would like an approach that's relatively easy to understand and maintain.\nThe infrastructure team came to consensus in a weekly planning meeting that, going forward, we should gate access to API endpoints using JWTs (minted by the service, provided with requests by the client).  This ADR is meant to capture and flesh out that decision.\n### Positive Consequences <!-- optional -->\n* More robust and less circumventable than restricting access solely by way of network and firewall configuration.\n* Does not preclude keeping appropriate firewall restrictions, which should remain part of our security practice.\n* Should a network re-configuration result in accidental loosening of firewall restrictions, token based authentication provides a robust additional line of access control.\n* For the ways we've built our applications, token based authentication is at least as easy to implement and maintain as client certificate based authenication, and should provide similarly adequate security.\n* Minting tokens has become a common approach in industry for authenticating client access to APIs, so there is a wealth of current information and advice available online for this practice.\n### Negative Consequences <!-- optional -->\n* More work for developers than solely relying on firewall rules and network configuration (tasks which typically fall to operations and which should happen anyway).\n",Implement OAuth2 as the authentication protocol for our services.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nAs described in [ADR-0001](0001-migrate-off-fedora-3.md)], it is necessary to migrate from Fedora 3 as a repository for DOR metadata. Since the drafting of that original ADR significant progress has been made in isolating Fedora 3 from other SDR systems. In addition, COCINA has been adopted as a data model for DOR digital objects and the Fedora data model has been mapped to the COCINA data model.\nThe purpose of this ADR is to propose an alternative datastore.\n## Decision Drivers\n* Support CRUD and querying/indexing of JSON data.\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\n* Support transactions.\n* Support for large number of records.\n* Currently 2 million digital objects.\n* Expect significant growth in number of digital objects.\n* Depending on implementation, may possibly have multiple records per digital object.\n* Ability to migrate JSON model changes.\n* Broad community support for datastore and expected long term viability.\n* High availability deployment configurations.\n* Local Docker deployment configurations.\n* Can be supported by Operations team.\n* Robust export support (e.g., if needed to exit the technology)\n\n## Decision\n","* Support CRUD and querying/indexing of JSON data.\n* Support for fulltext search (for metadata, not binary files) is desirable. (Note: There is not yet a clear use case for this.)\n* Support transactions.\n* Support for large number of records.\n* Currently 2 million digital objects.\n* Expect significant growth in number of digital objects.\n* Depending on implementation, may possibly have multiple records per digital object.\n* Ability to migrate JSON model changes.\n* Broad community support for datastore and expected long term viability.\n* High availability deployment configurations.\n* Local Docker deployment configurations.\n* Can be supported by Operations team.\n* Robust export support (e.g., if needed to exit the technology)\nNOT YET.\n",The alternative datastore selected is MongoDB.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nCurrently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.\n## Decision Drivers <!-- optional -->\n* Cannot accession large files (objects > 1GB or so)\n* Cannot accession objects with many pages, such as books\n* Blocker for Google Books project\n* Causes extreme delays accessioning other content\n\n## Decision\n","* Cannot accession large files (objects > 1GB or so)\n* Cannot accession objects with many pages, such as books\n* Blocker for Google Books project\n* Causes extreme delays accessioning other content\n**Preferred** (by Infrastructure Team) option: option 2, because:\n* Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers\n* Option 3 is an unsound preservation strategy and does not meet SDR user needs\n* Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines\n* Option 5 has been rejected as a general deployment strategy for now\nThus, option 2 is the only option that currently meets the department's and its customers' needs.\nAs part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.\nIn the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a “technical metadata strike team” in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.\nIn parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet.\n",Suspend JHOVE ingest until a future time when additional engineering resources are available to investigate the issue and develop an alternative that meets requirements.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n","[Describe your decision, e.g., in free form, following these guidelines, using one to three sentences. The set of braces should be relative to the text they describe, e.g., here, the keyword is **bold**.]\n\n- **Selected Option:** [Restate with **bold** the system or approach chosen from the Considered Options in the context section.]\n- **Rationale:** [Explain why this option was chosen.]\n- **Consequences:** [List the consequences of this decision, both positive and negative.]"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\n\n## Decision\n",Remove the datastream.\nThis was done in dor-services v9.0.0 ([commit](https://github.com/sul-dlss/dor-services/commit/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\n,"**Decision:** Store workflow state outside of Fedora (in the workflow service's database).\n\n**Reasoning:**\n\n* Decoupling workflow state from Fedora will allow for greater flexibility and independence in the evolution of both systems.\n* Storing workflow state in the workflow service's database will provide better performance and reliability compared to storing it in Fedora.\n* The workflow service is already responsible for managing workflow execution, so it makes sense to also store the workflow state in the same place."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n","**Decision:** [Provide a single sentence stating the selected or chosen decision.]\n\n**Rationale:** [Provide a justification for the chosen decision, e.g., in free form using two or more sentences. If applicable, explain how the decision addresses the decision drivers.]"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\n\n## Decision\n","* [driver 1, e.g., a force, facing concern, …]\n* [driver 2, e.g., a force, facing concern, …]\n* ... <!-- numbers of drivers can vary -->\nTBD\n### Positive Consequences <!-- optional -->\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, …]\n* ...\n### Negative Consequences <!-- optional -->\n* [e.g., compromising quality attribute, follow-up decisions required, …]\n* ...\n",[Describe the decision.]
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nCurrently the way to create new objects in the SDR is to:\n1. Register an object (typically using Argo)\n1. Move files into the `/dor` mount\n1. Run the assembly workflow (pre-assembly?)\nThe Google Books project will grow SDR by millions of new objects. This growth is an opportunity to inject an API into the SDR architecture that serves as the ""face"" of SDR. Creating an external-facing API allows SDR to accept deposits from on-campus stakeholders such as Big Local News and Open Neuro. Fronting SDR with a new API affords us a mechanism to validate the completeness and correctness of deposited data very early in the accessioning process. Furthermore, we might also use such an API to refactor internal projects (such as ETDs) so these projects no longer require intimate knowledge of the internals of SDR, which reduces coupling.\n## Decision Drivers <!-- optional -->\n* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\n\n## Decision\n","* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\nOption 2: we will create a deposit API as part of work on the 2020 Google Books work cycle.\n","**Decision:** Create a new web service API that will be the sole mechanism for creating new objects in SDR.\n\n**Rationale:**\n\n* An API will decouple the deposit process from the persistence layer, making it more flexible and extensible.\n* An API will allow on-campus stakeholders to deposit data to SDR in a self-service mode.\n* An API can be used to validate the completeness and correctness of deposited data early in the accessioning process, reducing the risk of bad data causing problems in the workflow."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement <!-- required -->\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management ""repository,"" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\n## Decision Drivers <!-- optional -->\n* Fedora 3 is unsupported and unlikely to be supported\n* Fedora 3 will be harder to install on newer operating systems\n* The Fedora 3 data model is not inherently validatable\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\n* Samvera software that supports Fedora 3 is outdated and maintained/supported only through our own efforts, preventing us from using mainstream Samvera software\n* We have (unverified) concerns about the scalability of Fedora 3\n\n## Decision\n","* Fedora 3 is unsupported and unlikely to be supported\n* Fedora 3 will be harder to install on newer operating systems\n* The Fedora 3 data model is not inherently validatable\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\n* Samvera software that supports Fedora 3 is outdated and maintained/supported only through our own efforts, preventing us from using mainstream Samvera software\n* We have (unverified) concerns about the scalability of Fedora 3\nNo decision made yet. See status field above.\n",Migrate the management repository to a supported version of Fedora.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe would like to generally flatten the directory structure in FDS as well as improving the\nimport statements for client applications. This will be a breaking change for a major\nversion, as clients will need to update imports once we make the change.\n## Decision Drivers\n- Remove unnecessary hierarchy in components dir\n- Simplify documentation\n- Adopt the unofficial CBI standard of organizing by component directories (each component\nhas its own directory).\n- Improve ergonomics and performance of import statements for client applications\n\n## Decision\n","- Remove unnecessary hierarchy in components dir\n- Simplify documentation\n- Adopt the unofficial CBI standard of organizing by component directories (each component\nhas its own directory).\n- Improve ergonomics and performance of import statements for client applications\n### Import statements\nBecause individual imports for each component has a negative performance impact, we decided\nto use destructured imports and rely on a tree shaking strategy:\n```diff\n-import Flex from '@cbinsights/fds/lib/components/layout/Flex';\n-import FlexItem from '@cbinsights/fds/lib/components/layout/FlexItem';\n-import TextInput from '@cbinsights/fds/lib/components/forms/TextInput';\n+import { Flex, FlexItem, TextInput } from '@cbinsights/fds/lib/components';\n```\nThe ""root"" file for each import corresponds to an FDS code module.\n| code module   | Import example                                                                        |\n| ------------- | ------------------------------------------------------------------------------------- |\n| `dictionary`  | `import { COLOR_TIMBERWOLF, COLOR_PURPLE } from '@cbinsights/fds/lib/dictionary';`    |\n| `components`  | `import { Avatar, TextInput } from '@cbinsights/fds/lib/components/';`                |\n| `icons`       | `import { CloudIcon, BellIcon } from '@cbinsights/fds/lib/icons';`                    |\n| `assets`      | Use path `@cbinsights/fds/lib/assets/<file>` where applicable                         |\n| `base-styles` | not applicable; we only publish this CSS as part of the main styles file in `assets/` |\n### `components` file structure\nCategorization directories will be removed.\nEach component will have its own directory under `components/`.\nTests, stories, private components, and helpers will be colocated in component\ndirectories.\n```\ncomponents/\n├── Foo/\n│   ├── FooHelper.jsx\n│   ├── foo.test.jsx\n│   ├── foo.stories.jsx\n│   ├── utils.js\n│   └── index.js  // contains Foo component code\n...\n```\n### Documentation\nThe ""Components"" section in Storybook will no longer have categorization. All components\nwill be documented flat under the section heading.\n","**Decision:** Flatten the directory structure in FDS and improve import statements for client applications.\n\n**Justification:**\n\n* **Remove unnecessary hierarchy in components dir:** The current directory structure contains unnecessary levels of hierarchy, making it difficult to navigate and find components. Flattening the structure will improve organization and ease of use.\n* **Simplify documentation:** The documentation for FDS will be simplified by removing references to the existing directory hierarchy.\n* **Adopt the unofficial CBI standard:** Adopting the unofficial CBI standard of organizing by component directories will make FDS more consistent with other projects and improve compatibility with other tools and libraries.\n* **Improve ergonomics and performance of import statements for client applications:** Flattening the directory structure and adopting the CBI standard will improve the ergonomics and performance of import statements for client applications, making it easier for developers to use FDS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nOur public-facing application is currently suffering from a ""zindex war"". In a ""zindex\nwar"", there are no winners. Engineers must regularly set insanely high and arbitrary\nzindex values to build our front end features.\n\n## Decision\n","We've decided to go with a 2 tier approach:\n- Use pre-defined z-index values & ranges that are defined in FDS.\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\nvalues are not useful)\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\n### Use values between `1` and `9` for fine layering control\nIn cases where one element needs to appear above another, use integers below 10. Rely on\nsurrounding stacking context to do the rest.\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\nvalues `1` through `5` to accomplish this.\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/The_stacking_context).\n### Pre-defined FDS z-index values / ranges\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\n#### Navigation\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to ""frame"" the content on a page.\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\n```\n.mainheader {\nz-index: calc(var(--zindex-navigation) + 9);\n}\n.subheader {\nz-index: var(--zindex-navigation)\n}\n```\n#### Modal\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\n#### Crazy\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\n### For standard values, continue using increments of `10`\nFor base stacking context, standard values are most helpful at increments of 10 because it\ngives engineers flexibility to insert layers between defined stack levels. For example:\n","**Decision:** Use a component library that includes a z-index system.\n\n**Rationale:**\n\n* A component library provides a consistent and structured approach to managing z-index values, preventing arbitrary and excessive usage.\n* It enforces a predefined hierarchy, ensuring that elements are always positioned correctly in relation to each other.\n* By centralizing z-index management, it eliminates the need for engineers to guess or compete for values, reducing the likelihood of z-index wars.\n* Standardizing z-index usage improves code readability, maintainability, and reduces potential conflicts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe [material-ui](https://material-ui.com) library is considered ""legacy"". In FDS, we\naim to provide component coverage so that in the future, consumers will no longer need to\nrely on material-ui.\nThe FDS node module however, still provides a `mui-theme` theming object for the\nmaterial-ui library.\n\n## Decision\n",Removed `mui-theme` from FDS.\n### Negative Consequences <!-- optional -->\n- Consumers will need to handle the breaking change. Consumers are now responsible for\ndefining their own material-ui base theme.\n,The FDS node module will gradually deprecate the use of the `mui-theme` object in favor of the `fds-theme` object.\n- All new components in the FDS node module will only support the `fds-theme` object.\n- Existing components in the FDS node module will be updated to support the `fds-theme` object and will deprecate the use of the `mui-theme` object.\n- A timeline for the deprecation and removal of the `mui-theme` object will be provided in a future announcement.\n- Consumers of the FDS node module are encouraged to migrate to the `fds-theme` object as soon as possible.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe take a utility class based approach (aka ""functional CSS"" aka ""expressive CSS"") in FDS.\nWe have a lot of utility classes that do only one thing (e.g. `display--none`).\nAt one point, we discussed adding a `position--relative` class, which led us to a key question\nwe needed to clarify:\n> When does it make sense to add a utility class?\n\n## Decision\n","Chosen option: ""Only add utility classes that are useful on their own""\n### Example of a utility class that is not ""useful on its own""\n```html\n<div class=""position--relative"">\n<div>\nIf this element is absolutely positioned, we need additional ad-hoc CSS rules beyond\nwhat the utility class above provides. `position--relative` is not helpful on its own.\n</div>\n</div>\n```\n### Example of a utility class that is ""useful on its own""\n```html\n<div class=""color--red"">\n<div>\nThis text will be red. No additional CSS rules are needed for the above utility class\nto be useful.\n</div>\n</div>\n```\n","**Decision:** Add a utility class when the need to apply a style is common and the style is simple, specific, and consistent.\n\n**Rationale:**\n\n* **Common need:** Utility classes should be used when the need for a particular style is common across multiple components or use cases.\n* **Simple style:** Utility classes should implement simple, isolated styles that can be easily combined with other classes.\n* **Specific:** Utility classes should target specific elements or behaviors, rather than broad, general styles.\n* **Consistent:** Utility classes should provide consistent styling and avoid introducing edge cases or exceptions.\n\nBy adhering to these principles, we can ensure that utility classes remain a valuable and efficient tool in our design system, allowing us to quickly and consistently apply common styles without the need for custom CSS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe'd like to change the spacing system to something more generic.\n## Decision Drivers <!-- optional -->\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half/half.\n\n## Decision\n","The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half/half.\nGeneric tshirt system was chosen over the other options.\n- Coding a direct ""arbitrary value"" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\n","The new spacing system will use absolute values instead of relative values. This will allow us to add additional spacing values that don't map cleanly to the current words, and it will also make it possible to change the default value in the future.\n\nThe new spacing values will be:\n\n- `default` (16px)\n- `small` (8px)\n- `medium` (12px)\n- `large` (24px)\n- `x-large` (32px)"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\narbitrary style overrides on the component.\nWe want all components in FDS to follow a similar prop interface, so this decision should\napply to _all_ components.\n\n## Decision\n","Chosen option: ""No components accept a `className`"", because...\n- It's too risky to introduce style overrides to low-level layout components.\n- Style customization should happen via props, where the component can be responsible for\nstyle changes and variations.\n### Positive Consequences <!-- optional -->\n- Prop API consistency across all components.\n### Negative Consequences <!-- optional -->\n- Consumers of FDS components must handle the breaking change. Any custom `className`\nprops on FDS components must be moved to a direct parent or direct child of the\ncomponent.\n",The decision is to not allow components in FDS to accept a `classNames` prop.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWhen should we spread props in components?\n**Example**\n```\nconst Component = ({ foo, bar, ...restProps}) => {\n<element foo={foo} bar={bar} {...restProps} />\n}\n```\nIf we spread rest props on every component, we create an opportunity for developers to\ncustomize components via code instead of explicitly defined props.\nFor components that render attribute-heavy DOM elements (like `button`), it may make\nsense to spread props. For more complex components, spreading props could lead to bugs,\nside effects, and unnecessary customization.\n\n## Decision\n","Chosen option: ""Only components that render attribute-heavy DOM elements spread rest props"", because...\n- Compromise that allows us to create the right prop interface for the job of each component\n- Components that conceptually map to attribute-heavy DOM elements (i.e. html form elements)\nwill allow spread props to keep the explicitly defined props easier to reason about.\n- Components that **do not** conceptually map to attribute-heavy DOM elements (e.g. `Toast`)\nwill be able to continue to define an explicit prop interface instead of spreading props.\n### Negative Consequences <!-- optional -->\n- Not all components will behave the same way.\n- **We must document when a component supports spread props**.\n",**Decision**: Spread props in components only when necessary.\n\n**Reasoning**: \n\n- Spreading props provides flexibility and customization for attribute-heavy DOM elements.\n- It can lead to bugs and side effects in complex components.\n- Explicitly defined props ensure clarity and maintainability.\n\n**Guidelines**: \n\n- **Use props**: For complex components with well-defined behavior and interactions.\n- **Spread props**: For attribute-heavy DOM elements and components that need flexibility and customization.\n- **Consider**: The potential consequences of spreading props and ensure it aligns with the component's purpose and design.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAssets in the Cosmos SDK are represented via a `Coins` type that consists of an `amount` and a `denom`,\nwhere the `amount` can be any arbitrarily large or small value. In addition, the Cosmos SDK uses an\naccount-based model where there are two types of primary accounts -- basic accounts and module accounts.\nAll account types have a set of balances that are composed of `Coins`. The `x/bank` module keeps\ntrack of all balances for all accounts and also keeps track of the total supply of balances in an\napplication.\nWith regards to a balance `amount`, the Cosmos SDK assumes a static and fixed unit of denomination,\nregardless of the denomination itself. In other words, clients and apps built atop a Cosmos-SDK-based\nchain may choose to define and use arbitrary units of denomination to provide a richer UX, however, by\nthe time a tx or operation reaches the Cosmos SDK state machine, the `amount` is treated as a single\nunit. For example, for the Cosmos Hub (Gaia), clients assume 1 ATOM = 10^6 uatom, and so all txs and\noperations in the Cosmos SDK work off of units of 10^6.\nThis clearly provides a poor and limited UX especially as interoperability of networks increases and\nas a result the total amount of asset types increases. We propose to have `x/bank` additionally keep\ntrack of metadata per `denom` in order to help clients, wallet providers, and explorers improve their\nUX and remove the requirement for making any assumptions on the unit of denomination.\n\n## Decision\n","The `x/bank` module will be updated to store and index metadata by `denom`, specifically the ""base"" or\nsmallest unit -- the unit the Cosmos SDK state-machine works with.\nMetadata may also include a non-zero length list of denominations. Each entry contains the name of\nthe denomination `denom`, the exponent to the base and a list of aliases. An entry is to be\ninterpreted as `1 denom = 10^exponent base_denom` (e.g. `1 ETH = 10^18 wei` and `1 uatom = 10^0 uatom`).\nThere are two denominations that are of high importance for clients: the `base`, which is the smallest\npossible unit and the `display`, which is the unit that is commonly referred to in human communication\nand on exchanges. The values in those fields link to an entry in the list of denominations.\nThe list in `denom_units` and the `display` entry may be changed via governance.\nAs a result, we can define the type as follows:\n```protobuf\nmessage DenomUnit {\nstring denom    = 1;\nuint32 exponent = 2;\nrepeated string aliases = 3;\n}\nmessage Metadata {\nstring description = 1;\nrepeated DenomUnit denom_units = 2;\nstring base = 3;\nstring display = 4;\n}\n```\nAs an example, the ATOM's metadata can be defined as follows:\n```json\n{\n""name"": ""atom"",\n""description"": ""The native staking token of the Cosmos Hub."",\n""denom_units"": [\n{\n""denom"": ""uatom"",\n""exponent"": 0,\n""aliases"": [\n""microatom""\n],\n},\n{\n""denom"": ""matom"",\n""exponent"": 3,\n""aliases"": [\n""milliatom""\n]\n},\n{\n""denom"": ""atom"",\n""exponent"": 6,\n}\n],\n""base"": ""uatom"",\n""display"": ""atom"",\n}\n```\nGiven the above metadata, a client may infer the following things:\n* 4.3atom = 4.3 * (10^6) = 4,300,000uatom\n* The string ""atom"" can be used as a display name in a list of tokens.\n* The balance 4300000 can be displayed as 4,300,000uatom or 4,300matom or 4.3atom.\nThe `display` denomination 4.3atom is a good default if the authors of the client don't make\nan explicit decision to choose a different representation.\nA client should be able to query for metadata by denom both via the CLI and REST interfaces. In\naddition, we will add handlers to these interfaces to convert from any unit to another given unit,\nas the base framework for this already exists in the Cosmos SDK.\nFinally, we need to ensure metadata exists in the `GenesisState` of the `x/bank` module which is also\nindexed by the base `denom`.\n```go\ntype GenesisState struct {\nSendEnabled   bool        `json:""send_enabled"" yaml:""send_enabled""`\nBalances      []Balance   `json:""balances"" yaml:""balances""`\nSupply        sdk.Coins   `json:""supply"" yaml:""supply""`\nDenomMetadata []Metadata  `json:""denom_metadata"" yaml:""denom_metadata""`\n}\n```\n","The Cosmos SDK should introduce a new field to the `x/bank` module's `Balance` struct to store metadata about each denomination. This metadata should include the following information:\n\n- decimal_units: The number of decimal units that the denomination has. For example, if the denomination is ""uatom"", then the decimal_units would be 6, because 1 ATOM = 10^6 uatom.\n- display: The user-friendly name of the denomination. For example, if the denomination is ""uatom"", then the display would be ""microatom"".\n\nThis change will allow clients, wallet providers, and explorers to improve their UX by providing more information about each denomination. It will also remove the requirement for clients to make any assumptions about the unit of denomination."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis idea was first conceived of in order to fulfill the use case of the\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\nmembers would be elected by a governing community and would fulfill the role of\ncoordinating the community under emergency situations. This thinking\ncan be further abstracted into the concept of ""blockchain specialization\ngroups"".\nThe creation of these groups are the beginning of specialization capabilities\nwithin a wider blockchain community which could be used to enable a certain\nlevel of delegated responsibilities. Examples of specialization which could be\nbeneficial to a blockchain community include: code auditing, emergency response,\ncode development etc. This type of community organization paves the way for\nindividual stakeholders to delegate votes by issue type, if in the future\ngovernance proposals include a field for issue type.\n\n## Decision\n","A specialization group can be broadly broken down into the following functions\n(herein containing examples):\n* Membership Admittance\n* Membership Acceptance\n* Membership Revocation\n* (probably) Without Penalty\n* member steps down (self-Revocation)\n* replaced by new member from governance\n* (probably) With Penalty\n* due to breach of soft-agreement (determined through governance)\n* due to breach of hard-agreement (determined by code)\n* Execution of Duties\n* Special transactions which only execute for members of a specialization\ngroup (for example, dCERT members voting to turn off transaction routes in\nan emergency scenario)\n* Compensation\n* Group compensation (further distribution decided by the specialization group)\n* Individual compensation for all constituents of a group from the\ngreater community\nMembership admission to a specialization group could take place over a wide\nvariety of mechanisms. The most obvious example is through a general vote among\nthe entire community, however in certain systems a community may want to allow\nthe members already in a specialization group to internally elect new members,\nor maybe the community may assign a permission to a particular specialization\ngroup to appoint members to other 3rd party groups. The sky is really the limit\nas to how membership admittance can be structured. We attempt to capture\nsome of these possibilities in a common interface dubbed the `Electionator`. For\nits initial implementation as a part of this ADR we recommend that the general\nelection abstraction (`Electionator`) is provided as well as a basic\nimplementation of that abstraction which allows for a continuous election of\nmembers of a specialization group.\n``` golang\n// The Electionator abstraction covers the concept space for\n// a wide variety of election kinds.\ntype Electionator interface {\n// is the election object accepting votes.\nActive() bool\n// functionality to execute for when a vote is cast in this election, here\n// the vote field is anticipated to be marshalled into a vote type used\n// by an election.\n//\n// NOTE There are no explicit ids here. Just votes which pertain specifically\n// to one electionator. Anyone can create and send a vote to the electionator item\n// which will presumably attempt to marshal those bytes into a particular struct\n// and apply the vote information in some arbitrary way. There can be multiple\n// Electionators within the Cosmos-Hub for multiple specialization groups, votes\n// would need to be routed to the Electionator upstream of here.\nVote(addr sdk.AccAddress, vote []byte)\n// here lies all functionality to authenticate and execute changes for\n// when a member accepts being elected\nAcceptElection(sdk.AccAddress)\n// Register a revoker object\nRegisterRevoker(Revoker)\n// No more revokers may be registered after this function is called\nSealRevokers()\n// register hooks to call when an election actions occur\nRegisterHooks(ElectionatorHooks)\n// query for the current winner(s) of this election based on arbitrary\n// election ruleset\nQueryElected() []sdk.AccAddress\n// query metadata for an address in the election this\n// could include for example position that an address\n// is being elected for within a group\n//\n// this metadata may be directly related to\n// voting information and/or privileges enabled\n// to members within a group.\nQueryMetadata(sdk.AccAddress) []byte\n}\n// ElectionatorHooks, once registered with an Electionator,\n// trigger execution of relevant interface functions when\n// Electionator events occur.\ntype ElectionatorHooks interface {\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\nAfterMemberAccepted(addr sdk.AccAddress)\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\n}\n// Revoker defines the function required for a membership revocation rule-set\n// used by a specialization group. This could be used to create self revoking,\n// and evidence based revoking, etc. Revokers types may be created and\n// reused for different election types.\n//\n// When revoking the ""cause"" bytes may be arbitrarily marshalled into evidence,\n// memos, etc.\ntype Revoker interface {\nRevokeName() string      // identifier for this revoker type\nRevokeMember(addr sdk.AccAddress, cause []byte) error\n}\n```\nCertain level of commonality likely exists between the existing code within\n`x/governance` and required functionality of elections. This common\nfunctionality should be abstracted during implementation. Similarly for each\nvote implementation client CLI/REST functionality should be abstracted\nto be reused for multiple elections.\nThe specialization group abstraction firstly extends the `Electionator`\nbut also further defines traits of the group.\n``` golang\ntype SpecializationGroup interface {\nElectionator\nGetName() string\nGetDescription() string\n// general soft contract the group is expected\n// to fulfill with the greater community\nGetContract() string\n// messages which can be executed by the members of the group\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\n// logic to be executed at endblock, this may for instance\n// include payment of a stipend to the group members\n// for participation in the security group.\nEndBlocker(ctx sdk.Context)\n}\n```\n","Utilize blockchain technology to establish specialized groups within the blockchain community to enable delegated responsibilities, such as code auditing, emergency response, and code development."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, a Cosmos SDK application's CLI directory stores key material and metadata in a plain text database in the user’s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user/computer.\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don’t provide a native secret store.\n\n## Decision\n",We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https://github.com/99designs/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\nThis appears to fulfill the requirement of protecting both key material and metadata from rogue software on a user’s machine.\n,Implement a secrets manager using a secure enclave.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe legacy amino multi-signature mechanism of the Cosmos SDK has certain limitations:\n* Key rotation is not possible, although this can be solved with [account rekeying](adr-034-account-rekeying.md).\n* Thresholds can't be changed.\n* UX is cumbersome for non-technical users ([#5661](https://github.com/cosmos/cosmos-sdk/issues/5661)).\n* It requires `legacy_amino` sign mode ([#8141](https://github.com/cosmos/cosmos-sdk/issues/8141)).\nWhile the group module is not meant to be a total replacement for the current multi-signature accounts, it provides a solution to the limitations described above, with a more flexible key management system where keys can be added, updated or removed, as well as configurable thresholds.\nIt's meant to be used with other access control modules such as [`x/feegrant`](./adr-029-fee-grant-module.md) and [`x/authz`](adr-030-authz-module.md) to simplify key management for individuals and organizations.\n\n## Decision\n","We propose merging the `x/group` module with its supporting ORM/Table Store package ([#7098](https://github.com/cosmos/cosmos-sdk/issues/7098)) into the Cosmos SDK and continuing development here. There will be a dedicated ADR for the ORM package.\n### Group\nA group is a composition of accounts with associated weights. It is not\nan account and doesn't have a balance. It doesn't in and of itself have any\nsort of voting or decision weight.\nGroup members can create proposals and vote on them through group accounts using different decision policies.\nIt has an `admin` account which can manage members in the group, update the group\nmetadata and set a new admin.\n```protobuf\nmessage GroupInfo {\n// group_id is the unique ID of this group.\nuint64 group_id = 1;\n// admin is the account address of the group's admin.\nstring admin = 2;\n// metadata is any arbitrary metadata to attached to the group.\nbytes metadata = 3;\n// version is used to track changes to a group's membership structure that\n// would break existing proposals. Whenever a member weight has changed,\n// or any member is added or removed, the version is incremented and will\n// invalidate all proposals from older versions.\nuint64 version = 4;\n// total_weight is the sum of the group members' weights.\nstring total_weight = 5;\n}\n```\n```protobuf\nmessage GroupMember {\n// group_id is the unique ID of the group.\nuint64 group_id = 1;\n// member is the member data.\nMember member = 2;\n}\n// Member represents a group member with an account address,\n// non-zero weight and metadata.\nmessage Member {\n// address is the member's account address.\nstring address = 1;\n// weight is the member's voting weight that should be greater than 0.\nstring weight = 2;\n// metadata is any arbitrary metadata to attached to the member.\nbytes metadata = 3;\n}\n```\n### Group Account\nA group account is an account associated with a group and a decision policy.\nA group account does have a balance.\nGroup accounts are abstracted from groups because a single group may have\nmultiple decision policies for different types of actions. Managing group\nmembership separately from decision policies results in the least overhead\nand keeps membership consistent across different policies. The pattern that\nis recommended is to have a single master group account for a given group,\nand then to create separate group accounts with different decision policies\nand delegate the desired permissions from the master account to\nthose ""sub-accounts"" using the [`x/authz` module](adr-030-authz-module.md).\n```protobuf\nmessage GroupAccountInfo {\n// address is the group account address.\nstring address = 1;\n// group_id is the ID of the Group the GroupAccount belongs to.\nuint64 group_id = 2;\n// admin is the account address of the group admin.\nstring admin = 3;\n// metadata is any arbitrary metadata of this group account.\nbytes metadata = 4;\n// version is used to track changes to a group's GroupAccountInfo structure that\n// invalidates active proposal from old versions.\nuint64 version = 5;\n// decision_policy specifies the group account's decision policy.\ngoogle.protobuf.Any decision_policy = 6 [(cosmos_proto.accepts_interface) = ""cosmos.group.v1.DecisionPolicy""];\n}\n```\nSimilarly to a group admin, a group account admin can update its metadata, decision policy or set a new group account admin.\nA group account can also be an admin or a member of a group.\nFor instance, a group admin could be another group account which could ""elects"" the members or it could be the same group that elects itself.\n### Decision Policy\nA decision policy is the mechanism by which members of a group can vote on\nproposals.\nAll decision policies should have a minimum and maximum voting window.\nThe minimum voting window is the minimum duration that must pass in order\nfor a proposal to potentially pass, and it may be set to 0. The maximum voting\nwindow is the maximum time that a proposal may be voted on and executed if\nit reached enough support before it is closed.\nBoth of these values must be less than a chain-wide max voting window parameter.\nWe define the `DecisionPolicy` interface that all decision policies must implement:\n```go\ntype DecisionPolicy interface {\ncodec.ProtoMarshaler\nValidateBasic() error\nGetTimeout() types.Duration\nAllow(tally Tally, totalPower string, votingDuration time.Duration) (DecisionPolicyResult, error)\nValidate(g GroupInfo) error\n}\ntype DecisionPolicyResult struct {\nAllow bool\nFinal bool\n}\n```\n#### Threshold decision policy\nA threshold decision policy defines a minimum support votes (_yes_), based on a tally\nof voter weights, for a proposal to pass. For\nthis decision policy, abstain and veto are treated as no support (_no_).\n```protobuf\nmessage ThresholdDecisionPolicy {\n// threshold is the minimum weighted sum of support votes for a proposal to succeed.\nstring threshold = 1;\n// voting_period is the duration from submission of a proposal to the end of voting period\n// Within this period, votes and exec messages can be submitted.\ngoogle.protobuf.Duration voting_period = 2 [(gogoproto.nullable) = false];\n}\n```\n### Proposal\nAny member of a group can submit a proposal for a group account to decide upon.\nA proposal consists of a set of `sdk.Msg`s that will be executed if the proposal\npasses as well as any metadata associated with the proposal. These `sdk.Msg`s get validated as part of the `Msg/CreateProposal` request validation. They should also have their signer set as the group account.\nInternally, a proposal also tracks:\n* its current `Status`: submitted, closed or aborted\n* its `Result`: unfinalized, accepted or rejected\n* its `VoteState` in the form of a `Tally`, which is calculated on new votes and when executing the proposal.\n```protobuf\n// Tally represents the sum of weighted votes.\nmessage Tally {\noption (gogoproto.goproto_getters) = false;\n// yes_count is the weighted sum of yes votes.\nstring yes_count = 1;\n// no_count is the weighted sum of no votes.\nstring no_count = 2;\n// abstain_count is the weighted sum of abstainers.\nstring abstain_count = 3;\n// veto_count is the weighted sum of vetoes.\nstring veto_count = 4;\n}\n```\n### Voting\nMembers of a group can vote on proposals. There are four choices to choose while voting - yes, no, abstain and veto. Not\nall decision policies will support them. Votes can contain some optional metadata.\nIn the current implementation, the voting window begins as soon as a proposal\nis submitted.\nVoting internally updates the proposal `VoteState` as well as `Status` and `Result` if needed.\n### Executing Proposals\nProposals will not be automatically executed by the chain in this current design,\nbut rather a user must submit a `Msg/Exec` transaction to attempt to execute the\nproposal based on the current votes and decision policy. A future upgrade could\nautomate this and have the group account (or a fee granter) pay.\n#### Changing Group Membership\nIn the current implementation, updating a group or a group account after submitting a proposal will make it invalid. It will simply fail if someone calls `Msg/Exec` and will eventually be garbage collected.\n### Notes on current implementation\nThis section outlines the current implementation used in the proof of concept of the group module but this could be subject to changes and iterated on.\n#### ORM\nThe [ORM package](https://github.com/cosmos/cosmos-sdk/discussions/9156) defines tables, sequences and secondary indexes which are used in the group module.\nGroups are stored in state as part of a `groupTable`, the `group_id` being an auto-increment integer. Group members are stored in a `groupMemberTable`.\nGroup accounts are stored in a `groupAccountTable`. The group account address is generated based on an auto-increment integer which is used to derive the group module `RootModuleKey` into a `DerivedModuleKey`, as stated in [ADR-033](adr-033-protobuf-inter-module-comm.md#modulekeys-and-moduleids). The group account is added as a new `ModuleAccount` through `x/auth`.\nProposals are stored as part of the `proposalTable` using the `Proposal` type. The `proposal_id` is an auto-increment integer.\nVotes are stored in the `voteTable`. The primary key is based on the vote's `proposal_id` and `voter` account address.\n#### ADR-033 to route proposal messages\nInter-module communication introduced by [ADR-033](adr-033-protobuf-inter-module-comm.md) can be used to route a proposal's messages using the `DerivedModuleKey` corresponding to the proposal's group account.\n",The Cosmos SDK should adopt the new Group Module as the primary multi-signature solution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nBaseApp's implementation of ABCI `{Check,Deliver}Tx()` and its own `Simulate()` method call the `runTx` method under the hood, which first runs antehandlers, then executes `Msg`s. However, the [transaction Tips](https://github.com/cosmos/cosmos-sdk/issues/9406) and [refunding unused gas](https://github.com/cosmos/cosmos-sdk/issues/2150) use cases require custom logic to be run after the `Msg`s execution. There is currently no way to achieve this.\nAn naive solution would be to add post-`Msg` hooks to BaseApp. However, the Cosmos SDK team thinks in parallel about the bigger picture of making app wiring simpler ([#9181](https://github.com/cosmos/cosmos-sdk/discussions/9182)), which includes making BaseApp more lightweight and modular.\n\n## Decision\n","We decide to transform Baseapp's implementation of ABCI `{Check,Deliver}Tx` and its own `Simulate` methods to use a middleware-based design.\nThe two following interfaces are the base of the middleware design, and are defined in `types/tx`:\n```go\ntype Handler interface {\nCheckTx(ctx context.Context, req Request, checkReq RequestCheckTx) (Response, ResponseCheckTx, error)\nDeliverTx(ctx context.Context, req Request) (Response, error)\nSimulateTx(ctx context.Context, req Request (Response, error)\n}\ntype Middleware func(Handler) Handler\n```\nwhere we define the following arguments and return types:\n```go\ntype Request struct {\nTx      sdk.Tx\nTxBytes []byte\n}\ntype Response struct {\nGasWanted uint64\nGasUsed   uint64\n// MsgResponses is an array containing each Msg service handler's response\n// type, packed in an Any. This will get proto-serialized into the `Data` field\n// in the ABCI Check/DeliverTx responses.\nMsgResponses []*codectypes.Any\nLog          string\nEvents       []abci.Event\n}\ntype RequestCheckTx struct {\nType abci.CheckTxType\n}\ntype ResponseCheckTx struct {\nPriority int64\n}\n```\nPlease note that because CheckTx handles separate logic related to mempool priotization, its signature is different than DeliverTx and SimulateTx.\nBaseApp holds a reference to a `tx.Handler`:\n```go\ntype BaseApp  struct {\n// other fields\ntxHandler tx.Handler\n}\n```\nBaseapp's ABCI `{Check,Deliver}Tx()` and `Simulate()` methods simply call `app.txHandler.{Check,Deliver,Simulate}Tx()` with the relevant arguments. For example, for `DeliverTx`:\n```go\nfunc (app *BaseApp) DeliverTx(req abci.RequestDeliverTx) abci.ResponseDeliverTx {\nvar abciRes abci.ResponseDeliverTx\nctx := app.getContextForTx(runTxModeDeliver, req.Tx)\nres, err := app.txHandler.DeliverTx(ctx, tx.Request{TxBytes: req.Tx})\nif err != nil {\nabciRes = sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\nreturn abciRes\n}\nabciRes, err = convertTxResponseToDeliverTx(res)\nif err != nil {\nreturn sdkerrors.ResponseDeliverTx(err, uint64(res.GasUsed), uint64(res.GasWanted), app.trace)\n}\nreturn abciRes\n}\n// convertTxResponseToDeliverTx converts a tx.Response into a abci.ResponseDeliverTx.\nfunc convertTxResponseToDeliverTx(txRes tx.Response) (abci.ResponseDeliverTx, error) {\ndata, err := makeABCIData(txRes)\nif err != nil {\nreturn abci.ResponseDeliverTx{}, nil\n}\nreturn abci.ResponseDeliverTx{\nData:   data,\nLog:    txRes.Log,\nEvents: txRes.Events,\n}, nil\n}\n// makeABCIData generates the Data field to be sent to ABCI Check/DeliverTx.\nfunc makeABCIData(txRes tx.Response) ([]byte, error) {\nreturn proto.Marshal(&sdk.TxMsgData{MsgResponses: txRes.MsgResponses})\n}\n```\nThe implementations are similar for `BaseApp.CheckTx` and `BaseApp.Simulate`.\n`baseapp.txHandler`'s three methods' implementations can obviously be monolithic functions, but for modularity we propose a middleware composition design, where a middleware is simply a function that takes a `tx.Handler`, and returns another `tx.Handler` wrapped around the previous one.\n### Implementing a Middleware\nIn practice, middlewares are created by Go function that takes as arguments some parameters needed for the middleware, and returns a `tx.Middleware`.\nFor example, for creating an arbitrary `MyMiddleware`, we can implement:\n```go\n// myTxHandler is the tx.Handler of this middleware. Note that it holds a\n// reference to the next tx.Handler in the stack.\ntype myTxHandler struct {\n// next is the next tx.Handler in the middleware stack.\nnext tx.Handler\n// some other fields that are relevant to the middleware can be added here\n}\n// NewMyMiddleware returns a middleware that does this and that.\nfunc NewMyMiddleware(arg1, arg2) tx.Middleware {\nreturn func (txh tx.Handler) tx.Handler {\nreturn myTxHandler{\nnext: txh,\n// optionally, set arg1, arg2... if they are needed in the middleware\n}\n}\n}\n// Assert myTxHandler is a tx.Handler.\nvar _ tx.Handler = myTxHandler{}\nfunc (h myTxHandler) CheckTx(ctx context.Context, req Request, checkReq RequestcheckTx) (Response, ResponseCheckTx, error) {\n// CheckTx specific pre-processing logic\n// run the next middleware\nres, checkRes, err := txh.next.CheckTx(ctx, req, checkReq)\n// CheckTx specific post-processing logic\nreturn res, checkRes, err\n}\nfunc (h myTxHandler) DeliverTx(ctx context.Context, req Request) (Response, error) {\n// DeliverTx specific pre-processing logic\n// run the next middleware\nres, err := txh.next.DeliverTx(ctx, tx, req)\n// DeliverTx specific post-processing logic\nreturn res, err\n}\nfunc (h myTxHandler) SimulateTx(ctx context.Context, req Request) (Response, error) {\n// SimulateTx specific pre-processing logic\n// run the next middleware\nres, err := txh.next.SimulateTx(ctx, tx, req)\n// SimulateTx specific post-processing logic\nreturn res, err\n}\n```\n### Composing Middlewares\nWhile BaseApp simply holds a reference to a `tx.Handler`, this `tx.Handler` itself is defined using a middleware stack. The Cosmos SDK exposes a base (i.e. innermost) `tx.Handler` called `RunMsgsTxHandler`, which executes messages.\nThen, the app developer can compose multiple middlewares on top on the base `tx.Handler`. Each middleware can run pre-and-post-processing logic around its next middleware, as described in the section above. Conceptually, as an example, given the middlewares `A`, `B`, and `C` and the base `tx.Handler` `H` the stack looks like:\n```text\nA.pre\nB.pre\nC.pre\nH # The base tx.handler, for example `RunMsgsTxHandler`\nC.post\nB.post\nA.post\n```\nWe define a `ComposeMiddlewares` function for composing middlewares. It takes the base handler as first argument, and middlewares in the ""outer to inner"" order. For the above stack, the final `tx.Handler` is:\n```go\ntxHandler := middleware.ComposeMiddlewares(H, A, B, C)\n```\nThe middleware is set in BaseApp via its `SetTxHandler` setter:\n```go\n// simapp/app.go\ntxHandler := middleware.ComposeMiddlewares(...)\napp.SetTxHandler(txHandler)\n```\nThe app developer can define their own middlewares, or use the Cosmos SDK's pre-defined middlewares from `middleware.NewDefaultTxHandler()`.\n### Middlewares Maintained by the Cosmos SDK\nWhile the app developer can define and compose the middlewares of their choice, the Cosmos SDK provides a set of middlewares that caters for the ecosystem's most common use cases. These middlewares are:\n| Middleware              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| RunMsgsTxHandler        | This is the base `tx.Handler`. It replaces the old baseapp's `runMsgs`, and executes a transaction's `Msg`s.                                                                                                                                                                                                                                                                                                                                                                             |\n| TxDecoderMiddleware     | This middleware takes in transaction raw bytes, and decodes them into a `sdk.Tx`. It replaces the `baseapp.txDecoder` field, so that BaseApp stays as thin as possible. Since most middlewares read the contents of the `sdk.Tx`, the TxDecoderMiddleware should be run first in the middleware stack.                                                                                                                                                                                   |\n| {Antehandlers}          | Each antehandler is converted to its own middleware. These middlewares perform signature verification, fee deductions and other validations on the incoming transaction.                                                                                                                                                                                                                                                                                                                 |\n| IndexEventsTxMiddleware | This is a simple middleware that chooses which events to index in Tendermint. Replaces `baseapp.indexEvents` (which unfortunately still exists in baseapp too, because it's used to index Begin/EndBlock events)                                                                                                                                                                                                                                                                         |\n| RecoveryTxMiddleware    | This index recovers from panics. It replaces baseapp.runTx's panic recovery described in [ADR-022](./adr-022-custom-panic-handling.md).                                                                                                                                                                                                                                                                                                                                                  |\n| GasTxMiddleware         | This replaces the [`Setup`](https://github.com/cosmos/cosmos-sdk/blob/v0.43.0/x/auth/ante/setup.go) Antehandler. It sets a GasMeter on sdk.Context. Note that before, GasMeter was set on sdk.Context inside the antehandlers, and there was some mess around the fact that antehandlers had their own panic recovery system so that the GasMeter could be read by baseapp's recovery system. Now, this mess is all removed: one middleware sets GasMeter, another one handles recovery. |\n### Similarities and Differences between Antehandlers and Middlewares\nThe middleware-based design builds upon the existing antehandlers design described in [ADR-010](./adr-010-modular-antehandler.md). Even though the final decision of ADR-010 was to go with the ""Simple Decorators"" approach, the middleware design is actually very similar to the other [Decorator Pattern](./adr-010-modular-antehandler.md#decorator-pattern) proposal, also used in [weave](https://github.com/iov-one/weave).\n#### Similarities with Antehandlers\n* Designed as chaining/composing small modular pieces.\n* Allow code reuse for `{Check,Deliver}Tx` and for `Simulate`.\n* Set up in `app.go`, and easily customizable by app developers.\n* Order is important.\n#### Differences with Antehandlers\n* The Antehandlers are run before `Msg` execution, whereas middlewares can run before and after.\n* The middleware approach uses separate methods for `{Check,Deliver,Simulate}Tx`, whereas the antehandlers pass a `simulate bool` flag and uses the `sdkCtx.Is{Check,Recheck}Tx()` flags to determine in which transaction mode we are.\n* The middleware design lets each middleware hold a reference to the next middleware, whereas the antehandlers pass a `next` argument in the `AnteHandle` method.\n* The middleware design use Go's standard `context.Context`, whereas the antehandlers use `sdk.Context`.\n","**Introduce a new `runTxWithCtx` method in BaseApp**\n\nThis method would take a `context.Context` as an argument and would be called instead of `runTx` by `ABCI {Check,Deliver}Tx()` and `Simulate()`. The `context.Context` would be passed to all antehandlers and `Msg`s, allowing them to access custom data and perform custom logic after the `Msg`s have been executed.\n\nThis solution is more flexible and modular than adding post-`Msg` hooks to BaseApp, and it is in line with the Cosmos SDK team's vision of making BaseApp more lightweight and modular."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIssue [\#3685](https://github.com/cosmos/cosmos-sdk/issues/3685) identified that public key\naddress spaces are currently overlapping. We confirmed that it significantly decreases security of Cosmos SDK.\n### Problem\nAn attacker can control an input for an address generation function. This leads to a birthday attack, which significantly decreases the security space.\nTo overcome this, we need to separate the inputs for different kind of account types:\na security break of one account type shouldn't impact the security of other account types.\n### Initial proposals\nOne initial proposal was extending the address length and\nadding prefixes for different types of addresses.\n@ethanfrey explained an alternate approach originally used in https://github.com/iov-one/weave:\n> I spent quite a bit of time thinking about this issue while building weave... The other cosmos Sdk.\n> Basically I define a condition to be a type and format as human readable string with some binary data appended. This condition is hashed into an Address (again at 20 bytes). The use of this prefix makes it impossible to find a preimage for a given address with a different condition (eg ed25519 vs secp256k1).\n> This is explained in depth here https://weave.readthedocs.io/en/latest/design/permissions.html\n> And the code is here, look mainly at the top where we process conditions. https://github.com/iov-one/weave/blob/master/conditions.go\nAnd explained how this approach should be sufficiently collision resistant:\n> Yeah, AFAIK, 20 bytes should be collision resistance when the preimages are unique and not malleable. A space of 2^160 would expect some collision to be likely around 2^80 elements (birthday paradox). And if you want to find a collision for some existing element in the database, it is still 2^160. 2^80 only is if all these elements are written to state.\n> The good example you brought up was eg. a public key bytes being a valid public key on two algorithms supported by the codec. Meaning if either was broken, you would break accounts even if they were secured with the safer variant. This is only as the issue when no differentiating type info is present in the preimage (before hashing into an address).\n> I would like to hear an argument if the 20 bytes space is an actual issue for security, as I would be happy to increase my address sizes in weave. I just figured cosmos and ethereum and bitcoin all use 20 bytes, it should be good enough. And the arguments above which made me feel it was secure. But I have not done a deeper analysis.\nThis led to the first proposal (which we proved to be not good enough):\nwe concatenate a key type with a public key, hash it and take the first 20 bytes of that hash, summarized as `sha256(keyTypePrefix || keybytes)[:20]`.\n### Review and Discussions\nIn [\#5694](https://github.com/cosmos/cosmos-sdk/issues/5694) we discussed various solutions.\nWe agreed that 20 bytes it's not future proof, and extending the address length is the only way to allow addresses of different types, various signature types, etc.\nThis disqualifies the initial proposal.\nIn the issue we discussed various modifications:\n* Choice of the hash function.\n* Move the prefix out of the hash function: `keyTypePrefix + sha256(keybytes)[:20]` [post-hash-prefix-proposal].\n* Use double hashing: `sha256(keyTypePrefix + sha256(keybytes)[:20])`.\n* Increase to keybytes hash slice from 20 byte to 32 or 40 bytes. We concluded that 32 bytes, produced by a good hash functions is future secure.\n### Requirements\n* Support currently used tools - we don't want to break an ecosystem, or add a long adaptation period. Ref: https://github.com/cosmos/cosmos-sdk/issues/8041\n* Try to keep the address length small - addresses are widely used in state, both as part of a key and object value.\n### Scope\nThis ADR only defines a process for the generation of address bytes. For end-user interactions with addresses (through the API, or CLI, etc.), we still use bech32 to format these addresses as strings. This ADR doesn't change that.\nUsing Bech32 for string encoding gives us support for checksum error codes and handling of user typos.\n\n## Decision\n","We define the following account types, for which we define the address function:\n1. simple accounts: represented by a regular public key (ie: secp256k1, sr25519)\n2. naive multisig: accounts composed by other addressable objects (ie: naive multisig)\n3. composed accounts with a native address key (ie: bls, group module accounts)\n4. module accounts: basically any accounts which cannot sign transactions and which are managed internally by modules\n### Legacy Public Key Addresses Don't Change\nCurrently (Jan 2021), the only officially supported Cosmos SDK user accounts are `secp256k1` basic accounts and legacy amino multisig.\nThey are used in existing Cosmos SDK zones. They use the following address formats:\n* secp256k1: `ripemd160(sha256(pk_bytes))[:20]`\n* legacy amino multisig: `sha256(aminoCdc.Marshal(pk))[:20]`\nWe don't want to change existing addresses. So the addresses for these two key types will remain the same.\nThe current multisig public keys use amino serialization to generate the address. We will retain\nthose public keys and their address formatting, and call them ""legacy amino"" multisig public keys\nin protobuf. We will also create multisig public keys without amino addresses to be described below.\n### Hash Function Choice\nAs in other parts of the Cosmos SDK, we will use `sha256`.\n### Basic Address\nWe start with defining a base algorithm for generating addresses which we will call `Hash`. Notably, it's used for accounts represented by a single key pair. For each public key schema we have to have an associated `typ` string, explained in the next section. `hash` is the cryptographic hash function defined in the previous section.\n```go\nconst A_LEN = 32\nfunc Hash(typ string, key []byte) []byte {\nreturn hash(hash(typ) + key)[:A_LEN]\n}\n```\nThe `+` is bytes concatenation, which doesn't use any separator.\nThis algorithm is the outcome of a consultation session with a professional cryptographer.\nMotivation: this algorithm keeps the address relatively small (length of the `typ` doesn't impact the length of the final address)\nand it's more secure than [post-hash-prefix-proposal] (which uses the first 20 bytes of a pubkey hash, significantly reducing the address space).\nMoreover the cryptographer motivated the choice of adding `typ` in the hash to protect against a switch table attack.\n`address.Hash` is a low level function to generate _base_ addresses for new key types. Example:\n* BLS: `address.Hash(""bls"", pubkey)`\n### Composed Addresses\nFor simple composed accounts (like a new naive multisig) we generalize the `address.Hash`. The address is constructed by recursively creating addresses for the sub accounts, sorting the addresses and composing them into a single address. It ensures that the ordering of keys doesn't impact the resulting address.\n```go\n// We don't need a PubKey interface - we need anything which is addressable.\ntype Addressable interface {\nAddress() []byte\n}\nfunc Composed(typ string, subaccounts []Addressable) []byte {\naddresses = map(subaccounts, \a -> LengthPrefix(a.Address()))\naddresses = sort(addresses)\nreturn address.Hash(typ, addresses[0] + ... + addresses[n])\n}\n```\nThe `typ` parameter should be a schema descriptor, containing all significant attributes with deterministic serialization (eg: utf8 string).\n`LengthPrefix` is a function which prepends 1 byte to the address. The value of that byte is the length of the address bits before prepending. The address must be at most 255 bits long.\nWe are using `LengthPrefix` to eliminate conflicts - it assures, that for 2 lists of addresses: `as = {a1, a2, ..., an}` and `bs = {b1, b2, ..., bm}` such that every `bi` and `ai` is at most 255 long, `concatenate(map(as, (a) => LengthPrefix(a))) = map(bs, (b) => LengthPrefix(b))` if `as = bs`.\nImplementation Tip: account implementations should cache addresses.\n#### Multisig Addresses\nFor a new multisig public keys, we define the `typ` parameter not based on any encoding scheme (amino or protobuf). This avoids issues with non-determinism in the encoding scheme.\nExample:\n```protobuf\npackage cosmos.crypto.multisig;\nmessage PubKey {\nuint32 threshold = 1;\nrepeated google.protobuf.Any pubkeys = 2;\n}\n```\n```go\nfunc (multisig PubKey) Address() {\n// first gather all nested pub keys\nvar keys []address.Addressable  // cryptotypes.PubKey implements Addressable\nfor _, _key := range multisig.Pubkeys {\nkeys = append(keys, key.GetCachedValue().(cryptotypes.PubKey))\n}\n// form the type from the message name (cosmos.crypto.multisig.PubKey) and the threshold joined together\nprefix := fmt.Sprintf(""%s/%d"", proto.MessageName(multisig), multisig.Threshold)\n// use the Composed function defined above\nreturn address.Composed(prefix, keys)\n}\n```\n### Derived Addresses\nWe must be able to cryptographically derive one address from another one. The derivation process must guarantee hash properties, hence we use the already defined `Hash` function:\n```go\nfunc Derive(address, derivationKey []byte) []byte {\nreturn Hash(address, derivationKey)\n}\n```\n### Module Account Addresses\nA module account will have `""module""` type. Module accounts can have sub accounts. The submodule account will be created based on module name, and sequence of derivation keys. Typically, the first derivation key should be a class of the derived accounts. The derivation process has a defined order: module name, submodule key, subsubmodule key... An example module account is created using:\n```go\naddress.Module(moduleName, key)\n```\nAn example sub-module account is created using:\n```go\ngroupPolicyAddresses := []byte{1}\naddress.Module(moduleName, groupPolicyAddresses, policyID)\n```\nThe `address.Module` function is using `address.Hash` with `""module""` as the type argument, and byte representation of the module name concatenated with submodule key. The two last component must be uniquely separated to avoid potential clashes (example: modulename=""ab"" & submodulekey=""bc"" will have the same derivation key as modulename=""a"" & submodulekey=""bbc"").\nWe use a null byte (`'\x00'`) to separate module name from the submodule key. This works, because null byte is not a part of a valid module name. Finally, the sub-submodule accounts are created by applying the `Derive` function recursively.\nWe could use `Derive` function also in the first step (rather than concatenating module name with zero byte and the submodule key). We decided to do concatenation to avoid one level of derivation and speed up computation.\nFor backward compatibility with the existing `authtypes.NewModuleAddress`, we add a special case in `Module` function: when no derivation key is provided, we fallback to the ""legacy"" implementation.\n```go\nfunc Module(moduleName string, derivationKeys ...[]byte) []byte{\nif len(derivationKeys) == 0 {\nreturn authtypes.NewModuleAddress(moduleName)  // legacy case\n}\nsubmoduleAddress := Hash(""module"", []byte(moduleName) + 0 + key)\nreturn fold((a, k) => Derive(a, k), subsubKeys, submoduleAddress)\n}\n```\n**Example 1**  A lending BTC pool address would be:\n```go\nbtcPool := address.Module(""lending"", btc.Address()})\n```\nIf we want to create an address for a module account depending on more than one key, we can concatenate them:\n```go\nbtcAtomAMM := address.Module(""amm"", btc.Address() + atom.Address()})\n```\n**Example 2**  a smart-contract address could be constructed by:\n```go\nsmartContractAddr = Module(""mySmartContractVM"", smartContractsNamespace, smartContractKey})\n// which equals to:\nsmartContractAddr = Derived(\nModule(""mySmartContractVM"", smartContractsNamespace),\n[]{smartContractKey})\n```\n### Schema Types\nA `typ` parameter used in `Hash` function SHOULD be unique for each account type.\nSince all Cosmos SDK account types are serialized in the state, we propose to use the protobuf message name string.\nExample: all public key types have a unique protobuf message type similar to:\n```protobuf\npackage cosmos.crypto.sr25519;\nmessage PubKey {\nbytes key = 1;\n}\n```\nAll protobuf messages have unique fully qualified names, in this example `cosmos.crypto.sr25519.PubKey`.\nThese names are derived directly from .proto files in a standardized way and used\nin other places such as the type URL in `Any`s. We can easily obtain the name using\n`proto.MessageName(msg)`.\n","**Decision:** Adopt a double-hashing approach with a 32-byte output size, prefixed by a 1-byte key type. This results in a 33-byte address.\n\n**Rationale:**\n\n* This approach provides a collision-resistant and future-proof solution, as the hash function output is 32 bytes long.\n* The use of double hashing further increases the security of the address generation process.\n* The addition of a 1-byte key type prefix ensures that addresses of different types are easily distinguishable, reducing the risk of birthday attacks.\n* This approach is compatible with existing tools and ecosystems, as it maintains the 20-byte address size for legacy addresses.\n* The 33-byte address length is still relatively small and manageable for use in state."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md) and\n[ADR 020](./adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nThis ADR continues from [ADD 020](./adr-020-protobuf-transaction-encoding.md)\nto specify the encoding of queries.\n\n## Decision\n","### Custom Query Definition\nModules define custom queries through a protocol buffers `service` definition.\nThese `service` definitions are generally associated with and used by the\nGRPC protocol. However, the protocol buffers specification indicates that\nthey can be used more generically by any request/response protocol that uses\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\nEach module with custom queries should define a service canonically named `Query`:\n```protobuf\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\n}\n```\n#### Handling of Interface Types\nModules that use interface types and need true polymorphism generally force a\n`oneof` up to the app-level that provides the set of concrete implementations of\nthat interface that the app supports. While app's are welcome to do the same for\nqueries and implement an app-level query service, it is recommended that modules\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\nThere is a concern on the transaction level that the overhead of `Any` is too\nhigh to justify its usage. However for queries this is not a concern, and\nproviding generic module-level queries that use `Any` does not preclude apps\nfrom also providing app-level queries that return use the app-level `oneof`s.\nA hypothetical example for the `gov` module would look something like:\n```protobuf\n// x/gov/types/types.proto\nimport ""google/protobuf/any.proto"";\nservice Query {\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\n}\nmessage AnyProposal {\nProposalBase base = 1;\ngoogle.protobuf.Any content = 2;\n}\n```\n### Custom Query Implementation\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https://github.com/cosmos/gogoproto)\ngrpc plugin, which for a service named `Query` generates an interface named\n`QueryServer` as below:\n```go\ntype QueryServer interface {\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\n}\n```\nThe custom queries for our module are implemented by implementing this interface.\nThe first parameter in this generated interface is a generic `context.Context`,\nwhereas querier methods generally need an instance of `sdk.Context` to read\nfrom the store. Since arbitrary values can be attached to `context.Context`\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\n`context.Context`.\nAn example implementation of `QueryBalance` for the bank module as above would\nlook something like:\n```go\ntype Querier struct {\nKeeper\n}\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\nreturn &balance, nil\n}\n```\n### Custom Query Registration and Routing\nQuery server implementations as above would be registered with `AppModule`s using\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\nas below:\n```go\n// x/bank/module.go\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\n}\n```\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\nquery routing table (with the routing method being described below).\nThe signature for this method matches the existing\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\nquery server implementation described above.\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\nand method name (ex. `QueryBalance`) combined with `/`s to form a full\nmethod name (ex. `/cosmos_sdk.x.bank.v1.Query/QueryBalance`). This gets translated\ninto an ABCI query as `custom/cosmos_sdk.x.bank.v1.Query/QueryBalance`. Service handlers\nregistered with `QueryRouter.RegisterService` will be routed this way.\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\n`sdk.Query` and `QueryRouter` infrastructure.\nThis basic specification allows us to reuse protocol buffer `service` definitions\nfor ABCI custom queries substantially reducing the need for manual decoding and\nencoding in query methods.\n### GRPC Protocol Support\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\nproxy server that routes requests in the GRPC protocol to ABCI query requests\nunder the hood. In this way, clients could use their host languages' existing\nGRPC implementations to make direct queries against Cosmos SDK app's using\nthese `service` definitions. In order for this server to work, the `QueryRouter`\non `BaseApp` will need to expose the service handlers registered with\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\nlaunch the proxy server on a separate port in the same process as the ABCI app\nwith a command-line flag.\n### REST Queries and Swagger Generation\n[grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway) is a project that\ntranslates REST calls into GRPC calls using special annotations on service\nmethods. Modules that want to expose REST queries should add `google.api.http`\nannotations to their `rpc` methods as in this example below.\n```protobuf\n// x/bank/types/types.proto\nservice Query {\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balance/{address}/{denom}""\n};\n}\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\noption (google.api.http) = {\nget: ""/x/bank/v1/balances/{address}""\n};\n}\n}\n```\ngrpc-gateway will work directly against the GRPC proxy described above which will\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\ngenerate Swagger definitions automatically.\nIn the current implementation of REST queries, each module needs to implement\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\napproach, there will be no need to generate separate REST query handlers, just\nquery servers as described above as grpc-gateway handles the translation of protobuf\nto REST as well as Swagger definitions.\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\na separate process or the same process as the ABCI app, as well as provide a\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\nfile.\n### Client Usage\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\ninterface like:\n```go\ntype QueryClient interface {\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\n}\n```\nVia a small patch to gogo protobuf ([gogo/protobuf#675](https://github.com/gogo/protobuf/pull/675))\nwe have tweaked the grpc codegen to use an interface rather than concrete type\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\nfor ABCI client queries.\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\nthat routes calls to ABCI queries\nClients (such as CLI methods) will then be able to call query methods like this:\n```go\nclientCtx := client.NewContext()\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\nparams := &types.QueryBalanceParams{addr, denom}\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\n```\n### Testing\nTests would be able to create a query client directly from keeper and `sdk.Context`\nreferences using a `QueryServerTestHelper` as below:\n```go\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\nqueryClient := types.NewQueryClient(queryHelper)\n```\n","To encode queries in Protocol Buffers, we will leverage the existing `QueryRequest` and `QueryResponse` types from the `google.cloud.grpc` protocol buffers package. These types provide a common interface for all queries, regardless of their specific type, and allow for the inclusion of arbitrary data in the query and response."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, in the Cosmos SDK, modules that require the use of parameters use the\n`x/params` module. The `x/params` works by having modules define parameters,\ntypically via a simple `Params` structure, and registering that structure in\nthe `x/params` module via a unique `Subspace` that belongs to the respective\nregistering module. The registering module then has unique access to its respective\n`Subspace`. Through this `Subspace`, the module can get and set its `Params`\nstructure.\nIn addition, the Cosmos SDK's `x/gov` module has direct support for changing\nparameters on-chain via a `ParamChangeProposal` governance proposal type, where\nstakeholders can vote on suggested parameter changes.\nThere are various tradeoffs to using the `x/params` module to manage individual\nmodule parameters. Namely, managing parameters essentially comes for ""free"" in\nthat developers only need to define the `Params` struct, the `Subspace`, and the\nvarious auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However,\nthere are some notable drawbacks. These drawbacks include the fact that parameters\nare serialized in state via JSON which is extremely slow. In addition, parameter\nchanges via `ParamChangeProposal` governance proposals have no way of reading from\nor writing to state. In other words, it is currently not possible to have any\nstate transitions in the application during an attempt to change param(s).\n\n## Decision\n","We will build off of the alignment of `x/gov` and `x/authz` work per\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810). Namely, module developers\nwill create one or more unique parameter data structures that must be serialized\nto state. The Param data structures must implement `sdk.Msg` interface with respective\nProtobuf Msg service method which will validate and update the parameters with all\nnecessary changes. The `x/gov` module via the work done in\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810), will dispatch Param\nmessages, which will be handled by Protobuf Msg services.\nNote, it is up to developers to decide how to structure their parameters and\nthe respective `sdk.Msg` messages. Consider the parameters currently defined in\n`x/auth` using the `x/params` module for parameter management:\n```protobuf\nmessage Params {\nuint64 max_memo_characters       = 1;\nuint64 tx_sig_limit              = 2;\nuint64 tx_size_cost_per_byte     = 3;\nuint64 sig_verify_cost_ed25519   = 4;\nuint64 sig_verify_cost_secp256k1 = 5;\n}\n```\nDevelopers can choose to either create a unique data structure for every field in\n`Params` or they can create a single `Params` structure as outlined above in the\ncase of `x/auth`.\nIn the former, `x/params`, approach, a `sdk.Msg` would need to be created for every single\nfield along with a handler. This can become burdensome if there are a lot of\nparameter fields. In the latter case, there is only a single data structure and\nthus only a single message handler, however, the message handler might have to be\nmore sophisticated in that it might need to understand what parameters are being\nchanged vs what parameters are untouched.\nParams change proposals are made using the `x/gov` module. Execution is done through\n`x/authz` authorization to the root `x/gov` module's account.\nContinuing to use `x/auth`, we demonstrate a more complete example:\n```go\ntype Params struct {\nMaxMemoCharacters      uint64\nTxSigLimit             uint64\nTxSizeCostPerByte      uint64\nSigVerifyCostED25519   uint64\nSigVerifyCostSecp256k1 uint64\n}\ntype MsgUpdateParams struct {\nMaxMemoCharacters      uint64\nTxSigLimit             uint64\nTxSizeCostPerByte      uint64\nSigVerifyCostED25519   uint64\nSigVerifyCostSecp256k1 uint64\n}\ntype MsgUpdateParamsResponse struct {}\nfunc (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {\nctx := sdk.UnwrapSDKContext(goCtx)\n// verification logic...\n// persist params\nparams := ParamsFromMsg(msg)\nms.SaveParams(ctx, params)\nreturn &types.MsgUpdateParamsResponse{}, nil\n}\nfunc ParamsFromMsg(msg *types.MsgUpdateParams) Params {\n// ...\n}\n```\nA gRPC `Service` query should also be provided, for example:\n```protobuf\nservice Query {\n// ...\nrpc Params(QueryParamsRequest) returns (QueryParamsResponse) {\noption (google.api.http).get = ""/cosmos/<module>/v1beta1/params"";\n}\n}\nmessage QueryParamsResponse {\nParams params = 1 [(gogoproto.nullable) = false];\n}\n```\n","**Decision:**\n\nReplace the `x/params` module with a more efficient and extensible parameter management system that addresses the drawbacks of the current approach, while preserving the benefits of parameter management in the Cosmos SDK.\n\n**Rationale:**\n\n* **Improved Performance:** The current JSON serialization of parameters in state is inefficient and can lead to performance bottlenecks. A more efficient binary serialization mechanism would significantly improve performance.\n* **State Transitions in Governance Proposals:** The current `ParamChangeProposal` governance proposals cannot read from or write to state. This limitation prevents more complex state transitions during parameter changes. A new system should allow for such state transitions to enable a wider range of parameter management use cases.\n* **Parameter Dependency and Validation:** The current system does not provide a mechanism to define dependencies or perform validation on parameter changes. A more robust parameter management system should allow for these capabilities to ensure the integrity of parameter changes.\n* **Extensibility:** The current `x/params` module is not easily extensible for modules that require additional parameter management functionality. A more extensible system would allow modules to customize parameter handling based on their specific needs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https://docs.cosmos.network/main/learn/advanced/ocap#ocaps-in-practice), it is stated that:\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\n### `x/bank` Case Study\nCurrently the `x/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module’s\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\nHowever, these permissions don’t really do much. They control what modules can be referenced in the `MintCoins`,\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access —\njust a simple string. So the `x/upgrade` module could mint tokens for the `x/staking` module simple by calling\n`MintCoins(“staking”)`. Furthermore, all modules which have access to these keeper methods, also have access to\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\n\n## Decision\n","Based on [ADR-021](./adr-021-protobuf-query-encoding.md) and [ADR-031](./adr-031-msg-service.md), we introduce the\nInter-Module Communication framework for secure module authorization and OCAPs.\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\nOf particular note — the decision is to _enable_ this functionality for modules to adopt at their own discretion.\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\naddressed as amendments to this ADR.\n### New ""Keeper"" Paradigm\nIn [ADR 021](./adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\nwas introduced and in [ADR 31](./adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg/Send` message type:\n```go\npackage bank\ntype MsgClient interface {\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\n}\ntype MsgServer interface {\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\n}\n```\n[ADR 021](./adr-021-protobuf-query-encoding.md) and [ADR 31](./adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\nbased service interfaces already used by clients for inter-module communication.\nUsing this `QueryClient`/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\n1. Protobuf types are checked for breaking changes using [buf](https://buf.build/docs/breaking-overview) and because of\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\nevolution.\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\nobject capability system (see below).\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\nenabling atomicy of operations ([currently a problem](https://github.com/cosmos/cosmos-sdk/issues/8030)). Any failure within a module-to-module call would result in a failure of the entire\ntransaction\nThis mechanism has the added benefits of:\n* reducing boilerplate through code generation, and\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\n### Inter-module Communication\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https://github.com/grpc/grpc-go/blob/v1.49.x/clientconn.go#L441-L450)\nimplementation. For this we introduce\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the ""private\nkey"" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\ndescribed in more detail below.\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole ""signer"". It's worth to note\nthat we don't use any cryptographic signature in this case.\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `/cosmos.bank.Msg/Send` transaction. `MsgSend` validation\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\nHere's an example of a hypothetical module `foo` interacting with `x/bank`:\n```go\npackage foo\ntype FooMsgServer {\n// ...\nbankQuery bank.QueryClient\nbankMsg   bank.MsgClient\n}\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\n// ...\nreturn FooMsgServer {\n// ...\nmodouleKey: moduleKey,\nbankQuery: bank.NewQueryClient(moduleKey),\nbankMsg: bank.NewMsgClient(moduleKey),\n}\n}\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: ""foo""})\n...\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\n...\n}\n```\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\ndenom prefix being restricted to certain modules (as discussed in\n[#7459](https://github.com/cosmos/cosmos-sdk/pull/7459#discussion_r529545528)).\n### `ModuleKey`s and `ModuleID`s\nA `ModuleKey` can be thought of as a ""private key"" for a module account and a `ModuleID` can be thought of as the\ncorresponding ""public key"". From the [ADR 028](./adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\nderivation path. `ModuleID` is a simple struct which contains the module name and optional ""derivation"" path,\nand forms its address based on the `AddressHash` method from [the ADR-028](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-028-public-key-addresses.md):\n```go\ntype ModuleID struct {\nModuleName string\nPath []byte\n}\nfunc (key ModuleID) Address() []byte {\nreturn AddressHash(key.ModuleName, key.Path)\n}\n```\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\nthe `ModuleKey` security.\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\n```go\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\ntype CallInfo {\nMethod string\nCaller ModuleID\n}\ntype RootModuleKey struct {\nmoduleName string\ninvoker Invoker\n}\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { /* ... */}\ntype DerivedModuleKey struct {\nmoduleName string\npath []byte\ninvoker Invoker\n}\n```\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\n```go\npackage foo\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\nbankMsgClient := bank.NewMsgClient(derivedKey)\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\n...\n}\n```\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\nfrom either the root or a derived module account to pass through.\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\nchecking permissions.\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\nname impersonation.\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\n```go\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\nreturn f(ctx, args, reply)\n}\n```\n### `AppModule` Wiring and Requirements\nIn [ADR 031](./adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\nspecify their dependencies on other modules using `RequireServer()`:\n```go\ntype Configurator interface {\nMsgServer() grpc.Server\nQueryServer() grpc.Server\nModuleKey() ModuleKey\nRequireServer(msgServer interface{})\n}\n```\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\nmodule `foo` could declare it's dependency on `x/bank` like this:\n```go\npackage foo\nfunc (am AppModule) RegisterServices(cfg Configurator) {\ncfg.RequireServer((*bank.QueryServer)(nil))\ncfg.RequireServer((*bank.MsgServer)(nil))\n}\n```\n### Security Considerations\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\nthe underlying router infrastructure.\n#### Recursion and Re-entry\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\ncalls Module B and Module B calls module A again in the same call.\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\nin the router could be used to perform this security check.\n#### Queries\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\n### Internal Methods\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\npurpose, we add the `InternalServer` method to `Configurator`:\n```go\ntype Configurator interface {\nMsgServer() grpc.Server\nQueryServer() grpc.Server\nInternalServer() grpc.Server\n}\n```\nAs an example, x/slashing's Slash must call x/staking's Slash, but we don't want to expose x/staking's Slash to end users\nand clients.\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\nproto package.\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\nAn alternative solution to internal-only methods could involve hooks / plugins as discussed [here](https://github.com/cosmos/cosmos-sdk/pull/7459#issuecomment-733807753).\nA more detailed evaluation of a hooks / plugin system will be addressed later in follow-ups to this ADR or as a separate\nADR.\n### Authorization\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-030-authz-module.md).\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\nAuthorization middleware should take into account the need to grant certain modules effectively ""admin"" privileges to\nother modules. This will be addressed in separate ADRs or updates to this ADR.\n### Future Work\nOther future improvements may include:\n* custom code generation that:\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\n* code generation which makes inter-module communication more performant\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\n* inter-module hooks and plugins\n","Implement a proper object-capability system for the Cosmos SDK. This system should include the following features:\n- A unique object capability token for each object.\n- A set of permissions associated with each object capability token.\n- A mechanism for enforcing the permissions associated with each object capability token.\n\nThis system will provide a more secure and flexible way to manage access to objects in the Cosmos SDK. It will also make it easier to build and compose modules, and it will help to promote the development of a thriving ecosystem of Cosmos SDK modules."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to support building highly secure, robust and interoperable blockchain\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\nevidence can be submitted, evaluated and verified resulting in some agreed upon\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\nFurthermore, such a mechanism is paramount for any\n[IBC](https://github.com/cosmos/ibc) or\ncross-chain validation protocol implementation in order to support the ability\nfor any misbehavior to be relayed back from a collateralized chain to a primary\nchain so that the equivocating validator(s) can be slashed.\n\n## Decision\n","We will implement an evidence module in the Cosmos SDK supporting the following\nfunctionality:\n* Provide developers with the abstractions and interfaces necessary to define\ncustom evidence messages, message handlers, and methods to slash and penalize\naccordingly for misbehavior.\n* Support the ability to route evidence messages to handlers in any module to\ndetermine the validity of submitted misbehavior.\n* Support the ability, through governance, to modify slashing penalties of any\nevidence type.\n* Querier implementation to support querying params, evidence types, params, and\nall submitted valid misbehavior.\n### Types\nFirst, we define the `Evidence` interface type. The `x/evidence` module may implement\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\nIn addition, other modules may implement their own `Evidence` types in a similar\nmanner in which governance is extensible. It is important to note any concrete\ntype implementing the `Evidence` interface may include arbitrary fields such as\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\nWhen submitting evidence to the `x/evidence` module, the concrete type must provide\nthe validator's consensus address, which should be known by the `x/slashing`\nmodule (assuming the infraction is valid), the height at which the infraction\noccurred and the validator's power at same height in which the infraction occurred.\n```go\ntype Evidence interface {\nRoute() string\nType() string\nString() string\nHash() HexBytes\nValidateBasic() error\n// The consensus address of the malicious validator at time of infraction\nGetConsensusAddress() ConsAddress\n// Height at which the infraction occurred\nGetHeight() int64\n// The total power of the malicious validator at time of infraction\nGetValidatorPower() int64\n// The total validator set power at time of infraction\nGetTotalPower() int64\n}\n```\n### Routing & Handling\nEach `Evidence` type must map to a specific unique route and be registered with\nthe `x/evidence` module. It accomplishes this through the `Router` implementation.\n```go\ntype Router interface {\nAddRoute(r string, h Handler) Router\nHasRoute(r string) bool\nGetRoute(path string) Handler\nSeal()\n}\n```\nUpon successful routing through the `x/evidence` module, the `Evidence` type\nis passed through a `Handler`. This `Handler` is responsible for executing all\ncorresponding business logic necessary for verifying the evidence as valid. In\naddition, the `Handler` may execute any necessary slashing and potential jailing.\nSince slashing fractions will typically result from some form of static functions,\nallow the `Handler` to do this provides the greatest flexibility. An example could\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\nby governance. The `Evidence` type should provide all the external information\nnecessary in order for the `Handler` to make the necessary state transitions.\nIf no error is returned, the `Evidence` is considered valid.\n```go\ntype Handler func(Context, Evidence) error\n```\n### Submission\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\nhandled by the `x/evidence` module's `SubmitEvidence`.\n```go\ntype MsgSubmitEvidence struct {\nEvidence\n}\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\nreturn err.Result()\n}\n// emit events...\nreturn Result{\n// ...\n}\n}\n```\nThe `x/evidence` module's keeper is responsible for matching the `Evidence` against\nthe module's router and invoking the corresponding `Handler` which may include\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\n```go\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\nhandler := keeper.router.GetRoute(evidence.Route())\nif err := handler(ctx, evidence); err != nil {\nreturn ErrInvalidEvidence(keeper.codespace, err)\n}\nkeeper.setEvidence(ctx, evidence)\nreturn nil\n}\n```\n### Genesis\nFinally, we need to represent the genesis state of the `x/evidence` module. The\nmodule only needs a list of all submitted valid infractions and any necessary params\nfor which the module needs in order to handle submitted evidence. The `x/evidence`\nmodule will naturally define and route native evidence types for which it'll most\nlikely need slashing penalty constants for.\n```go\ntype GenesisState struct {\nParams       Params\nInfractions  []Evidence\n}\n```\n","Implement a fully decentralized slashing protocol, encapsulating the functionality into a separate module and define a corresponding message set and client service interface. The slashing module should provide the following functionality:\n\n- A mechanism for proposing and storing evidence of validator misbehavior.\n- A mechanism for evaluating and verifying evidence, resulting in some agreed upon penalty for any misbehavior committed by a validator.\n- A mechanism for updating the validator set to reflect the penalty (e.g. slashing the validator's stake)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nHaving the ability to sign messages off-chain has proven to be a fundamental aspect of nearly any blockchain. The notion of signing messages off-chain has many added benefits such as saving on computational costs and reducing transaction throughput and overhead. Within the context of the Cosmos, some of the major applications of signing such data includes, but is not limited to, providing a cryptographic secure and verifiable means of proving validator identity and possibly associating it with some other framework or organization. In addition, having the ability to sign Cosmos messages with a Ledger or similar HSM device.\nFurther context and use cases can be found in the references links.\n\n## Decision\n","The aim is being able to sign arbitrary messages, even using Ledger or similar HSM devices.\nAs a result signed messages should look roughly like Cosmos SDK messages but **must not** be a valid on-chain transaction. `chain-id`, `account_number` and `sequence` can all be assigned invalid values.\nCosmos SDK 0.40 also introduces a concept of “auth_info” this can specify SIGN_MODES.\nA spec should include an `auth_info` that supports SIGN_MODE_DIRECT and SIGN_MODE_LEGACY_AMINO.\nCreate the `offchain` proto definitions, we extend the auth module with `offchain` package to offer functionalities to verify and sign offline messages.\nAn offchain transaction follows these rules:\n* the memo must be empty\n* nonce, sequence number must be equal to 0\n* chain-id must be equal to “”\n* fee gas must be equal to 0\n* fee amount must be an empty array\nVerification of an offchain transaction follows the same rules as an onchain one, except for the spec differences highlighted above.\nThe first message added to the `offchain` package is `MsgSignData`.\n`MsgSignData` allows developers to sign arbitrary bytes valid offchain only. Where `Signer` is the account address of the signer. `Data` is arbitrary bytes which can represent `text`, `files`, `object`s. It's applications developers decision how `Data` should be deserialized, serialized and the object it can represent in their context.\nIt's applications developers decision how `Data` should be treated, by treated we mean the serialization and deserialization process and the Object `Data` should represent.\nProto definition:\n```protobuf\n// MsgSignData defines an arbitrary, general-purpose, off-chain message\nmessage MsgSignData {\n// Signer is the sdk.AccAddress of the message signer\nbytes Signer = 1 [(gogoproto.jsontag) = ""signer"", (gogoproto.casttype) = ""github.com/cosmos/cosmos-sdk/types.AccAddress""];\n// Data represents the raw bytes of the content that is signed (text, json, etc)\nbytes Data = 2 [(gogoproto.jsontag) = ""data""];\n}\n```\nSigned MsgSignData json example:\n```json\n{\n""type"": ""cosmos-sdk/StdTx"",\n""value"": {\n""msg"": [\n{\n""type"": ""sign/MsgSignData"",\n""value"": {\n""signer"": ""cosmos1hftz5ugqmpg9243xeegsqqav62f8hnywsjr4xr"",\n""data"": ""cmFuZG9t""\n}\n}\n],\n""fee"": {\n""amount"": [],\n""gas"": ""0""\n},\n""signatures"": [\n{\n""pub_key"": {\n""type"": ""tendermint/PubKeySecp256k1"",\n""value"": ""AqnDSiRoFmTPfq97xxEb2VkQ/Hm28cPsqsZm9jEVsYK9""\n},\n""signature"": ""8y8i34qJakkjse9pOD2De+dnlc4KvFgh0wQpes4eydN66D9kv7cmCEouRrkka9tlW9cAkIL52ErB+6ye7X5aEg==""\n}\n],\n""memo"": """"\n}\n}\n```\n","**Decision:** Implement an off-chain message signing capability.\n\n**Rationale:**\n\n* Off-chain message signing provides significant benefits, including:\n    * Reduced computational costs\n    * Reduced transaction throughput and overhead\n* Key applications within the Cosmos context include:\n    * Cryptographically secure and verifiable validator identity\n    * Association with other frameworks or organizations\n    * Signing messages with Ledger or HSM devices\n\n**Implications:**\n\n* New API endpoints or functionality will need to be developed to support off-chain message signing.\n* Security considerations must be addressed, including key management and authorization.\n* Interoperability with other Cosmos applications and protocols must be ensured.\n\n**Additional Considerations:**\n\n* The specific implementation approach should be carefully considered, balancing security, performance, and usability.\n* The off-chain message signing capability should be well-documented and easily accessible to developers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTelemetry is paramount into debugging and understanding what the application is doing and how it is\nperforming. We aim to expose metrics from modules and other core parts of the Cosmos SDK.\nIn addition, we should aim to support multiple configurable sinks that an operator may choose from.\nBy default, when telemetry is enabled, the application should track and expose metrics that are\nstored in-memory. The operator may choose to enable additional sinks, where we support only\n[Prometheus](https://prometheus.io/) for now, as it's battle-tested, simple to setup, open source,\nand is rich with ecosystem tooling.\nWe must also aim to integrate metrics into the Cosmos SDK in the most seamless way possible such that\nmetrics may be added or removed at will and without much friction. To do this, we will use the\n[go-metrics](https://github.com/hashicorp/go-metrics) library.\nFinally, operators may enable telemetry along with specific configuration options. If enabled, metrics\nwill be exposed via `/metrics?format={text|prometheus}` via the API server.\n\n## Decision\n","We will add an additional configuration block to `app.toml` that defines telemetry settings:\n```toml\n###############################################################################\n###                         Telemetry Configuration                         ###\n###############################################################################\n[telemetry]\n# Prefixed with keys to separate services\nservice-name = {{ .Telemetry.ServiceName }}\n# Enabled enables the application telemetry functionality. When enabled,\n# an in-memory sink is also enabled by default. Operators may also enabled\n# other sinks such as Prometheus.\nenabled = {{ .Telemetry.Enabled }}\n# Enable prefixing gauge values with hostname\nenable-hostname = {{ .Telemetry.EnableHostname }}\n# Enable adding hostname to labels\nenable-hostname-label = {{ .Telemetry.EnableHostnameLabel }}\n# Enable adding service to labels\nenable-service-label = {{ .Telemetry.EnableServiceLabel }}\n# PrometheusRetentionTime, when positive, enables a Prometheus metrics sink.\nprometheus-retention-time = {{ .Telemetry.PrometheusRetentionTime }}\n```\nThe given configuration allows for two sinks -- in-memory and Prometheus. We create a `Metrics`\ntype that performs all the bootstrapping for the operator, so capturing metrics becomes seamless.\n```go\n// Metrics defines a wrapper around application telemetry functionality. It allows\n// metrics to be gathered at any point in time. When creating a Metrics object,\n// internally, a global metrics is registered with a set of sinks as configured\n// by the operator. In addition to the sinks, when a process gets a SIGUSR1, a\n// dump of formatted recent metrics will be sent to STDERR.\ntype Metrics struct {\nmemSink           *metrics.InmemSink\nprometheusEnabled bool\n}\n// Gather collects all registered metrics and returns a GatherResponse where the\n// metrics are encoded depending on the type. Metrics are either encoded via\n// Prometheus or JSON if in-memory.\nfunc (m *Metrics) Gather(format string) (GatherResponse, error) {\nswitch format {\ncase FormatPrometheus:\nreturn m.gatherPrometheus()\ncase FormatText:\nreturn m.gatherGeneric()\ncase FormatDefault:\nreturn m.gatherGeneric()\ndefault:\nreturn GatherResponse{}, fmt.Errorf(""unsupported metrics format: %s"", format)\n}\n}\n```\nIn addition, `Metrics` allows us to gather the current set of metrics at any given point in time. An\noperator may also choose to send a signal, SIGUSR1, to dump and print formatted metrics to STDERR.\nDuring an application's bootstrapping and construction phase, if `Telemetry.Enabled` is `true`, the\nAPI server will create an instance of a reference to `Metrics` object and will register a metrics\nhandler accordingly.\n```go\nfunc (s *Server) Start(cfg config.Config) error {\n// ...\nif cfg.Telemetry.Enabled {\nm, err := telemetry.New(cfg.Telemetry)\nif err != nil {\nreturn err\n}\ns.metrics = m\ns.registerMetrics()\n}\n// ...\n}\nfunc (s *Server) registerMetrics() {\nmetricsHandler := func(w http.ResponseWriter, r *http.Request) {\nformat := strings.TrimSpace(r.FormValue(""format""))\ngr, err := s.metrics.Gather(format)\nif err != nil {\nrest.WriteErrorResponse(w, http.StatusBadRequest, fmt.Sprintf(""failed to gather metrics: %s"", err))\nreturn\n}\nw.Header().Set(""Content-Type"", gr.ContentType)\n_, _ = w.Write(gr.Metrics)\n}\ns.Router.HandleFunc(""/metrics"", metricsHandler).Methods(""GET"")\n}\n```\nApplication developers may track counters, gauges, summaries, and key/value metrics. There is no\nadditional lifting required by modules to leverage profiling metrics. To do so, it's as simple as:\n```go\nfunc (k BaseKeeper) MintCoins(ctx sdk.Context, moduleName string, amt sdk.Coins) error {\ndefer metrics.MeasureSince(time.Now(), ""MintCoins"")\n// ...\n}\n```\n","Enable telemetry by default, exposing metrics in-memory. Support multiple configurable sinks, including Prometheus. Use the go-metrics library for seamless metrics integration within the Cosmos SDK. Allow operators to enable telemetry and configure it through specific configuration options. Expose metrics via `/metrics?format={text|prometheus}` if telemetry is enabled."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\nIdeally, we would have:\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\n\n## Decision\n","Re-structure the `/docs` folder of the Cosmos SDK github repo as follows:\n```text\ndocs/\n├── README\n├── intro/\n├── concepts/\n│   ├── baseapp\n│   ├── types\n│   ├── store\n│   ├── server\n│   ├── modules/\n│   │   ├── keeper\n│   │   ├── handler\n│   │   ├── cli\n│   ├── gas\n│   └── commands\n├── clients/\n│   ├── lite/\n│   ├── service-providers\n├── modules/\n├── spec/\n├── translations/\n└── architecture/\n```\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\n* `README`: Landing page of the docs.\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https://github.com/cosmos/sdk-application-tutorial/) will be highlighted, as well as the `godocs`.\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\n* `spec`: Contains specs of modules, and others.\n* `modules`: Contains links to `godocs` and the spec of the modules.\n* `architecture`: Contains architecture-related docs like the present one.\n* `translations`: Contains different translations of the documentation.\nWebsite docs sidebar will only include the following sections:\n* `README`\n* `intro`\n* `concepts`\n* `clients`\n`architecture` need not be displayed on the website.\n","All documentation related to development frameworks or tools should be moved to their respective GitHub repositories. This includes the SDK repository, the Hub repository, the Lotion repository, and so on. All other documentation, such as FAQs, whitepapers, and high-level material about Cosmos, should be moved to the Cosmos website."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFull implementation of the [IBC specification](https://github.com/cosmos/ibc) requires the ability to create and authenticate object-capability keys at runtime (i.e., during transaction execution),\nas described in [ICS 5](https://github.com/cosmos/ibc/tree/master/spec/core/ics-005-port-allocation#technical-specification). In the IBC specification, capability keys are created for each newly initialised\nport & channel, and are used to authenticate future usage of the port or channel. Since channels and potentially ports can be initialised during transaction execution, the state machine must be able to create\nobject-capability keys at this time.\nAt present, the Cosmos SDK does not have the ability to do this. Object-capability keys are currently pointers (memory addresses) of `StoreKey` structs created at application initialisation in `app.go` ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L132))\nand passed to Keepers as fixed arguments ([example](https://github.com/cosmos/gaia/blob/dcbddd9f04b3086c0ad07ee65de16e7adedc7da4/app/app.go#L160)). Keepers cannot create or store capability keys during transaction execution — although they could call `NewKVStoreKey` and take the memory address\nof the returned struct, storing this in the Merklised store would result in a consensus fault, since the memory address will be different on each machine (this is intentional — were this not the case, the keys would be predictable and couldn't serve as object capabilities).\nKeepers need a way to keep a private map of store keys which can be altered during transaction execution, along with a suitable mechanism for regenerating the unique memory addresses (capability keys) in this map whenever the application is started or restarted, along with a mechanism to revert capability creation on tx failure.\nThis ADR proposes such an interface & mechanism.\n\n## Decision\n","The Cosmos SDK will include a new `CapabilityKeeper` abstraction, which is responsible for provisioning,\ntracking, and authenticating capabilities at runtime. During application initialisation in `app.go`,\nthe `CapabilityKeeper` will be hooked up to modules through unique function references\n(by calling `ScopeToModule`, defined below) so that it can identify the calling module when later\ninvoked.\nWhen the initial state is loaded from disk, the `CapabilityKeeper`'s `Initialise` function will create\nnew capability keys for all previously allocated capability identifiers (allocated during execution of\npast transactions and assigned to particular modes), and keep them in a memory-only store while the\nchain is running.\nThe `CapabilityKeeper` will include a persistent `KVStore`, a `MemoryStore`, and an in-memory map.\nThe persistent `KVStore` tracks which capability is owned by which modules.\nThe `MemoryStore` stores a forward mapping that map from module name, capability tuples to capability names and\na reverse mapping that map from module name, capability name to the capability index.\nSince we cannot marshal the capability into a `KVStore` and unmarshal without changing the memory location of the capability,\nthe reverse mapping in the KVStore will simply map to an index. This index can then be used as a key in the ephemeral\ngo-map to retrieve the capability at the original memory location.\nThe `CapabilityKeeper` will define the following types & functions:\nThe `Capability` is similar to `StoreKey`, but has a globally unique `Index()` instead of\na name. A `String()` method is provided for debugging.\nA `Capability` is simply a struct, the address of which is taken for the actual capability.\n```go\ntype Capability struct {\nindex uint64\n}\n```\nA `CapabilityKeeper` contains a persistent store key, memory store key, and mapping of allocated module names.\n```go\ntype CapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleNames   map[string]interface{}\nsealed        bool\n}\n```\nThe `CapabilityKeeper` provides the ability to create *scoped* sub-keepers which are tied to a\nparticular module name. These `ScopedCapabilityKeeper`s must be created at application initialisation\nand passed to modules, which can then use them to claim capabilities they receive and retrieve\ncapabilities which they own by name, in addition to creating new capabilities & authenticating capabilities\npassed by other modules.\n```go\ntype ScopedCapabilityKeeper struct {\npersistentKey StoreKey\nmemKey        StoreKey\ncapMap        map[uint64]*Capability\nmoduleName    string\n}\n```\n`ScopeToModule` is used to create a scoped sub-keeper with a particular name, which must be unique.\nIt MUST be called before `InitialiseAndSeal`.\n```go\nfunc (ck CapabilityKeeper) ScopeToModule(moduleName string) ScopedCapabilityKeeper {\nif ck.sealed {\npanic(""cannot scope to module via a sealed capability keeper"")\n}\nif _, ok := ck.scopedModules[moduleName]; ok {\npanic(fmt.Sprintf(""cannot create multiple scoped keepers for the same module name: %s"", moduleName))\n}\nck.scopedModules[moduleName] = struct{}{}\nreturn ScopedKeeper{\ncdc:      ck.cdc,\nstoreKey: ck.storeKey,\nmemKey:   ck.memKey,\ncapMap:   ck.capMap,\nmodule:   moduleName,\n}\n}\n```\n`InitialiseAndSeal` MUST be called exactly once, after loading the initial state and creating all\nnecessary `ScopedCapabilityKeeper`s, in order to populate the memory store with newly-created\ncapability keys in accordance with the keys previously claimed by particular modules and prevent the\ncreation of any new `ScopedCapabilityKeeper`s.\n```go\nfunc (ck CapabilityKeeper) InitialiseAndSeal(ctx Context) {\nif ck.sealed {\npanic(""capability keeper is sealed"")\n}\npersistentStore := ctx.KVStore(ck.persistentKey)\nmap := ctx.KVStore(ck.memKey)\n// initialise memory store for all names in persistent store\nfor index, value := range persistentStore.Iter() {\ncapability = &CapabilityKey{index: index}\nfor moduleAndCapability := range value {\nmoduleName, capabilityName := moduleAndCapability.Split(""/"")\nmemStore.Set(moduleName + ""/fwd/"" + capability, capabilityName)\nmemStore.Set(moduleName + ""/rev/"" + capabilityName, index)\nck.capMap[index] = capability\n}\n}\nck.sealed = true\n}\n```\n`NewCapability` can be called by any module to create a new unique, unforgeable object-capability\nreference. The newly created capability is automatically persisted; the calling module need not\ncall `ClaimCapability`.\n```go\nfunc (sck ScopedCapabilityKeeper) NewCapability(ctx Context, name string) (Capability, error) {\n// check name not taken in memory store\nif capStore.Get(""rev/"" + name) != nil {\nreturn nil, errors.New(""name already taken"")\n}\n// fetch the current index\nindex := persistentStore.Get(""index"")\n// create a new capability\ncapability := &CapabilityKey{index: index}\n// set persistent store\npersistentStore.Set(index, Set.singleton(sck.moduleName + ""/"" + name))\n// update the index\nindex++\npersistentStore.Set(""index"", index)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to index\nmemStore.Set(sck.moduleName + ""/rev/"" + name, index)\n// set the in-memory mapping from index to capability pointer\ncapMap[index] = capability\n// return the newly created capability\nreturn capability\n}\n```\n`AuthenticateCapability` can be called by any module to check that a capability\ndoes in fact correspond to a particular name (the name can be untrusted user input)\nwith which the calling module previously associated it.\n```go\nfunc (sck ScopedCapabilityKeeper) AuthenticateCapability(name string, capability Capability) bool {\n// return whether forward mapping in memory store matches name\nreturn memStore.Get(sck.moduleName + ""/fwd/"" + capability) === name\n}\n```\n`ClaimCapability` allows a module to claim a capability key which it has received from another module\nso that future `GetCapability` calls will succeed.\n`ClaimCapability` MUST be called if a module which receives a capability wishes to access it by name\nin the future. Capabilities are multi-owner, so if multiple modules have a single `Capability` reference,\nthey will all own it.\n```go\nfunc (sck ScopedCapabilityKeeper) ClaimCapability(ctx Context, capability Capability, name string) error {\npersistentStore := ctx.KVStore(sck.persistentKey)\n// set forward mapping in memory store from capability to name\nmemStore.Set(sck.moduleName + ""/fwd/"" + capability, name)\n// set reverse mapping in memory store from name to capability\nmemStore.Set(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.add(sck.moduleName + ""/"" + name)\npersistentStore.Set(capability.Index(), owners)\n}\n```\n`GetCapability` allows a module to fetch a capability which it has previously claimed by name.\nThe module is not allowed to retrieve capabilities which it does not own.\n```go\nfunc (sck ScopedCapabilityKeeper) GetCapability(ctx Context, name string) (Capability, error) {\n// fetch the index of capability using reverse mapping in memstore\nindex := memStore.Get(sck.moduleName + ""/rev/"" + name)\n// fetch capability from go-map using index\ncapability := capMap[index]\n// return the capability\nreturn capability\n}\n```\n`ReleaseCapability` allows a module to release a capability which it had previously claimed. If no\nmore owners exist, the capability will be deleted globally.\n```go\nfunc (sck ScopedCapabilityKeeper) ReleaseCapability(ctx Context, capability Capability) err {\npersistentStore := ctx.KVStore(sck.persistentKey)\nname := capStore.Get(sck.moduleName + ""/fwd/"" + capability)\nif name == nil {\nreturn error(""capability not owned by module"")\n}\n// delete forward mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/fwd/"" + capability, name)\n// delete reverse mapping in memory store\nmemoryStore.Delete(sck.moduleName + ""/rev/"" + name, capability)\n// update owner set in persistent store\nowners := persistentStore.Get(capability.Index())\nowners.remove(sck.moduleName + ""/"" + name)\nif owners.size() > 0 {\n// there are still other owners, keep the capability around\npersistentStore.Set(capability.Index(), owners)\n} else {\n// no more owners, delete the capability\npersistentStore.Delete(capability.Index())\ndelete(capMap[capability.Index()])\n}\n}\n```\n### Usage patterns\n#### Initialisation\nAny modules which use dynamic capabilities must be provided a `ScopedCapabilityKeeper` in `app.go`:\n```go\nck := NewCapabilityKeeper(persistentKey, memoryKey)\nmod1Keeper := NewMod1Keeper(ck.ScopeToModule(""mod1""), ....)\nmod2Keeper := NewMod2Keeper(ck.ScopeToModule(""mod2""), ....)\n// other initialisation logic ...\n// load initial state...\nck.InitialiseAndSeal(initialContext)\n```\n#### Creating, passing, claiming and using capabilities\nConsider the case where `mod1` wants to create a capability, associate it with a resource (e.g. an IBC channel) by name, then pass it to `mod2` which will use it later:\nModule 1 would have the following code:\n```go\ncapability := scopedCapabilityKeeper.NewCapability(ctx, ""resourceABC"")\nmod2Keeper.SomeFunction(ctx, capability, args...)\n```\n`SomeFunction`, running in module 2, could then claim the capability:\n```go\nfunc (k Mod2Keeper) SomeFunction(ctx Context, capability Capability) {\nk.sck.ClaimCapability(ctx, capability, ""resourceABC"")\n// other logic...\n}\n```\nLater on, module 2 can retrieve that capability by name and pass it to module 1, which will authenticate it against the resource:\n```go\nfunc (k Mod2Keeper) SomeOtherFunction(ctx Context, name string) {\ncapability := k.sck.GetCapability(ctx, name)\nmod1.UseResource(ctx, capability, ""resourceABC"")\n}\n```\nModule 1 will then check that this capability key is authenticated to use the resource before allowing module 2 to use it:\n```go\nfunc (k Mod1Keeper) UseResource(ctx Context, capability Capability, resource string) {\nif !k.sck.AuthenticateCapability(name, capability) {\nreturn errors.New(""unauthenticated"")\n}\n// do something with the resource\n}\n```\nIf module 2 passed the capability key to module 3, module 3 could then claim it and call module 1 just like module 2 did\n(in which case module 1, module 2, and module 3 would all be able to use this capability).\n","**Decision:** Implement an interface and mechanism for Keepers to create and store object-capability keys during transaction execution. This interface should include methods for creating new keys, storing keys, and regenerating keys on application start or restart. The mechanism should be able to revert capability creation on tx failure."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](./adr-019-protobuf-state-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nSpecifically, the client-side migration path primarily includes tx generation and\nsigning, message construction and routing, in addition to CLI & REST handlers and\nbusiness logic (i.e. queriers).\nWith this in mind, we will tackle the migration path via two main areas, txs and\nquerying. However, this ADR solely focuses on transactions. Querying should be\naddressed in a future ADR, but it should build off of these proposals.\nBased on detailed discussions ([\#6030](https://github.com/cosmos/cosmos-sdk/issues/6030)\nand [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078)), the original\ndesign for transactions was changed substantially from an `oneof` /JSON-signing\napproach to the approach described below.\n\n## Decision\n","### Transactions\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\n`sdk.Msg`s are encoding with `Any` in transactions.\nOne of the main goals of using `Any` to encode interface values is to have a\ncore set of types which is reused by apps so that\nclients can safely be compatible with as many chains as possible.\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\nformat that can serve a wide variety of use cases without breaking client\ncompatibility.\nIn order to facilitate signing, transactions are separated into `TxBody`,\nwhich will be re-used by `SignDoc` below, and `signatures`:\n```protobuf\n// types/types.proto\npackage cosmos_sdk.v1;\nmessage Tx {\nTxBody body = 1;\nAuthInfo auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\n// A variant of Tx that pins the signer's exact binary representation of body and\n// auth_info. This is used for signing, broadcasting and verification. The binary\n// `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\n// becomes the ""txhash"", commonly used as the transaction ID.\nmessage TxRaw {\n// A protobuf serialization of a TxBody that matches the representation in SignDoc.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\nbytes auth_info = 2;\n// A list of signatures that matches the length and order of AuthInfo's signer_infos to\n// allow connecting signature meta information like public key and signing mode by position.\nrepeated bytes signatures = 3;\n}\nmessage TxBody {\n// A list of messages to be executed. The required signers of those messages define\n// the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\n// Each required signer address is added to the list only the first time it occurs.\n//\n// By convention, the first required signer (usually from the first message) is referred\n// to as the primary signer and pays the fee for the whole transaction.\nrepeated google.protobuf.Any messages = 1;\nstring memo = 2;\nint64 timeout_height = 3;\nrepeated google.protobuf.Any extension_options = 1023;\n}\nmessage AuthInfo {\n// This list defines the signing modes for the required signers. The number\n// and order of elements must match the required signers from TxBody's messages.\n// The first element is the primary signer and the one which pays the fee.\nrepeated SignerInfo signer_infos = 1;\n// The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\nFee fee = 2;\n}\nmessage SignerInfo {\n// The public key is optional for accounts that already exist in state. If unset, the\n// verifier can use the required signer address for this position and lookup the public key.\ngoogle.protobuf.Any public_key = 1;\n// ModeInfo describes the signing mode of the signer and is a nested\n// structure to support nested multisig pubkey's\nModeInfo mode_info = 2;\n// sequence is the sequence of the account, which describes the\n// number of committed transactions signed by a given address. It is used to prevent\n// replay attacks.\nuint64 sequence = 3;\n}\nmessage ModeInfo {\noneof sum {\nSingle single = 1;\nMulti multi = 2;\n}\n// Single is the mode info for a single signer. It is structured as a message\n// to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\nmessage Single {\nSignMode mode = 1;\n}\n// Multi is the mode info for a multisig public key\nmessage Multi {\n// bitarray specifies which keys within the multisig are signing\nCompactBitArray bitarray = 1;\n// mode_infos is the corresponding modes of the signers of the multisig\n// which could include nested multisig public keys\nrepeated ModeInfo mode_infos = 2;\n}\n}\nenum SignMode {\nSIGN_MODE_UNSPECIFIED = 0;\nSIGN_MODE_DIRECT = 1;\nSIGN_MODE_TEXTUAL = 2;\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\n}\n```\nAs will be discussed below, in order to include as much of the `Tx` as possible\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\nraw signatures themselves live outside of what is signed over.\nBecause we are aiming for a flexible, extensible cross-chain transaction\nformat, new transaction processing options should be added to `TxBody` as soon\nthose use cases are discovered, even if they can't be implemented yet.\nBecause there is coordination overhead in this, `TxBody` includes an\n`extension_options` field which can be used for any transaction processing\noptions that are not already covered. App developers should, nevertheless,\nattempt to upstream important improvements to `Tx`.\n### Signing\nAll of the signing modes below aim to provide the following guarantees:\n* **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\nis signed\n* **Predictable Gas**: if I am signing a transaction where I am paying a fee,\nthe final gas is fully dependent on what I am signing\nThese guarantees give the maximum amount confidence to message signers that\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\n#### `SIGN_MODE_DIRECT`\nThe ""direct"" signing behavior is to sign the raw `TxBody` bytes as broadcast over\nthe wire. This has the advantages of:\n* requiring the minimum additional client capabilities beyond a standard protocol\nbuffers implementation\n* leaving effectively zero holes for transaction malleability (i.e. there are no\nsubtle differences between the signing and encoding formats which could\npotentially be exploited by an attacker)\nSignatures are structured using the `SignDoc` below which reuses the serialization of\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\n```protobuf\n// types/types.proto\nmessage SignDoc {\n// A protobuf serialization of a TxBody that matches the representation in TxRaw.\nbytes body = 1;\n// A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\nbytes auth_info = 2;\nstring chain_id = 3;\nuint64 account_number = 4;\n}\n```\nIn order to sign in the default mode, clients take the following steps:\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\n2. Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n3. Sign the encoded `SignDoc` bytes.\n4. Build a `TxRaw` and serialize it for broadcasting.\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\nbytes encoded in `TxRaw` not based on any [""canonicalization""](https://github.com/regen-network/canonical-proto3)\nalgorithm which creates added complexity for clients in addition to preventing\nsome forms of upgradeability (to be addressed later in this document).\nSignature verifiers do:\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\n2. Create a list of required signer addresses from the messages.\n3. For each required signer:\n* Pull account number and sequence from the state.\n* Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\n* Create a `SignDoc` and serialize it using [ADR 027](./adr-027-deterministic-protobuf-serialization.md).\n* Verify the signature at the same list position against the serialized `SignDoc`.\n#### `SIGN_MODE_LEGACY_AMINO`\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\nsupported transaction signing. Once wallets and exchanges have had a\nchance to upgrade to protobuf based signing, this option will be disabled. In\nthe meantime, it is foreseen that disabling the current Amino signing would cause\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\nCosmos Hub and other chains may choose to disable Amino signing immediately.\nLegacy clients will be able to sign a transaction using the current Amino\nJSON format and have it encoded to protobuf using the REST `/tx/encode`\nendpoint before broadcasting.\n#### `SIGN_MODE_TEXTUAL`\nAs was discussed extensively in [\#6078](https://github.com/cosmos/cosmos-sdk/issues/6078),\nthere is a desire for a human-readable signing encoding, especially for hardware\nwallets like the [Ledger](https://www.ledger.com) which display\ntransaction contents to users before signing. JSON was an attempt at this but\nfalls short of the ideal.\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\nencoding which will replace Amino JSON. This new encoding should be even more\nfocused on readability than JSON, possibly based on formatting strings like\n[MessageFormat](http://userguide.icu-project.org/formatparse/messages).\nIn order to ensure that the new human-readable format does not suffer from\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\nto generate sign bytes.\nMultiple human-readable formats (maybe even localized messages) may be supported\nby `SIGN_MODE_TEXTUAL` when it is implemented.\n### Unknown Field Filtering\nUnknown fields in protobuf messages should generally be rejected by transaction\nprocessors because:\n* important data may be present in the unknown fields, that if ignored, will\ncause unexpected behavior for clients\n* they present a malleability vulnerability where attackers can bloat tx size\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\nnot `TxBody`)\nThere are also scenarios where we may choose to safely ignore unknown fields\n(https://github.com/cosmos/cosmos-sdk/issues/6078#issuecomment-624400188) to\nprovide graceful forwards compatibility with newer clients.\nWe propose that field numbers with bit 11 set (for most use cases this is\nthe range of 1024-2047) be considered non-critical fields that can safely be\nignored if unknown.\nTo handle this we will need an unknown field filter that:\n* always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\nunsigned parts of `AuthInfo` if present based on the signing mode)\n* rejects unknown fields in all messages (including nested `Any`s) other than\nfields with bit 11 set\nThis will likely need to be a custom protobuf parser pass that takes message bytes\nand `FileDescriptor`s and returns a boolean result.\n### Public Key Encoding\nPublic keys in the Cosmos SDK implement the `cryptotypes.PubKey` interface.\nWe propose to use `Any` for protobuf encoding as we are doing with other interfaces (for example, in `BaseAccount.PubKey` and `SignerInfo.PublicKey`).\nThe following public keys are implemented: secp256k1, secp256r1, ed25519 and legacy-multisignature.\nEx:\n```protobuf\nmessage PubKey {\nbytes key = 1;\n}\n```\n`multisig.LegacyAminoPubKey` has an array of `Any`'s member to support any\nprotobuf public key type.\nApps should only attempt to handle a registered set of public keys that they\nhave tested. The provided signature verification ante handler decorators will\nenforce this.\n### CLI & REST\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\nin the client can be interfaces, similar to how we described in [ADR 019](./adr-019-protobuf-state-encoding.md),\nthe client logic will now need to take a codec interface that knows not only how\nto handle all the types, but also knows how to generate transactions, signatures,\nand messages.\nIf the account is sending its first transaction, the account number must be set to 0. This is due to the account not being created yet.\n```go\ntype AccountRetriever interface {\nGetAccount(clientCtx Context, addr sdk.AccAddress) (client.Account, error)\nGetAccountWithHeight(clientCtx Context, addr sdk.AccAddress) (client.Account, int64, error)\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\n}\ntype Generator interface {\nNewTx() TxBuilder\nNewFee() ClientFee\nNewSignature() ClientSignature\nMarshalTx(tx types.Tx) ([]byte, error)\n}\ntype TxBuilder interface {\nGetTx() sdk.Tx\nSetMsgs(...sdk.Msg) error\nGetSignatures() []sdk.Signature\nSetSignatures(...sdk.Signature)\nGetFee() sdk.Fee\nSetFee(sdk.Fee)\nGetMemo() string\nSetMemo(string)\n}\n```\nWe then update `Context` to have new fields: `Codec`, `TxGenerator`,\nand `AccountRetriever`, and we update `AppModule.GetTxCmd` to take\na `Context` which should have all of these fields pre-populated.\nEach client method should then use one of the `Init` methods to re-initialize\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\ngenerate or broadcast a transaction. For example:\n```go\nimport ""github.com/spf13/cobra""\nimport ""github.com/cosmos/cosmos-sdk/client""\nimport ""github.com/cosmos/cosmos-sdk/client/tx""\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\nreturn &cobra.Command{\nRunE: func(cmd *cobra.Command, args []string) error {\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\nmsg := NewSomeMsg{...}\ntx.GenerateOrBroadcastTx(clientCtx, msg)\n},\n}\n}\n```\n",The decision for this ADR is not provided in the given context.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, in the Cosmos SDK, the address of an auth `BaseAccount` is based on the hash of the public key.  Once an account is created, the public key for the account is set in stone, and cannot be changed.  This can be a problem for users, as key rotation is a useful security practice, but is not possible currently.  Furthermore, as multisigs are a type of pubkey, once a multisig for an account is set, it can not be updated.  This is problematic, as multisigs are often used by organizations or companies, who may need to change their set of multisig signers for internal reasons.\nTransferring all the assets of an account to a new account with the updated pubkey is not sufficient, because some ""engagements"" of an account are not easily transferable.  For example, in staking, to transfer bonded Atoms, an account would have to unbond all delegations and wait the three week unbonding period.  Even more significantly, for validator operators, ownership over a validator is not transferrable at all, meaning that the operator key for a validator can never be updated, leading to poor operational security for validators.\n\n## Decision\n","We propose the addition of a new feature to `x/auth` that allows accounts to update the public key associated with their account, while keeping the address the same.\nThis is possible because the Cosmos SDK `BaseAccount` stores the public key for an account in state, instead of making the assumption that the public key is included in the transaction (whether explicitly or implicitly through the signature) as in other blockchains such as Bitcoin and Ethereum.  Because the public key is stored on chain, it is okay for the public key to not hash to the address of an account, as the address is not pertinent to the signature checking process.\nTo build this system, we design a new Msg type as follows:\n```protobuf\nservice Msg {\nrpc ChangePubKey(MsgChangePubKey) returns (MsgChangePubKeyResponse);\n}\nmessage MsgChangePubKey {\nstring address = 1;\ngoogle.protobuf.Any pub_key = 2;\n}\nmessage MsgChangePubKeyResponse {}\n```\nThe MsgChangePubKey transaction needs to be signed by the existing pubkey in state.\nOnce, approved, the handler for this message type, which takes in the AccountKeeper, will update the in-state pubkey for the account and replace it with the pubkey from the Msg.\nAn account that has had its pubkey changed cannot be automatically pruned from state.  This is because if pruned, the original pubkey of the account would be needed to recreate the same address, but the owner of the address may not have the original pubkey anymore.  Currently, we do not automatically prune any accounts anyways, but we would like to keep this option open the road (this is the purpose of account numbers).  To resolve this, we charge an additional gas fee for this operation to compensate for this this externality (this bound gas amount is configured as parameter `PubKeyChangeCost`). The bonus gas is charged inside the handler, using the `ConsumeGas` function.  Furthermore, in the future, we can allow accounts that have rekeyed manually prune themselves using a new Msg type such as `MsgDeleteAccount`.  Manually pruning accounts can give a gas refund as an incentive for performing the action.\n```go\namount := ak.GetParams(ctx).PubKeyChangeCost\nctx.GasMeter().ConsumeGas(amount, ""pubkey change fee"")\n```\nEvery time a key for an address is changed, we will store a log of this change in the state of the chain, thus creating a stack of all previous keys for an address and the time intervals for which they were active.  This allows dapps and clients to easily query past keys for an account which may be useful for features such as verifying timestamped off-chain signed messages.\n","Do not implement key rotation in BaseAccount. Instead, introduce a concept of the 'primary account' and 'subaccounts' that share a common root key."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion/consideration.\n\n## Decision\n","We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\nThus, we propose the following mechanism:\n### Params\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\n* There is a new gov param called `MaxVotingPeriodExtension`.\n### Mechanism\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the conentiousness/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\n### Dealing with Unbonding/Redelegation\nThere is one thing that needs to be addressed.  How to deal with redelegation/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\n### Delegators\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and ""unextend"" as that would contradict the ""voting power length can only be ratcheted up"" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\n",Extend the proposal submission form to allow for different voting periods on a proposal-by-proposal basis.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order for the Cosmos SDK to implement the [IBC specification](https://github.com/cosmos/ics), modules within the Cosmos SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\n\n## Decision\n","The application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\n```go\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper) error {\ninfo := HistoricalInfo{\napphash: ctx.HeaderInfo().AppHash,\nTime: ctx.HeaderInfo().Time,\nNextValidatorsHash: ctx.CometInfo().NextValidatorsHash,\n}\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\nn := keeper.GetParamRecentHeadersToStore()\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\n// continue handling request\n}\n```\nAlternatively, the application MAY store only the hash of the validator set.\nThe application MUST make these past `n` committed headers available for querying by Cosmos SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x/staking` or `x/ibc`).\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\n",The Cosmos SDK should leverage the Tendermint gRPC APIs to retrieve consensus states from the Tendermint core.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the Cosmos SDK utilizes [go-amino](https://github.com/tendermint/go-amino/) for binary\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\nFrom the Amino docs:\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\n> support. See the [Proto3 spec](https://developers.google.com/protocol-buffers/docs/proto3) for more\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\n>\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\nAmino also aims to have the following goals (not a complete list):\n* Binary bytes must be decode-able with a schema.\n* Schema must be upgradeable.\n* The encoder and decoder logic must be reasonably simple.\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\nclients written in various languages while providing virtually little in the way of true backwards\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1</sup>. This is\nlargely reflected in the performance of simulations and application transaction throughput.\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\n* Language agnostic\n* Platform agnostic\n* Rich client support and thriving ecosystem\n* High performance\n* Minimal encoded message size\n* Codegen-based over reflection-based\n* Supports backward and forward compatibility\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\nmade to address client-side encoding.\n\n## Decision\n","We will adopt [Protocol Buffers](https://developers.google.com/protocol-buffers) for serializing\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\nwill provide two concrete implementations of the `Marshaler` interface: `AminoCodec` and `ProtoCodec`.\n* `AminoCodec`: Uses Amino for both binary and JSON encoding.\n* `ProtoCodec`: Uses Protobuf for both binary and JSON encoding.\nModules will use whichever codec that is instantiated in the app. By default, the Cosmos SDK's `simapp`\ninstantiates a `ProtoCodec` as the concrete implementation of `Marshaler`, inside the `MakeTestEncodingConfig`\nfunction. This can be easily overwritten by app developers if they so desire.\nThe ultimate goal will be to replace Amino JSON encoding with Protobuf encoding and thus have\nmodules accept and/or extend `ProtoCodec`. Until then, Amino JSON is still provided for legacy use-cases.\nA handful of places in the Cosmos SDK still have Amino JSON hardcoded, such as the Legacy API REST endpoints\nand the `x/params` store. They are planned to be converted to Protobuf in a gradual manner.\n### Module Codecs\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\n`Marshaler` that will be a `ProtoCodec`. This migration is simple as things will just work as-is.\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\n[gogoprotobuf](https://github.com/cosmos/gogoproto) Value types.\nExample:\n```go\nts, err := gogotypes.TimestampProto(completionTime)\nif err != nil {\n// ...\n}\nbz := cdc.MustMarshal(ts)\n```\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\nto the module and will contain method contracts that know how to serialize the needed interfaces.\nExample:\n```go\n// x/auth/types/codec.go\ntype Codec interface {\ncodec.Codec\nMarshalAccount(acc exported.Account) ([]byte, error)\nUnmarshalAccount(bz []byte) (exported.Account, error)\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\n}\n```\n### Usage of `Any` to encode interfaces\nIn general, module-level .proto files should define messages which encode interfaces\nusing [`google.protobuf.Any`](https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/any.proto).\nAfter [extension discussion](https://github.com/cosmos/cosmos-sdk/issues/6030),\nthis was chosen as the preferred alternative to application-level `oneof`s\nas in our original protobuf design. The arguments in favor of `Any` can be\nsummarized as follows:\n* `Any` provides a simpler, more consistent client UX for dealing with\ninterfaces than app-level `oneof`s that will need to be coordinated more\ncarefully across applications. Creating a generic transaction\nsigning library using `oneof`s may be cumbersome and critical logic may need\nto be reimplemented for each chain\n* `Any` provides more resistance against human error than `oneof`\n* `Any` is generally simpler to implement for both modules and apps\nThe main counter-argument to using `Any` centers around its additional space\nand possibly performance overhead. The space overhead could be dealt with using\ncompression at the persistence layer in the future and the performance impact\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\nwith user experience as the higher order concern.\nNote, that given the Cosmos SDK's decision to adopt the `Codec` interfaces described\nabove, apps can still choose to use `oneof` to encode state and transactions\nbut it is not the recommended approach. If apps do choose to use `oneof`s\ninstead of `Any` they will likely lose compatibility with client apps that\nsupport multiple chains. Thus developers should think carefully about whether\nthey care more about what is possibly a pre-mature optimization or end-user\nand client developer UX.\n### Safe usage of `Any`\nBy default, the [gogo protobuf implementation of `Any`](https://pkg.go.dev/github.com/cosmos/gogoproto/types)\nuses [global type registration]( https://github.com/cosmos/gogoproto/blob/master/proto/properties.go#L540)\nto decode values packed in `Any` into concrete\ngo types. This introduces a vulnerability where any malicious module\nin the dependency tree could register a type with the global protobuf registry\nand cause it to be loaded and unmarshaled by a transaction that referenced\nit in the `type_url` field.\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\nvalues into concrete types through the `InterfaceRegistry` interface which\nbears some similarity to type registration with Amino:\n```go\ntype InterfaceRegistry interface {\n// RegisterInterface associates protoName as the public name for the\n// interface passed in as iface\n// Ex:\n//   registry.RegisterInterface(""cosmos_sdk.Msg"", (*sdk.Msg)(nil))\nRegisterInterface(protoName string, iface interface{})\n// RegisterImplementations registers impls as a concrete implementations of\n// the interface iface\n// Ex:\n//  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\nRegisterImplementations(iface interface{}, impls ...proto.Message)\n}\n```\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\nto communicate the list of concrete types that satisfy an interface to clients.\nIn .proto files:\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\nmay be used via code generation, reflection &/or static linting.\nThe same struct that implements `InterfaceRegistry` will also implement an\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\n```go\ntype InterfaceUnpacker interface {\n// UnpackAny unpacks the value in any to the interface pointer passed in as\n// iface. Note that the type in any must have been registered with\n// RegisterImplementations as a concrete type for that interface\n// Ex:\n//    var msg sdk.Msg\n//    err := ctx.UnpackAny(any, &msg)\n//    ...\nUnpackAny(any *Any, iface interface{}) error\n}\n```\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\nusage of `Any`, it just introduces a security and introspection layer for\ngolang usage.\n`InterfaceRegistry` will be a member of `ProtoCodec`\ndescribed above. In order for modules to register interface types, app modules\ncan optionally implement the following interface:\n```go\ntype InterfaceModule interface {\nRegisterInterfaceTypes(InterfaceRegistry)\n}\n```\nThe module manager will include a method to call `RegisterInterfaceTypes` on\nevery module that implements it in order to populate the `InterfaceRegistry`.\n### Using `Any` to encode state\nThe Cosmos SDK will provide support methods `MarshalInterface` and `UnmarshalInterface` to hide a complexity of wrapping interface types into `Any` and allow easy serialization.\n```go\nimport ""github.com/cosmos/cosmos-sdk/codec""\n// note: eviexported.Evidence is an interface type\nfunc MarshalEvidence(cdc codec.BinaryCodec, e eviexported.Evidence) ([]byte, error) {\nreturn cdc.MarshalInterface(e)\n}\nfunc UnmarshalEvidence(cdc codec.BinaryCodec, bz []byte) (eviexported.Evidence, error) {\nvar evi eviexported.Evidence\nerr := cdc.UnmarshalInterface(&evi, bz)\nreturn err, nil\n}\n```\n### Using `Any` in `sdk.Msg`s\nA similar concept is to be applied for messages that contain interfaces fields.\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\nan interface:\n```protobuf\n// x/evidence/types/types.proto\nmessage MsgSubmitEvidence {\nbytes submitter = 1\n[\n(gogoproto.casttype) = ""github.com/cosmos/cosmos-sdk/types.AccAddress""\n];\ngoogle.protobuf.Any evidence = 2;\n}\n```\nNote that in order to unpack the evidence from `Any` we do need a reference to\n`InterfaceRegistry`. In order to reference evidence in methods like\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\ninterfaces before they're needed.\n### Unpacking Interfaces\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\ninterfaces wrapped in `Any` before they're needed, we create an interface\nthat `sdk.Msg`s and other types can implement:\n```go\ntype UnpackInterfacesMessage interface {\nUnpackInterfaces(InterfaceUnpacker) error\n}\n```\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\nstruct itself with a public getter `GetCachedValue() interface{}`.\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\nand stored in `cachedValue` for reference later.\nThen unpacked interface values can safely be used in any code afterwards\nwithout knowledge of the `InterfaceRegistry`\nand messages can introduce a simple getter to cast the cached value to the\ncorrect interface type.\nThis has the added benefit that unmarshaling of `Any` values only happens once\nduring initial deserialization rather than every time the value is read. Also,\nwhen `Any` values are first packed (for instance in a call to\n`NewMsgSubmitEvidence`), the original interface value is cached so that\nunmarshaling isn't needed to read it again.\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\n`GetEvidence` as follows:\n```go\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\nvar evi eviexported.Evidence\nreturn ctx.UnpackAny(msg.Evidence, *evi)\n}\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\n}\n```\n### Amino Compatibility\nOur custom implementation of `Any` can be used transparently with Amino if used\nwith the proper codec instance. What this means is that interfaces packed within\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\nhave been registered properly with Amino).\nIn order for this functionality to work:\n* **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\nnow a wrapper which properly handles `Any`**\n* **all new code should use `Marshaler` which is compatible with both amino and\nprotobuf**\n* Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\n### Why Wasn't X Chosen Instead\nFor a more complete comparison to alternative protocols, see [here](https://codeburst.io/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\n### Cap'n Proto\nWhile [Cap’n Proto](https://capnproto.org/) does seem like an advantageous alternative to Protobuf\ndue to it's native support for interfaces/generics and built in canonicalization, it does lack the\nrich client ecosystem compared to Protobuf and is a bit less mature.\n### FlatBuffers\n[FlatBuffers](https://google.github.io/flatbuffers/) is also a potentially viable alternative, with the\nprimary difference being that FlatBuffers does not need a parsing/unpacking step to a secondary\nrepresentation before you can access data, often coupled with per-object memory allocation.\nHowever, it would require great efforts into research and full understanding the scope of the migration\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\nuntrusted inputs.\n","**Migrate State Serialization to Protobuf**\n\n**Rationale:**\n\nProtobuf meets all the criteria specified in the context:\n\n* **Language agnostic:** Protobuf supports multiple languages, including Go, Python, Java, and more.\n* **Platform agnostic:** Protobuf can be used on various platforms, including Linux, Windows, and macOS.\n* **Rich client support and thriving ecosystem:** Protobuf has extensive client support and a thriving ecosystem with numerous tools and libraries.\n* **High performance:** Protobuf is highly performant for serialization and deserialization.\n* **Minimal encoded message size:** Protobuf generates compact encoded messages.\n* **Codegen-based over reflection-based:** Protobuf uses code generation, which improves performance and is more developer-friendly.\n* **Supports backward and forward compatibility:** Protobuf provides mechanisms for backward and forward compatibility through message versioning and field presence.\n\nFurthermore, adopting Protobuf for state serialization aligns with the Cosmos SDK's goal of providing a flexible and extensible ecosystem. It will also improve interoperability with other blockchains that use Protobuf for state serialization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, Cosmos SDK uses IAVL for both state [commitments](https://cryptography.fandom.com/wiki/Commitment_scheme) and data storage.\nIAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems:\n* Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level.\n* Each edge traversal requires a DB query.\n* Creating snapshots is [expensive](https://github.com/cosmos/cosmos-sdk/issues/7215#issuecomment-684804950). It takes about 30 seconds to export less than 100 MB of state (as of March 2020).\n* Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck.\n* The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, [ref](https://github.com/cosmos/iavl/blob/master/docs/node/node.md\n).\nMoreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\n\n## Decision\n","We propose to separate the concerns of state commitment (**SC**), needed for consensus, and state storage (**SS**), needed for state machine. Finally we replace IAVL with [Celestia's SMT](https://github.com/lazyledger/smt). Celestia SMT is based on Diem (called jellyfish) design [*] - it uses a compute-optimised SMT by replacing subtrees with only default values with a single node (same approach is used by Ethereum2) and implements compact proofs.\nThe storage model presented here doesn't deal with data structure nor serialization. It's a Key-Value database, where both key and value are binaries. The storage user is responsible for data serialization.\n### Decouple state commitment from storage\nSeparation of storage and commitment (by the SMT) will allow the optimization of different components according to their usage and access patterns.\n`SC` (SMT) is used to commit to a data and compute Merkle proofs. `SS` is used to directly access data. To avoid collisions, both `SS` and `SC` will use a separate storage namespace (they could use the same database underneath). `SS` will store each record directly (mapping `(key, value)` as `key → value`).\nSMT is a merkle tree structure: we don't store keys directly. For every `(key, value)` pair, `hash(key)` is used as leaf path (we hash a key to uniformly distribute leaves in the tree) and `hash(value)` as the leaf contents. The tree structure is specified in more depth [below](#smt-for-state-commitment).\nFor data access we propose 2 additional KV buckets (implemented as namespaces for the key-value pairs, sometimes called [column family](https://github.com/facebook/rocksdb/wiki/Terminology)):\n1. B1: `key → value`: the principal object storage, used by a state machine, behind the Cosmos SDK `KVStore` interface: provides direct access by key and allows prefix iteration (KV DB backend must support it).\n2. B2: `hash(key) → key`: a reverse index to get a key from an SMT path. Internally the SMT will store `(key, value)` as `prefix || hash(key) || hash(value)`. So, we can get an object value by composing `hash(key) → B2 → B1`.\n3. We could use more buckets to optimize the app usage if needed.\nWe propose to use a KV database for both `SS` and `SC`. The store interface will allow to use the same physical DB backend for both `SS` and `SC` as well two separate DBs. The latter option allows for the separation of `SS` and `SC` into different hardware units, providing support for more complex setup scenarios and improving overall performance: one can use different backends (eg RocksDB and Badger) as well as independently tuning the underlying DB configuration.\n### Requirements\nState Storage requirements:\n* range queries\n* quick (key, value) access\n* creating a snapshot\n* historical versioning\n* pruning (garbage collection)\nState Commitment requirements:\n* fast updates\n* tree path should be short\n* query historical commitment proofs using ICS-23 standard\n* pruning (garbage collection)\n### SMT for State Commitment\nA Sparse Merkle tree is based on the idea of a complete Merkle tree of an intractable size. The assumption here is that as the size of the tree is intractable, there would only be a few leaf nodes with valid data blocks relative to the tree size, rendering a sparse tree.\nThe full specification can be found at [Celestia](https://github.com/celestiaorg/celestia-specs/blob/ec98170398dfc6394423ee79b00b71038879e211/src/specs/data_structures.md#sparse-merkle-tree). In summary:\n* The SMT consists of a binary Merkle tree, constructed in the same fashion as described in [Certificate Transparency (RFC-6962)](https://tools.ietf.org/html/rfc6962), but using as the hashing function SHA-2-256 as defined in [FIPS 180-4](https://doi.org/10.6028/NIST.FIPS.180-4).\n* Leaves and internal nodes are hashed differently: the one-byte `0x00` is prepended for leaf nodes while `0x01` is prepended for internal nodes.\n* Default values are given to leaf nodes with empty leaves.\n* While the above rule is sufficient to pre-compute the values of intermediate nodes that are roots of empty subtrees, a further simplification is to extend this default value to all nodes that are roots of empty subtrees. The 32-byte zero is used as the default value. This rule takes precedence over the above one.\n* An internal node that is the root of a subtree that contains exactly one non-empty leaf is replaced by that leaf's leaf node.\n### Snapshots for storage sync and state versioning\nBelow, with simple _snapshot_ we refer to a database snapshot mechanism, not to a _ABCI snapshot sync_. The latter will be referred as _snapshot sync_ (which will directly use DB snapshot as described below).\nDatabase snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a _copy on write_ and it allows DB state to be efficiently delivered at a certain stage.\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs.\nOne of the Stargate core features is a _snapshot sync_ delivered in the `/snapshot` package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a `SS` at a certain version together with a header chain.\nA new database snapshot will be created in every `EndBlocker` and identified by a block height. The `root` store keeps track of the available snapshots to offer `SS` at a certain version. The `root` store implements the `RootStore` interface described below. In essence, `RootStore` encapsulates a `Committer` interface. `Committer` has a `Commit`, `SetPruning`, `GetPruning` functions which will be used for creating and removing snapshots. The `rootStore.Commit` function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the `Committer` interface.\nNOTE: `Commit` must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into `Committer` and `PruningCommitter` - only the multiroot should implement `PruningCommitter` (cache and prefix store don't need pruning).\nNumber of historical versions for `abci.RequestQuery` and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions.\nPruning old snapshots is effectively done by a database. Whenever we update a record in `SC`, SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions.\nTo manage the active snapshots we will either use a DB _max number of snapshots_ option (if available), or we will remove DB snapshots in the `EndBlocker`. The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\n#### Accessing old state versions\nOne of the functional requirements is to access old state. This is done through `abci.RequestQuery` structure.  The version is specified by a block height (so we query for an object by a key `K` at block height `H`). The number of old versions supported for `abci.RequestQuery` is configurable. Accessing an old state is done by using available snapshots.\n`abci.RequestQuery` doesn't need old state of `SC` unless the `prove=true` parameter is set. The SMT merkle proof must be included in the `abci.ResponseQuery` only if both `SC` and `SS` have a snapshot for requested version.\nMoreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution.\nWe positively [validated](https://github.com/cosmos/cosmos-sdk/discussions/8297) a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\n### State Proofs\nFor any object stored in State Store (SS), we have corresponding object in `SC`. A proof for object `V` identified by a key `K` is a branch of `SC`, where the path corresponds to the key `hash(K)`, and the leaf is `hash(K, V)`.\n### Rollbacks\nWe need to be able to process transactions and roll-back state updates if a transaction fails. This can be done in the following way: during transaction processing, we keep all state change requests (writes) in a `CacheWrapper` abstraction (as it's done today). Once we finish the block processing, in the `Endblocker`,  we commit a root store - at that time, all changes are written to the SMT and to the `SS` and a snapshot is created.\n### Committing to an object without saving it\nWe identified use-cases, where modules will need to save an object commitment without storing an object itself. Sometimes clients are receiving complex objects, and they have no way to prove a correctness of that object without knowing the storage layout. For those use cases it would be easier to commit to the object without storing it directly.\n### Refactor MultiStore\nThe Stargate `/store` implementation (store/v1) adds an additional layer in the SDK store construction - the `MultiStore` structure. The multistore exists to support the modularity of the Cosmos SDK - each module is using its own instance of IAVL, but in the current implementation, all instances share the same database. The latter indicates, however, that the implementation doesn't provide true modularity. Instead it causes problems related to race condition and atomic DB commits (see: [\#6370](https://github.com/cosmos/cosmos-sdk/issues/6370) and [discussion](https://github.com/cosmos/cosmos-sdk/discussions/8297#discussioncomment-757043)).\nWe propose to reduce the multistore concept from the SDK, and to use a single instance of `SC` and `SS` in a `RootStore` object. To avoid confusion, we should rename the `MultiStore` interface to `RootStore`. The `RootStore` will have the following interface; the methods for configuring tracing and listeners are omitted for brevity.\n```go\n// Used where read-only access to versions is needed.\ntype BasicRootStore interface {\nStore\nGetKVStore(StoreKey) KVStore\nCacheRootStore() CacheRootStore\n}\n// Used as the main app state, replacing CommitMultiStore.\ntype CommitRootStore interface {\nBasicRootStore\nCommitter\nSnapshotter\nGetVersion(uint64) (BasicRootStore, error)\nSetInitialVersion(uint64) error\n... // Trace and Listen methods\n}\n// Replaces CacheMultiStore for branched state.\ntype CacheRootStore interface {\nBasicRootStore\nWrite()\n... // Trace and Listen methods\n}\n// Example of constructor parameters for the concrete type.\ntype RootStoreConfig struct {\nUpgrades        *StoreUpgrades\nInitialVersion  uint64\nReservePrefix(StoreKey, StoreType)\n}\n```\n<!-- TODO: Review whether these types can be further reduced or simplified -->\n<!-- TODO: RootStorePersistentCache type -->\nIn contrast to `MultiStore`, `RootStore` doesn't allow to dynamically mount sub-stores or provide an arbitrary backing DB for individual sub-stores.\nNOTE: modules will be able to use a special commitment and their own DBs. For example: a module which will use ZK proofs for state can store and commit this proof in the `RootStore` (usually as a single record) and manage the specialized store privately or using the `SC` low level interface.\n#### Compatibility support\nTo ease the transition to this new interface for users, we can create a shim which wraps a `CommitMultiStore` but provides a `CommitRootStore` interface, and expose functions to safely create and access the underlying `CommitMultiStore`.\nThe new `RootStore` and supporting types can be implemented in a `store/v2alpha1` package to avoid breaking existing code.\n#### Merkle Proofs and IBC\nCurrently, an IBC (v1.0) Merkle proof path consists of two elements (`[""<store-key>"", ""<record-key>""]`), with each key corresponding to a separate proof. These are each verified according to individual [ICS-23 specs](https://github.com/cosmos/ibc-go/blob/f7051429e1cf833a6f65d51e6c3df1609290a549/modules/core/23-commitment/types/merkle.go#L17), and the result hash of each step is used as the committed value of the next step, until a root commitment hash is obtained.\nThe root hash of the proof for `""<record-key>""` is hashed with the `""<store-key>""` to validate against the App Hash.\nThis is not compatible with the `RootStore`, which stores all records in a single Merkle tree structure, and won't produce separate proofs for the store- and record-key. Ideally, the store-key component of the proof could just be omitted, and updated to use a ""no-op"" spec, so only the record-key is used. However, because the IBC verification code hardcodes the `""ibc""` prefix and applies it to the SDK proof as a separate element of the proof path, this isn't possible without a breaking change. Breaking this behavior would severely impact the Cosmos ecosystem which already widely adopts the IBC module. Requesting an update of the IBC module across the chains is a time consuming effort and not easily feasible.\nAs a workaround, the `RootStore` will have to use two separate SMTs (they could use the same underlying DB): one for IBC state and one for everything else. A simple Merkle map that reference these SMTs will act as a Merkle Tree to create a final App hash. The Merkle map is not stored in a DBs - it's constructed in the runtime. The IBC substore key must be `""ibc""`.\nThe workaround can still guarantee atomic syncs: the [proposed DB backends](#evaluated-kv-databases) support atomic transactions and efficient rollbacks, which will be used in the commit phase.\nThe presented workaround can be used until the IBC module is fully upgraded to supports single-element commitment proofs.\n### Optimization: compress module key prefixes\nWe consider a compression of prefix keys by creating a mapping from module key to an integer, and serializing the integer using varint coding. Varint coding assures that different values don't have common byte prefix. For Merkle Proofs we can't use prefix compression - so it should only apply for the `SS` keys. Moreover, the prefix compression should be only applied for the module namespace. More precisely:\n* each module has it's own namespace;\n* when accessing a module namespace we create a KVStore with embedded prefix;\n* that prefix will be compressed only when accessing and managing `SS`.\nWe need to assure that the codes won't change. We can fix the mapping in a static variable (provided by an app) or SS state under a special key.\nTODO: need to make decision about the key compression.\n","We should move to Poseidon Tree for state commitments and BadgerDB for data storage.\n\n**Benefits:**\n\n* Poseidon Tree is a state-of-the-art Merkle Tree for state commitments that is more efficient, secure, and easier to maintain than IAVL.\n* BadgerDB is a high-performance, embedded NoSQL database that is well-suited for storing the large amounts of data that are typically found in a blockchain system.\n\n**Risks:**\n\n* Moving to a new state commitment data structure and storage engine will require significant development effort.\n* There is a risk that the new solutions will not perform as well as expected.\n* There is a risk that the new solutions will introduce new security vulnerabilities.\n\n**Mitigation:**\n\n* The development effort will be carefully planned and executed.\n* The new solutions will be thoroughly tested before being deployed in production.\n* The new solutions will be audited by a security expert."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, the Cosmos SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\n\n## Decision\n","In summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`’s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\nDetailed changes:\n### 1) (Un)Marshal accounts directly using amino\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\n```go\n// GenesisState - all auth state that must be provided at genesis\ntype GenesisState struct {\nParams   Params           `json:""params"" yaml:""params""`\nAccounts []GenesisAccount `json:""accounts"" yaml:""accounts""`\n}\n```\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\n```go\n// InitGenesis - Init store state from genesis data\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\nak.SetParams(ctx, data.Params)\n// load the accounts\nfor _, a := range data.Accounts {\nacc := ak.NewAccount(ctx, a) // set account number\nak.SetAccount(ctx, acc)\n}\n}\n// ExportGenesis returns a GenesisState for a given context and keeper\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\nparams := ak.GetParams(ctx)\nvar genAccounts []exported.GenesisAccount\nak.IterateAccounts(ctx, func(account exported.Account) bool {\ngenAccount := account.(exported.GenesisAccount)\ngenAccounts = append(genAccounts, genAccount)\nreturn false\n})\nreturn NewGenesisState(params, genAccounts)\n}\n```\n### 2) Register custom account types on the `auth` codec\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\nAn example custom account definition:\n```go\nimport authtypes ""cosmossdk.io/x/auth/types""\n// Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\nfunc init() {\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, ""cosmos-sdk/ModuleAccount"")\n}\ntype ModuleAccount struct {\n...\n```\nThe `auth` codec definition:\n```go\nvar ModuleCdc *codec.LegacyAmino\nfunc init() {\nModuleCdc = codec.NewLegacyAmino()\n// register module msg's and Account interface\n...\n// leave the codec unsealed\n}\n// RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\nModuleCdc.RegisterConcrete(o, name, nil)\n}\n```\n### 3) Genesis validation for custom account types\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\n```go\ntype GenesisAccount interface {\nexported.Account\nValidate() error\n}\n```\nThen the `auth` `ValidateGenesis` function becomes:\n```go\n// ValidateGenesis performs basic validation of auth genesis data returning an\n// error for any failed validation criteria.\nfunc ValidateGenesis(data GenesisState) error {\n// Validate params\n...\n// Validate accounts\naddrMap := make(map[string]bool, len(data.Accounts))\nfor _, acc := range data.Accounts {\n// check for duplicated accounts\naddrStr := acc.GetAddress().String()\nif _, ok := addrMap[addrStr]; ok {\nreturn fmt.Errorf(""duplicate account found in genesis state; address: %s"", addrStr)\n}\naddrMap[addrStr] = true\n// check account specific validation\nif err := acc.Validate(); err != nil {\nreturn fmt.Errorf(""invalid account found in genesis state; address: %s, error: %s"", addrStr, err.Error())\n}\n}\nreturn nil\n}\n```\n### 4) Move add-genesis-account cli to `auth`\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\n### 5) Update module and vesting accounts\nUnder the new scheme, module and vesting account types need some minor updates:\n* Type registration on `auth`'s codec (shown above)\n* A `Validate` method for each `Account` concrete type\n",Modify `genaccounts` to support any type of account supported by `auth` by introducing an `AccountFactory` interface that can be registered in `auth` and is responsible for loading and exporting accounts that use that factory.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe Cosmos SDK maintains a set of [Protobuf definitions](https://github.com/cosmos/cosmos-sdk/tree/main/proto/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf's](https://docs.buf.build/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. For example:\n* Adding fields to `Msg`s. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for `reserved` fields. See [#9446](https://github.com/cosmos/cosmos-sdk/issues/9446) for more details on this issue.\nMoreover, module developers often face other questions around Protobuf definitions such as ""Can I rename a field?"" or ""Can I deprecate a field?"" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\n\n## Decision\n","We decide to keep [Buf's](https://docs.buf.build/) recommendations with the following exceptions:\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn't use the `-Service` suffix.\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don't use a version suffix.\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don't have the `-Request` suffix to keep backwards compatibility.\nOn top of Buf's recommendations we add the following guidelines that are specific to the Cosmos SDK.\n### Updating Protobuf Definition Without Bumping Version\n#### 1. Module developers MAY add new Protobuf definitions\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\n```protobuf\n// Since: cosmos-sdk <version>{, <version>...}\n```\nWhere each `version` denotes a minor (""0.45"") or patch (""0.44.5"") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show/hide these fields depending on the targeted node version.\nAs examples, the following comments are valid:\n```protobuf\n// Since: cosmos-sdk 0.44\n// Since: cosmos-sdk 0.42.11, 0.44.5\n```\nand the following ones are NOT valid:\n```protobuf\n// Since cosmos-sdk v0.44\n// since: cosmos-sdk 0.44\n// Since: cosmos-sdk 0.42.11 0.44.5\n// Since: Cosmos SDK 0.42.11, 0.44.5\n```\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\nProtobuf supports the [`deprecated` field option](https://developers.google.com/protocol-buffers/docs/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\n* The Cosmos SDK recently removed support for [time-based software upgrades](https://github.com/cosmos/cosmos-sdk/pull/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\n* The Cosmos SDK now supports [governance split votes](./adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\n#### 3. Fields MUST NOT be renamed\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\n### Incrementing Protobuf Package Version\nTODO, needs architecture review. Some topics:\n* Bumping versions frequency\n* When bumping versions, should the Cosmos SDK support both versions?\n* i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\n* mention ADR-023 Protobuf naming\n","To avoid breaking changes within the same version, the Cosmos SDK will follow a set of Protobuf definition update guidelines. These guidelines will be based on the Protobuf specification and will take into account the specific needs of the Cosmos SDK.\n\n**Allowed Updates:**\n\n* Adding new fields to messages\n* Adding new enum values\n* Changing the order of fields or enum values\n* Deprecating fields or enum values\n\n**Disallowed Updates:**\n\n* Removing fields or enum values\n* Renaming fields or enum values\n* Changing the type of fields or enum values\n* Making fields required or optional\n\n**Guidelines:**\n\n* Deprecating deprecated fields or enum values for two major versions to give developers time to update their code\n* Only making breaking changes in major versions"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n[Rosetta API](https://www.rosetta-api.org/) is an open-source specification and set of tools developed by Coinbase to\nstandardise blockchain interactions.\nThrough the use of a standard API for integrating blockchain applications it will\n* Be easier for a user to interact with a given blockchain\n* Allow exchanges to integrate new blockchains quickly and easily\n* Enable application developers to build cross-blockchain applications such as block explorers, wallets and dApps at\nconsiderably lower cost and effort.\n\n## Decision\n","It is clear that adding Rosetta API support to the Cosmos SDK will bring value to all the developers and\nCosmos SDK based chains in the ecosystem. How it is implemented is key.\nThe driving principles of the proposed design are:\n1. **Extensibility:** it must be as riskless and painless as possible for application developers to set-up network\nconfigurations to expose Rosetta API-compliant services.\n2. **Long term support:** This proposal aims to provide support for all the supported Cosmos SDK release series.\n3. **Cost-efficiency:** Backporting changes to Rosetta API specifications from `master` to the various stable\nbranches of Cosmos SDK is a cost that needs to be reduced.\nWe will achieve these delivering on these principles by the following:\n1. There will be a package `rosetta/lib`\nfor the implementation of the core Rosetta API features, particularly:\na. The types and interfaces (`Client`, `OfflineClient`...), this separates design from implementation detail.\nb. The `Server` functionality as this is independent of the Cosmos SDK version.\nc. The `Online/OfflineNetwork`, which is not exported, and implements the rosetta API using the `Client` interface to query the node, build tx and so on.\nd. The `errors` package to extend rosetta errors.\n2. Due to differences between the Cosmos release series, each series will have its own specific implementation of `Client` interface.\n3. There will be two options for starting an API service in applications:\na. API shares the application process\nb. API-specific process.\n","**Decision:** Adopt Rosetta API as the standard for blockchain interactions.\n\n**Rationale:**\n\n* **Standardization:** Rosetta API provides a standardized interface for interacting with different blockchains, making it easier for users, exchanges, and application developers to integrate with and utilize various blockchain platforms.\n* **Ease of Integration:** Using Rosetta API, exchanges can quickly and easily integrate with new blockchains, reducing the time and effort required for onboarding.\n* **Cross-Blockchain Applications:** Rosetta API enables developers to build cross-blockchain applications, such as block explorers, wallets, and dApps, with reduced costs and effort compared to developing separate applications for each blockchain.\n* **Enhanced User Experience:** By simplifying the interaction with blockchains through a standardized API, Rosetta API improves the user experience for blockchain users and developers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https://github.com/cosmos/cosmos-sdk/discussions/9065.\nAs discussed in [#9065](https://github.com/cosmos/cosmos-sdk/discussions/9065), several potential solutions can be considered:\n* irismod/nft and modules/incubator/nft\n* CW721\n* DID NFTs\n* interNFT\nSince functions/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\nThe current design is based on the work done by [IRISnet team](https://github.com/irisnet/irismod/tree/master/modules/nft) and an older implementation in the [Cosmos repository](https://github.com/cosmos/modules/tree/master/incubator/nft).\n\n## Decision\n","We create a `x/nft` module, which contains the following functionality:\n* Store NFTs and track their ownership.\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\n* Query NFTs and their supply information.\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\n### Types\nWe propose two main types:\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\n#### Class\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\n```protobuf\nmessage Class {\nstring id          = 1;\nstring name        = 2;\nstring symbol      = 3;\nstring description = 4;\nstring uri         = 5;\nstring uri_hash    = 6;\ngoogle.protobuf.Any data = 7;\n}\n```\n* `id` is used as the primary index for storing the class; _required_\n* `name` is a descriptive name of the NFT class; _optional_\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\n* `description` is a detailed description of the NFT class; _optional_\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https://docs.opensea.io/docs/contract-level-metadata)); _optional_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is app specific metadata of the class; _optional_\n#### NFT\nWe define a general model for `NFT` as follows.\n```protobuf\nmessage NFT {\nstring class_id           = 1;\nstring id                 = 2;\nstring uri                = 3;\nstring uri_hash           = 4;\ngoogle.protobuf.Any data  = 10;\n}\n```\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\n```text\n{class_id}/{id} --> NFT (bytes)\n```\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https://docs.opensea.io/docs/metadata-standards)); _required_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational/UI functionality.\n### `Keeper` Interface\n```go\ntype Keeper interface {\nNewClass(ctx sdk.Context,class Class)\nUpdateClass(ctx sdk.Context,class Class)\nMint(ctx sdk.Context,nft NFT，receiver sdk.AccAddress)   // updates totalSupply\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\nBurn(ctx sdk.Context, classId string, nftId string)    // updates totalSupply\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\nUpdate(ctx sdk.Context, nft NFT)\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\nGetClass(ctx sdk.Context, classId string) Class\nGetClasses(ctx sdk.Context) []Class\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\nGetTotalSupply(ctx sdk.Context, classId string) uint64\n}\n```\nOther business logic implementations should be defined in composing modules that import `x/nft` and use its `Keeper`.\n### `Msg` Service\n```protobuf\nservice Msg {\nrpc Send(MsgSend)         returns (MsgSendResponse);\n}\nmessage MsgSend {\nstring class_id = 1;\nstring id       = 2;\nstring sender   = 3;\nstring reveiver = 4;\n}\nmessage MsgSendResponse {}\n```\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\nThe implementation outline of the server is as follows:\n```go\ntype msgServer struct{\nk Keeper\n}\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\n// check current ownership\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\n// transfer ownership\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\nreturn &types.MsgSendResponse{}, nil\n}\n```\nThe query service methods for the `x/nft` module are:\n```protobuf\nservice Query {\n// Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/balance/{owner}/{class_id}"";\n}\n// Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/owner/{class_id}/{id}"";\n}\n// Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/supply/{class_id}"";\n}\n// NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/nfts"";\n}\n// NFT queries an NFT based on its class and id.\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/nfts/{class_id}/{id}"";\n}\n// Class queries an NFT class based on its id\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/classes/{class_id}"";\n}\n// Classes queries all NFT classes\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\noption (google.api.http).get = ""/cosmos/nft/v1beta1/classes"";\n}\n}\n// QueryBalanceRequest is the request type for the Query/Balance RPC method\nmessage QueryBalanceRequest {\nstring class_id = 1;\nstring owner    = 2;\n}\n// QueryBalanceResponse is the response type for the Query/Balance RPC method\nmessage QueryBalanceResponse {\nuint64 amount = 1;\n}\n// QueryOwnerRequest is the request type for the Query/Owner RPC method\nmessage QueryOwnerRequest {\nstring class_id = 1;\nstring id       = 2;\n}\n// QueryOwnerResponse is the response type for the Query/Owner RPC method\nmessage QueryOwnerResponse {\nstring owner = 1;\n}\n// QuerySupplyRequest is the request type for the Query/Supply RPC method\nmessage QuerySupplyRequest {\nstring class_id = 1;\n}\n// QuerySupplyResponse is the response type for the Query/Supply RPC method\nmessage QuerySupplyResponse {\nuint64 amount = 1;\n}\n// QueryNFTstRequest is the request type for the Query/NFTs RPC method\nmessage QueryNFTsRequest {\nstring                                class_id   = 1;\nstring                                owner      = 2;\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\n}\n// QueryNFTsResponse is the response type for the Query/NFTs RPC methods\nmessage QueryNFTsResponse {\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n// QueryNFTRequest is the request type for the Query/NFT RPC method\nmessage QueryNFTRequest {\nstring class_id = 1;\nstring id       = 2;\n}\n// QueryNFTResponse is the response type for the Query/NFT RPC method\nmessage QueryNFTResponse {\ncosmos.nft.v1beta1.NFT nft = 1;\n}\n// QueryClassRequest is the request type for the Query/Class RPC method\nmessage QueryClassRequest {\nstring class_id = 1;\n}\n// QueryClassResponse is the response type for the Query/Class RPC method\nmessage QueryClassResponse {\ncosmos.nft.v1beta1.Class class = 1;\n}\n// QueryClassesRequest is the request type for the Query/Classes RPC method\nmessage QueryClassesRequest {\n// pagination defines an optional pagination for the request.\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\n}\n// QueryClassesResponse is the response type for the Query/Classes RPC method\nmessage QueryClassesResponse {\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n```\n### Interoperability\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x/nft. This requires creation of a new IBC standard and implementation of it.\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x/nft interoperability, custom NFT implementations (example: x/cryptokitty) should use the canonical x/nft module and proxy all NFT balance keeping functionality to x/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x/nft becomes the standard NFT registry for all Cosmos NFTs (example: x/cryptokitty will register a kitty NFT in x/nft and use x/nft for book keeping). This was [discussed](https://github.com/cosmos/cosmos-sdk/discussions/9065#discussioncomment-873206) in the context of using x/bank as a general asset balance book. Not using x/nft will require implementing another module for IBC.\n","The decision is to implement a generic NFT module in Cosmos Hub, based on the work done by the IRISnet team and the previous implementation in the Cosmos repository. This module will handle the generic NFT logic, while application-specific functions can be managed by other modules on Cosmos Hub or other Zones by importing the NFT module. This design will allow for composability and support the generic use cases and compatibility of interchain protocols such as IBC and Gravity Bridge."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current proof of stake module takes the design decision to apply staking weight changes to the consensus engine immediately. This means that delegations and unbonds get applied immediately to the validator set. This decision was primarily done as it was implementationally simplest, and because we at the time believed that this would lead to better UX for clients.\nAn alternative design choice is to allow buffering staking updates (delegations, unbonds, validators joining) for a number of blocks. This 'epoch'd proof of stake consensus provides the guarantee that the consensus weights for validators will not change mid-epoch, except in the event of a slash condition.\nAdditionally, the UX hurdle may not be as significant as was previously thought. This is because it is possible to provide users immediate acknowledgement that their bond was recorded and will be executed.\nFurthermore, it has become clearer over time that immediate execution of staking events comes with limitations, such as:\n* Threshold based cryptography. One of the main limitations is that because the validator set can change so regularly, it makes the running of multiparty computation by a fixed validator set difficult. Many threshold-based cryptographic features for blockchains such as randomness beacons and threshold decryption require a computationally-expensive DKG process (will take much longer than 1 block to create). To productively use these, we need to guarantee that the result of the DKG will be used for a reasonably long time. It wouldn't be feasible to rerun the DKG every block. By epoching staking, it guarantees we'll only need to run a new DKG once every epoch.\n* Light client efficiency. This would lessen the overhead for IBC when there is high churn in the validator set. In the Tendermint light client bisection algorithm, the number of headers you need to verify is related to bounding the difference in validator sets between a trusted header and the latest header. If the difference is too great, you verify more header in between the two. By limiting the frequency of validator set changes, we can reduce the worst case size of IBC lite client proofs, which occurs when a validator set has high churn.\n* Fairness of deterministic leader election. Currently we have no ways of reasoning of fairness of deterministic leader election in the presence of staking changes without epochs (tendermint/spec#217). Breaking fairness of leader election is profitable for validators, as they earn additional rewards from being the proposer. Adding epochs at least makes it easier for our deterministic leader election to match something we can prove secure. (Albeit, we still haven’t proven if our current algorithm is fair with > 2 validators in the presence of stake changes)\n* Staking derivative design. Currently, reward distribution is done lazily using the F1 fee distribution. While saving computational complexity, lazy accounting requires a more stateful staking implementation. Right now, each delegation entry has to track the time of last withdrawal. Handling this can be a challenge for some staking derivatives designs that seek to provide fungibility for all tokens staked to a single validator. Force-withdrawing rewards to users can help solve this, however it is infeasible to force-withdraw rewards to users on a per block basis. With epochs, a chain could more easily alter the design to have rewards be forcefully withdrawn (iterating over delegator accounts only once per-epoch), and can thus remove delegation timing from state. This may be useful for certain staking derivative designs.\n\n## Decision\n","**Step-1**:  Implement buffering of all staking and slashing messages.\nFirst we create a pool for storing tokens that are being bonded, but should be applied at the epoch boundary called the `EpochDelegationPool`. Then, we have two separate queues, one for staking, one for slashing. We describe what happens on each message being delivered below:\n### Staking messages\n* **MsgCreateValidator**: Move user's self-bond to `EpochDelegationPool` immediately. Queue a message for the epoch boundary to handle the self-bond, taking the funds from the `EpochDelegationPool`. If Epoch execution fail, return back funds from `EpochDelegationPool` to user's account.\n* **MsgEditValidator**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **MsgDelegate**: Move user's funds to `EpochDelegationPool` immediately. Queue a message for the epoch boundary to handle the delegation, taking the funds from the `EpochDelegationPool`. If Epoch execution fail, return back funds from `EpochDelegationPool` to user's account.\n* **MsgBeginRedelegate**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **MsgUndelegate**: Validate message and if valid queue the message for execution at the end of the Epoch.\n### Slashing messages\n* **MsgUnjail**: Validate message and if valid queue the message for execution at the end of the Epoch.\n* **Slash Event**: Whenever a slash event is created, it gets queued in the slashing module to apply at the end of the epoch. The queues should be setup such that this slash applies immediately.\n### Evidence Messages\n* **MsgSubmitEvidence**: This gets executed immediately, and the validator gets jailed immediately. However in slashing, the actual slash event gets queued.\nThen we add methods to the end blockers, to ensure that at the epoch boundary the queues are cleared and delegation updates are applied.\n**Step-2**: Implement querying of queued staking txs.\nWhen querying the staking activity of a given address, the status should return not only the amount of tokens staked, but also if there are any queued stake events for that address. This will require more work to be done in the querying logic, to trace the queued upcoming staking events.\nAs an initial implementation, this can be implemented as a linear search over all queued staking events. However, for chains that need long epochs, they should eventually build additional support for nodes that support querying to be able to produce results in constant time. (This is do-able by maintaining an auxiliary hashmap for indexing upcoming staking events by address)\n**Step-3**: Adjust gas\nCurrently gas represents the cost of executing a transaction when its done immediately. (Merging together costs of p2p overhead, state access overhead, and computational overhead) However, now a transaction can cause computation in a future block, namely at the epoch boundary.\nTo handle this, we should initially include parameters for estimating the amount of future computation (denominated in gas), and add that as a flat charge needed for the message.\nWe leave it as out of scope for how to weight future computation versus current computation in gas pricing, and have it set such that the are weighted equally for now.\n",Use epochs for proof of stake updates.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained.\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto ""circuit-break"" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established.\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to ""pre-approve"" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.\n\n## Decision\n","The dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the\nimplementation of:\n* continuous voting\n* slashing due to breach of soft contract\n* revoking a member due to breach of soft contract\n* emergency disband of the entire dCERT group (ex. for colluding maliciously)\n* compensation stipend from the community pool or other means decided by\ngovernance\nThis system necessitates the following new parameters:\n* blockly stipend allowance per dCERT member\n* maximum number of dCERT members\n* required staked slashable tokens for each dCERT member\n* quorum for suspending a particular member\n* proposal wager for disbanding the dCERT group\n* stabilization period for dCERT member transition\n* circuit break dCERT privileges enabled\nThese parameters are expected to be implemented through the param keeper such\nthat governance may change them at any given point.\n### Continuous Voting Electionator\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n* All delegation addresses may submit votes at any point which updates their\npreferred representation on the dCERT group.\n* Preferred representation may be arbitrarily split between addresses (ex. 50%\nto John, 25% to Sally, 25% to Carol)\n* In order for a new member to be added to the dCERT group they must\nsend a transaction accepting their admission at which point the validity of\ntheir admission is to be confirmed.\n* A sequence number is assigned when a member is added to dCERT group.\nIf a member leaves the dCERT group and then enters back, a new sequence number\nis assigned.\n* Addresses which control the greatest amount of preferred-representation are\neligible to join the dCERT group (up the _maximum number of dCERT members_).\nIf the dCERT group is already full and new member is admitted, the existing\ndCERT member with the lowest amount of votes is kicked from the dCERT group.\n* In the split situation where the dCERT group is full but a vying candidate\nhas the same amount of vote as an existing dCERT member, the existing\nmember should maintain its position.\n* In the split situation where somebody must be kicked out but the two\naddresses with the smallest number of votes have the same number of votes,\nthe address with the smallest sequence number maintains its position.\n* A stabilization period can be optionally included to reduce the\n""flip-flopping"" of the dCERT membership tail members. If a stabilization\nperiod is provided which is greater than 0, when members are kicked due to\ninsufficient support, a queue entry is created which documents which member is\nto replace which other member. While this entry is in the queue, no new entries\nto kick that same dCERT member can be made. When the entry matures at the\nduration of the  stabilization period, the new member is instantiated, and old\nmember kicked.\n### Staking/Slashing\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed.\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes.\n### dCERT membership transactions\nActive dCERT members\n* change of the description of the dCERT group\n* circuit break a message route\n* vote to suspend a dCERT member.\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: ""disable all staking-delegation messages"", or\n""disable all distribution messages"". This could be accomplished by verifying\nthat the message route has not been ""circuit-broken"" at CheckTx time (in\n`baseapp/baseapp.go`).\n""unbreaking"" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired.\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n",FAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILEDFAILED
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\n\n## Decision\n","### Design\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\n```text\nslash_amount = k * power // power is the faulting validator's voting power and k is some on-chain constant\n```\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts (sybil attack), so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\n```text\nslash_amount = k * (power_1 + power_2 + ... + power_n) // where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\n```\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will get slashed at the sum 10% amount.\nHowever in practice, we likely don't want a linear relation between amount of stake at fault, and the percentage of stake to slash. In particular, solely 5% of stake double signing effectively did nothing to majorly threaten security, whereas 30% of stake being at fault clearly merits a large slashing factor, due to being very close to the point at which Tendermint security is threatened. A linear relation would require a factor of 6 gap between these two, whereas the difference in risk posed to the network is much larger. We propose using S-curves (formally [logistic functions](https://en.wikipedia.org/wiki/Logistic_function) to solve this). S-Curves capture the desired criterion quite well. They allow the slashing factor to be minimal for small values, and then grow very rapidly near some threshold point where the risk posed becomes notable.\n#### Parameterization\nThis requires parameterizing a logistic function. It is very well understood how to parameterize this. It has four parameters:\n1) A minimum slashing factor\n2) A maximum slashing factor\n3) The inflection point of the S-curve (essentially where do you want to center the S)\n4) The rate of growth of the S-curve (How elongated is the S)\n#### Correlation across non-sybil validators\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\n#### Griefing\nGriefing, the act of intentionally getting oneself slashed in order to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker also gets equally impacted by the grief as the victim, so it would not provide much benefit to the griefer.\n### Implementation\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define ""recent slashes"" as ones that have occurred within the last `unbonding period`.  For liveness faults, we will define ""recent slashes"" as ones that have occurred within the last `jail period`.\n```go\ntype SlashEvent struct {\nAddress                     sdk.ValAddress\nValidatorVotingPercent      sdk.Dec\nSlashedSoFar                sdk.Dec\n}\n```\nThese slash events will be pruned from the queue once they are older than their respective ""recent slash period"".\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\nWe then will iterate over all the SlashEvents in the queue, adding their `ValidatorVotingPercent` to calculate the new percent to slash all the validators in the queue at, using the ""Square of Sum of Roots"" formula introduced above.\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occurred, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\n","Implement a progressive fee for delegation on validators based on the share of consensus power they hold.  The fee would be collected by the protocol and used to fund public goods in the ecosystem.  The fee would be tiered, with higher fees for validators that hold a larger share of consensus power.  This would incentivize delegators to spread their delegations across a wider range of validators, reducing the risk of centralization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen a chain upgrade introduces state-breaking changes inside modules, the current procedure consists of exporting the whole state into a JSON file (via the `simd genesis export` command), running migration scripts on the JSON file (`simd genesis migrate` command), clearing the stores (`simd unsafe-reset-all` command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height). An example of such a procedure can be seen [in the Cosmos Hub 3->4 migration guide](https://github.com/cosmos/gaia/blob/v4.0.3/docs/migration/cosmoshub-3.md#upgrade-procedure).\nThis procedure is cumbersome for multiple reasons:\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https://github.com/cosmos/cosmos-sdk/issues/6936)).\n\n## Decision\n","We propose a migration procedure based on modifying the KV store in-place without involving the JSON export-process-import flow described above.\n### Module `ConsensusVersion`\nWe introduce a new method on the `AppModule` interface:\n```go\ntype AppModule interface {\n// --snip--\nConsensusVersion() uint64\n}\n```\nThis methods returns an `uint64` which serves as state-breaking version of the module. It MUST be incremented on each consensus-breaking change introduced by the module. To avoid potential errors with default values, the initial version of a module MUST be set to 1. In the Cosmos SDK, version 1 corresponds to the modules in the v0.41 series.\n### Module-Specific Migration Functions\nFor each consensus-breaking change introduced by the module, a migration script from ConsensusVersion `N` to version `N+1` MUST be registered in the `Configurator` using its newly-added `RegisterMigration` method. All modules receive a reference to the configurator in their `RegisterServices` method on `AppModule`, and this is where the migration functions should be registered. The migration functions should be registered in increasing order.\n```go\nfunc (am AppModule) RegisterServices(cfg module.Configurator) {\n// --snip--\ncfg.RegisterMigration(types.ModuleName, 1, func(ctx sdk.Context) error {\n// Perform in-place store migrations from ConsensusVersion 1 to 2.\n})\ncfg.RegisterMigration(types.ModuleName, 2, func(ctx sdk.Context) error {\n// Perform in-place store migrations from ConsensusVersion 2 to 3.\n})\n// etc.\n}\n```\nFor example, if the new ConsensusVersion of a module is `N` , then `N-1` migration functions MUST be registered in the configurator.\nIn the Cosmos SDK, the migration functions are handled by each module's keeper, because the keeper holds the `sdk.StoreKey` used to perform in-place store migrations. To not overload the keeper, a `Migrator` wrapper is used by each module to handle the migration functions:\n```go\n// Migrator is a struct for handling in-place store migrations.\ntype Migrator struct {\nBaseKeeper\n}\n```\nMigration functions should live inside the `migrations/` folder of each module, and be called by the Migrator's methods. We propose the format `Migrate{M}to{N}` for method names.\n```go\n// Migrate1to2 migrates from version 1 to 2.\nfunc (m Migrator) Migrate1to2(ctx sdk.Context) error {\nreturn v2bank.MigrateStore(ctx, m.keeper.storeKey) // v043bank is package `x/bank/migrations/v2`.\n}\n```\nEach module's migration functions are specific to the module's store evolutions, and are not described in this ADR. An example of x/bank store key migrations after the introduction of ADR-028 length-prefixed addresses can be seen in this [store.go code](https://github.com/cosmos/cosmos-sdk/blob/36f68eb9e041e20a5bb47e216ac5eb8b91f95471/x/bank/legacy/v043/store.go#L41-L62).\n### Tracking Module Versions in `x/upgrade`\nWe introduce a new prefix store in `x/upgrade`'s store. This store will track each module's current version, it can be modelized as a `map[string]uint64` of module name to module ConsensusVersion, and will be used when running the migrations (see next section for details). The key prefix used is `0x1`, and the key/value format is:\n```text\n0x2 | {bytes(module_name)} => BigEndian(module_consensus_version)\n```\nThe initial state of the store is set from `app.go`'s `InitChainer` method.\nThe UpgradeHandler signature needs to be updated to take a `VersionMap`, as well as return an upgraded `VersionMap` and an error:\n```diff\n- type UpgradeHandler func(ctx sdk.Context, plan Plan)\n+ type UpgradeHandler func(ctx sdk.Context, plan Plan, versionMap VersionMap) (VersionMap, error)\n```\nTo apply an upgrade, we query the `VersionMap` from the `x/upgrade` store and pass it into the handler. The handler runs the actual migration functions (see next section), and if successful, returns an updated `VersionMap` to be stored in state.\n```diff\nfunc (k UpgradeKeeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {\n// --snip--\n-   handler(ctx, plan)\n+   updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx)) // k.GetModuleVersionMap() fetches the VersionMap stored in state.\n+   if err != nil {\n+       return err\n+   }\n+\n+   // Set the updated consensus versions to state\n+   k.SetModuleVersionMap(ctx, updatedVM)\n}\n```\nA gRPC query endpoint to query the `VersionMap` stored in `x/upgrade`'s state will also be added, so that app developers can double-check the `VersionMap` before the upgrade handler runs.\n### Running Migrations\nOnce all the migration handlers are registered inside the configurator (which happens at startup), running migrations can happen by calling the `RunMigrations` method on `module.Manager`. This function will loop through all modules, and for each module:\n* Get the old ConsensusVersion of the module from its `VersionMap` argument (let's call it `M`).\n* Fetch the new ConsensusVersion of the module from the `ConsensusVersion()` method on `AppModule` (call it `N`).\n* If `N>M`, run all registered migrations for the module sequentially `M -> M+1 -> M+2...` until `N`.\n* There is a special case where there is no ConsensusVersion for the module, as this means that the module has been newly added during the upgrade. In this case, no migration function is run, and the module's current ConsensusVersion is saved to `x/upgrade`'s store.\nIf a required migration is missing (e.g. if it has not been registered in the `Configurator`), then the `RunMigrations` function will error.\nIn practice, the `RunMigrations` method should be called from inside an `UpgradeHandler`.\n```go\napp.UpgradeKeeper.SetUpgradeHandler(""my-plan"", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap)  (module.VersionMap, error) {\nreturn app.mm.RunMigrations(ctx, vm)\n})\n```\nAssuming a chain upgrades at block `n`, the procedure should run as follows:\n* the old binary will halt in `BeginBlock` when starting block `N`. In its store, the ConsensusVersions of the old binary's modules are stored.\n* the new binary will start at block `N`. The UpgradeHandler is set in the new binary, so will run at `BeginBlock` of the new binary. Inside `x/upgrade`'s `ApplyUpgrade`, the `VersionMap` will be retrieved from the (old binary's) store, and passed into the `RunMigrations` function, migrating all module stores in-place before the modules' own `BeginBlock`s.\n","**Introduce a new `migrate` command to the Cosmos SDK which would take a genesis JSON file and a list of migration scripts as inputs and output a new genesis file with the applied migrations.**\n\nThis new `migrate` command would simplify the migration process by automating the steps of exporting the state, running the migration scripts, and updating the genesis file. It would also make the migration process more efficient by reducing the time and resources required to complete the migration.\n\nThe `migrate` command would be implemented as a new subcommand of the `simd` tool. It would take the following arguments:\n\n* `genesis-file`: The path to the genesis JSON file to be migrated.\n* `migration-scripts`: A list of paths to the migration scripts to be applied.\n* `output-file`: The path to the output genesis JSON file.\n\nThe `migrate` command would work by first loading the genesis JSON file into memory. It would then iterate over the list of migration scripts, applying each script to the in-memory genesis state. Once all of the migration scripts have been applied, the `migrate` command would write the updated genesis state to the output file.\n\nThe `migrate` command would provide a number of benefits over the existing migration procedure. First, it would be more efficient, reducing the time and resources required to complete a migration. Second, it would be more user-friendly, providing a simple and straightforward way to apply migrations to a genesis file. Third, it would be more extensible, allowing for the development of new migration scripts to support future state-breaking changes."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCosmos SDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\nmediocre.\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\nwhich are repetitive and hard to maintain.\nSecond, this makes it harder to align with the object capability theorem: the right to access the\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\nFinally, because the getter/setter functions are defined as methods of a module's `Keeper`, the reviewers\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\nThere is no static way to know which part of the state that the function is accessing (and which is not).\n\n## Decision\n","We will define a type named `Value`:\n```go\ntype Value struct {\nm   Mapping\nkey []byte\n}\n```\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\nspace it will access and `Value.key` defines the exact key for the reference.\nWe will define a type named `Mapping`:\n```go\ntype Mapping struct {\nstoreKey sdk.StoreKey\ncdc      *codec.LegacyAmino\nprefix   []byte\n}\n```\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\nWe will define the following core methods for the `Value` type:\n```go\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Value) Get(ctx Context, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\n// Get stored data as raw byte slice\nfunc (Value) GetRaw(ctx Context) []byte {}\n// Marshal and set a raw value\nfunc (Value) Set(ctx Context, o interface{}) {}\n// Check if a raw value exists\nfunc (Value) Exists(ctx Context) bool {}\n// Delete a raw value value\nfunc (Value) Delete(ctx Context) {}\n```\nWe will define the following core methods for the `Mapping` type:\n```go\n// Constructs key-value pair reference corresponding to the key argument in the Mapping space\nfunc (Mapping) Value(key []byte) Value {}\n// Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\n// Get and unmarshal stored data, return error if not exists or cannot unmarshal\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\n// Get stored data as raw byte slice\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\n// Marshal and set a raw value\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\n// Check if a raw value exists\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\n// Delete a raw value value\nfunc (Mapping) Delete(ctx Context, key []byte) {}\n```\nEach method of the `Mapping` type that is passed the arguments `ctx`, `key`, and `args...` will proxy\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\nIn addition, we will define and provide a common set of types derived from the `Value` type:\n```go\ntype Boolean struct { Value }\ntype Enum struct { Value }\ntype Integer struct { Value; enc IntEncoding }\ntype String struct { Value }\n// ...\n```\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\nin core methods are replaced by explicit return types.\nFinally, we will define a family of types derived from the `Mapping` type:\n```go\ntype Indexer struct {\nm   Mapping\nenc IntEncoding\n}\n```\nWhere the `key` argument in core method is typed.\nSome of the properties of the accessor types are:\n* State access happens only when a function which takes a `Context` as an argument is invoked\n* Accessor type structs give rights to access the state only that the struct is referring, no other\n* Marshalling/Unmarshalling happens implicitly within the core methods\n","The module state should be modeled as Rust structs, which are stored in a KV store as serialized bytes. The module structs should implement the `Codable` trait to allow for easy serialization/deserialization. Additionally, a module should only have access to the keyspace that it needs, and should not be able to access other modules' keyspaces. This can be enforced using the object capability theorem. Finally, the getter/setter functions should be defined as static functions, so that the reviewers can easily see which part of the state that the function is accessing."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nProtocol Buffers provide a basic [style guide](https://developers.google.com/protocol-buffers/docs/style)\nand [Buf](https://buf.build/docs/style-guide) builds upon that. To the\nextent possible, we want to follow industry accepted guidelines and wisdom for\nthe effective usage of protobuf, deviating from those only when there is clear\nrationale for our use case.\n### Adoption of `Any`\nThe adoption of `google.protobuf.Any` as the recommended approach for encoding\ninterface types (as opposed to `oneof`) makes package naming a central part\nof the encoding as fully-qualified message names now appear in encoded\nmessages.\n### Current Directory Organization\nThus far we have mostly followed [Buf's](https://buf.build) [DEFAULT](https://buf.build/docs/lint-checkers#default)\nrecommendations, with the minor deviation of disabling [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout)\nwhich although being convenient for developing code comes with the warning\nfrom Buf that:\n> you will have a very bad time with many Protobuf plugins across various languages if you do not do this\n### Adoption of gRPC Queries\nIn [ADR 021](adr-021-protobuf-query-encoding.md), gRPC was adopted for Protobuf\nnative queries. The full gRPC service path thus becomes a key part of ABCI query\npath. In the future, gRPC queries may be allowed from within persistent scripts\nby technologies such as CosmWasm and these query routes would be stored within\nscript binaries.\n\n## Decision\n","The goal of this ADR is to provide thoughtful naming conventions that:\n* encourage a good user experience for when users interact directly with\n.proto files and fully-qualified protobuf names\n* balance conciseness against the possibility of either over-optimizing (making\nnames too short and cryptic) or under-optimizing (just accepting bloated names\nwith lots of redundant information)\nThese guidelines are meant to act as a style guide for both the Cosmos SDK and\nthird-party modules.\nAs a starting point, we should adopt all of the [DEFAULT](https://buf.build/docs/lint-checkers#default)\ncheckers in [Buf's](https://buf.build) including [`PACKAGE_DIRECTORY_MATCH`](https://buf.build/docs/lint-checkers#file_layout),\nexcept:\n* [PACKAGE_VERSION_SUFFIX](https://buf.build/docs/lint-checkers#package_version_suffix)\n* [SERVICE_SUFFIX](https://buf.build/docs/lint-checkers#service_suffix)\nFurther guidelines to be described below.\n### Principles\n#### Concise and Descriptive Names\nNames should be descriptive enough to convey their meaning and distinguish\nthem from other names.\nGiven that we are using fully-qualifed names within\n`google.protobuf.Any` as well as within gRPC query routes, we should aim to\nkeep names concise, without going overboard. The general rule of thumb should\nbe if a shorter name would convey more or else the same thing, pick the shorter\nname.\nFor instance, `cosmos.bank.MsgSend` (19 bytes) conveys roughly the same information\nas `cosmos_sdk.x.bank.v1.MsgSend` (28 bytes) but is more concise.\nSuch conciseness makes names both more pleasant to work with and take up less\nspace within transactions and on the wire.\nWe should also resist the temptation to over-optimize, by making names\ncryptically short with abbreviations. For instance, we shouldn't try to\nreduce `cosmos.bank.MsgSend` to `csm.bk.MSnd` just to save a few bytes.\nThe goal is to make names **_concise but not cryptic_**.\n#### Names are for Clients First\nPackage and type names should be chosen for the benefit of users, not\nnecessarily because of legacy concerns related to the go code-base.\n#### Plan for Longevity\nIn the interests of long-term support, we should plan on the names we do\nchoose to be in usage for a long time, so now is the opportunity to make\nthe best choices for the future.\n### Versioning\n#### Guidelines on Stable Package Versions\nIn general, schema evolution is the way to update protobuf schemas. That means that new fields,\nmessages, and RPC methods are _added_ to existing schemas and old fields, messages and RPC methods\nare maintained as long as possible.\nBreaking things is often unacceptable in a blockchain scenario. For instance, immutable smart contracts\nmay depend on certain data schemas on the host chain. If the host chain breaks those schemas, the smart\ncontract may be irreparably broken. Even when things can be fixed (for instance in client software),\nthis often comes at a high cost.\nInstead of breaking things, we should make every effort to evolve schemas rather than just breaking them.\n[Buf](https://buf.build) breaking change detection should be used on all stable (non-alpha or beta) packages\nto prevent such breakage.\nWith that in mind, different stable versions (i.e. `v1` or `v2`) of a package should more or less be considered\ndifferent packages and this should be last resort approach for upgrading protobuf schemas. Scenarios where creating\na `v2` may make sense are:\n* we want to create a new module with similar functionality to an existing module and adding `v2` is the most natural\nway to do this. In that case, there are really just two different, but similar modules with different APIs.\n* we want to add a new revamped API for an existing module and it's just too cumbersome to add it to the existing package,\nso putting it in `v2` is cleaner for users. In this case, care should be made to not deprecate support for\n`v1` if it is actively used in immutable smart contracts.\n#### Guidelines on unstable (alpha and beta) package versions\nThe following guidelines are recommended for marking packages as alpha or beta:\n* marking something as `alpha` or `beta` should be a last resort and just putting something in the\nstable package (i.e. `v1` or `v2`) should be preferred\n* a package _should_ be marked as `alpha` _if and only if_ there are active discussions to remove\nor significantly alter the package in the near future\n* a package _should_ be marked as `beta` _if and only if_ there is an active discussion to\nsignificantly refactor/rework the functionality in the near future but not remove it\n* modules _can and should_ have types in both stable (i.e. `v1` or `v2`) and unstable (`alpha` or `beta`) packages.\n_`alpha` and `beta` should not be used to avoid responsibility for maintaining compatibility._\nWhenever code is released into the wild, especially on a blockchain, there is a high cost to changing things. In some\ncases, for instance with immutable smart contracts, a breaking change may be impossible to fix.\nWhen marking something as `alpha` or `beta`, maintainers should ask the questions:\n* what is the cost of asking others to change their code vs the benefit of us maintaining the optionality to change it?\n* what is the plan for moving this to `v1` and how will that affect users?\n`alpha` or `beta` should really be used to communicate ""changes are planned"".\nAs a case study, gRPC reflection is in the package `grpc.reflection.v1alpha`. It hasn't been changed since\n2017 and it is now used in other widely used software like gRPCurl. Some folks probably use it in production services\nand so if they actually went and changed the package to `grpc.reflection.v1`, some software would break and\nthey probably don't want to do that... So now the `v1alpha` package is more or less the de-facto `v1`. Let's not do that.\nThe following are guidelines for working with non-stable packages:\n* [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix)\n(ex. `v1alpha1`) _should_ be used for non-stable packages\n* non-stable packages should generally be excluded from breaking change detection\n* immutable smart contract modules (i.e. CosmWasm) _should_ block smart contracts/persistent\nscripts from interacting with `alpha`/`beta` packages\n#### Omit v1 suffix\nInstead of using [Buf's recommended version suffix](https://buf.build/docs/lint-checkers#package_version_suffix),\nwe can omit `v1` for packages that don't actually have a second version. This\nallows for more concise names for common use cases like `cosmos.bank.Send`.\nPackages that do have a second or third version can indicate that with `.v2`\nor `.v3`.\n### Package Naming\n#### Adopt a short, unique top-level package name\nTop-level packages should adopt a short name that is known to not collide with\nother names in common usage within the Cosmos ecosystem. In the near future, a\nregistry should be created to reserve and index top-level package names used\nwithin the Cosmos ecosystem. Because the Cosmos SDK is intended to provide\nthe top-level types for the Cosmos project, the top-level package name `cosmos`\nis recommended for usage within the Cosmos SDK instead of the longer `cosmos_sdk`.\n[ICS](https://github.com/cosmos/ics) specifications could consider a\nshort top-level package like `ics23` based upon the standard number.\n#### Limit sub-package depth\nSub-package depth should be increased with caution. Generally a single\nsub-package is needed for a module or a library. Even though `x` or `modules`\nis used in source code to denote modules, this is often unnecessary for .proto\nfiles as modules are the primary thing sub-packages are used for. Only items which\nare known to be used infrequently should have deep sub-package depths.\nFor the Cosmos SDK, it is recommended that we simply write `cosmos.bank`,\n`cosmos.gov`, etc. rather than `cosmos.x.bank`. In practice, most non-module\ntypes can go straight in the `cosmos` package or we can introduce a\n`cosmos.base` package if needed. Note that this naming _will not_ change\ngo package names, i.e. the `cosmos.bank` protobuf package will still live in\n`x/bank`.\n### Message Naming\nMessage type names should be as concise possible without losing clarity. `sdk.Msg`\ntypes which are used in transactions will retain the `Msg` prefix as that provides\nhelpful context.\n### Service and RPC Naming\n[ADR 021](adr-021-protobuf-query-encoding.md) specifies that modules should\nimplement a gRPC query service. We should consider the principle of conciseness\nfor query service and RPC names as these may be called from persistent script\nmodules such as CosmWasm. Also, users may use these query paths from tools like\n[gRPCurl](https://github.com/fullstorydev/grpcurl). As an example, we can shorten\n`/cosmos_sdk.x.bank.v1.QueryService/QueryBalance` to\n`/cosmos.bank.Query/Balance` without losing much useful information.\nRPC request and response types _should_ follow the `ServiceNameMethodNameRequest`/\n`ServiceNameMethodNameResponse` naming convention. i.e. for an RPC method named `Balance`\non the `Query` service, the request and response types would be `QueryBalanceRequest`\nand `QueryBalanceResponse`. This will be more self-explanatory than `BalanceRequest`\nand `BalanceResponse`.\n#### Use just `Query` for the query service\nInstead of [Buf's default service suffix recommendation](https://github.com/cosmos/cosmos-sdk/pull/6033),\nwe should simply use the shorter `Query` for query services.\nFor other types of gRPC services, we should consider sticking with Buf's\ndefault recommendation.\n#### Omit `Get` and `Query` from query service RPC names\n`Get` and `Query` should be omitted from `Query` service names because they are\nredundant in the fully-qualified name. For instance, `/cosmos.bank.Query/QueryBalance`\njust says `Query` twice without any new information.\n","**Decision:**\n\nWe will continue to follow the style guide for Protobuf and Buf to the extent feasible for our use case. However, we will deviate from these guidelines in certain situations, where there is clear rationale for doing so.\n\n**Rationale:**\n\nFollowing industry-accepted guidelines and wisdom for the effective usage of protobuf is important for ensuring that our code is readable, maintainable, and extensible. However, it is also important to deviate from these guidelines when necessary to accommodate our specific use case.\n\nIn the case of Protobuf Any, we will adopt it as the recommended approach for encoding interface types. This is because package naming is a central part of the encoding, and fully-qualified message names now appear in encoded messages.\n\nIn terms of directory organization, we will continue to follow Buf's DEFAULT recommendations, but we will deviate in one minor way. We will disable the `PACKAGE_DIRECTORY_MATCH` lint checker. This is because it is convenient for developing code, and the warning from Buf about potential issues with protobuf plugins is not a major concern for our use case.\n\nFinally, we will continue to support gRPC queries in Protobuf. This is because gRPC is a powerful RPC framework that provides a number of benefits over other approaches. Additionally, gRPC queries may be allowed from within persistent scripts by technologies such as CosmWasm in the future, and it is important to be prepared for this possibility."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, KVStore data can be remotely accessed through [Queries](https://github.com/cosmos/cosmos-sdk/blob/main/docs/build/building-modules/02-messages-and-queries.md#queries)\nwhich proceed either through Tendermint and the ABCI, or through the gRPC server.\nIn addition to these request/response queries, it would be beneficial to have a means of listening to state changes as they occur in real time.\n\n## Decision\n","We will modify the `CommitMultiStore` interface and its concrete (`rootmulti`) implementations and introduce a new `listenkv.Store` to allow listening to state changes in underlying KVStores. We don't need to listen to cache stores, because we can't be sure that the writes will be committed eventually, and the writes are duplicated in `rootmulti.Store` eventually, so we should only listen to `rootmulti.Store`.\nWe will introduce a plugin system for configuring and running streaming services that write these state changes and their surrounding ABCI message context to different destinations.\n### Listening\nIn a new file, `store/types/listening.go`, we will create a `MemoryListener` struct for streaming out protobuf encoded KV pairs state changes from a KVStore.\nThe `MemoryListener` will be used internally by the concrete `rootmulti` implementation to collect state changes from KVStores.\n```go\n// MemoryListener listens to the state writes and accumulate the records in memory.\ntype MemoryListener struct {\nstateCache []StoreKVPair\n}\n// NewMemoryListener creates a listener that accumulate the state writes in memory.\nfunc NewMemoryListener() *MemoryListener {\nreturn &MemoryListener{}\n}\n// OnWrite writes state change events to the internal cache\nfunc (fl *MemoryListener) OnWrite(storeKey StoreKey, key []byte, value []byte, delete bool) {\nfl.stateCache = append(fl.stateCache, StoreKVPair{\nStoreKey: storeKey.Name(),\nDelete:   delete,\nKey:      key,\nValue:    value,\n})\n}\n// PopStateCache returns the current state caches and set to nil\nfunc (fl *MemoryListener) PopStateCache() []StoreKVPair {\nres := fl.stateCache\nfl.stateCache = nil\nreturn res\n}\n```\nWe will also define a protobuf type for the KV pairs. In addition to the key and value fields this message\nwill include the StoreKey for the originating KVStore so that we can collect information from separate KVStores and determine the source of each KV pair.\n```protobuf\nmessage StoreKVPair {\noptional string store_key = 1; // the store key for the KVStore this pair originates from\nrequired bool set = 2; // true indicates a set operation, false indicates a delete operation\nrequired bytes key = 3;\nrequired bytes value = 4;\n}\n```\n### ListenKVStore\nWe will create a new `Store` type `listenkv.Store` that the `rootmulti` store will use to wrap a `KVStore` to enable state listening.\nWe will configure the `Store` with a `MemoryListener` which will collect state changes for output to specific destinations.\n```go\n// Store implements the KVStore interface with listening enabled.\n// Operations are traced on each core KVStore call and written to any of the\n// underlying listeners with the proper key and operation permissions\ntype Store struct {\nparent    types.KVStore\nlistener  *types.MemoryListener\nparentStoreKey types.StoreKey\n}\n// NewStore returns a reference to a new traceKVStore given a parent\n// KVStore implementation and a buffered writer.\nfunc NewStore(parent types.KVStore, psk types.StoreKey, listener *types.MemoryListener) *Store {\nreturn &Store{parent: parent, listener: listener, parentStoreKey: psk}\n}\n// Set implements the KVStore interface. It traces a write operation and\n// delegates the Set call to the parent KVStore.\nfunc (s *Store) Set(key []byte, value []byte) {\ntypes.AssertValidKey(key)\ns.parent.Set(key, value)\ns.listener.OnWrite(s.parentStoreKey, key, value, false)\n}\n// Delete implements the KVStore interface. It traces a write operation and\n// delegates the Delete call to the parent KVStore.\nfunc (s *Store) Delete(key []byte) {\ns.parent.Delete(key)\ns.listener.OnWrite(s.parentStoreKey, key, nil, true)\n}\n```\n### MultiStore interface updates\nWe will update the `CommitMultiStore` interface to allow us to wrap a `Memorylistener` to a specific `KVStore`.\nNote that the `MemoryListener` will be attached internally by the concrete `rootmulti` implementation.\n```go\ntype CommitMultiStore interface {\n...\n// AddListeners adds a listener for the KVStore belonging to the provided StoreKey\nAddListeners(keys []StoreKey)\n// PopStateCache returns the accumulated state change messages from MemoryListener\nPopStateCache() []StoreKVPair\n}\n```\n### MultiStore implementation updates\nWe will adjust the `rootmulti` `GetKVStore` method to wrap the returned `KVStore` with a `listenkv.Store` if listening is turned on for that `Store`.\n```go\nfunc (rs *Store) GetKVStore(key types.StoreKey) types.KVStore {\nstore := rs.stores[key].(types.KVStore)\nif rs.TracingEnabled() {\nstore = tracekv.NewStore(store, rs.traceWriter, rs.traceContext)\n}\nif rs.ListeningEnabled(key) {\nstore = listenkv.NewStore(store, key, rs.listeners[key])\n}\nreturn store\n}\n```\nWe will implement `AddListeners` to manage KVStore listeners internally and implement `PopStateCache`\nfor a means of retrieving the current state.\n```go\n// AddListeners adds state change listener for a specific KVStore\nfunc (rs *Store) AddListeners(keys []types.StoreKey) {\nlistener := types.NewMemoryListener()\nfor i := range keys {\nrs.listeners[keys[i]] = listener\n}\n}\n```\n```go\nfunc (rs *Store) PopStateCache() []types.StoreKVPair {\nvar cache []types.StoreKVPair\nfor _, ls := range rs.listeners {\ncache = append(cache, ls.PopStateCache()...)\n}\nsort.SliceStable(cache, func(i, j int) bool {\nreturn cache[i].StoreKey < cache[j].StoreKey\n})\nreturn cache\n}\n```\nWe will also adjust the `rootmulti` `CacheMultiStore` and `CacheMultiStoreWithVersion` methods to enable listening in\nthe cache layer.\n```go\nfunc (rs *Store) CacheMultiStore() types.CacheMultiStore {\nstores := make(map[types.StoreKey]types.CacheWrapper)\nfor k, v := range rs.stores {\nstore := v.(types.KVStore)\n// Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\n// set same listeners on cache store will observe duplicated writes.\nif rs.ListeningEnabled(k) {\nstore = listenkv.NewStore(store, k, rs.listeners[k])\n}\nstores[k] = store\n}\nreturn cachemulti.NewStore(rs.db, stores, rs.keysByName, rs.traceWriter, rs.getTracingContext())\n}\n```\n```go\nfunc (rs *Store) CacheMultiStoreWithVersion(version int64) (types.CacheMultiStore, error) {\n// ...\n// Wire the listenkv.Store to allow listeners to observe the writes from the cache store,\n// set same listeners on cache store will observe duplicated writes.\nif rs.ListeningEnabled(key) {\ncacheStore = listenkv.NewStore(cacheStore, key, rs.listeners[key])\n}\ncachedStores[key] = cacheStore\n}\nreturn cachemulti.NewStore(rs.db, cachedStores, rs.keysByName, rs.traceWriter, rs.getTracingContext()), nil\n}\n```\n### Exposing the data\n#### Streaming Service\nWe will introduce a new `ABCIListener` interface that plugs into the BaseApp and relays ABCI requests and responses\nso that the service can group the state changes with the ABCI requests.\n```go\n// baseapp/streaming.go\n// ABCIListener is the interface that we're exposing as a streaming service.\ntype ABCIListener interface {\n// ListenFinalizeBlock updates the streaming service with the latest FinalizeBlock messages\nListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error\n// ListenCommit updates the steaming service with the latest Commit messages and state changes\nListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []*StoreKVPair) error\n}\n```\n#### BaseApp Registration\nWe will add a new method to the `BaseApp` to enable the registration of `StreamingService`s:\n```go\n// SetStreamingService is used to set a streaming service into the BaseApp hooks and load the listeners into the multistore\nfunc (app *BaseApp) SetStreamingService(s ABCIListener) {\n// register the StreamingService within the BaseApp\n// BaseApp will pass BeginBlock, DeliverTx, and EndBlock requests and responses to the streaming services to update their ABCI context\napp.abciListeners = append(app.abciListeners, s)\n}\n```\nWe will add two new fields to the `BaseApp` struct:\n```go\ntype BaseApp struct {\n...\n// abciListenersAsync for determining if abciListeners will run asynchronously.\n// When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\n// When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\nabciListenersAsync bool\n// stopNodeOnABCIListenerErr halts the node when ABCI streaming service listening results in an error.\n// stopNodeOnABCIListenerErr=true must be paired with abciListenersAsync=false.\nstopNodeOnABCIListenerErr bool\n}\n```\n#### ABCI Event Hooks\nWe will modify the `FinalizeBlock` and `Commit` methods to pass ABCI requests and responses\nto any streaming service hooks registered with the `BaseApp`.\n```go\nfunc (app *BaseApp) FinalizeBlock(req abci.RequestFinalizeBlock) abci.ResponseFinalizeBlock {\nvar abciRes abci.ResponseFinalizeBlock\ndefer func() {\n// call the streaming service hook with the FinalizeBlock messages\nfor _, abciListener := range app.abciListeners {\nctx := app.finalizeState.ctx\nblockHeight := ctx.BlockHeight()\nif app.abciListenersAsync {\ngo func(req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) {\nif err := app.abciListener.FinalizeBlock(blockHeight, req, res); err != nil {\napp.logger.Error(""FinalizeBlock listening hook failed"", ""height"", blockHeight, ""err"", err)\n}\n}(req, abciRes)\n} else {\nif err := app.abciListener.ListenFinalizeBlock(blockHeight, req, res); err != nil {\napp.logger.Error(""FinalizeBlock listening hook failed"", ""height"", blockHeight, ""err"", err)\nif app.stopNodeOnABCIListenerErr {\nos.Exit(1)\n}\n}\n}\n}\n}()\n...\nreturn abciRes\n}\n```\n```go\nfunc (app *BaseApp) Commit() abci.ResponseCommit {\n...\nres := abci.ResponseCommit{\nData:         commitID.Hash,\nRetainHeight: retainHeight,\n}\n// call the streaming service hook with the Commit messages\nfor _, abciListener := range app.abciListeners {\nctx := app.deliverState.ctx\nblockHeight := ctx.BlockHeight()\nchangeSet := app.cms.PopStateCache()\nif app.abciListenersAsync {\ngo func(res abci.ResponseCommit, changeSet []store.StoreKVPair) {\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\napp.logger.Error(""ListenCommit listening hook failed"", ""height"", blockHeight, ""err"", err)\n}\n}(res, changeSet)\n} else {\nif err := app.abciListener.ListenCommit(ctx, res, changeSet); err != nil {\napp.logger.Error(""ListenCommit listening hook failed"", ""height"", blockHeight, ""err"", err)\nif app.stopNodeOnABCIListenerErr {\nos.Exit(1)\n}\n}\n}\n}\n...\nreturn res\n}\n```\n#### Go Plugin System\nWe propose a plugin architecture to load and run `Streaming` plugins and other types of implementations. We will introduce a plugin\nsystem over gRPC that is used to load and run Cosmos-SDK plugins. The plugin system uses [hashicorp/go-plugin](https://github.com/hashicorp/go-plugin).\nEach plugin must have a struct that implements the `plugin.Plugin` interface and an `Impl` interface for processing messages over gRPC.\nEach plugin must also have a message protocol defined for the gRPC service:\n```go\n// streaming/plugins/abci/{plugin_version}/interface.go\n// Handshake is a common handshake that is shared by streaming and host.\n// This prevents users from executing bad plugins or executing a plugin\n// directory. It is a UX feature, not a security feature.\nvar Handshake = plugin.HandshakeConfig{\nProtocolVersion:  1,\nMagicCookieKey:   ""ABCI_LISTENER_PLUGIN"",\nMagicCookieValue: ""ef78114d-7bdf-411c-868f-347c99a78345"",\n}\n// ListenerPlugin is the base struct for all kinds of go-plugin implementations\n// It will be included in interfaces of different Plugins\ntype ABCIListenerPlugin struct {\n// GRPCPlugin must still implement the Plugin interface\nplugin.Plugin\n// Concrete implementation, written in Go. This is only used for plugins\n// that are written in Go.\nImpl baseapp.ABCIListener\n}\nfunc (p *ListenerGRPCPlugin) GRPCServer(_ *plugin.GRPCBroker, s *grpc.Server) error {\nRegisterABCIListenerServiceServer(s, &GRPCServer{Impl: p.Impl})\nreturn nil\n}\nfunc (p *ListenerGRPCPlugin) GRPCClient(\n_ context.Context,\n_ *plugin.GRPCBroker,\nc *grpc.ClientConn,\n) (interface{}, error) {\nreturn &GRPCClient{client: NewABCIListenerServiceClient(c)}, nil\n}\n```\nThe `plugin.Plugin` interface has two methods `Client` and `Server`. For our GRPC service these are `GRPCClient` and `GRPCServer`\nThe `Impl` field holds the concrete implementation of our `baseapp.ABCIListener` interface written in Go.\nNote: this is only used for plugin implementations written in Go.\nThe advantage of having such a plugin system is that within each plugin authors can define the message protocol in a way that fits their use case.\nFor example, when state change listening is desired, the `ABCIListener` message protocol can be defined as below (*for illustrative purposes only*).\nWhen state change listening is not desired than `ListenCommit` can be omitted from the protocol.\n```protobuf\nsyntax = ""proto3"";\n...\nmessage Empty {}\nmessage ListenFinalizeBlockRequest {\nRequestFinalizeBlock  req = 1;\nResponseFinalizeBlock res = 2;\n}\nmessage ListenCommitRequest {\nint64                block_height = 1;\nResponseCommit       res          = 2;\nrepeated StoreKVPair changeSet    = 3;\n}\n// plugin that listens to state changes\nservice ABCIListenerService {\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\n}\n```\n```protobuf\n...\n// plugin that doesn't listen to state changes\nservice ABCIListenerService {\nrpc ListenFinalizeBlock(ListenFinalizeBlockRequest) returns (Empty);\nrpc ListenCommit(ListenCommitRequest) returns (Empty);\n}\n```\nImplementing the service above:\n```go\n// streaming/plugins/abci/{plugin_version}/grpc.go\nvar (\n_ baseapp.ABCIListener = (*GRPCClient)(nil)\n)\n// GRPCClient is an implementation of the ABCIListener and ABCIListenerPlugin interfaces that talks over RPC.\ntype GRPCClient struct {\nclient ABCIListenerServiceClient\n}\nfunc (m *GRPCClient) ListenFinalizeBlock(goCtx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\nctx := sdk.UnwrapSDKContext(goCtx)\n_, err := m.client.ListenDeliverTx(ctx, &ListenDeliverTxRequest{BlockHeight: ctx.BlockHeight(), Req: req, Res: res})\nreturn err\n}\nfunc (m *GRPCClient) ListenCommit(goCtx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\nctx := sdk.UnwrapSDKContext(goCtx)\n_, err := m.client.ListenCommit(ctx, &ListenCommitRequest{BlockHeight: ctx.BlockHeight(), Res: res, ChangeSet: changeSet})\nreturn err\n}\n// GRPCServer is the gRPC server that GRPCClient talks to.\ntype GRPCServer struct {\n// This is the real implementation\nImpl baseapp.ABCIListener\n}\nfunc (m *GRPCServer) ListenFinalizeBlock(ctx context.Context, req *ListenFinalizeBlockRequest) (*Empty, error) {\nreturn &Empty{}, m.Impl.ListenFinalizeBlock(ctx, req.Req, req.Res)\n}\nfunc (m *GRPCServer) ListenCommit(ctx context.Context, req *ListenCommitRequest) (*Empty, error) {\nreturn &Empty{}, m.Impl.ListenCommit(ctx, req.Res, req.ChangeSet)\n}\n```\nAnd the pre-compiled Go plugin `Impl`(*this is only used for plugins that are written in Go*):\n```go\n// streaming/plugins/abci/{plugin_version}/impl/plugin.go\n// Plugins are pre-compiled and loaded by the plugin system\n// ABCIListener is the implementation of the baseapp.ABCIListener interface\ntype ABCIListener struct{}\nfunc (m *ABCIListenerPlugin) ListenFinalizeBlock(ctx context.Context, req abci.RequestFinalizeBlock, res abci.ResponseFinalizeBlock) error {\n// send data to external system\n}\nfunc (m *ABCIListenerPlugin) ListenCommit(ctx context.Context, res abci.ResponseCommit, changeSet []store.StoreKVPair) error {\n// send data to external system\n}\nfunc main() {\nplugin.Serve(&plugin.ServeConfig{\nHandshakeConfig: grpc_abci_v1.Handshake,\nPlugins: map[string]plugin.Plugin{\n""grpc_plugin_v1"": &grpc_abci_v1.ABCIListenerGRPCPlugin{Impl: &ABCIListenerPlugin{}},\n},\n// A non-nil value here enables gRPC serving for this streaming...\nGRPCServer: plugin.DefaultGRPCServer,\n})\n}\n```\nWe will introduce a plugin loading system that will return `(interface{}, error)`.\nThis provides the advantage of using versioned plugins where the plugin interface and gRPC protocol change over time.\nIn addition, it allows for building independent plugin that can expose different parts of the system over gRPC.\n```go\nfunc NewStreamingPlugin(name string, logLevel string) (interface{}, error) {\nlogger := hclog.New(&hclog.LoggerOptions{\nOutput: hclog.DefaultOutput,\nLevel:  toHclogLevel(logLevel),\nName:   fmt.Sprintf(""plugin.%s"", name),\n})\n// We're a host. Start by launching the streaming process.\nenv := os.Getenv(GetPluginEnvKey(name))\nclient := plugin.NewClient(&plugin.ClientConfig{\nHandshakeConfig: HandshakeMap[name],\nPlugins:         PluginMap,\nCmd:             exec.Command(""sh"", ""-c"", env),\nLogger:          logger,\nAllowedProtocols: []plugin.Protocol{\nplugin.ProtocolNetRPC, plugin.ProtocolGRPC},\n})\n// Connect via RPC\nrpcClient, err := client.Client()\nif err != nil {\nreturn nil, err\n}\n// Request streaming plugin\nreturn rpcClient.Dispense(name)\n}\n```\nWe propose a `RegisterStreamingPlugin` function for the App to register `NewStreamingPlugin`s with the App's BaseApp.\nStreaming plugins can be of `Any` type; therefore, the function takes in an interface vs a concrete type.\nFor example, we could have plugins of `ABCIListener`, `WasmListener` or `IBCListener`. Note that `RegisterStreamingPluing` function\nis helper function and not a requirement. Plugin registration can easily be moved from the App to the BaseApp directly.\n```go\n// baseapp/streaming.go\n// RegisterStreamingPlugin registers streaming plugins with the App.\n// This method returns an error if a plugin is not supported.\nfunc RegisterStreamingPlugin(\nbApp *BaseApp,\nappOpts servertypes.AppOptions,\nkeys map[string]*types.KVStoreKey,\nstreamingPlugin interface{},\n) error {\nswitch t := streamingPlugin.(type) {\ncase ABCIListener:\nregisterABCIListenerPlugin(bApp, appOpts, keys, t)\ndefault:\nreturn fmt.Errorf(""unexpected plugin type %T"", t)\n}\nreturn nil\n}\n```\n```go\nfunc registerABCIListenerPlugin(\nbApp *BaseApp,\nappOpts servertypes.AppOptions,\nkeys map[string]*store.KVStoreKey,\nabciListener ABCIListener,\n) {\nasyncKey := fmt.Sprintf(""%s.%s.%s"", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIAsync)\nasync := cast.ToBool(appOpts.Get(asyncKey))\nstopNodeOnErrKey := fmt.Sprintf(""%s.%s.%s"", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIStopNodeOnErrTomlKey)\nstopNodeOnErr := cast.ToBool(appOpts.Get(stopNodeOnErrKey))\nkeysKey := fmt.Sprintf(""%s.%s.%s"", StreamingTomlKey, StreamingABCITomlKey, StreamingABCIKeysTomlKey)\nexposeKeysStr := cast.ToStringSlice(appOpts.Get(keysKey))\nexposedKeys := exposeStoreKeysSorted(exposeKeysStr, keys)\nbApp.cms.AddListeners(exposedKeys)\napp.SetStreamingManager(\nstoretypes.StreamingManager{\nABCIListeners: []storetypes.ABCIListener{abciListener},\nStopNodeOnErr: stopNodeOnErr,\n},\n)\n}\n```\n```go\nfunc exposeAll(list []string) bool {\nfor _, ele := range list {\nif ele == ""*"" {\nreturn true\n}\n}\nreturn false\n}\nfunc exposeStoreKeys(keysStr []string, keys map[string]*types.KVStoreKey) []types.StoreKey {\nvar exposeStoreKeys []types.StoreKey\nif exposeAll(keysStr) {\nexposeStoreKeys = make([]types.StoreKey, 0, len(keys))\nfor _, storeKey := range keys {\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\n}\n} else {\nexposeStoreKeys = make([]types.StoreKey, 0, len(keysStr))\nfor _, keyStr := range keysStr {\nif storeKey, ok := keys[keyStr]; ok {\nexposeStoreKeys = append(exposeStoreKeys, storeKey)\n}\n}\n}\n// sort storeKeys for deterministic output\nsort.SliceStable(exposeStoreKeys, func(i, j int) bool {\nreturn exposeStoreKeys[i].Name() < exposeStoreKeys[j].Name()\n})\nreturn exposeStoreKeys\n}\n```\nThe `NewStreamingPlugin` and `RegisterStreamingPlugin` functions are used to register a plugin with the App's BaseApp.\ne.g. in `NewSimApp`:\n```go\nfunc NewSimApp(\nlogger log.Logger,\ndb dbm.DB,\ntraceStore io.Writer,\nloadLatest bool,\nappOpts servertypes.AppOptions,\nbaseAppOptions ...func(*baseapp.BaseApp),\n) *SimApp {\n...\nkeys := sdk.NewKVStoreKeys(\nauthtypes.StoreKey, banktypes.StoreKey, stakingtypes.StoreKey,\nminttypes.StoreKey, distrtypes.StoreKey, slashingtypes.StoreKey,\ngovtypes.StoreKey, paramstypes.StoreKey, ibchost.StoreKey, upgradetypes.StoreKey,\nevidencetypes.StoreKey, ibctransfertypes.StoreKey, capabilitytypes.StoreKey,\n)\n...\n// register streaming services\nstreamingCfg := cast.ToStringMap(appOpts.Get(baseapp.StreamingTomlKey))\nfor service := range streamingCfg {\npluginKey := fmt.Sprintf(""%s.%s.%s"", baseapp.StreamingTomlKey, service, baseapp.StreamingPluginTomlKey)\npluginName := strings.TrimSpace(cast.ToString(appOpts.Get(pluginKey)))\nif len(pluginName) > 0 {\nlogLevel := cast.ToString(appOpts.Get(flags.FlagLogLevel))\nplugin, err := streaming.NewStreamingPlugin(pluginName, logLevel)\nif err != nil {\ntmos.Exit(err.Error())\n}\nif err := baseapp.RegisterStreamingPlugin(bApp, appOpts, keys, plugin); err != nil {\ntmos.Exit(err.Error())\n}\n}\n}\nreturn app\n```\n#### Configuration\nThe plugin system will be configured within an App's TOML configuration files.\n```toml\n# gRPC streaming\n[streaming]\n# ABCI streaming service\n[streaming.abci]\n# The plugin version to use for ABCI listening\nplugin = ""abci_v1""\n# List of kv store keys to listen to for state changes.\n# Set to [""*""] to expose all keys.\nkeys = [""*""]\n# Enable abciListeners to run asynchronously.\n# When abciListenersAsync=false and stopNodeOnABCIListenerErr=false listeners will run synchronized but will not stop the node.\n# When abciListenersAsync=true stopNodeOnABCIListenerErr will be ignored.\nasync = false\n# Whether to stop the node on message deliver error.\nstop-node-on-err = true\n```\nThere will be four parameters for configuring `ABCIListener` plugin: `streaming.abci.plugin`, `streaming.abci.keys`, `streaming.abci.async` and `streaming.abci.stop-node-on-err`.\n`streaming.abci.plugin` is the name of the plugin we want to use for streaming, `streaming.abci.keys` is a set of store keys for stores it listens to,\n`streaming.abci.async` is bool enabling asynchronous listening and `streaming.abci.stop-node-on-err` is a bool that stops the node when true and when operating\non synchronized mode `streaming.abci.async=false`. Note that `streaming.abci.stop-node-on-err=true` will be ignored if `streaming.abci.async=true`.\nThe configuration above support additional streaming plugins by adding the plugin to the `[streaming]` configuration section\nand registering the plugin with `RegisterStreamingPlugin` helper function.\nNote the that each plugin must include `streaming.{service}.plugin` property as it is a requirement for doing the lookup and registration of the plugin\nwith the App. All other properties are unique to the individual services.\n#### Encoding and decoding streams\nADR-038 introduces the interfaces and types for streaming state changes out from KVStores, associating this\ndata with their related ABCI requests and responses, and registering a service for consuming this data and streaming it to some destination in a final format.\nInstead of prescribing a final data format in this ADR, it is left to a specific plugin implementation to define and document this format.\nWe take this approach because flexibility in the final format is necessary to support a wide range of streaming service plugins. For example,\nthe data format for a streaming service that writes the data out to a set of files will differ from the data format that is written to a Kafka topic.\n",Create a new KVStore subscription API that allows clients to subscribe to changes made to a KVStore and receive notifications when those changes occur.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\nthe same serialization of a `SignDoc` as defined in\n[ADR-020](./adr-020-protobuf-transaction-encoding.md) without transmitting the\nserialization.\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https://github.com/cosmos/cosmos-sdk/blob/9e85e81e0e8140067dd893421290c191529c148c/proto/cosmos/tx/v1beta1/tx.proto#L30)\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https://github.com/cosmos/cosmos-sdk/blob/main/docs/architecture/adr-020-protobuf-transaction-encoding.md#transactions))\nby converting all [Tx](https://github.com/cosmos/cosmos-sdk/blob/9e85e81e0e8140067dd893421290c191529c148c/proto/cosmos/tx/v1beta1/tx.proto#L13)\nfields to bytes on the client side. This adds an additional manual\nstep when sending and signing transactions.\n### Decision\nThe following encoding scheme is to be used by other ADRs,\nand in particular for `SignDoc` serialization.\n\n## Decision\n","The following encoding scheme is to be used by other ADRs,\nand in particular for `SignDoc` serialization.\n","The following encoding scheme is to be used by other ADRs, and in particular for `SignDoc` serialization."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently `Msg` handlers in the Cosmos SDK do have return values that are placed in the `data` field of the response.\nThese return values, however, are not specified anywhere except in the golang handler code.\nIn early conversations [it was proposed](https://docs.google.com/document/d/1eEgYgvgZqLE45vETjhwIw4VOqK-5hwQtZtjVbiXnIGc/edit)\nthat `Msg` return types be captured using a protobuf extension field, ex:\n```protobuf\npackage cosmos.gov;\nmessage MsgSubmitProposal\noption (cosmos_proto.msg_return) = “uint64”;\nstring delegator_address = 1;\nstring validator_address = 2;\nrepeated sdk.Coin amount = 3;\n}\n```\nThis was never adopted, however.\nHaving a well-specified return value for `Msg`s would improve client UX. For instance,\nin `x/gov`,  `MsgSubmitProposal` returns the proposal ID as a big-endian `uint64`.\nThis isn’t really documented anywhere and clients would need to know the internals\nof the Cosmos SDK to parse that value and return it to users.\nAlso, there may be cases where we want to use these return values programmatically.\nFor instance, https://github.com/cosmos/cosmos-sdk/issues/7093 proposes a method for\ndoing inter-module Ocaps using the `Msg` router. A well-defined return type would\nimprove the developer UX for this approach.\nIn addition, handler registration of `Msg` types tends to add a bit of\nboilerplate on top of keepers and is usually done through manual type switches.\nThis isn't necessarily bad, but it does add overhead to creating modules.\n\n## Decision\n","We decide to use protobuf `service` definitions for defining `Msg`s as well as\nthe code generated by them as a replacement for `Msg` handlers.\nBelow we define how this will look for the `SubmitProposal` message from `x/gov` module.\nWe start with a `Msg` `service` definition:\n```protobuf\npackage cosmos.gov;\nservice Msg {\nrpc SubmitProposal(MsgSubmitProposal) returns (MsgSubmitProposalResponse);\n}\n// Note that for backwards compatibility this uses MsgSubmitProposal as the request\n// type instead of the more canonical MsgSubmitProposalRequest\nmessage MsgSubmitProposal {\ngoogle.protobuf.Any content = 1;\nstring proposer = 2;\n}\nmessage MsgSubmitProposalResponse {\nuint64 proposal_id;\n}\n```\nWhile this is most commonly used for gRPC, overloading protobuf `service` definitions like this does not violate\nthe intent of the [protobuf spec](https://developers.google.com/protocol-buffers/docs/proto3#services) which says:\n> If you don’t want to use gRPC, it’s also possible to use protocol buffers with your own RPC implementation.\nWith this approach, we would get an auto-generated `MsgServer` interface:\nIn addition to clearly specifying return types, this has the benefit of generating client and server code. On the server\nside, this is almost like an automatically generated keeper method and could maybe be used instead of keepers eventually\n(see [\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093)):\n```go\npackage gov\ntype MsgServer interface {\nSubmitProposal(context.Context, *MsgSubmitProposal) (*MsgSubmitProposalResponse, error)\n}\n```\nOn the client side, developers could take advantage of this by creating RPC implementations that encapsulate transaction\nlogic. Protobuf libraries that use asynchronous callbacks, like [protobuf.js](https://github.com/protobufjs/protobuf.js#using-services)\ncould use this to register callbacks for specific messages even for transactions that include multiple `Msg`s.\nEach `Msg` service method should have exactly one request parameter: its corresponding `Msg` type. For example, the `Msg` service method `/cosmos.gov.v1beta1.Msg/SubmitProposal` above has exactly one request parameter, namely the `Msg` type `/cosmos.gov.v1beta1.MsgSubmitProposal`. It is important the reader understands clearly the nomenclature difference between a `Msg` service (a Protobuf service) and a `Msg` type (a Protobuf message), and the differences in their fully-qualified name.\nThis convention has been decided over the more canonical `Msg...Request` names mainly for backwards compatibility, but also for better readability in `TxBody.messages` (see [Encoding section](#encoding) below): transactions containing `/cosmos.gov.MsgSubmitProposal` read better than those containing `/cosmos.gov.v1beta1.MsgSubmitProposalRequest`.\nOne consequence of this convention is that each `Msg` type can be the request parameter of only one `Msg` service method. However, we consider this limitation a good practice in explicitness.\n### Encoding\nEncoding of transactions generated with `Msg` services do not differ from current Protobuf transaction encoding as defined in [ADR-020](./adr-020-protobuf-transaction-encoding.md). We are encoding `Msg` types (which are exactly `Msg` service methods' request parameters) as `Any` in `Tx`s which involves packing the\nbinary-encoded `Msg` with its type URL.\n### Decoding\nSince `Msg` types are packed into `Any`, decoding transactions messages are done by unpacking `Any`s into `Msg` types. For more information, please refer to [ADR-020](./adr-020-protobuf-transaction-encoding.md#transactions).\n### Routing\nWe propose to add a `msg_service_router` in BaseApp. This router is a key/value map which maps `Msg` types' `type_url`s to their corresponding `Msg` service method handler. Since there is a 1-to-1 mapping between `Msg` types and `Msg` service method, the `msg_service_router` has exactly one entry per `Msg` service method.\nWhen a transaction is processed by BaseApp (in CheckTx or in DeliverTx), its `TxBody.messages` are decoded as `Msg`s. Each `Msg`'s `type_url` is matched against an entry in the `msg_service_router`, and the respective `Msg` service method handler is called.\nFor backward compatibility, the old handlers are not removed yet. If BaseApp receives a legacy `Msg` with no corresponding entry in the `msg_service_router`, it will be routed via its legacy `Route()` method into the legacy handler.\n### Module Configuration\nIn [ADR 021](./adr-021-protobuf-query-encoding.md), we introduced a method `RegisterQueryService`\nto `AppModule` which allows for modules to register gRPC queriers.\nTo register `Msg` services, we attempt a more extensible approach by converting `RegisterQueryService`\nto a more generic `RegisterServices` method:\n```go\ntype AppModule interface {\nRegisterServices(Configurator)\n...\n}\ntype Configurator interface {\nQueryServer() grpc.Server\nMsgServer() grpc.Server\n}\n// example module:\nfunc (am AppModule) RegisterServices(cfg Configurator) {\ntypes.RegisterQueryServer(cfg.QueryServer(), keeper)\ntypes.RegisterMsgServer(cfg.MsgServer(), keeper)\n}\n```\nThe `RegisterServices` method and the `Configurator` interface are intended to\nevolve to satisfy the use cases discussed in [\#7093](https://github.com/cosmos/cosmos-sdk/issues/7093)\nand [\#7122](https://github.com/cosmos/cosmos-sdk/issues/7421).\nWhen `Msg` services are registered, the framework _should_ verify that all `Msg` types\nimplement the `sdk.Msg` interface and throw an error during initialization rather\nthan later when transactions are processed.\n### `Msg` Service Implementation\nJust like query services, `Msg` service methods can retrieve the `sdk.Context`\nfrom the `context.Context` parameter method using the `sdk.UnwrapSDKContext`\nmethod:\n```go\npackage gov\nfunc (k Keeper) SubmitProposal(goCtx context.Context, params *types.MsgSubmitProposal) (*MsgSubmitProposalResponse, error) {\nctx := sdk.UnwrapSDKContext(goCtx)\n...\n}\n```\nThe `sdk.Context` should have an `EventManager` already attached by BaseApp's `msg_service_router`.\nSeparate handler definition is no longer needed with this approach.\n","**Adopt a protobuf extension field to specify `Msg` return types.**\n\n**Rationale:**\n\n* **Consistency with protobuf best practices:** Protobuf extensions are a well-supported mechanism for extending protobuf messages. By using an extension field, we can specify return types in a consistent and structured manner.\n* **Improved client UX:** A well-defined return value for `Msg`s will make it easier for clients to parse and return the value to users. This will enhance the overall user experience.\n* **Programmability:** A well-defined return type will enable programmatic use of return values. This will be useful for features such as inter-module Ocaps.\n* **Reduced boilerplate:** Using a protobuf extension field for return types will reduce the boilerplate code for handler registration, simplifying the creation of modules."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn order to make blockchain transactions, the signing account must possess a sufficient balance of the right denomination\nin order to pay fees. There are classes of transactions where needing to maintain a wallet with sufficient fees is a\nbarrier to adoption.\nFor instance, when proper permissions are setup, someone may temporarily delegate the ability to vote on proposals to\na ""burner"" account that is stored on a mobile phone with only minimal security.\nOther use cases include workers tracking items in a supply chain or farmers submitting field data for analytics\nor compliance purposes.\nFor all of these use cases, UX would be significantly enhanced by obviating the need for these accounts to always\nmaintain the appropriate fee balance. This is especially true if we wanted to achieve enterprise adoption for something\nlike supply chain tracking.\nWhile one solution would be to have a service that fills up these accounts automatically with the appropriate fees, a better UX\nwould be provided by allowing these accounts to pull from a common fee pool account with proper spending limits.\nA single pool would reduce the churn of making lots of small ""fill up"" transactions and also more effectively leverages\nthe resources of the organization setting up the pool.\n\n## Decision\n","As a solution we propose a module, `x/feegrant` which allows one account, the ""granter"" to grant another account, the ""grantee""\nan allowance to spend the granter's account balance for fees within certain well-defined limits.\nFee allowances are defined by the extensible `FeeAllowanceI` interface:\n```go\ntype FeeAllowanceI {\n// Accept can use fee payment requested as well as timestamp of the current block\n// to determine whether or not to process this. This is checked in\n// Keeper.UseGrantedFees and the return values should match how it is handled there.\n//\n// If it returns an error, the fee payment is rejected, otherwise it is accepted.\n// The FeeAllowance implementation is expected to update it's internal state\n// and will be saved again after an acceptance.\n//\n// If remove is true (regardless of the error), the FeeAllowance will be deleted from storage\n// (eg. when it is used up). (See call to RevokeFeeAllowance in Keeper.UseGrantedFees)\nAccept(ctx sdk.Context, fee sdk.Coins, msgs []sdk.Msg) (remove bool, err error)\n// ValidateBasic should evaluate this FeeAllowance for internal consistency.\n// Don't allow negative amounts, or negative periods for example.\nValidateBasic() error\n}\n```\nTwo basic fee allowance types, `BasicAllowance` and `PeriodicAllowance` are defined to support known use cases:\n```protobuf\n// BasicAllowance implements FeeAllowanceI with a one-time grant of tokens\n// that optionally expires. The delegatee can use up to SpendLimit to cover fees.\nmessage BasicAllowance {\n// spend_limit specifies the maximum amount of tokens that can be spent\n// by this allowance and will be updated as tokens are spent. If it is\n// empty, there is no spend limit and any amount of coins can be spent.\nrepeated cosmos_sdk.v1.Coin spend_limit = 1;\n// expiration specifies an optional time when this allowance expires\ngoogle.protobuf.Timestamp expiration = 2;\n}\n// PeriodicAllowance extends FeeAllowanceI to allow for both a maximum cap,\n// as well as a limit per time period.\nmessage PeriodicAllowance {\nBasicAllowance basic = 1;\n// period specifies the time duration in which period_spend_limit coins can\n// be spent before that allowance is reset\ngoogle.protobuf.Duration period = 2;\n// period_spend_limit specifies the maximum number of coins that can be spent\n// in the period\nrepeated cosmos_sdk.v1.Coin period_spend_limit = 3;\n// period_can_spend is the number of coins left to be spent before the period_reset time\nrepeated cosmos_sdk.v1.Coin period_can_spend = 4;\n// period_reset is the time at which this period resets and a new one begins,\n// it is calculated from the start time of the first transaction after the\n// last period ended\ngoogle.protobuf.Timestamp period_reset = 5;\n}\n```\nAllowances can be granted and revoked using `MsgGrantAllowance` and `MsgRevokeAllowance`:\n```protobuf\n// MsgGrantAllowance adds permission for Grantee to spend up to Allowance\n// of fees from the account of Granter.\nmessage MsgGrantAllowance {\nstring granter = 1;\nstring grantee = 2;\ngoogle.protobuf.Any allowance = 3;\n}\n// MsgRevokeAllowance removes any existing FeeAllowance from Granter to Grantee.\nmessage MsgRevokeAllowance {\nstring granter = 1;\nstring grantee = 2;\n}\n```\nIn order to use allowances in transactions, we add a new field `granter` to the transaction `Fee` type:\n```protobuf\npackage cosmos.tx.v1beta1;\nmessage Fee {\nrepeated cosmos.base.v1beta1.Coin amount = 1;\nuint64 gas_limit = 2;\nstring payer = 3;\nstring granter = 4;\n}\n```\n`granter` must either be left empty or must correspond to an account which has granted\na fee allowance to fee payer (either the first signer or the value of the `payer` field).\nA new `AnteDecorator` named `DeductGrantedFeeDecorator` will be created in order to process transactions with `fee_payer`\nset and correctly deduct fees based on fee allowances.\n",**Decision:** Implement a common fee pool account with proper spending limits to allow signing accounts to pull from a shared resource for transaction fees.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https://github.com/cosmos/cosmos-sdk/issues/5467) and [4982](https://github.com/cosmos/cosmos-sdk/issues/4982) for additional context.\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\n\n## Decision\n","Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\n### Account interface (x/auth)\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\nnow be stored in & managed by the bank module.\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\naccount balance.\nVesting accounts will continue to store original vesting, delegated free, and delegated\nvesting coins (which is safe since these cannot contain arbitrary denominations).\n### Bank keeper (x/bank)\nThe following APIs will be added to the `x/bank` keeper:\n* `GetAllBalances(ctx Context, addr AccAddress) Coins`\n* `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\n* `SetBalance(ctx Context, addr AccAddress, coin Coin)`\n* `LockedCoins(ctx Context, addr AccAddress) Coins`\n* `SpendableCoins(ctx Context, addr AccAddress) Coins`\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\ncore functionality or persistence.\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\nbut retrieval of all balances for a single account is presumed to be more frequent):\n```go\nvar BalancesPrefix = []byte(""balances"")\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\nif !balance.IsValid() {\nreturn err\n}\nstore := ctx.KVStore(k.storeKey)\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\nbz := Marshal(balance)\naccountStore.Set([]byte(balance.Denom), bz)\nreturn nil\n}\n```\nThis will result in the balances being indexed by the byte representation of\n`balances/{address}/{denom}`.\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\naccount balance by denomination found in the (un)delegation amount. As a result,\nany mutations to the account balance by will made by denomination.\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\ndirectly instead of calling `GetCoins()` / `SetCoins()` (which no longer exist).\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\naccount balances.\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\npossible as to not load the entire account balance.\n### Supply module\nThe supply module, in order to implement the total supply invariant, will now need\nto scan all accounts & call `GetAllBalances` using the `x/bank` Keeper, then sum\nthe balances and check that they match the expected total supply.\n","Regarding the potential denial-of-service (DoS) concern resulting from a high number of denominations within the `sdk.Coins` structure, the decision is to:\n\n**Introduce a soft cap on the number of distinct denominations an account can hold.**\n\n**Rationale:**\n\n* Implementing a hard limit could still cause griefing vectors where a malicious actor could fill an account with a large number of different denominations to prevent the account holder from using the account.\n* A soft cap allows the modification of account balances using a bounded number of denominations, allowing the system to handle normal operations while mitigating the DoS risk.\n* This approach allows for flexibility and customization, as the soft cap can be adjusted if the system's capabilities evolve."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe concrete use cases which motivated this module include:\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\ndelegated stake\n* ""sub-keys"" functionality, as originally proposed in [\#4480](https://github.com/cosmos/cosmos-sdk/issues/4480) which\nis a term used to describe the functionality provided by this module together with\nthe `fee_grant` module from [ADR 029](./adr-029-fee-grant-module.md) and the [group module](https://github.com/cosmos/cosmos-sdk/tree/main/x/group).\nThe ""sub-keys"" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\nkeys.\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https://github.com/cosmos-gaians/cosmos-sdk/tree/hackatom/x/delegation).\n\n## Decision\n","We will create a module named `authz` which provides functionality for\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\nmust be granted for a particular `Msg` service methods one by one using an implementation\nof `Authorization` interface.\n### Types\nAuthorizations determine exactly what privileges are granted. They are extensible\nand can be defined for any `Msg` service method even outside of the module where\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\n#### Authorization\n```go\ntype Authorization interface {\nproto.Message\n// MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\n// which will process and accept or reject a request.\nMsgTypeURL() string\n// Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\n// so provides an upgraded authorization instance.\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\n// ValidateBasic does a simple validation check that\n// doesn't require access to any other information.\nValidateBasic() error\n}\n// AcceptResponse instruments the controller of an authz message if the request is accepted\n// and if it should be updated or deleted.\ntype AcceptResponse struct {\n// If Accept=true, the controller can accept and authorization and handle the update.\nAccept bool\n// If Delete=true, the controller must delete the authorization object and release\n// storage resources.\nDelete bool\n// Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\n// it must use the updated version and handle the update on the storage level.\nUpdated Authorization\n}\n```\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\na `SpendLimit` and updates it down to zero:\n```go\ntype SendAuthorization struct {\n// SpendLimit specifies the maximum amount of tokens that can be spent\n// by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\n// can be used with bank msg type url to create limit less bank authorization).\nSpendLimit sdk.Coins\n}\nfunc (a SendAuthorization) MsgTypeURL() string {\nreturn sdk.MsgTypeURL(&MsgSend{})\n}\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\nmSend, ok := msg.(*MsgSend)\nif !ok {\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(""type mismatch"")\n}\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\nif isNegative {\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(""requested amount is more than spend limit"")\n}\nif limitLeft.IsZero() {\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\n}\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\n}\n```\nA different type of capability for `MsgSend` could be implemented\nusing the `Authorization` interface with no need to change the underlying\n`bank` module.\n##### Small notes on `AcceptResponse`\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\nthe field will be `nil`.\n### `Msg` Service\n```protobuf\nservice Msg {\n// Grant grants the provided authorization to the grantee on the granter's\n// account with the provided expiration time.\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\n// Exec attempts to execute the provided messages using\n// authorizations granted to the grantee. Each message should have only\n// one signer corresponding to the granter of the authorization.\nrpc Exec(MsgExec) returns (MsgExecResponse);\n// Revoke revokes any authorization corresponding to the provided method name on the\n// granter's account that has been granted to the grantee.\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\n}\n// Grant gives permissions to execute\n// the provided method with expiration time.\nmessage Grant {\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = ""cosmos.authz.v1beta1.Authorization""];\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\n}\nmessage MsgGrant {\nstring granter = 1;\nstring grantee = 2;\nGrant grant = 3 [(gogoproto.nullable) = false];\n}\nmessage MsgExecResponse {\ncosmos.base.abci.v1beta1.Result result = 1;\n}\nmessage MsgExec {\nstring   grantee                  = 1;\n// Authorization Msg requests to execute. Each msg must implement Authorization interface\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = ""cosmos.base.v1beta1.Msg""];;\n}\n```\n### Router Middleware\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\nto the router based on `Authorization` grants:\n```go\ntype Keeper interface {\n// DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\n// to send those messages by the first (and only) signer of each msg.\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\n}\n```\n### CLI\n#### `tx exec` Method\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\nwould send a transaction like this:\n```go\nMsgExec {\nGrantee: mykey,\nMsgs: []sdk.Msg{\nMsgVote {\nProposalID: 1,\nVoter: cosmos3thsdgh983egh823\nOption: Yes\n}\n}\n}\n```\n#### `tx grant <grantee> <authorization> --from <granter>`\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\nJSON on the CLI.\n#### `tx revoke <grantee> <method-name> --from <granter>`\nThis CLI command will send a `MsgRevoke` transaction.\n### Built-in Authorizations\n#### `SendAuthorization`\n```protobuf\n// SendAuthorization allows the grantee to spend up to spend_limit coins from\n// the granter's account.\nmessage SendAuthorization {\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\n}\n```\n#### `GenericAuthorization`\n```protobuf\n// GenericAuthorization gives the grantee unrestricted permissions to execute\n// the provided method on behalf of the granter's account.\nmessage GenericAuthorization {\noption (cosmos_proto.implements_interface) = ""Authorization"";\n// Msg, identified by it's type URL, to grant unrestricted permissions to execute\nstring msg = 1;\n}\n```\n","The decision is to implement a delegation module that allows users to delegate their voting power to other accounts. This will enable users to cast votes on proposals without having to delegate their stake to those accounts. The module will also allow users to create ""sub-keys"" that have limited permissions, such as the ability to spend small amounts of funds or vote on proposals. This will allow users to grant specific permissions to other accounts without giving them full access to their accounts."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\n[runTx()](https://github.com/cosmos/cosmos-sdk/blob/bad4ca75f58b182f600396ca350ad844c18fc80b/baseapp/baseapp.go#L539)\nmethod. We think that this method can be more flexible and can give Cosmos SDK users more options for customizations without\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\nmight be handled in a ""standard"" way (middleware) alongside the others.\nWe propose middleware-solution, which could help developers implement the following cases:\n* add external logging (let's say sending reports to external services like [Sentry](https://sentry.io));\n* call panic for specific error cases;\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#recovery-middleware)).\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\nin order to customize it.\n\n## Decision\n","### Design\n#### Overview\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\ncan be found [here](https://github.com/cosmos/cosmos-sdk/pull/6053).\n#### Implementation details\n##### Recovery handler\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\n`recover()` from the `builtin` package.\n```go\ntype RecoveryHandler func(recoveryObj interface{}) error\n```\nHandler should type assert (or other methods) an object to define if object should be handled.\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\nAn example:\n```go\nfunc exampleErrHandler(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(error)\nif !ok { return nil }\nif someSpecificError.Is(err) {\npanic(customPanicMsg)\n} else {\nreturn nil\n}\n}\n```\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\n##### Recovery middleware\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\nchain processing.\n```go\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\nif err := handler(recoveryObj); err != nil {\nreturn nil, err\n}\nreturn next, nil\n}\n}\n```\nFunction receives a `recoveryObj` object and returns:\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\n`OutOfGas` middleware example:\n```go\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\nif !ok { return nil }\nreturn errorsmod.Wrap(\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\n""out of gas in location: %v; gasWanted: %d, gasUsed: %d"", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\n),\n)\n}\nreturn newRecoveryMiddleware(handler, next)\n}\n```\n`Default` middleware example:\n```go\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\nhandler := func(recoveryObj interface{}) error {\nreturn errorsmod.Wrap(\nsdkerrors.ErrPanic, fmt.Sprintf(""recovered: %v\nstack:\n%v"", recoveryObj, string(debug.Stack())),\n)\n}\nreturn newRecoveryMiddleware(handler, nil)\n}\n```\n##### Recovery processing\nBasic chain of middlewares processing would look like:\n```go\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\nif middleware == nil { return nil }\nnext, err := middleware(recoveryObj)\nif err != nil { return err }\nif next == nil { return nil }\nreturn processRecovery(recoveryObj, next)\n}\n```\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\n`default` handler which must return an `error`.\n##### BaseApp changes\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\n```go\ntype BaseApp struct {\n// ...\nrunTxRecoveryMiddleware recoveryMiddleware\n}\nfunc NewBaseApp(...) {\n// ...\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\n}\nfunc (app *BaseApp) runTx(...) {\n// ...\ndefer func() {\nif r := recover(); r != nil {\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\nerr, result = processRecovery(r, recoveryMW), nil\n}\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\n}()\n// ...\n}\n```\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\n```go\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\nfor _, h := range handlers {\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\n}\n}\n```\nThis method would prepend handlers to an existing chain.\n","**Implement a middleware-based approach to error handling in BaseApp.**\n\n**Benefits:**\n\n* Allows developers to write custom error handlers for panic recovery.\n* Provides a more flexible and customizable error handling mechanism.\n* Enables specific cases like logging and panicking to be handled in a standard way.\n* Addresses the specific need for node crashing when node <-> sidecar communication breaks, ensuring deterministic state machine execution.\n\n**Implementation Details:**\n\n* Introduce a new `RecoveryHandler` interface with a `Handle` method, which takes a `Handler` function as an argument.\n* Add a new `Middleware` field to the `BaseApp` struct, which is a slice of `RecoveryHandler` middleware.\n* Update the `runTx` method to iterate over the `Middleware` slice and call the `Handle` method for each middleware.\n* Provide a default `RecoveryHandler` middleware that wraps the recovery object as an error and logs it.\n* Allow developers to write and register custom `RecoveryHandler` middleware to handle specific error cases.\n\n**Example Middleware Implementation:**\n\n```go\ntype outOfGasMiddleware struct{}\n\nfunc (oom outOfGasMiddleware) Handle(next sdk.Handler) sdk.Handler {\n    return func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n        res, err := next(ctx, msg)\n        if err == sdk.ErrorOutOfGas {\n            // Handle OutOfGas error\n        }\n        return res, err\n    }\n}\n```"
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https://github.com/tendermint/tendermint/issues/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\n\n## Decision\n","### Pseudo procedure for consensus key rotation\n* create new random consensus key.\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\n* start validating with new consensus key.\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\n### Considerations\n* consensus key mapping information management strategy\n* store history of each key mapping changes in the kvstore.\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\n* the state machine does not need any historical mapping information which is past more than unbonding period.\n* key rotation costs related to LCD and IBC\n* LCD and IBC will have traffic/computation burden when there exists frequent power changes\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\n* limits\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\n* parameters can be decided by governance and stored in genesis file.\n* key rotation fee\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\n* evidence module\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\n* abci.ValidatorUpdate\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\n* new genesis parameters in `staking` module\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\n### Workflow\n1. The validator generates a new consensus keypair.\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\n```go\ntype MsgRotateConsPubKey struct {\nValidatorAddress  sdk.ValAddress\nNewPubKey         crypto.PubKey\n}\n```\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\n4. `RotateConsPubKey`\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\n* checks if the signing account has enough balance to pay `KeyRotationFee`\n* pays `KeyRotationFee` to community fund\n* overwrites `NewPubKey` in `validator.ConsPubKey`\n* deletes old `ValidatorByConsAddr`\n* `SetValidatorByConsAddr` for `NewPubKey`\n* Add `ConsPubKeyRotationHistory` for tracking rotation\n```go\ntype ConsPubKeyRotationHistory struct {\nOperatorAddress         sdk.ValAddress\nOldConsPubKey           crypto.PubKey\nNewConsPubKey           crypto.PubKey\nRotatedHeight           int64\n}\n```\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\n```go\nabci.ValidatorUpdate{\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\nPower:  0,\n}\nabci.ValidatorUpdate{\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\nPower:  v.ConsensusPower(),\n}\n```\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\n* Note : All above features shall be implemented in `staking` module.\n","**Decision:** Implement a simple form of validator consensus key rotation feature in Cosmos SDK.\n\n**Rationale:**\n\n* The requested feature has been requested for a long time and is considered important for safer validator key management.\n* The implementation is relatively simple and does not require changes to Tendermint's consensus logic.\n* This proposal does not include the more complex concept of multiple consensus keys, which will be considered as a long-term goal."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\n[Our platform](http://github.com/ovrclk/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https://github.com/ovrclk/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\n\n## Decision\n","**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\n```go\n// types/events.go\n// EmitTypedEvent takes typed event and emits converting it into sdk.Event\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\nevtType := proto.MessageName(event)\nevtJSON, err := codec.ProtoMarshalJSON(event)\nif err != nil {\nreturn err\n}\nvar attrMap map[string]json.RawMessage\nerr = json.Unmarshal(evtJSON, &attrMap)\nif err != nil {\nreturn err\n}\nvar attrs []abci.EventAttribute\nfor k, v := range attrMap {\nattrs = append(attrs, abci.EventAttribute{\nKey:   []byte(k),\nValue: v,\n})\n}\nem.EmitEvent(Event{\nType:       evtType,\nAttributes: attrs,\n})\nreturn nil\n}\n// ParseTypedEvent converts abci.Event back to typed event\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\nconcreteGoType := proto.MessageType(event.Type)\nif concreteGoType == nil {\nreturn nil, fmt.Errorf(""failed to retrieve the message of type %q"", event.Type)\n}\nvar value reflect.Value\nif concreteGoType.Kind() == reflect.Ptr {\nvalue = reflect.New(concreteGoType.Elem())\n} else {\nvalue = reflect.Zero(concreteGoType)\n}\nprotoMsg, ok := value.Interface().(proto.Message)\nif !ok {\nreturn nil, fmt.Errorf(""%q does not implement proto.Message"", event.Type)\n}\nattrMap := make(map[string]json.RawMessage)\nfor _, attr := range event.Attributes {\nattrMap[string(attr.Key)] = attr.Value\n}\nattrBytes, err := json.Marshal(attrMap)\nif err != nil {\nreturn nil, err\n}\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\nif err != nil {\nreturn nil, err\n}\nreturn protoMsg, nil\n}\n```\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\n**Step-2**: Add proto definitions for typed events for msgs in each module:\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\n```protobuf\n// proto/cosmos/gov/v1beta1/gov.proto\n// Add typed event definition\npackage cosmos.gov.v1beta1;\nmessage EventSubmitProposal {\nstring from_address   = 1;\nuint64 proposal_id    = 2;\nTextProposal proposal = 3;\n}\n```\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\n```go\n// x/gov/handler.go\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\n...\ntypes.Context.EventManager().EmitTypedEvent(\n&EventSubmitProposal{\nFromAddress: fromAddress,\nProposalId: id,\nProposal: proposal,\n},\n)\n...\n}\n```\n### How to subscribe to these typed events in `Client`\n> NOTE: Full code example below\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\nAkash Network has built a simple [`pubsub`](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/pubsub/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https://github.com/ovrclk/akash/blob/90d258caeb933b611d575355b8df281208a214f8/events/publish.go#L21) them as typed events.\nPlease see the below code sample for more detail on this flow looks for clients.\n","The proposal to implement typed events in the Cosmos SDK is accepted. This will enable Cosmos SDK applications to quickly and easily build event-driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nCurrently, an address can cast a vote with only one options (Yes/No/Abstain/NoWithVeto) and use their full voting power behind that choice.\nHowever, often times the entity owning that address might not be a single individual.  For example, a company might have different stakeholders who want to vote differently, and so it makes sense to allow them to split their voting power.  Another example use case is exchanges.  Many centralized exchanges often stake a portion of their users' tokens in their custody.  Currently, it is not possible for them to do ""passthrough voting"" and giving their users voting rights over their tokens.  However, with this system, exchanges can poll their users for voting preferences, and then vote on-chain proportionally to the results of the poll.\n\n## Decision\n","We modify the vote structs to be\n```go\ntype WeightedVoteOption struct {\nOption string\nWeight sdk.Dec\n}\ntype Vote struct {\nProposalID int64\nVoter      sdk.Address\nOptions    []WeightedVoteOption\n}\n```\nAnd for backwards compatibility, we introduce `MsgVoteWeighted` while keeping `MsgVote`.\n```go\ntype MsgVote struct {\nProposalID int64\nVoter      sdk.Address\nOption     Option\n}\ntype MsgVoteWeighted struct {\nProposalID int64\nVoter      sdk.Address\nOptions    []WeightedVoteOption\n}\n```\nThe `ValidateBasic` of a `MsgVoteWeighted` struct would require that\n1. The sum of all the Rates is equal to 1.0\n2. No Option is repeated\nThe governance tally function will iterate over all the options in a vote and add to the tally the result of the voter's voting power * the rate for that option.\n```go\ntally() {\nresults := map[types.VoteOption]sdk.Dec\nfor _, vote := range votes {\nfor i, weightedOption := range vote.Options {\nresults[weightedOption.Option] += getVotingPower(vote.voter) * weightedOption.Weight\n}\n}\n}\n```\nThe CLI command for creating a multi-option vote would be as such:\n```shell\nsimd tx gov vote 1 ""yes=0.6,no=0.3,abstain=0.05,no_with_veto=0.05"" --from mykey\n```\nTo create a single-option vote a user can do either\n```shell\nsimd tx gov vote 1 ""yes=1"" --from mykey\n```\nor\n```shell\nsimd tx gov vote 1 yes --from mykey\n```\nto maintain backwards compatibility.\n","**Decision:**\n\n**Implement a voting system that allows addresses to split their voting power across multiple options.**\n\n**Justification:**\n\n* **Supports diverse stakeholder interests:** Allows entities with multiple stakeholders to cast proportional votes based on their preferences.\n* **Enables passthrough voting:** Facilitates exchanges and other custodians to aggregate user preferences and vote proportionally on their behalf.\n* **Promotes flexibility and granularity:** Provides fine-grained control over vote allocation, allowing users to express nuanced preferences.\n* **Improves representation and inclusivity:** Ensures that all stakeholders have a fair opportunity to participate in decision-making.\n\n**Consequences:**\n\n* **Increased complexity:** Introducing splitting of voting power requires careful design and implementation to avoid potential confusion or abuse.\n* **Potential for fragmentation:** Allowing multiple options per address could lead to dispersed voting power and difficulty in reaching consensus.\n* **Security considerations:** Implementation must ensure proper validation of vote splits to prevent double-counting or fraudulent voting."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nPreviously as part of the xpub project we used `Formik` as our form framework of choice. This relied on using Wrapper components and HOC's for form/ input control. Between then and now React has introduced [Hooks](https://reactjs.org/blog/2019/02/06/react-v16.8.0.html) which many libraries, including `Formik` have implemented to provide easier to use APIs allowing for more readable and concise components. A number of new form frameworks built entirely around hooks have also emerged which we felt we should also consider before settling on a library.\n#### formik\nhttps://github.com/libero/reviewer-client/issues/83\nFormik seems to have improved a lot since we last worked with it. The introduction of hooks has made implementation simpler and more readable and some of the performance issues we had encountered relating to unnecessary re-renders when using the `Field` component have been fixed.\n`Formik` is quite a mature library with its initial release &gt; 3 years ago. The community is very active and there are plenty of resources / examples out there of different use cases.\n#### unform\nThis is a relatively new library first being published 9 months ago. We didn't do to much investigating here as the issues backlog and tagged release dates hint at the community not being widely active and updates not being too frequent (last updated 4 months ago as of writing this).\nUnform offers hooks however to register your own form components you need to create your own refs and use `useEffect` to register fields. This makes implementation code more verbose and complicated to follow.\nUnform doesn't include any extra dependencies and is quite light weight but this is probably down to it missing some key features which the other libraries do offer (see [RFC](https://github.com/Rocketseat/unform/issues/118))\n#### react-hook-form\nhttps://github.com/libero/reviewer-client/issues/84\nDesigned entirely around hooks and with performance in mind `react-hook-form` uses refs to the DOM elements to maintain form state which makes registering form fields straight forward and is intended to limit any unnecessary re-renders. This may offer us more control over what is rendered in the DOM so we can be more accessibility conscious when developing components.\n`react-hook-form` is very light weight and ships with no dependencies. By default it only requires the use of a single `useForm` hook and then can be used with `form` and `input` elements by passing `ref` props. This helps to make form code clean and readable.\nThe library is relatively new first published 10 months ago. The community seems responsive and updates are released frequently.\n### Decision\nWe have decided to go with `react-hook-form` based on the following:\n- little difference in functionality between `formik` and `react-hook-form` but `react-hook-form` api made code easier to follow and slightly less verbose in most use-cases\n- `react-hook-form` has 0 dependencies and a slightly smaller bundle size.\n- seems to have a greater focus on implementation through hooks rather than `Input` components meaning we should find it easier to control what is rendered to the DOM (good for accessibility)\n\n## Decision\n",We have decided to go with `react-hook-form` based on the following:\n- little difference in functionality between `formik` and `react-hook-form` but `react-hook-form` api made code easier to follow and slightly less verbose in most use-cases\n- `react-hook-form` has 0 dependencies and a slightly smaller bundle size.\n- seems to have a greater focus on implementation through hooks rather than `Input` components meaning we should find it easier to control what is rendered to the DOM (good for accessibility)\n,Use `react-hook-form` as the form framework for the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nA microservices architecture is complex and we'll need to make many decisions.\nWe'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future.\nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.\n\n## Decision\n","We've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) format.\nLADR is text based and is lightweight enough to meet our needs.\nWe'll keep each LADR record in its own text file and manage the files like code.\nWe also considered the following alternative solutions:\n* Project management tooling (not selected, because we didn't want to install tools)\n* Informal or ""word of mouth"" record keeping (not reliable)\n",**Use an Architectural Decision Record (ADR) template to document important architectural decisions.**\n\n**Benefits:**\n\n* Provides a standardized format for documenting decisions\n* Facilitates easy revisions and tracking of changes\n* Ensures consistency and transparency in decision-making\n* Promotes knowledge sharing and understanding across the team\n* Allows for future review and re-evaluation of decisions without requiring additional software tools
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe resources, database, storage and API endpoints needs to be managed as a unit.\nWhen build our application with the Serverless framework we get CloudFormation templates that manages our infrastructure in AWS.\n\n## Decision\n",To help package the individual AWS resource into a serverless application we will use [Serverless](https://www.serverless.com/) framework.\nWe also considered the following alternative solutions:\n* Manual creation of resource in AWS throurgh the AWS Console.\n* AWS Serverless Application Model (AWS SAM)\n,"Use the Serverless framework to manage the infrastructure for the application. This will provide a consistent experience for managing the infrastructure, and will ensure that all resources are created and managed in a consistent way."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\n\n## Decision\n,"The frontend will be build with [React](https://reactjs.org/) and [Snowpack](https://www.snowpack.dev) will be used as build tool.\nThe application will be styled with [Tailwind CSS](https://tailwindcss.com)\nWe also considered the following alternative solutions:\n* Vanilla HTML would work for the scope of this application.\n* Vanilla CSS would work, but take more time to generate nice layouts.\n","**Decision:** Use a Progressive Web App (PWA)\n\n**Rationale:**\n\n* PWAs offer a responsive and device-agnostic experience, making them suitable for use on various devices.\n* They provide a native-like experience while being easy to install and update.\n* PWAs can be deployed and distributed through web browsers, eliminating the need for app stores or software installations.\n* They support offline caching, enabling users to access the API even without an internet connection.\n* PWAs integrate with the device's operating system, allowing for notifications, push messages, and other native features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n### Context\nComponents and Projects pages display entire contents of the database in one table. Reading data, generating HTML\npage, and rendering the table (which is paginated by datatables on the client) all take a long time with thousands of\nentities.\nDatatables supports server-side processing for displaying and paginating data. For that, the server must be able to\ntell datatables how many rows are in the table in total and to load a page of data starting from given index.\nHowever, CouchDB cookbook strongly discourages loading of data starting from some index because of performance\nconcerns. Instead, loading data starting from a specific key should be used. This is incompatible with what datatables\nrequires and also makes going to previous pages highly complicated.\nTo support sorting of the table by multiple columns would require creating a CouchDB view per column.\nIn addition, projects are filtered by visibility in backend after loading from CouchDB. This filtering cannot be\nimplemented in CouchDB. This complicates the matters even further with regards to pagination of projects table in\nbackend.\n### Decision\nWe will not use datatables' server-side processing as it's not worth the effort.\nWe will load only some number of latest components by default and let the user increase that number up to all\navailable components. We will make this choice sticky between sessions.\nWe will not change the projects table for now as the users have the option of loading only the projects from their\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\n### Status\nAccepted\n### Consequences\nUsers will not see some components that are already created in the system and may try to create the ""missing""\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\nthe total number of components in the system.\nLoading time of unfiltered projects table will still be slow with thousands of projects.\n\n## Decision\n","We will not use datatables' server-side processing as it's not worth the effort.\nWe will load only some number of latest components by default and let the user increase that number up to all\navailable components. We will make this choice sticky between sessions.\nWe will not change the projects table for now as the users have the option of loading only the projects from their\ngroup and are not disturbed much by the performance of the page when all projects are displayed.\n### Status\nAccepted\n### Consequences\nUsers will not see some components that are already created in the system and may try to create the ""missing""\ncomponents. Quick Filter will not help them find the component as it works on client-side only.\nTo really make sure that a component is not in the system, users will have to use Advanced Search.\nLoading time of the components page (with default settings) will improve dramatically and will be independent of\nthe total number of components in the system.\nLoading time of unfiltered projects table will still be slow with thousands of projects.\n",We will not use datatables' server-side processing as it's not worth the effort. We will load only some number of latest components by default and let the user increase that number up to all available components. We will make this choice sticky between sessions. We will not change the projects table for now as the users have the option of loading only the projects from their group and are not disturbed much by the performance of the page when all projects are displayed.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe have a bash script (`cx-server`), which orchestrates the Docker containers used by our Cx Server.\nUsing bash is inconvenient for Windows users, as Windows does not include bash by default.\nThere is options for running bash on Windows, such as the Windows Subsystem for Linux, but this is not trivial to setup and requires switching Windows to Developer Mode.\nOther options include running a virtual machine locally, or connecting to a remove Linux system, but both are not always possible and have too much overhead.\nRecently, we added a ""companion"" Docker image which is used by `cx-server` to run scripts.\nUnrelated, the idea was born to move `cx-server` into this image, so the remaining `cx-server` is a very thin wrapper which can also be added as a Windows compatible script file.\n\n## Decision\n","We move the bash script inside the `s4sdk/cxserver-companion` Docker image.\nThe old `cx-server` script just delegates the command to the script inside the companion container.\nA new `cx-server.bat` script is added, doing the same for Windows.\nWe don't use PowerShell to increase compatibility with Windows.\n","Move `cx-server` into the ""companion"" Docker image and provide a Windows compatible script file as a wrapper."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the ""build once"" principle.\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\nFor this document, the term ""release"" (noun) refers to a uniquely identifiable version of software.\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\nThe verb ""to release"" refers to the process of creating a new release.\nPart of this process is to determine the version number of the release candidate.\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http://maven.apache.org/maven-release/maven-release-plugin/).\nUsing this plugin does not satisfy our requirements as described below.\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\nArtifact uploads and deployments happen only for commits on the so co called ""productive branch"" (`master` by default).\nMaven's versioning schema appends `SNAPSHOT` suffix to versions which are not released.\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\n\n## Decision\n","We implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\nThis feature is enabled by default, but can be disabled.\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\nAn auto-created release does not imply creating a tag in the repository.\nCreating tags may be done manually to mark noteworthy versions by the developer.\n### Reasoning\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team's definition of done.\nMerging to `master` is implicitly approval for release.\n* Feature can be disabled: You might still have builds which don't follow this release approach.\nFor those, it must be possible to disable automatic versioning.\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\n* Don't create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\nIf we created tags automatically for each version, tags would be cluttered very quickly.\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\n","We build and test everything on the productive branch (`master` by default).\nThe artifact produced by the build step doesn't have a `SNAPSHOT` suffix in its version.\nEach commit on the productive branch produces a new release candidate.\nAt most one release candidate exists for any point in time.\nIf the build pipeline for a release candidate fails, the release candidate is discarded.\nIf the build pipeline for a release candidate succeeds, the artifact is deployed to the Cloud Platform and uploaded to the artifact repository.\nThe release candidate becomes a release.\nWe use version tags to identify releases.\nThe version tag is created as part of the build pipeline.\nIt contains the version of the release candidate and the Git hash of the commit from which the artifact was built.\nA version tag might look like `1.0.2-commit-hash`."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nDeploying artifacts to nexus was done with this plugin before, but a bug was reported.\nWhen reading a child pom without `version`, for example, it does not retrieve the information inherited from the parent pom.\nWe have to investigate alternatives.\n### Alternatives\n* [Apache Maven Deploy Plugin](http://maven.apache.org/plugins/maven-deploy-plugin/)\n* Maven lifecycle phase : deploy\n* [Nexus Artifact Uploader](https://wiki.jenkins.io/display/JENKINS/Nexus+Artifact+Uploader)\n### Pros and Cons\n#### Apache Maven Deploy Plugin (deploy:deploy-file)\nFor this option, we only consider the goal `deploy:deploy-file`.\n##### :+1:\n- Official maven plugin for deployment, which is perfect for any maven projects if you only care whether your artifacts are deployed correctly.\n##### :-1:\n- A list of parameters has to be generated before using the plugin, including `artifactId` and `version`, which is the same case as the `Nexus Artifact Uploader`.\n- Credential info has to be stored in the `settings.xml`, which introduces additional implementation.\nLet's assume users have saved all the credentials in the Jenkins server.\nWe may inject a list of `server` tags under the `servers` tag with credentials info into the global `settings.xml`.\nTo make it as secrets, `mvn --encrypt-master-password <password>` has to be executed afterwards.\n#### Maven lifecycle phase: deploy\nBy default, the maven lifecycle phase `deploy` binds to the goal `deploy:deploy` of the `Apache Maven Deploy Plugin`.\n##### :+1:\n- Same as the `Apache Maven Deploy Plugin`\n- You don't have to pass the parameters as `Apache Maven Deploy Plugin` and `Nexus Artifact Uploader`,\nbecause `package` phase is executed implicitly and makes the parameters ready before `deploy` phase.\n##### :-1:\n- Same case as the `Apache Maven Deploy Plugin` for handling credentials.\n- As a maven phase, a list of phases is triggered implicitly before this phase, including `compile`, `test` and `package`.\nTo follow the build-once principle, all these phases have to be skipped.\nHowever, it's not possible to skip some of the maven goals binding to certain phases.\nFor example, if the `<packaging>` tag of the `pom.xml` is set to `jar`, then the `jar:jar` goal of the [`Apache Maven JAR Plugin`](https://maven.apache.org/plugins/maven-jar-plugin/) is bound to `package` phase.\nUnfortunately, however, `Apache Maven JAR Plugin` does not provide an option to skip the the `jar:jar` goal.\n**This is the main reason why we cannot use this option.**\n#### Nexus Artifact Uploader\n##### :+1:\n- Without the pain of handling the credentials, which was mentioned above in `Apache Maven Deploy Plugin` section.\n- It's promising, when the plugin is used properly\n##### :-1:\n- Same as the `Apache Maven Deploy Plugin`. A list of parameters has to be prepared.\n### Decision\n`Nexus Artifact Uploader` is chosen, because:\n- `Maven lifecycle phase: deploy` does not meet our build-once principle.\n- `Nexus Artifact Uploader` has the same situation regarding parameters as `Apache Maven Deploy Plugin`, but can handle credentials as a Jenkins plugin.\n\n## Decision\n","`Nexus Artifact Uploader` is chosen, because:\n- `Maven lifecycle phase: deploy` does not meet our build-once principle.\n- `Nexus Artifact Uploader` has the same situation regarding parameters as `Apache Maven Deploy Plugin`, but can handle credentials as a Jenkins plugin.\n",Use `Nexus Artifact Uploader` to deploy artifacts to Nexus.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\n* **Flexibility** - We should be able to follow different patterns, like ""Given ... When .. Then"", but also like ""Does X"".\n\n## Decision\n","We use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\nGood:\ntest_given_first_time_visitor_then_return_main_banner\nBad:\ntest_given_first_time_visitor_return_main_banner\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\n","The team decided to adopt a naming scheme for unit test methods that prioritizes readability and flexibility. Specifically, the following guidelines will be followed:\n\n- Test method names should be written in sentence format, using clear word boundaries.\n- The ""Given ... When ... Then"" pattern is preferred for tests that follow a specific scenario.\n- Other patterns, such as ""Does X"", are acceptable as long as they are clear and concise.\n\nThis naming scheme will help ensure that our unit tests are easy to read and understand, regardless of the pattern that is being followed."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur current dns naming follows a couple of very similar patterns. Sometimes using 'gcp', 'frankfurt', 'oregon-b' as ways to separate different environments.  We should have one pattern and stick to it the best we can.\nThings the pattern needs to solve for:\n* Should be 'the same' for all of the meao services (for example: nucleus/bedrock/snippets).\n* should allow for multiple 'environments' of a service to be deployed in the same region ('prod'|'stg'|'dev')\n* should allow for multiple regions/deployments of the same service + environment ('or', 'fr', 'ia')\n* should also have a good 'user facing' pattern, that is not the same as the above pattern. (www.mozilla.org -> 'bedrock' 'prod' 'or' && 'bedrock' 'prod' 'fr' with some mechanism for choosing between the two deployments.)\n\n## Decision\n","For the backend deployments follow this pattern: 'service'.'environment'.'region'.'domain'. An incomplete list of each of examples of values for those variables:\n| Service  |\n|----------|\n| bedrock  |\n| nucleus  |\n| snippets |\n| prom     |\n| Environments |\n|--------------|\n| dev          |\n| stg          |\n| prod         |\n| demo1        |\n| Region | Description           |\n|--------|-----------------------|\n| or     | oregon eks cluster    |\n| fr     | frankfurt eks cluster |\n| ia     | iowa gcp cluster      |\n| Domain     |\n|------------|\n| moz.works  |\n| mozmar.org |\n| ramzom.org |\nThis leads to a few examples:\n| Examples                 |\n|--------------------------|\n| bedrock.dev.or.moz.works |\n| prom.prod.fr.mozmar.org  |\n| nucleus.stg.ia.moz.works |\nNote that these are for 'internal' use primarily.  The user facing domains will stay as they are.  A few examples, nucleus.mozilla.org (prod) and nucleus.allizom.org (stg), www.mozilla.org (bedrock prod) www.allizom.org (bedrock stg).  The connection between the new dns entries and the user facing will stay the same. (If we're using a r53 traffic policy now, we will continue to after this change, if we're just using cname/alias records we will again after this change,etc, including cloudflare vs cloudfront etc.)\n","Use a combination of a naming pattern and a user-facing pattern to provide a consistent and easy-to-use naming scheme for our DNS records.\n\n**Naming Pattern**\n\nThe naming pattern will be as follows:\n\n```\n{service}-{environment}-{region}\n```\n\nFor example, the production deployment of our nucleus service in the Oregon-B region would be named:\n\n```\nnucleus-prod-or-b\n```\n\nThis pattern meets all of the requirements outlined in the context:\n\n* It is the same for all of the meao services.\n* It allows for multiple environments of a service to be deployed in the same region.\n* It allows for multiple regions/deployments of the same service + environment.\n\n**User-Facing Pattern**\n\nThe user-facing pattern will be as follows:\n\n```\n{service}.{environment}.mozilla.org\n```\n\nFor example, the production deployment of our nucleus service would be accessible at:\n\n```\nnucleus.prod.mozilla.org\n```\n\nThis pattern is simple and easy to remember, and it provides a clear way to identify the service, environment, and region of a deployment.\n\n**Choosing Between Deployments**\n\nWhen multiple deployments of the same service and environment are available, users can choose between them using a variety of methods, such as:\n\n* DNS round-robin\n* Load balancing\n* Manual selection\n\nThe specific method used will depend on the specific needs of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nChanging networking can be hard.  It usually requires a full redeploy of all services and all infrastructure to make it 'real'.  Our current network has overlaps, which makes it more difficult to setup vpns, usually requring syncing of live IP addresses to their desired state.\n\n## Decision\n","Netops will reserve a /16 block of ips for mozmeao. For each VPC make a block of /20 ipv4 addresses.  Where VPC maps to a region within a cloud provider we use.  Divide that into /24 subnets, where we'll have just one subnet per AZ.\nOur /16 is - 10.154.0.0/16\nFor example, in oregon, the network would look like:\n| Label           | CIDR          | Range Start  | Range End      | Description                     |\n|-----------------|---------------|--------------|----------------|---------------------------------|\n| Oregon VPC      | 10.154.0.0/20 | 10.154.0.1   | 10.154.15.254  | A large block for the whole VPC |\n| Oregon Subnet A | 10.154.0.0/24 | 10.154.0.1   | 10.154.0.254   | Subnet for oregon-a az          |\n| Oregon Subnet B | 10.154.1.0/24 | 10.154.1.1   | 10.154.1.254   | Subnet for oregon-b az          |\n| Oregon Subnet C | 10.154.2.0/24 | 10.154.2.1   | 10.154.3.254   | Subnet for oregon-c az          |\nand Frankfurt would be:\n| Label              | CIDR           | Range Start   | Range End       | Description                        |\n|--------------------|----------------|---------------|-----------------|------------------------------------|\n| Frankfurt VPC      | 10.154.16.0/20 | 10.154.31.1   | 10.154.255.254  | A large block for the whole VPC    |\n| Frankfurt Subnet A | 10.154.16.0/24 | 10.154.16.1   | 10.154.16.254   | Subnet for frankfurt-a az          |\n| Frankfurt Subnet B | 10.154.17.0/24 | 10.154.17.1   | 10.154.17.254   | Subnet for frankfurt-b az          |\n| Frankfurt Subnet C | 10.154.18.0/24 | 10.154.18.1   | 10.154.18.254   | Subnet for frankfurt-c az          |\nThe next few vpc blocks would be 10.154.32.0/20, 19.154.48.0/20, 19.154.128.0/20\nIn oregon we could continue with 10.154.3 and 10.154.4 until 15 for the subnets.  Essentially the same for frankfurt 10.154.19, 10.154.20.\n",Use a cloud-native networking platform to manage networking infrastructure.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to have load balancers between our CDNs and our services, in order to deal with k8s node failures (and for handing traffic, and as a place to log stuff, and as a way to block bad actors if needed).  We'd prefer to only pay AWS/GCP for a single load balancer thing, while still having the ability to generate unique dns addresses for each of our services.  They must be able to host certs correctly.\n\n## Decision\n","We're currently managing our ELBs in aws with some out of band terraform that connects the nodes of each k8s cluster to a load balancer. Since this is out of band, upgrading clusters or services implies doing a bunch of k8s stuff, and then also running terraform.  We could possibly simplify the whole experience by moving the full definition of the load balancer, and dns, and certs to objects inside k8s.  Voyager + External Ingress seem like the most common way to do this.  Deploying one 'ingress' object per group of services you want to have behind an ALB, and listing all the DNS to point at those services solves the problem outlined above.\nThe primary advantage of doing this work, is that it allows dynamic things (deployments created in response to events, such as pull requests and ephemeral 'demo' branches) to be created simply by writing the yaml and deploying it to kubernetes.\n","Utilize a managed load balancer service, such as AWS Elastic Load Balancing or Google Cloud Load Balancing, to provide the required functionality while optimizing cost and flexibility. This approach allows for a single load balancer to handle traffic for multiple services, while enabling the generation of unique DNS addresses for each service. Additionally, the managed service will handle certificate management, traffic logging, and the ability to block malicious traffic."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\n\n## Decision\n","Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev/staging/prod versions of the service.\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\n","The decision is to use Helm charts to manage Kubernetes deployments.\n\nHelm charts provide a way to package and manage Kubernetes applications. They define the resources that need to be created or updated in a Kubernetes cluster, and can be used to deploy applications in a consistent way across multiple environments.\n\nBy using Helm charts, we can reduce the amount of drift between different environments by ensuring that the same deployment configuration is used in each environment. We can also improve code reuse by defining our deployments in a single place, rather than having to create separate sets of yml files for each environment."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe want to have a flexible, easy to run, but not too expensive alerting and monitoring solution.  Since we are primarily kubernetes based, something built around that seems ideal.  We are in a little bit of a rush given that our current monitoring solution has yearly renewals, and that renewal would be coming up at the end of the month.\nPossible options include, influx cloud (v1, v2), influx we host (rejected mostly because v2 is not yet ready for primetime, and team has no influx familiarity).  New Relic, DataDog, HoneyComb are all great products, but are a bit expensive for us (saas prices for our data load seem to be too much).  Tools like Nagios are hard to run in the cloud because they assume mostly assume 100% network reliability.  Given all that, and our team's familiarity with prometheus, it seems like the best choice.\n\n## Decision\n","Run prometheus and alertmanager, on our clusters.  Run one prom per k8s cluster to collect metrics. Run one prom/grafana/alertmanager deployment to collect and display the information all in one place.  Send alerts to slack and/or pagerduty depending on severity.  Monitor this stack with dead man's snitch or other negative alerting services.\n",Use Prometheus as the primary alerting and monitoring solution.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMozilla IT SREs have built a VPC module at https://github.com/mozilla-it/itsre-deploy/tree/master/modules/vpc. This module does almost everything described in our previous ADR (#5), with one exception around public/private subnets. That difference will be described in this document.\n\n## Decision\n","We will create two subnets per az. One 'public' and one 'private'. In general this is mostly by convention, rather than something that is strictly enforced.  We may later validate instances in private are not accessible to the general public with automated auditing solution. But, for now, we'll just do our best to only assign public things to the public subnet, and everything else can go in private.\nThe one functional item for the subnets, 'public ips' will be assigned by default in the public subnets, but not the private subnets.\n",The Mozilla IT SRE VPC module will provide the following functionality:\n\n\n- **Create a VPC with the specified CIDR block.**\n- **Create a public subnet with the specified CIDR block and availability zone.**\n- **Create a private subnet with the specified CIDR block and availability zone.**\n- **Associate the specified public subnet with the VPC.**\n- **Associate the specified private subnet with the VPC.**\n- **Create a NAT Gateway in the specified public subnet.**\n- **Create a route table for the specified VPC.**\n- **Add a route to the specified route table for the specified destination CIDR block and next hop.**\n- **Associate the specified route table with the specified subnet.**\n- **Delete the specified VPC.**\n- **Delete the specified subnet.**\n- **Delete the specified NAT Gateway.**\n- **Delete the specified route table.**\n\nThe module will not provide the following functionality:\n\n\n- **Create a VPC with a CIDR block that overlaps with an existing VPC.**\n- **Create a subnet with a CIDR block that overlaps with an existing subnet in the same VPC.**\n- **Create a NAT Gateway in a private subnet.**\n- **Create a route table for a VPC that is not associated with the specified VPC.**\n- **Add a route to a route table for a destination CIDR block that is not within the VPC's CIDR block.**\n- **Associate a route table with a subnet that is not associated with the same VPC.**\n
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",We will use an Architectural Decision Record (ADR) template to document our architectural decisions. The template will include the following sections:\n\n\n1. Title: A brief title for the decision.\n2. Date: The date the decision was made.\n3. Context: A brief description of the context in which the decision was made.\n4. Decision: A clear statement of the decision that was made.\n5. Rationale: The reasons for making the decision.\n6. Consequences: The potential consequences of the decision.\n7. Trade-offs: The trade-offs that were considered before making the decision.\n8. Alternatives: The alternative options that were considered before making the decision.\n9. Related decisions: Any other architectural decisions that are related to this decision.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to\nupdate individual components such as container images used by JupyterHub. Nevertheless it is required to create\nreleases of ODH as a whole.\n\n## Decision\n,"Chosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\nproject/product level while maintianing freedom of others to update.\n### Positive Consequences <!-- optional -->\n* Operators of ODH have a clear base line of versions, these versions have been tested with each other and have\nundergone ODH integration testing.\n* Operators of ODH can update individual components, they could maintain a repository analog to odh-manifests declaring\nthe exact versions (container image tags, git repository tags) of components they want to deploy.\n* Operators can mix in their builds of container images following the method mentioned above.\n### Negative Consequences <!-- optional -->\n* An ODH release (a tag in the odh-manifests repository) might not contain the latest versions of components, for example\nsecurity updates might have forced the build of a S2I image used with JupyterHub component of ODH.\n<!-- markdownlint-disable-file MD013 -->\n",**Decision:** ODH will create releases for the platform as a whole as well as for individual components.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\nAnother option was Rust, which could be a useful learning exercise.\nPython is used in FIAAS, so we have some experience using it for an operator.\n\n## Decision\n",We will use Python for Kafkarator.\n,"We will use Python for this project, as we have some experience using it for an operator in FIAAS."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur application needs to supply some web endpoints for healthchecks, so we need to select a web framework.\nThere are a multitude to choose from, but we have some preferences:\n- We want something that is well maintained and has a community\n- We want something that has decent performance\n- As a learning experience, we want to use one of the many async frameworks\nThe front contenders seems to be Sanic, FastAPI, Tornado, Vibora and Quart.\nTornado seems to be somewhat dated, as it was created before Python had async built in.\nVibora is a relatively new project, with uncertain community adoption.\nQuart is API compatible with Flask, which is the most common non-async web framework.\nSanic seems to be the most popular of the async frameworks.\nFastAPI is by far the most performant of them all.\n\n## Decision\n","We will use FastAPI, because of the performance, and some of the interesting features.\n","We have decided to use **FastAPI** for our web framework.\n\n**Reasons:**\n\n* **High performance:** FastAPI is the most performant of the async frameworks we considered, which is important for our application's performance requirements.\n* **Well-maintained and active community:** FastAPI is a well-maintained project with an active community, ensuring that we will have support and resources available as we use it.\n* **Learning experience:** FastAPI is an async framework, which aligns with our goal of using this project as a learning experience.\n\n**Additional Considerations:**\n\n* **API compatibility with Flask:** While Quart offers API compatibility with Flask, we did not find this to be a strong enough reason to choose it over FastAPI given the other advantages it offers.\n* **Popularity of Sanic:** Sanic's popularity was a factor in our decision, but we ultimately prioritized performance and community support over popularity.\n* **Maturity of Vibora:** While Vibora is a promising project, its relatively new status and uncertain community adoption made us hesitant to choose it for our application.\n* **Age of Tornado:** Tornado's age and lack of built-in async support in Python led us to exclude it from our final decision."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n\n## Decision\n","Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\n",NAV will move Kafka to Aiven.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\nspace, and the Python community has not landed on a single standard. Each project needs to select which\ntools to use, usually a selection of tools that integrate well.\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\nclaims to support the use case well. It is also following the new pyproject.toml standard.\n\n## Decision\n",We will use Poetry for this project. (https://python-poetry.org/)\n,Use Poetry to manage Python dependencies in virtualenvs.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhile CRDs are a good API for Kubernetes Operators, they are not in common use for most developers, and a better\nuser experience is wanted.\nUsers today are used to creating an Application object to deploy their application. For users who are not owning a\ntopic, it would be easier if they can declare their topics as part of the Application resources. However, if we design\nour CRDs to be simple enough, this might not be a real problem.\nThere are other use cases where parts of the Application object are translated into a specific CRD handled by a separate\noperator. This model could also be used here. This will move some of the responsibility from the users to Naiserator,\nbut at the same time create an unnecessary coupling to Naiserator.\nBy making sure our flow is built around a set of CRDs, details of the UI can be worked on iteratively, refining the\nexperience as we get more experience.\nIn order to do this, we will need more than the two CRDs detailed in [6. Kafkarator API is focused around dedicated CRDs](0006-kafkarator-api-is-focused-around-dedicated-crds.md).\nThe flow can be separated into three distinct parts:\n- Topic creation\n- Getting topic access\n- Managing topic access\n### A suggested flow\n![Suggested sequence diagram](./0008-user-experience-flow.png)\n#### Topic creation\n1. A developer creates a Topic CRD in their team namespace, detailing the topic name and configuration (1)\n2. Kafkarator sees the Topic and\n1. Creates the topic in the Aiven cluster\n2. Creates a TopicAccess CRD object, recording the team as owner of this Topic\n#### Getting topic access\n1. A developer creates an AppTopic CRD in their team namespace, connecting an application with topics for either\nproducing or consuming (2)\n2. Kafkarator sees the AppTopic and\n1. If the application does not already have a service user in Aiven, it is created\n2. Downloads credentials for service user\n3. Creates a Kubernetes secret in the team namespace\n4. Adds an access request to the associated TopicAccess\n#### Managing topic access\n1. A topic owner edits the TopicAccess CRD to accept an access request (4)\n2. Kafkarator sees the TopicAccess change and\n1. Updates ACL for all relevant service users, granting or removing access to a Topic (5)\n#### Future work\n(1) It could be useful to create a UI for topic creation, with links to documentation for each option and easy\naccess to ""common"" options, but it does not seem worth the effort at this time.\n(2) As discussed, this could be done indirectly by specifying the topics in the Application CRD and letting Naiserator\ncreate the AppTopic CRD.\n(3) Application developers need to make sure their application requests access to the correct secrets to get the Aiven\ncredentials. This could be handled by Naiserator automatically, by simply mounting the secret in a predefined location\nif it exists.\n(4) Later iterations might want to provide a UI for teams to manage access, instead of editing CRDs directly. This would\nreduce the chance of errors, but requires proper access control in the UI which is already provided by Kubernetes.\n(5) Credentials for topic access is provided as service users per application, but we want access to be granted on a\nteam basis. This allows for limiting access to individual applications should the need arise at a later point.\n\n## Decision\n","- We will keep the CRDs as detailed in [6. Kafkarator API is focused around dedicated CRDs](0006-kafkarator-api-is-focused-around-dedicated-crds.md)\nwith further refinements as described above.\n- Kafkarator will create topic and provide credentials as Secrets based on CRDs\n- We make no integration with Naiserator at this time, but leave an opening for doing so in the future, as detailed\nunder [Future work](#future-work)\n","**Topic CRD and AppTopic CRD as the primary mechanism for user interaction**\n\nThis decision is based on the following factors:\n\n- CRDs are a well-established API in Kubernetes and are familiar to most users.\n- CRDs provide a flexible way to define custom resources, which allows us to model the desired user experience.\n- By using CRDs, we can leverage the existing Kubernetes ecosystem of tools and integrations, such as UIs, editors, and monitoring.\n\nThe suggested flow in the Context section provides a good starting point for designing the user experience for topic creation, access management, and team ownership. We will further refine this flow through user research and feedback.\n\nWe will also explore the use of a UI for topic creation and management in the future. However, for now, we will focus on building a solid foundation with CRDs."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe project requires dealing with two relatively separate concerns:\n1. Create topics when needed\n2. Supply credentials for working with topics.\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\nso it makes sense to keep them in the same application.\n\n## Decision\n","We will ignore the SRP in this instance, and keep the two concerns in the same application.\n","**Decision:** Create a single application that contains separate components for each of the two concerns.\n\n**Rationale:**\n\n* **Separation of concerns:** The application will be organized into separate components for each concern, ensuring that the code is maintainable and testable.\n* **Conceptual cohesion:** While the two concerns are implemented separately, they are conceptually related. Keeping them in the same application allows for easy collaboration and knowledge sharing between teams working on each component.\n* **Efficiency:** Having all components within a single application streamlines communication and reduces the overhead of coordination between multiple applications."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n",**Decision:** To use an Architectural Decision Record (ADR) template to capture and document architectural decisions throughout the project lifecycle.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\nWe need to make a decicion, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\nFrom various discussions, we have a few findings that guide our decision:\n- Piggybacking on the existing NAIS CRDs are not encouraged and should be avoided\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\n- We have two separate concerns that needs two separate configurations\n\n## Decision\n",- We will define two new CRD objects (see github issues #3 and #12)\n- App developers will create these in the cluster when deploying their application\n- Kafkarator will watch these two CRDs and take needed actions\n,"As the findings suggest there are two separate concerns with separate configurations, and that the recommended approach is to use a separate CRD.\n\nTherefore **the decision** is:\n\n* Create a new CRD for Kafkarator, with appropriate versions, and with a spec that encompasses both concerns."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nGP Data changes on a daily basis.\nNightly ETLs are run to obtain Syndication and POMI data.\nCurrently [profiles-db](https://github.com/nhsuk/profiles-db) requires manually updating from the ETL outputs, and re-deploying to refresh the data.\nNo automated validation of data is performed during the update.\n\n## Decision\n","To avoid a new release of [profiles-db](https://github.com/nhsuk/profiles-db) each time data changes, data will be inserted into a running mongodb instance on a daily schedule.\nThe output from the nightly [gp-data-etl](https://github.com/nhsuk/gp-data-etl) and [pomi-data-etl](https://github.com/nhsuk/pomi-data-etl) will be validated and combined.\nThe contents of the existing database will only be replaced if the new data is of a comparable size.\n","An automated process should be developed to update [profiles-db](https://github.com/nhsuk/profiles-db) data from Nightly ETL outputs, without the need for manual intervention.\n\nTo ensure data quality, automated validation checks should be implemented as part of the process."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe merge of the data sources is a common operation from which the generated asset can be used by several different processes.\nCurrently the generated asset is only available for the immediately following process within this application i.e. updating MongoDB.\nThere is an immediate need to use the same generated asset for updating Elasticsearch.\nThe [mongodb-updater](https://github.com/nhsuk/mongodb-updater) service is able to update a MongoDB database from a JSON file available at a URL.\n\n## Decision\n,"The `gp-data-merged.json` file will be written to the team's preferred cloud hosting platform, enabling the merged data to be used as a\nsource for both the `mongodb-updater` and the forthcoming `elasticsearch-updater`.\n",The data source merge output JSON file will be made available as a URL that can be consumed by the [mongodb-updater](https://github.com/nhsuk/mongodb-updater) service. This would involve implementing a new endpoint (or modifying an existing one) that returns the JSON file. It would also require setting up a process to generate the JSON file and make it available at the URL.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\n,"We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n","We will record the architectural decisions made on this project using an Architectural Decision Record (ADR) template. The ADR template will include the following sections:\n\n- Title: A brief title for the decision.\n- Date: The date the decision was made.\n- Authors: The people who made the decision.\n- Status: The current status of the decision (e.g., proposed, accepted, rejected).\n- Context: The context in which the decision was made.\n- Decision: The decision that was made.\n- Rationale: The rationale for the decision.\n- Consequences: The potential consequences of the decision.\n- Alternatives: The other alternatives that were considered."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\n\n## Decision\n","We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\n1. The Java package name will include the version number.\n1. The library name will be part of the group ID.\n1. The group ID will include the version number.\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\nhttp://jakewharton.com/java-interoperability-policy-for-major-version-updates/\n","**Decision:** Upgrade `kotlin-guice` to a new major version when Guice makes a corresponding major version upgrade.\n\n**Reasoning:**\n\n* `kotlin-guice` is heavily tied to Guice, and major version changes in Guice will necessitate corresponding changes in `kotlin-guice`.\n* Upgrading both Guice and `kotlin-guice` simultaneously minimizes the likelihood of dependency conflicts and ensures compatibility between the two libraries.\n* This approach aligns `kotlin-guice`'s versioning with Guice's, simplifying dependency management for consumers."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n\n## Decision\n",All pointer event interactions have a mouse event fallback.\n,Use a cross-browser library like [Hammer.js](http://hammerjs.github.io/) or [Velocity.js](http://velocityjs.org/) as a fallback if PointerEvents isn't available.
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and Problem Statement\nWe want to be able to centralise in a single tool all tasks to be called:\n* During the development cycle.\n* During the build cycle.\n* In the CI/CD pipelines.\n## Decision Drivers\n* Must have a significant user base and community.\n* Must not require significant installation.\n* Must be sufficiently simple to learn and use.\n\n## Decision\n,"* Must have a significant user base and community.\n* Must not require significant installation.\n* Must be sufficiently simple to learn and use.\nChosen option: `GNU Make`, because compared to the other evaluated tools (see\n[Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as:\n* `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has\na sufficient community.\n* `Rake` required to install and learn Ruby.\n* `SCons` is more a build tool and seems difficult to apprehend/get to grips with.\n","**Decision:** Adopt Make as the central task runner.\n\n**Rationale:**\n\nMake is a widely-used and well-supported task runner that meets all of the decision drivers:\n\n* It has a large and active user base and community, ensuring that support and resources are readily available.\n* It is straightforward to install, requiring only a single command to set up.\n* Its syntax is relatively simple, making it easy to learn and use even for developers who are new to task runners.\n* It provides a robust set of features for defining, executing, and automating tasks, meeting the requirements for development, build, and CI/CD pipelines."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to record the architectural decisions made on Opinionated Digital Center.\n\n## Decision\n,"We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n","The architectural decision for Opinionated Digital Center is to record the architectural decisions made on the system. This will help to ensure that the system is built in a consistent and maintainable way. The architectural decisions will be documented in a central location, such as a wiki or a document repository. This will make it easy for stakeholders to understand the rationale behind the decisions that were made."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\n### Technological forces\n* METS Reader & Writer (henceforth [metsrw][0] already exists, is being used in\nArchivematica, and seems like a good tool for abstracting METS file creation.\n* Currently, [Archivematica][1] (AM) and the [Archivematica Storage Service][2]\n(SS) both create or modify METS files (pointer files are METS files too),\nusing mets-rw or various lxml APIs. This inconsistency makes it difficult to\nreliably create predictable METS files.\n* Artefactual has created a [METS validator][3] project, which contains a\nSchematron file that validates AM-generated AIP METS files.\n* metsrw (v. 0.1.1) cannot create pointer files and it does not support METS\nfile validation, either via XMLSchema or Schematron.\n* metsrw (v. 0.2.0) does have support for pointer file creation and for METS\nfile validation generally, both via XMLSchema and Schematron.\n* Changing how we write METS files may inadvertently affect the METS files\nproduced; METS validation and related testing will need to prevent this, or\nformally recognize it if it is intentional.\n* At present in AM and SS, METS/PREMIS data structures are encoded at various\npoints as XML strings, MySQL/SQLite database rows, and Python objects (i.e.,\nmetsrw or lxml instances).\n### Project forces\n* In the context of the project ""AIP Encryption via Mirror Locations"", mirror\nlocations in the Storage Service must replicate stored AIPs and give them\ntheir own pointer files; if replica pointer files are to be generated in the\nSS then it makes sense to generate all pointer files there.\n* An [analysis of pointer file creation in Archivematica][4] has been performed.\n### Political forces\n* Other projects under discussion or on the horizon will involve the need to\nrecognize various ""flavours"" or versions of AM-generated and\nexternally-generated (e.g., DSpace, from-LOCKSS) METS files and process them\naccordingly. One such project relates to re-ingesting old AM-generated AIPs\nwhose METS files may be significantly different from their current\ncounterparts. This implies more sophisticated METS file validation and\nrecognition capabilities than AM currently implements.\n* The Mirror Locations project’s budget does not cover wholesale refactoring of\nhow AM creates METS files, or even significant alterations to metsrw.\n\n## Decision\n","We will use metsrw to create and modify METS\nfiles---including pointer files---in all future code. We will not rewrite\nexisting METS creation code to use metsrw unless a clear opportunity arises to\ndo so, e.g., direct funding for this purpose or a project that requires\nnon-trivial changes to such code.\nIn the AIP Mirror project, metsrw will be used to create METS pointer files,\nboth when AIPs are stored and when they are replicated. This will involve\nremoving the Create Pointer File micro-service in AM and calling a\n`create_pointer_file` method of Package at the end of AIP storage, reingest and\nreplication.\n","**Decision:**\n\nUpgrade mets-rw to version 0.2.0 in both Archivematica and Archivematica Storage Service. This will enable the creation of pointer files and support for METS file validation in both systems, using either XMLSchema or Schematron.\n\n**Additional Considerations:**\n\n* Conduct thorough testing to ensure that changing how METS files are written does not inadvertently affect the METS files produced. METS validation and related testing should be used to prevent this or formally recognize any intentional changes.\n* Explore ways to consolidate METS/PREMIS data structures into a single data structure to improve consistency and maintainability."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nArchivematica code style is inconsistent and controversial. Formatting is a\ndistraction from content and a constant dilemma in our code review process.\nThe style guide described by [PEP 8][0] is not specific enough and Python is too\nflexible. Every project ends up with its own style and memorizing the rules is\nhard.\nWith the recent rise of auto formatters such *gofmt* or *Prettier*, we see the\nvalue of ceding control to an opinionated tool.\n\n## Decision\n","Chosen option: ""black"", because:\n* It prioritizes consistency over style.\n* It can check that the reformatted code still produces a valid AST that is\nequivalent to the original.\n* Zero configuration, sane defaults.\n* Adopted by popular projects such [responder][5] or [pytest][6].\n","**Adopt a strict and opinionated code style enforced by an auto formatter.**\n\n### Rationale\n\n* Improves code consistency and readability, reducing distractions and simplifying the code review process.\n* Enforces a specific style, eliminating the need for individual project styling and memorization.\n* Auto formatters like *gofmt* or *Prettier* provide automated formatting, ensuring consistency and reducing the burden on developers.\n\n### Consequences\n\n* Some developers may initially resist the imposition of a strict style.\n* May require modification to existing code to comply with the new style.\n* May require ongoing maintenance efforts to ensure compliance with the code style as the codebase evolves."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nThe Simon Fraser University Archives is a long-time Archivematica user. They\nrecognized that there is often a long-period between receiving digital materials\nfrom donors and an archivist creating a Submission Information Package (SIP) for\nlong-term archival storage. SFU Archives wanted the ability to perform minimal\ndigital preservation tasks, such as those provided by Archivematica's Transfer\nfunctionality, and then return to the creation of SIPs from backlog at an\nundetermined future time, perhaps several years in the future.\nThis meant that transfer backlogs are used for long-term storage of content and\nthat users should expect it to be as durable as Archival Information Package (AIP)\nstorage for maintaining accurate metadata over pipeline upgrades, migrations or\nre-indexes. To enable this, the decision was made to package backlog transfers as\nBagIt packages, supported by a verbose METS file, as is already done for AIPs.\n\n## Decision\n","* Preservation of original content and metadata over long-term gaps in\nprocessing\n* Re-use of existing standards and protocols\nChosen option: Option 2 was chosen because SFU Archives wanted the ability to\ncreate multiple SIPs from multiple transfers. In their use cases, one transfer\ndoes not automatically equal one SIP and one AIP. By sending transfers to\nbacklog they are able to use the Appraisal tab functionality where users can\ncreate SIPs by combining files from different transfers in the Archivematica\nbacklog.\nHowever, this creates a new expectation, namely, that transfer\nbacklogs can be used for long-term storage of content and that users should\nexpect it to be as durable as AIP storage for maintaining accurate metadata\nover pipeline upgrades, migrations or re-indexes. However, prior to release\n1.10, transfers stored using Archivematica's backlog functionality could only be\naccessed by using the same pipeline that the transfer was created in. If the\npipeline was accidently lost or decomissioned, you would be unable to reindex\nand repopulate the backlogged material in a new Archivematica pipeline. The\nreliance on maintaining this backlog information in the pipeline's transactional\ndatabase (MCPServer) and search index (Elasticsearch) is a significant\npreservation risk if the files are kept in backlog for an indefinitely long\nperiod of time, as is the case for SFU Archives and an increasing number of\nArchivematica users.\n![before_archivematica_backlog](/assets/before_archivematica_backlog.png)\nThe decision was made to convert the materials in transfer backlogs to mirror\nArchivematica's AIPs, namely to use the Bagit and METS standards to\npackage and describe the transfer materials. As materials go through transfer\nprocessing, event and object information is written to the transfer METS file\nwherever feasible. Other relevant information is captured in a logs subdirectory\nin an IETF-compliant Bagit package. Functionality was then added in Archivematica\nrelease 1.10 to rebuild backlogs from these self-describing packages in the event\nof a lost or destroyed pipeline.\n![after_archivematica_backlog](/assets/after_archivematica_backlog.png)\n### Positive consequences\nArchivematica users that rely on transfer backlog for long-term storage of\ndigital materials can now be assured that they can rebuild their backlogs from\nthe transfer Bagit packages that Archivematica creates as of release 1.10. This\nsignificantly reduces the risk of loss of materials during the transfer backlog\nphase.\n### Negative consequences\nSome major changes were introduced to the Archivematica codebase to enable it\nto restore from backlog using transfer Bagit packages. Previously, the\ninformation used to accomplish this was maintained primarily in the MCPServer\ndatabase and ElasticSearch index. There is a risk that not all previous\nfunctionality to restore from backlog is fully enabled and that some\nregressions may have been introduced as a result of this architectural change.\n","**Decision:**\n\nPackage transfer backlogs as BagIt packages with verbose METS files to enable long-term storage and maintain accurate metadata over pipeline upgrades, migrations, or re-indexes."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nArtefactual have been working on a client application to provide easier access\nto the functions exposed by the Archivematica Application Programming Interface\n(API). This application is called [AMClient][amclient-1]. The application also\nacts as a Python library which makes it easier to make Archivematica specific\nAPI calls in code. The library was released as a [Python package][amclient-2]\nearly in 2019.\n\n## Decision\n,"* Desire to simplify development processes in Archivematica and the Storage\nService.\n* Desire to make it easier to maintain components of Archivematica downstream\nand push the effects upstream (modularity).\nOption 2. The AMClient library helps us to create consistency across the\nArchivematica code-base. It is already widely used in the default Automation\nTools, and Archivematica Automated Acceptance Tests (AMAUAT).\n### Positive consequences\n* As the AMClient library is adapted as a Python package its use on the command\nline is extended and so it becomes more useful to those embedding it outside\nof Archivematica.\n* As AMClient is extended, tests are added to the package itself, providing a\ngreater level of lower-level testing, thusly the Archivematica API. The focus\nof testing in Archivematica can be on manipulating well understood AMClient\nresponses where this is appropriate for the unit being tested.\n* The number of lines of code required to talk to the AMClient package versus\nthat of creating lower-level API calls via a HTTP requests library should be\nfewer.\n* Missing functionality, e.g. as described by issue [#905][amclient-3] can be\nadded as it is required by Archivematica and the Storage Service. As an\nexternal package, and something used inside Archivematica, the two take on a\nsymbiotic relationship.\n### Negative consequences\n* The release candidate for version 1.0.0 of AMClient does not yet adhere to\nmore traditional Python best practices i.e. it might not be considered to be\nentirely ""Pythonic"" in how it exposes errors to the calling code. This is\ndescribed in issue [#488][amclient-4]. This effect of not being 'Pythonic'\nmay travel upstream to Archivematica and is something for maintainers to be\naware of. Work on an AMClient 2.0.0 should replace these patterns and then\ncascading those changes down to Archivematica should still be easier than\nworking with and replacing the calls that exist in Archivematica using a HTTP\nrequests library.\n",Continue developing the AMClient package as a Python package.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nThere is no open-source software system capable of implementing the functional\nrequirements of the OAIS reference model (ISO 14721:2003). Digital preservation\nspecialists must use multiple tools, which can be difficult to install and use,\nto perform discrete preservation actions. Those tools produce metadata that do\nnot conform to digital preservation and exchange standards and schemas, and do\nnot provide a way to automatically generate standardized, system-independent,\nself-documenting Archival Information Packages (AIPs) that package content and\nPreservation Description Information (PDI) as described by OAIS. Repository\napplications such as Fedora are capable of performing some but not all OAIS\npreservation actions, and tend to be complex to develop and maintain, posing a\nrisk to future retrieval and readability of the AIPs. In fact, any middleware\nrepository or database software that is required to access and read AIPs is\ninherently a risk to their long-term usability.\n\n## Decision\n","Artefactual designed an open-source, web-based archival description and access\nsystem called ICA-AtoM (Access To Memory) that has a broad user base around the\nworld. ICA-AtoM does not provide digital preservation functionality as described\nby OAIS. It would benefit ICA-AtoM users to be able to integrate with a back-end\nsystem designed to preserve digital objects that are linked to access copies in\nICA-AtoM. The system should also be usable on its own or in conjunction with\nother access tools.\nDesign principles:\n1. The application will perform a set of configurable preservation actions on\ningested digital objects, using the file system as the focal point of\npreservation action operations. Making the file system the focal point of\nmicro-services operations is noteworthy as a long-term preservation strategy\nbecause it provides users with the option of direct, unmediated access to\narchival storage. This might be necessary one day because the various layers\nand generations of digital preservation system components are just as\nsusceptible to the risk of technology obsolescence and incompatibility as the\ndigital objects they are attempting to preserve.\n2. The information packages ingested by the application will be moved from one\nmicro-service to the next using the Unix pipeline pattern.\n3. Micro-service functionality will be provided by one or more of the\nopen-source software utilities and applications bundled into the application.\nWhere necessary, these will be supplemented by integration code written as\nPython scripts.\n4. The application will provide a graphical user interface so that the end user\ncan determine and control the status of digital objects moving through the\npipeline.\n5. The application will generate AIPs that are system-independent,\nself-documenting and self-contained. The AIPs will contain PDI that conforms\nto recognized standards and schemas, such as PREMIS, METS and Dublin Core.\n6. The file-based AIP is the canonical source of the preserved digital objects\nand metadata. The preservation application may be supported by databases and\nindexes that are derived and updated from this source, but these are not\nessential to long-term preservation of the AIP.\n7. The application will be designed to integrate with diverse external systems\ncapable of serving as sources of digital objects to be ingested.\n8. The application will be designed to integrate with diverse storage systems\nand protocols for deposit and long-term preservation of AIPs.\n9. The application will be designed to integrate with diverse external systems\ncapable of providing search, browse and display capabilities for\nDissemination Information Packages (DIPs) generated by the application.\n",Develop a modular digital preservation framework tailored to support OAIS-compliant preservation actions.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nArchivematica users rely on spreadsheets created in a specific way to perform\ntasks within or after Archivematica. Documentations and examples can be found\n[here](https://www.archivematica.org/en/docs/archivematica-1.9/user-manual/transfer/import-metadata/)\nbut there is no clear validation as can be performed by a machine. As one\nexample, the metadata.csv and rights.csv files are ""special"" and are utilized by\nArchivematica to add metadata or rights metadata into the AIP's METS file.\nAnother example is the Avalon Media System having a specific Manifest.csv file\nthat is used to recreate hierarchical information and additional metadata, which\nis used after a DIP is created from a stored AIP. It would be beneficial if this\nmanifest could be validated prior to going through the preservation process.\nBoth of these examples would benefit from a validation service that a user (or\nautomated system) could access prior to ingest into Archivematica.\n\n## Decision\n","* More closely integrate two open source projects: Archivematica and Avalon\n* [Feature request](https://github.com/archivematica/Issues/issues/563) for\nvalidation of Archivematica-specific CSVs within Archivematica\n* Creation of something for a specific purpose that can later be extended to\nsuit many use cases\nChosen option: ""1. API endpoint for pre-ingest CSV validation"", because it is\nflexible, it lays the groundwork for future work around CSV validation as a step\nto be taken by Archivematica. It doesn't have the complications of the long-term\nmaintenance and testing of a GUI component. It can be more easily automated. The\nsolution allows for custom or institutionally-specific CSV to be used/added.\nTechnically, this would live in the Archivematica codebase and be a new endpoint\nin the [Archivematica API](https://wiki.archivematica.org/Archivematica_API)\nProposed endpoint below:\n```yaml\nURL: /api/transfer/validate_csv\nVerb: POST\nValidates local CSV with validator service Python script\nParameters: CSV\ninput: Path to the CSV\nvalidator: Name of service CSV should be checked against, i.e. ""avalon""\nor ""rights""\nResponse: JSON\nmessage: Approval or non-approval, depending on service output\n```\n### Positive consequences\n* Fulfills requirement of Avalon/Archivematica integration work.\n* Relatively small feature with big potential benefits.\n* Lays foundation for more development in the future.\n### Negative consequences\n* Code must be maintained and tested over time.\n* User would need API expertise to benefit from this feature.\n* Implementation -- timeouts on very big CSVs?\n","Develop a service to validate a single file (metadata.csv, rights.csv, Manifest.csv, etc)."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nPython 2 will not be supported by the Python development team past January 1,\n2020\. Operating system vendors will provide bugfix support for some number of\nyears, depending on the OS:\n* [Ubuntu 16.04 through 2021](https://wiki.ubuntu.com/Releases)\n* [Ubuntu 18.04 through 2023](https://wiki.ubuntu.com/Releases)\n* [RHEL/CentOS 7 through 2024](https://wiki.centos.org/About/Product)\nArchivematica is not fully compatible with Python 3 yet. Archivematica and all\nof its required components will eventually need to run in a Python 3\nenvironment, especially as downstream dependencies begin to drop support for\nPython 2 (i.e.\n[Django](https://docs.djangoproject.com/en/2.2/faq/install/#what-python-version-should-i-use-with-django)).\nComponents:\n* [am/compose](https://github.com/artefactual-labs/am)\n* [archivematica-storage-service](https://github.com/artefactual/archivematica-storage-service)\n* [MCPServer](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/MCPServer)\n* [MCPClient](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/MCPClient)\n* [archivematica-common](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/archivematicaCommon)\n* [Dashboard](https://github.com/artefactual/archivematica/tree/a889605d8c97e114f8c0bc707d6a371030fb5c0b/src/dashboard)\n* [mets-reader-writer](https://github.com/artefactual-labs/mets-reader-writer)\n* [ammcpc](https://github.com/artefactual-labs/ammcpc)\n* [agentarchives](https://github.com/artefactual-labs/agentarchives)\n* [fixity](https://github.com/artefactual/fixity)\n* [automation-tools](https://github.com/artefactual/automation-tools)\n* [archivematica-acceptance-tests](https://github.com/artefactual-labs/archivematica-acceptance-tests)\n* [amclient](https://github.com/artefactual-labs/amclient)\n\n## Decision\n","* Archivematica is built on Python 2\n* Python 2 will no longer be supported after January 1, 2020\n* Archivematica should be refactored to use Python 3 (3.5 at minimum)\nChosen option: 1, because refactoring Archivematica and all required components\nto Python 3 is the only option to ensure that Archivematica continues to be a\nsafe, stable environment.\nPython 3.5 has been set as the minimum version for the following reasons:\n* [Python 3.4](https://www.python.org/dev/peps/pep-0429/) has already been\nretired\n* [Python 3.5](https://www.python.org/dev/peps/pep-0478/) is the oldest version\nthis is still supported\n* Ubuntu 16.04 ships Python 3.5\n* Ubuntu 18.04 ships Python 3.6\n* For CentOS 7, The Software Collections Repository provides packages for\nPython 3.5 and up\nDuring the course of the project, a higher version of Python may be selected.\n[Epic for Python 3 refactoring](https://github.com/archivematica/Issues/issues/805)\n### Positive consequences\n* Archivematica will be upgraded to a maintained version of Python.\n* Archivematica developers will be able to take advantage of new features in\nPython.\n* Python 3 support will be maintained for many years (for example, the latest\nstable release is [3.7, which will be maintained until 2023](https://www.python.org/dev/peps/pep-0537/#release-schedule);\nfurther releases are expected).\n### Negative consequences\n* This is a very large task and components need to be tackled in priority order,\nso it will take a considerable amount of time to get everything refactored.\n* Releasing the Python 3 work once all components are refactored will take the\nplace of a feature release, leaving some development work unreleased for at\nleast 4 extra months (note: all code resides in public branches).\n* We will need to support Archivematica running Python 2.7 for some time because\nusers will need time to upgrade. This results in a dual support burden.\n* Development work currently in progress may need to be refactored to Python 3\nbefore it are releasable, which is unsponsored work.\n","The decision is to migrate Archivematica and all of its required components to a Python 3 environment as soon as possible.\n\n**Rationale**\n\nPython 2 will not be supported by the Python development team past January 1, 2020. Operating system vendors will provide bugfix support for some number of years, depending on the OS, but this support will eventually end. Archivematica and all of its required components will eventually need to run in a Python 3 environment, especially as downstream dependencies begin to drop support for Python 2.\n\nMigrating to Python 3 will require a significant amount of work, but it is necessary to ensure the long-term viability of Archivematica. The earlier we start the migration, the less disruptive it will be.\n\n**Consequences**\n\nMigrating to Python 3 will require a significant amount of work, but it is necessary to ensure the long-term viability of Archivematica. The earlier we start the migration, the less disruptive it will be.\n\n**Alternatives**\n\nOne alternative to migrating to Python 3 is to continue to use Python 2 and rely on operating system vendors to provide bugfix support for as long as possible. However, this is a risky strategy, as there is no guarantee that operating system vendors will continue to support Python 2 indefinitely. Additionally, continuing to use Python 2 will make it more difficult to keep Archivematica up-to-date with the latest security patches and bug fixes.\n\nAnother alternative is to fork Archivematica and continue to develop it using Python 2. However, this would be a significant undertaking, and it would be difficult to maintain compatibility with the main Archivematica codebase. Additionally, forking Archivematica would reduce the number of developers working on the project, which could slow down development and make it more difficult to fix bugs and implement new features."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nThe core of Archivematica is a server application that coordinates work across\nmultiple workers. The collection of related tasks (also known as *workflow\ndata*) is stored in the application database using a relation model.\nIntroducing changes to the workflow data is hard because the model is clumsy and\nconvoluted. Changes to the data are delivered as data and schema migrations\nwhich are difficult to implement and they add risk to the process of upgrading\nthe software.\nAdditionally, we want to add full internationalisation support in order to make\nthe system more attractive to a wider range of users and increase adoption. We\nneed a simpler mechanism to deliver translations and other workflow data changes\nin general.\n\n## Decision\n",* Archivematica 1.9 will implement the proposed solution replacing the existing\ndatabase tables.\n* We do not currently intend to expose the ability to modify the workflow data\nto Archivematica users - but this may change in the future as the solution\nmatures and becomes interoperable.\n* The visualization and manipulation of the workflow data will be facilitated by\na new tool that we're calling amflow.\n,The Archivematica core application will use a configuration file format based on YAML\nto represent workflow data. This format will make it easy to add internationalisation\nsupport.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context and problem statement\nQuarantine is an optional workflow step in Archivematica that places digital\nobjects in a separate space for a specified period of time for virus definitions\nto be updated. It adds complexity to the workflow and is arguably unnecessary\nbecause ingested digital objects generally speaking pre-date the available virus\ndefinitions, which are updated daily. This proposal is to remove quarantine\nto simplify Archivematica's workflow and ease maintenance and testing.\n\n## Decision\n","* Desire to simplify Archivematica workflows\n* Need to fix known bugs related to quarantine if it remains supported\n* Little evidence of need for or use of quarantine in the user community\nOption 1, because there is no evidence of use in the community to justify the\ncontinued maintenance of the feature and the positive consequences of the\nremoval of the code outweigh the benefits of the functionality. The virus\nscanning micro-service remains unchanged by this decision.\n### Positive consequences\n* Simplified, easier to maintain workflow\n* Removes two processing configurations that users are required to make but\nwhich are rarely implemented.\n* Resolution of the following issues:\n* DSpace workflow issue: [#426][1]\n* Virus scan not triggered after quarantine: [691][2]\n[1]: https://github.com/archivematica/Issues/issues/426\n[2]: https://github.com/archivematica/Issues/issues/691\n","**Decision: Remove quarantine**\n\n**Rationale:**\n\n* Quarantine adds complexity to the workflow without providing significant benefits.\n* Ingested digital objects generally pre-date available virus definitions.\n* Removing quarantine would simplify Archivematica's workflow and ease maintenance and testing.\n\n**Consequences:**\n\n* **Reduced workflow complexity:** The workflow will be simplified by removing an unnecessary step.\n* **Easier maintenance and testing:** The reduced complexity will make it easier to maintain and test Archivematica.\n* **Potential security risk:** There is a small potential security risk if new viruses are detected after ingestion. However, this risk is mitigated by the fact that ingested digital objects generally pre-date available virus definitions.\n\n**Additional Considerations:**\n\n* **Phased removal:** The quarantine step can be removed in a phased approach to minimize the impact on existing workflows.\n* **Monitoring for new viruses:** Archivematica can monitor for new viruses and take appropriate action if necessary."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some ""events bus"" to do that.\n\n## Decision\n",Solution number 1 - In Memory Events Bus</br>\nAt that moment we don't see more advanced integration scenarios in our system than simple publish/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\n,Use Apache Kafka as the event bus for inter-modular communication.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAs it is monolith, only one language (or platform) must be selected for implementation.\n\n## Decision\n","I decided to use:\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\n- F# will not be used, I don't have commercial experience with it\n",Select a language (or platform) that meets the following criteria:\n- It is supported by a majority of the team.\n- It has a large and active community.\n- It is well-documented and easy to learn.\n- It is appropriate for the size and complexity of the project.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nEach module should be autonomous. However, communication between them must take place. We have to decide what will be the preferred way of communication and integration between modules.\n\n## Decision\n","Solution number 2 - Event-driven (asynchronous)</br>\nWe want to achieve the maximum level of autonomy and loose coupling between modules. Moreover, we don't want dependencies between modules. We allow direct calls in the future, but this should be an exception, not a rule.\n","**Decision:** Utilize a Message Bus architecture for communication and integration between modules.\n\n**Rationale:**\n\n* **Decoupling and Autonomy:** The message bus architecture effectively decouples modules, allowing them to communicate and integrate without direct dependencies. This preserves their autonomy and facilitates changes without impacting the entire system.\n* **Loose Coupling:** Modules can subscribe to and publish messages on the message bus without requiring specific knowledge about other modules. This flexibility enhances the system's resilience and maintainability.\n* **Asynchronous Communication:** The message bus enables asynchronous communication, allowing modules to process messages at their own pace. This eliminates bottlenecks and ensures that messages are delivered reliably even in the presence of failures or delays.\n* **Message Standardization:** The message bus provides a common interface for message exchange, ensuring compatibility and consistency between modules. This simplifies integration and reduces the likelihood of communication errors.\n* **Event-Driven Architecture:** The message bus facilitates event-driven architecture, where modules react to events published by other modules. This promotes a reactive and adaptive system that can respond to changes in real time."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur application should handle 2 types of requests - reading and writing. </br>\nFor now, it looks like:</br>\n- for reading, we need data model in relational form to return data in tabular/flattened way (tables, lists, dictionaries).\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\n\n## Decision\n","We applied the CQRS architectural style/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\n","We will use two different data models for reading and writing:\n\n- For reading, we will use a relational data model. This will allow us to return data in a tabular/flattened way, which is what we need for our reading requests.\n- For writing, we will use a graph of objects. This will allow us to perform more sophisticated work like validations, business rules checks, and calculations, which is what we need for our writing requests."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAn advanced example of Modular Monolith architecture and tactical DDD implementation in .NET is missing on the internet.\n\n## Decision\n,I decided to create nontrivial application using Modular Monolith architecture and Domain-Driven Design tactical patterns.\n,"**Decision: Create a case study showcasing Modular Monolith architecture and tactical Domain-Driven Design (DDD) implementation in .NET.**\n\n**Details:**\n\n* Develop a real-world application that demonstrates the principles of Modular Monolith architecture and tactical DDD.\n* Utilize a modern .NET stack, such as .NET Core or .NET 6, for implementation.\n* Structure the application into independent modules with clearly defined boundaries and responsibilities.\n* Employ DDD techniques, such as domain models, bounded contexts, and repositories, to encapsulate business logic and maintain data integrity.\n* Provide comprehensive documentation, code samples, and architecture diagrams to illustrate the design and implementation.\n* Publish the case study on a publicly accessible platform, such as GitHub, to serve as a resource for the community."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always ""void"". However, sometimes we need to return some data immediately as part of the same request.\n\n## Decision\n","We decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\n","In cases where it is necessary to return data immediately as part of the command processing, the following options should be considered:\n\n1. **Use a separate query for the data.** This is the preferred approach, as it keeps the command and query models separate.\n2. **Use a ""Command Query Separation"" (CQS) design pattern.** This pattern allows commands to return data, but it does so in a way that keeps the command and query models separate.\n3. **Use a ""Query on Commit"" (QoC) design pattern.** This pattern allows commands to return data after the command has been committed to the database.\n\nThe best option for a particular application will depend on the specific requirements of the application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nThe MyMeetings domain contains 4 main subdomains: Meetings (core domain), Administration (supporting subdomain), Payments (supporting subdomain) and User Access (generic domain).\nWe use Modular Monolith architecture so we need to implement one application which solves all requirements from all domains listed above.\nWe need to modularize our system.\n2. Create 4 modules based on Bounded Contexts which in this scenario maps 1:1 to domains. This solution is more difficult at the beginning. We need to set modules boundaries, communication strategy between modules and have more advanced infrastructure code. It is a more complex solution. On the other hand, it supports autonomy, maintainability, readability. We can develop our Domain Models in all of the Bounded Contexts independently.\n\n## Decision\n","Solution 2.\nWe created 4 modules: Meetings, Administration, Payments, User Access. The key factor here is module autonomy and maintainability. We want to develop each module independently. This is more cleaner solution. It involves more work at the beginning but we want to invest.\n","Implement the 4 modules based on the Bounded Contexts to support autonomy, maintainability, readability, and independent development of the Domain Models in each Bounded Context."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\n\n## Decision\n",Solution 1.\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\n,"We will create a REST API using Django Rest Framework. We chose Django Rest Framework because it is a popular and well-maintained framework that provides a robust set of features for building REST APIs. It is also well-integrated with Django, which makes it a good choice for our application."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nOur API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API/contract/interface).\n\n## Decision\n","Each module will provide implementation for one interface with 3 methods:</br>\n```csharp\nTask<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);\nTask ExecuteCommandAsync(ICommand command);\nTask<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query);\n```\nThis interface will act as a façade (Façade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the façade (module encapsulation).\n","**Decision:** The communication between the API layer and business modules will be based on **fine-grained interfaces**.\n\n**Rationale:**\n\n* **Modularity and Autonomy:** Fine-grained interfaces allow each module to expose only the operations it needs, promoting modularity and autonomy.\n* **Flexibility:** Fine-grained interfaces provide greater flexibility in evolving the system over time, as modules can be easily added, removed, or modified without affecting the API layer or other modules.\n* **Code Reusability:** Fine-grained interfaces encourage code reuse, as operations can be shared among multiple modules.\n* **Maintainability:** By isolating operations into individual interfaces, it becomes easier to maintain and understand the codebase."
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nIn some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.\n\n## Decision\n",We decided to implement Unit Tests for our architecture. </br>\nWe will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.\n,Enforce coding conventions by means of a static code analyzer.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nAggregates should check business invariants. When the invariant is broken, we should stop processing and return an error immediately to the client.\n\n## Decision\n","Solution number 1 - Use exceptions. </br>\nPerformance cost of throwing an exception is irrelevant, we don't want too many if/else statements in entities, more familiar with exceptions approach.\n","**Decision:** Invariants are checked within the aggregates. When an invariant is broken, an exception is thrown."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe need to create Domain Models for all of the modules. Each Domain Model should represent a solution that solves a particular set of Domain problems (implements business logic).\n\n## Decision\n,"Solution number 3 - Rich Domain Model</br>\n1 - no, because the procedural style of coding will not be enough. We want to focus on behavior, not on the data.</br>\n2 - no, keeping business logic in the database is not a good idea in that case, object-oriented programming is better than T-SQL to model our domain and we don't have performance architectural drivers to resign from OOD.</br>\nWe expect complex business logic with different rules, calculations and processing so we want to get as much as possible from Object-Oriented Design principles like abstraction, encapsulation, polymorphism. We want to mutate the state of our objects only through methods (abstraction) to encapsulate all logic and hide implementation details from the client (the Application Service Layer and Unit Tests).</br>\n",We should **create a Domain Model for each module**. Each Domain Model will be responsible for representing a solution that solves a particular set of Domain problems (implements business logic). This will allow us to easily identify and reuse Domain logic across different modules. It will also help us to keep our codebase organized and maintainable.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe applied the CQRS style (see [ADR 7. Use CQRS architectural style](007-use-cqrs-architectural-style.md)), now we need to decide how to handle reading (querying) requests.\n\n## Decision\n","We will use 2 layered architecture to handle queries: API layer and Application Service layer. As we applied the CQRS and created a separated read model, querying should be straightforward so 2 layers are enough. The API layer is responsible for Query creation based on HTTP request and the module Application layer is responsible for query handling.\n","We have decided to use a read-only database replica for handling read (querying) requests. The replica will be updated asynchronously from the primary database using a log-based replication mechanism. This approach will provide us with the following benefits:\n\n* **High availability:** The read-only replica will be available even if the primary database is unavailable, ensuring that our application can continue to serve read requests.\n* **Scalability:** The read-only replica can be scaled independently of the primary database, allowing us to handle increased read traffic without impacting the performance of write operations.\n* **Low latency:** The read-only replica will be located close to the application servers, reducing the latency of read operations.\n\nWe will use a database replication tool, such as [Bucardo](https://wiki.postgresql.org/wiki/Bucardo) or [Slony](https://www.slony.info/), to implement the log-based replication between the primary database and the read-only replica."
This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nWe decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)) for each module. We need to define or use some construction elements / building blocks to implement our architecture and business logic.\n\n## Decision\n,"We decided to use **Domain-Driven Design** tactical patterns. They focus on the Domain Model implementation. Especially we will use the following building blocks:\n- Command - public method on Aggregate (behavior)\n- Domain Event - the immutable class which represents important fact occurred on a special point of time (behavior)\n- Entity - class with identity (identity cannot change) with mutable attributes which represents concept from domain\n- Value Object - immutable class without an identity which represents concept from domain\n- Aggregate - cluster of domain objects (Entities, Value Objects) with one class entry point (Entity as Aggregate Root) which defines the boundary of transaction/consistency and protects business rules and invariants\n- Repository - collection-like abstraction to persist and load particular Aggregate\n- Domain Service - stateless service to execute some business logic which does not belong to any of Entity/Value Object\n",Use an Event Sourcing architecture style as the foundation for building our rich domain models.
"This is an Architectural Decision Record. Provide a Decision for the Context given below.\n## Context\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\n\n## Decision\n",Solution number 2 - IoC Container per module</br>\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\n,Use Dependency Injection (DI) framework to resolve object dependencies.
