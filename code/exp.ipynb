{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "# from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "# import t5tokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "# from transformers import pipeline\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '/scratch/adyansh/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../ADR-data/context_decision.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, input_ids, output_ids):\n",
    "        self.input_ids = input_ids['input_ids']\n",
    "        self.attention_mask = input_ids['attention_mask']\n",
    "        self.output_ids = output_ids['input_ids']\n",
    "        self.output_attention_mask = output_ids['attention_mask']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'decoder_input_ids': self.output_ids[idx][:-1],\n",
    "            'decoder_attention_mask': self.output_attention_mask[idx][:-1],\n",
    "            'labels': self.output_ids[idx][1:]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data: pd.DataFrame, max_length = -1):\n",
    "    context = data['Context'].tolist()\n",
    "    decision = data['Decision'].tolist()\n",
    "    for i in range(len(context)):\n",
    "        context[i] = f\"This is an Architectural Decision Record. Provide a Decision for the Context given below.\\n{context[i]}\\n## Decision\\n\"\n",
    "    if max_length != -1:\n",
    "        removed = []\n",
    "        context_new = []\n",
    "        decision_new = []\n",
    "        for i, (c, d) in enumerate(zip(context, decision)):\n",
    "            if len(c) <= max_length and len(d) <= max_length:\n",
    "                context_new.append(c)\n",
    "                decision_new.append(d)\n",
    "            else:\n",
    "                removed.append(i)\n",
    "        context = context_new\n",
    "        decision = decision_new\n",
    "        \n",
    "    return context, decision, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4856, 4856)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "model_max_length = 2000\n",
    "\n",
    "context, decision, removed = get_data(data, model_max_length)\n",
    "len(context), len(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR, model_max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=CACHE_DIR, device_map=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_decision = []\n",
    "\n",
    "# for c in tqdm(context):\n",
    "#     # print(c)\n",
    "#     input_ids = tokenizer(c, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "#     outputs = model.generate(input_ids, max_length=len(input_ids[0])*4, min_length= int(len(input_ids[0])/8))\n",
    "#     predicted_decision.append(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_decision = []\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "inputs = tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True, max_length=model_max_length, return_attention_mask=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(context), BATCH_SIZE)):\n",
    "        input_ids = inputs['input_ids'][i:i+BATCH_SIZE].to(device)\n",
    "        attention_mask = inputs['attention_mask'][i:i+BATCH_SIZE].to(device)\n",
    "\n",
    "        outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=model_max_length, min_length= int(model_max_length/8))\n",
    "        predicted_decision.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "len(predicted_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in removed:\n",
    "    predicted_decision.insert(i, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "context_2 = [c.replace(\"\\n\", \"\\\\n\") for c in data['Context'].tolist()]\n",
    "df = pd.DataFrame({'Context': context_2 , 'Decision': data['Decision'].tolist(), model_name.split('/')[-1]: predicted_decision})\n",
    "\n",
    "df.to_csv(f'../results/results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
